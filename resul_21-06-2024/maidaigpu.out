====================================================================================================
STL + ARIMA + ES + LSTM
Epoch 34: reducing lr to 5.324477699058609e-06
Epoch 42: reducing lr to 4.814999015523203e-06
Epoch 47: reducing lr to 4.376860711052749e-06
Epoch 53: reducing lr to 3.7608712520519415e-06
Epoch 56: reducing lr to 3.42715863054746e-06
Epoch 59: reducing lr to 3.0832376924750566e-06
Epoch 65: reducing lr to 2.386538938594925e-06
Epoch 68: reducing lr to 2.0447479738635276e-06
Epoch 71: reducing lr to 1.7145503929356359e-06
Epoch 74: reducing lr to 1.4011521164382144e-06
Epoch 77: reducing lr to 1.1094965457617986e-06
Epoch 86: reducing lr to 4.088349950235692e-07
Epoch 89: reducing lr to 2.456676450305213e-07
Epoch 92: reducing lr to 1.2246501296949797e-07
Epoch 95: reducing lr to 4.1170440933682266e-08
Epoch 98: reducing lr to 3.065776520088612e-09
[I 2024-06-20 21:12:50,011] Trial 0 finished with value: 1.074925422668457 and parameters: {'hidden_size': 93, 'n_layers': 6, 'rnn_dropout': 9.149985387590931e-05, 'bidirectional': True, 'fc_dropout': 0.07387087581503825, 'learning_rate_model': 5.5595654267125665e-05}. Best is trial 0 with value: 1.074925422668457.
Epoch 5: reducing lr to 0.000538736592834246
Epoch 8: reducing lr to 0.0010075004585150499
Epoch 11: reducing lr to 0.001572440058027613
Epoch 14: reducing lr to 0.0021542111575494666
Epoch 17: reducing lr to 0.0026711060141683144
Epoch 20: reducing lr to 0.003050528129843052
Epoch 23: reducing lr to 0.0032391887918001598
Epoch 26: reducing lr to 0.003249300514968436
Epoch 29: reducing lr to 0.0032203906595456937
Epoch 32: reducing lr to 0.0031663554241285288
Epoch 35: reducing lr to 0.003088046975867921
Epoch 38: reducing lr to 0.0029867002647879217
Epoch 41: reducing lr to 0.0028639136019450928
Epoch 44: reducing lr to 0.002721623406995635
Epoch 47: reducing lr to 0.0025620737443956287
Epoch 50: reducing lr to 0.002387780757081576
Epoch 53: reducing lr to 0.002201493290979546
Epoch 56: reducing lr to 0.0020061486359460945
Epoch 59: reducing lr to 0.0018048283601241847
Epoch 62: reducing lr to 0.0016007066661924165
Epoch 65: reducing lr to 0.0013970032765975716
Epoch 68: reducing lr to 0.0011969298187882783
Epoch 71: reducing lr to 0.0010036427556606015
Epoch 74: reducing lr to 0.0008201894660173594
Epoch 77: reducing lr to 0.000649463658328351
Epoch 80: reducing lr to 0.000494157275319063
Epoch 83: reducing lr to 0.00035672004899921716
Epoch 86: reducing lr to 0.00023931888074365076
Epoch 89: reducing lr to 0.00014380595242401742
Epoch 92: reducing lr to 7.168708694427484e-05
Epoch 95: reducing lr to 2.4099854376222196e-05
Epoch 98: reducing lr to 1.794607150387012e-06
[I 2024-06-20 21:13:03,400] Trial 1 finished with value: 1.1490776538848877 and parameters: {'hidden_size': 79, 'n_layers': 3, 'rnn_dropout': 0.4310533872026856, 'bidirectional': False, 'fc_dropout': 0.16356179978521396, 'learning_rate_model': 0.032543911150884876}. Best is trial 0 with value: 1.074925422668457.
Epoch 23: reducing lr to 0.0015883448168686608
Epoch 26: reducing lr to 0.001593303127148259
Epoch 29: reducing lr to 0.0015791271028506397
Epoch 32: reducing lr to 0.001552630781821005
Epoch 35: reducing lr to 0.0015142320264824375
Epoch 38: reducing lr to 0.0014645363978552646
Epoch 41: reducing lr to 0.0014043276320060158
Epoch 44: reducing lr to 0.001334555257449977
Epoch 47: reducing lr to 0.0012563196571461655
Epoch 50: reducing lr to 0.001170854628458233
Epoch 53: reducing lr to 0.0010795080752780714
Epoch 56: reducing lr to 0.0009837203054787861
Epoch 59: reducing lr to 0.0008850023741739571
Epoch 62: reducing lr to 0.000784910760067453
Epoch 65: reducing lr to 0.0006850242626022226
Epoch 68: reducing lr to 0.0005869177118173966
Epoch 71: reducing lr to 0.0004921388876674173
Epoch 74: reducing lr to 0.00040218208043222935
Epoch 77: reducing lr to 0.00031846622773632936
Epoch 80: reducing lr to 0.00024231133083625378
Epoch 83: reducing lr to 0.0001749186223215349
Epoch 86: reducing lr to 0.00011735064802960646
Epoch 89: reducing lr to 7.05156302546384e-05
Epoch 92: reducing lr to 3.515195325218233e-05
Epoch 95: reducing lr to 1.1817427524651174e-05
Epoch 98: reducing lr to 8.799903768647033e-07
[I 2024-06-20 21:13:19,825] Trial 2 finished with value: 1.08838951587677 and parameters: {'hidden_size': 21, 'n_layers': 5, 'rnn_dropout': 0.3338438418937016, 'bidirectional': True, 'fc_dropout': 0.15848119126790305, 'learning_rate_model': 0.015957993164212966}. Best is trial 0 with value: 1.074925422668457.
[I 2024-06-20 21:14:02,717] Trial 3 finished with value: 1.1042189598083496 and parameters: {'hidden_size': 195, 'n_layers': 3, 'rnn_dropout': 0.5538580925354513, 'bidirectional': False, 'fc_dropout': 0.06803536909582233, 'learning_rate_model': 1.432910723846784e-05}. Best is trial 0 with value: 1.074925422668457.
Epoch 16: reducing lr to 0.00045170179443101527
Epoch 29: reducing lr to 0.000579358347284702
Epoch 46: reducing lr to 0.0004708109126181557
Epoch 50: reducing lr to 0.0004295692229141394
Epoch 53: reducing lr to 0.00039605552538778
Epoch 56: reducing lr to 0.00036091241125794135
Epoch 59: reducing lr to 0.00032469426426718587
Epoch 62: reducing lr to 0.0002879721334006325
Epoch 65: reducing lr to 0.0002513252567920008
Epoch 68: reducing lr to 0.00021533141625953577
Epoch 71: reducing lr to 0.00018055846934601988
Epoch 74: reducing lr to 0.00014755464902484056
Epoch 77: reducing lr to 0.00011684054249606821
Epoch 80: reducing lr to 8.890043867160885e-05
Epoch 83: reducing lr to 6.417505199839924e-05
Epoch 86: reducing lr to 4.305421480797177e-05
Epoch 89: reducing lr to 2.587114040935477e-05
Epoch 92: reducing lr to 1.2896731050495921e-05
Epoch 95: reducing lr to 4.335639143599953e-06
Epoch 98: reducing lr to 3.228554366818161e-07
[I 2024-06-20 21:14:21,221] Trial 4 finished with value: 1.106611967086792 and parameters: {'hidden_size': 47, 'n_layers': 7, 'rnn_dropout': 0.07867746706644008, 'bidirectional': False, 'fc_dropout': 0.42653222797841367, 'learning_rate_model': 0.005854751355295724}. Best is trial 0 with value: 1.074925422668457.
Epoch 5: reducing lr to 0.00016276811997621943
Epoch 16: reducing lr to 0.0007585880160435583
Epoch 19: reducing lr to 0.0008892004182831998
Epoch 22: reducing lr to 0.0009667362000109951
Epoch 25: reducing lr to 0.000982906233110818
Epoch 31: reducing lr to 0.0009629206701292978
Epoch 34: reducing lr to 0.0009416701076214945
Epoch 37: reducing lr to 0.0009133221117093077
Epoch 40: reducing lr to 0.000878323742648951
Epoch 43: reducing lr to 0.0008372269714390209
Epoch 46: reducing lr to 0.0007906798700778879
Epoch 49: reducing lr to 0.0007394165831232806
Epoch 52: reducing lr to 0.0006842455301608364
Epoch 55: reducing lr to 0.0006260367027408444
Epoch 58: reducing lr to 0.0005657083072118177
Epoch 61: reducing lr to 0.0005042114915107441
Epoch 64: reducing lr to 0.00044251641565862964
Epoch 67: reducing lr to 0.00038159563823810655
Epoch 70: reducing lr to 0.0003224103231223221
Epoch 73: reducing lr to 0.00026589367311469767
Epoch 76: reducing lr to 0.0002129369120129873
Epoch 79: reducing lr to 0.00016437535559937836
Epoch 82: reducing lr to 0.00012097470491400901
Epoch 85: reducing lr to 8.341954201440075e-05
Epoch 88: reducing lr to 5.230198179731159e-05
Epoch 91: reducing lr to 2.8112951071909004e-05
Epoch 94: reducing lr to 1.1233802417135938e-05
Epoch 97: reducing lr to 1.9307798208778246e-06
[I 2024-06-20 21:14:39,661] Trial 5 finished with value: 1.1009104251861572 and parameters: {'hidden_size': 74, 'n_layers': 5, 'rnn_dropout': 0.6677005375178984, 'bidirectional': False, 'fc_dropout': 0.7910888711251958, 'learning_rate_model': 0.009832469717408681}. Best is trial 0 with value: 1.074925422668457.
Epoch 24: reducing lr to 1.4160579746521335e-05
Epoch 27: reducing lr to 1.4109464279046376e-05
Epoch 30: reducing lr to 1.394691497167045e-05
Epoch 33: reducing lr to 1.3676081240671011e-05
Epoch 36: reducing lr to 1.330123428558033e-05
Epoch 39: reducing lr to 1.282828556974911e-05
Epoch 42: reducing lr to 1.2264693839686062e-05
Epoch 45: reducing lr to 1.16193472765576e-05
Epoch 48: reducing lr to 1.090242366354274e-05
Epoch 51: reducing lr to 1.0125229083800952e-05
Epoch 54: reducing lr to 9.300020984543719e-06
Epoch 57: reducing lr to 8.43981085578762e-06
Epoch 60: reducing lr to 7.558168539659002e-06
Epoch 63: reducing lr to 6.66899482348316e-06
Epoch 66: reducing lr to 5.786315127674328e-06
Epoch 69: reducing lr to 4.9240459632486e-06
Epoch 72: reducing lr to 4.0957909368699045e-06
Epoch 75: reducing lr to 3.314608370414355e-06
Epoch 78: reducing lr to 2.592820281183509e-06
Epoch 81: reducing lr to 1.9418075624497384e-06
Epoch 84: reducing lr to 1.3718389917411558e-06
Epoch 87: reducing lr to 8.919009829898424e-07
Epoch 90: reducing lr to 5.095652877595537e-07
Epoch 93: reducing lr to 2.3085965256694832e-07
Epoch 96: reducing lr to 6.018025291100915e-08
Epoch 99: reducing lr to 2.1829977215812295e-10
[I 2024-06-20 21:14:59,600] Trial 6 finished with value: 1.1071107387542725 and parameters: {'hidden_size': 67, 'n_layers': 6, 'rnn_dropout': 0.08258080526211363, 'bidirectional': False, 'fc_dropout': 0.2348913186989436, 'learning_rate_model': 0.00014161242322273667}. Best is trial 0 with value: 1.074925422668457.
[I 2024-06-20 21:15:03,184] Trial 7 finished with value: 1.1084123849868774 and parameters: {'hidden_size': 40, 'n_layers': 1, 'rnn_dropout': 0.5430684263519128, 'bidirectional': False, 'fc_dropout': 0.39325852742427064, 'learning_rate_model': 1.634745612637677e-05}. Best is trial 0 with value: 1.074925422668457.
Epoch 18: reducing lr to 0.000518512060163731
Epoch 24: reducing lr to 0.0005992117359028589
Epoch 27: reducing lr to 0.0005970487603365034
Epoch 30: reducing lr to 0.0005901704082925863
Epoch 33: reducing lr to 0.0005787099488341317
Epoch 36: reducing lr to 0.0005628481198215894
Epoch 39: reducing lr to 0.0005428350676670072
Epoch 42: reducing lr to 0.0005189864128127074
Epoch 45: reducing lr to 0.000491678262915374
Epoch 48: reducing lr to 0.0004613412957604864
Epoch 51: reducing lr to 0.0004284539336893273
Epoch 54: reducing lr to 0.000393534856470155
Epoch 57: reducing lr to 0.00035713465155483103
Epoch 60: reducing lr to 0.0003198275333330205
Epoch 63: reducing lr to 0.00028220172029949625
Epoch 66: reducing lr to 0.0002448507048580734
Epoch 69: reducing lr to 0.00020836337085905574
Epoch 72: reducing lr to 0.00017331536145473976
Epoch 75: reducing lr to 0.00014025924580962496
Epoch 78: reducing lr to 0.00010971643600635609
Epoch 81: reducing lr to 8.216852001208808e-05
Epoch 84: reducing lr to 5.8050026081904026e-05
Epoch 87: reducing lr to 3.7741218639166135e-05
Epoch 90: reducing lr to 2.1562499989398463e-05
Epoch 93: reducing lr to 9.768937122687713e-06
Epoch 96: reducing lr to 2.546556317564383e-06
Epoch 99: reducing lr to 9.237459748338838e-09
[I 2024-06-20 21:15:33,592] Trial 8 finished with value: 1.1880626678466797 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.47144442952262744, 'bidirectional': True, 'fc_dropout': 0.3312447902556547, 'learning_rate_model': 0.005992397731141758}. Best is trial 0 with value: 1.074925422668457.
Epoch 23: reducing lr to 0.00022089540274429563
Epoch 27: reducing lr to 0.00022112066512525275
Epoch 30: reducing lr to 0.00021857322531805952
Epoch 40: reducing lr to 0.00019824953034799984
Epoch 52: reducing lr to 0.00015444345679190095
Epoch 57: reducing lr to 0.00013226700554000198
Epoch 60: reducing lr to 0.00011845008581226695
Epoch 63: reducing lr to 0.00010451513550910891
Epoch 66: reducing lr to 9.06819581772339e-05
Epoch 69: reducing lr to 7.716865055732903e-05
Epoch 72: reducing lr to 6.418840561647934e-05
Epoch 75: reducing lr to 5.1945870729068435e-05
Epoch 78: reducing lr to 4.063415405338776e-05
Epoch 81: reducing lr to 3.0431614642646947e-05
Epoch 84: reducing lr to 2.149918269746401e-05
Epoch 87: reducing lr to 1.3977691476030298e-05
Epoch 90: reducing lr to 7.985803934559354e-06
Epoch 93: reducing lr to 3.6179856950343483e-06
Epoch 96: reducing lr to 9.431327290611563e-07
Epoch 99: reducing lr to 3.4211497943175544e-09
[I 2024-06-20 21:15:42,618] Trial 9 finished with value: 0.9844352006912231 and parameters: {'hidden_size': 92, 'n_layers': 1, 'rnn_dropout': 0.4287171247324093, 'bidirectional': True, 'fc_dropout': 0.7556758047926507, 'learning_rate_model': 0.002219321201267233}. Best is trial 9 with value: 0.9844352006912231.
Epoch 13: reducing lr to 0.0030945996847008034
Epoch 16: reducing lr to 0.003957105978566978
Epoch 19: reducing lr to 0.004638433796627053
Epoch 22: reducing lr to 0.005042892210072786
Epoch 25: reducing lr to 0.005127241729574371
Epoch 28: reducing lr to 0.005095069301757017
Epoch 31: reducing lr to 0.005022988842517612
Epoch 34: reducing lr to 0.004912137199505747
Epoch 37: reducing lr to 0.004764262435164537
Epoch 40: reducing lr to 0.0045816965990060085
Epoch 43: reducing lr to 0.004367318997969301
Epoch 46: reducing lr to 0.0041245102411927815
Epoch 49: reducing lr to 0.0038570999275589547
Epoch 52: reducing lr to 0.0035693051049355148
Epoch 55: reducing lr to 0.0032656640057913944
Epoch 58: reducing lr to 0.0029509663707425996
Epoch 61: reducing lr to 0.002630173777230844
Epoch 64: reducing lr to 0.002308346977519673
Epoch 67: reducing lr to 0.0019905592357530562
Epoch 70: reducing lr to 0.001681824376600479
Epoch 73: reducing lr to 0.001387010368332641
Epoch 76: reducing lr to 0.0011107661995227182
Epoch 79: reducing lr to 0.0008574492196222937
Epoch 82: reducing lr to 0.0006310536390586915
Epoch 85: reducing lr to 0.0004351505184014788
Epoch 88: reducing lr to 0.00027282857161450223
Epoch 91: reducing lr to 0.00014664867412751692
Epoch 94: reducing lr to 5.860011727938444e-05
Epoch 97: reducing lr to 1.007173882384832e-05
[I 2024-06-20 21:16:03,167] Trial 10 finished with value: 1.337050199508667 and parameters: {'hidden_size': 183, 'n_layers': 1, 'rnn_dropout': 0.11142107780060684, 'bidirectional': True, 'fc_dropout': 0.13228335769354624, 'learning_rate_model': 0.051290191619112704}. Best is trial 9 with value: 0.9844352006912231.
Epoch 24: reducing lr to 2.486411739779812e-05
Epoch 27: reducing lr to 2.47743653532568e-05
Epoch 30: reducing lr to 2.448895012775951e-05
Epoch 33: reducing lr to 2.401340168247018e-05
Epoch 40: reducing lr to 2.2211882788842103e-05
Epoch 88: reducing lr to 1.3226620583002466e-06
[I 2024-06-20 21:16:55,088] Trial 11 finished with value: 1.0510315895080566 and parameters: {'hidden_size': 80, 'n_layers': 6, 'rnn_dropout': 0.5807983882803612, 'bidirectional': True, 'fc_dropout': 0.6007539472218698, 'learning_rate_model': 0.0002486528079375971}. Best is trial 9 with value: 0.9844352006912231.
[I 2024-06-20 21:17:44,012] Trial 12 finished with value: 1.0780189037322998 and parameters: {'hidden_size': 65, 'n_layers': 7, 'rnn_dropout': 0.34247295189703597, 'bidirectional': True, 'fc_dropout': 0.49735657616729745, 'learning_rate_model': 2.877291692654975e-05}. Best is trial 9 with value: 0.9844352006912231.
Epoch 22: reducing lr to 0.0001938042561893984
Epoch 30: reducing lr to 0.00019413093468388534
Epoch 33: reducing lr to 0.00019036112570106425
Epoch 36: reducing lr to 0.00018514352812461278
Epoch 39: reducing lr to 0.0001785604252342349
Epoch 42: reducing lr to 0.00017071563736828125
Epoch 45: reducing lr to 0.00016173288155815134
Epoch 48: reducing lr to 0.00015175382515935452
Epoch 51: reducing lr to 0.00014093584064428497
Epoch 54: reducing lr to 0.00012944954278250545
Epoch 57: reducing lr to 0.00011747604207219275
Epoch 60: reducing lr to 0.00010520422086767768
Epoch 63: reducing lr to 9.282756804028289e-05
Epoch 66: reducing lr to 8.054130726347882e-05
Epoch 69: reducing lr to 6.853914627095284e-05
Epoch 72: reducing lr to 5.701043739489492e-05
Epoch 75: reducing lr to 4.613694299897917e-05
Epoch 78: reducing lr to 3.6090176621561664e-05
Epoch 81: reducing lr to 2.7028552037515953e-05
Epoch 84: reducing lr to 1.909500317765344e-05
Epoch 87: reducing lr to 1.2414614402181021e-05
Epoch 90: reducing lr to 7.092778997804211e-06
Epoch 93: reducing lr to 3.2133988215066145e-06
Epoch 96: reducing lr to 8.376654457890702e-07
Epoch 99: reducing lr to 3.0385744013376043e-09
[I 2024-06-20 21:20:08,802] Trial 13 finished with value: 1.1114684343338013 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.4627116915097054, 'bidirectional': True, 'fc_dropout': 0.7227036164498031, 'learning_rate_model': 0.001971142158600765}. Best is trial 9 with value: 0.9844352006912231.
Epoch 11: reducing lr to 0.002080870038743925
Epoch 14: reducing lr to 0.002850749974212268
Epoch 17: reducing lr to 0.003534776697410965
Epoch 22: reducing lr to 0.004234342343265513
Epoch 25: reducing lr to 0.004305167720287503
Epoch 32: reducing lr to 0.004190159173601812
Epoch 35: reducing lr to 0.004086530610507061
Epoch 38: reducing lr to 0.003952414633535491
Epoch 41: reducing lr to 0.003789926348807179
Epoch 44: reducing lr to 0.003601628294477047
Epoch 47: reducing lr to 0.0033904901268240932
Epoch 50: reducing lr to 0.00315984155398125
Epoch 55: reducing lr to 0.002742065228160209
Epoch 58: reducing lr to 0.002477824497662145
Epoch 61: reducing lr to 0.0022084660411399934
Epoch 64: reducing lr to 0.001938239197406807
Epoch 67: reducing lr to 0.0016714038110693132
Epoch 70: reducing lr to 0.0014121698174613072
Epoch 73: reducing lr to 0.0011646246813382575
Epoch 76: reducing lr to 0.0009326719977699606
Epoch 79: reducing lr to 0.00071997048253273
Epoch 82: reducing lr to 0.0005298739361116439
Epoch 85: reducing lr to 0.0003653808546771872
Epoch 88: reducing lr to 0.000229084724621402
Epoch 91: reducing lr to 0.00012313582455749788
Epoch 94: reducing lr to 4.92044936873326e-05
Epoch 97: reducing lr to 8.456891084634294e-06
[I 2024-06-20 21:20:15,076] Trial 14 finished with value: 1.1169662475585938 and parameters: {'hidden_size': 16, 'n_layers': 5, 'rnn_dropout': 0.26131592141767696, 'bidirectional': False, 'fc_dropout': 0.28581580800199985, 'learning_rate_model': 0.04306660168805714}. Best is trial 9 with value: 0.9844352006912231.
[I 2024-06-20 21:20:22,393] Trial 15 finished with value: 1.0964847803115845 and parameters: {'hidden_size': 131, 'n_layers': 1, 'rnn_dropout': 0.7435497869950091, 'bidirectional': False, 'fc_dropout': 0.13787240667626285, 'learning_rate_model': 3.5362503098265445e-05}. Best is trial 9 with value: 0.9844352006912231.
Epoch 5: reducing lr to 0.00011614640295176609
Epoch 22: reducing lr to 0.0006898336864180824
Epoch 25: reducing lr to 0.0007013721325242966
Epoch 30: reducing lr to 0.0006909964773420609
Epoch 33: reducing lr to 0.0006775780866480502
Epoch 36: reducing lr to 0.0006590063863088579
Epoch 39: reducing lr to 0.0006355742583245225
Epoch 42: reducing lr to 0.0006076512444591817
Epoch 45: reducing lr to 0.000575677707466064
Epoch 48: reducing lr to 0.000540157903113426
Epoch 51: reducing lr to 0.0005016519885149809
Epoch 54: reducing lr to 0.0004607672558827729
Epoch 57: reducing lr to 0.00041814835629445533
Epoch 60: reducing lr to 0.00037446760424584625
Epoch 63: reducing lr to 0.00033041371083137553
Epoch 66: reducing lr to 0.0002866815620612585
Epoch 69: reducing lr to 0.0002439606480563244
Epoch 72: reducing lr to 0.0002029249561681142
Epoch 75: reducing lr to 0.00016422145774726144
Epoch 78: reducing lr to 0.00012846064411506687
Epoch 81: reducing lr to 9.620637883391604e-05
Epoch 84: reducing lr to 6.796742596474623e-05
Epoch 87: reducing lr to 4.418901517903827e-05
Epoch 90: reducing lr to 2.5246287048631255e-05
Epoch 93: reducing lr to 1.1437884794465465e-05
Epoch 96: reducing lr to 2.9816158520740153e-06
Epoch 99: reducing lr to 1.0815608603859105e-08
[I 2024-06-20 21:23:19,229] Trial 16 finished with value: 1.1791834831237793 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.052800138177649995, 'bidirectional': True, 'fc_dropout': 0.7384196284371867, 'learning_rate_model': 0.007016152732955534}. Best is trial 9 with value: 0.9844352006912231.
Epoch 19: reducing lr to 0.00012931869048086794
Epoch 24: reducing lr to 0.0001429894307255552
Epoch 27: reducing lr to 0.00014247328154760153
Epoch 30: reducing lr to 0.00014083190574642877
Epoch 33: reducing lr to 0.0001380971052149462
Epoch 36: reducing lr to 0.00013431200928829166
Epoch 39: reducing lr to 0.00012953631021031417
Epoch 42: reducing lr to 0.0001238453242418094
Epoch 45: reducing lr to 0.00011732880166051458
Epoch 48: reducing lr to 0.00011008951477158011
Epoch 51: reducing lr to 0.00010224162912639205
Epoch 54: reducing lr to 9.390891687484064e-05
Epoch 57: reducing lr to 8.522276427254996e-05
Epoch 60: reducing lr to 7.632019565294522e-05
Epoch 63: reducing lr to 6.734157713816699e-05
Epoch 66: reducing lr to 5.842853335917083e-05
Epoch 69: reducing lr to 4.97215892113704e-05
Epoch 72: reducing lr to 4.135810997270696e-05
Epoch 75: reducing lr to 3.346995479335086e-05
Epoch 78: reducing lr to 2.6181547833250327e-05
Epoch 81: reducing lr to 1.9607810054632486e-05
Epoch 84: reducing lr to 1.385243259721591e-05
Epoch 87: reducing lr to 9.006157664738978e-06
Epoch 90: reducing lr to 5.1454426102957865e-06
Epoch 93: reducing lr to 2.3311538714477144e-06
Epoch 96: reducing lr to 6.07682754428119e-07
Epoch 99: reducing lr to 2.204327838773064e-09
[I 2024-06-20 21:23:23,218] Trial 17 finished with value: 1.0921207666397095 and parameters: {'hidden_size': 38, 'n_layers': 1, 'rnn_dropout': 0.020968789502175424, 'bidirectional': False, 'fc_dropout': 0.6880223589463105, 'learning_rate_model': 0.0014299612122349629}. Best is trial 9 with value: 0.9844352006912231.
Epoch 6: reducing lr to 3.664919117329854e-05
Epoch 29: reducing lr to 0.00017360398492527435
Epoch 35: reducing lr to 0.00016646963593005255
Epoch 38: reducing lr to 0.00016100626369898284
Epoch 42: reducing lr to 0.00015194143864759131
Epoch 48: reducing lr to 0.0001350649235795868
Epoch 51: reducing lr to 0.0001254366308477317
Epoch 56: reducing lr to 0.00010814694065774532
Epoch 59: reducing lr to 9.729421941246965e-05
Epoch 62: reducing lr to 8.62904800458764e-05
Epoch 65: reducing lr to 7.530929052105039e-05
Epoch 68: reducing lr to 6.452378241801423e-05
Epoch 71: reducing lr to 5.410411352039001e-05
Epoch 74: reducing lr to 4.421456113477657e-05
Epoch 77: reducing lr to 3.501111854729276e-05
Epoch 80: reducing lr to 2.663890230861232e-05
Epoch 83: reducing lr to 1.922997193692629e-05
Epoch 86: reducing lr to 1.2901140189872312e-05
Epoch 89: reducing lr to 7.752254007687928e-06
Epoch 92: reducing lr to 3.864488901159113e-06
Epoch 95: reducing lr to 1.2991687028496117e-06
Epoch 98: reducing lr to 9.674321708738955e-08
[I 2024-06-20 21:24:03,736] Trial 18 finished with value: 1.0942280292510986 and parameters: {'hidden_size': 118, 'n_layers': 6, 'rnn_dropout': 0.09933865209592892, 'bidirectional': False, 'fc_dropout': 0.7756765986557397, 'learning_rate_model': 0.0017543687266950109}. Best is trial 9 with value: 0.9844352006912231.
Epoch 28: reducing lr to 0.0009673916732312766
Epoch 31: reducing lr to 0.0009537058856705771
Epoch 34: reducing lr to 0.0009326586829609488
Epoch 37: reducing lr to 0.0009045819665843155
Epoch 40: reducing lr to 0.0008699185186003281
Epoch 43: reducing lr to 0.0008292150278551303
Epoch 46: reducing lr to 0.0007831133645446349
Epoch 49: reducing lr to 0.0007323406477424669
Epoch 52: reducing lr to 0.0006776975607664018
Epoch 55: reducing lr to 0.0006200457696785909
Epoch 58: reducing lr to 0.0005602946939421341
Epoch 61: reducing lr to 0.0004993863793701378
Epoch 64: reducing lr to 0.00043828170192131594
Epoch 67: reducing lr to 0.00037794391316268587
Epoch 70: reducing lr to 0.00031932497899481496
Epoch 73: reducing lr to 0.00026334917182534385
Epoch 76: reducing lr to 0.00021089918677935854
Epoch 79: reducing lr to 0.0001628023459848164
Epoch 82: reducing lr to 0.00011981702301422161
Epoch 85: reducing lr to 8.262124873526198e-05
Epoch 88: reducing lr to 5.180147173041126e-05
Epoch 91: reducing lr to 2.7843920826051312e-05
Epoch 94: reducing lr to 1.1126299202035215e-05
Epoch 97: reducing lr to 1.9123029925797315e-06
[I 2024-06-20 21:24:20,827] Trial 19 finished with value: 1.0845681428909302 and parameters: {'hidden_size': 19, 'n_layers': 6, 'rnn_dropout': 0.18637941907281635, 'bidirectional': True, 'fc_dropout': 0.6908334836475429, 'learning_rate_model': 0.009738376723091037}. Best is trial 9 with value: 0.9844352006912231.
[I 2024-06-20 21:24:32,747] Trial 20 finished with value: 1.0543397665023804 and parameters: {'hidden_size': 118, 'n_layers': 1, 'rnn_dropout': 0.04793415160976933, 'bidirectional': True, 'fc_dropout': 0.08599530328487433, 'learning_rate_model': 7.995347689531267e-05}. Best is trial 9 with value: 0.9844352006912231.
Epoch 58: reducing lr to 3.7419921953662207e-06
Epoch 61: reducing lr to 3.3352090503078104e-06
Epoch 65: reducing lr to 2.7919048799306206e-06
Epoch 68: reducing lr to 2.3920589579061525e-06
Epoch 71: reducing lr to 2.005775615688144e-06
Epoch 74: reducing lr to 1.6391450263585848e-06
Epoch 77: reducing lr to 1.2979502535174435e-06
Epoch 80: reducing lr to 9.875711328155595e-07
Epoch 83: reducing lr to 7.129034428577779e-07
Epoch 86: reducing lr to 4.782777264739417e-07
Epoch 89: reducing lr to 2.8739556095639843e-07
Epoch 92: reducing lr to 1.4326632672987593e-07
Epoch 95: reducing lr to 4.8163452560015116e-08
Epoch 98: reducing lr to 3.5865144661133493e-09
[I 2024-06-20 21:25:09,069] Trial 21 finished with value: 1.0916944742202759 and parameters: {'hidden_size': 147, 'n_layers': 4, 'rnn_dropout': 0.010044784127292684, 'bidirectional': False, 'fc_dropout': 0.4544803695359537, 'learning_rate_model': 6.503886274016054e-05}. Best is trial 9 with value: 0.9844352006912231.
Epoch 27: reducing lr to 9.074029913394491e-06
Epoch 30: reducing lr to 8.969491764506644e-06
Epoch 33: reducing lr to 8.795314111263295e-06
Epoch 36: reducing lr to 8.554243832748966e-06
Epoch 39: reducing lr to 8.250082688847332e-06
Epoch 42: reducing lr to 7.887627522840171e-06
Epoch 45: reducing lr to 7.47259447108707e-06
Epoch 48: reducing lr to 7.011529034337875e-06
Epoch 51: reducing lr to 6.511702341727141e-06
Epoch 54: reducing lr to 5.980997360351219e-06
Epoch 57: reducing lr to 5.427781994709759e-06
Epoch 60: reducing lr to 4.860783234781916e-06
Epoch 63: reducing lr to 4.288940906879651e-06
Epoch 66: reducing lr to 3.721274990916424e-06
Epoch 69: reducing lr to 3.1667354253698893e-06
Epoch 72: reducing lr to 2.634070914751943e-06
Epoch 75: reducing lr to 2.1316794819059173e-06
Epoch 78: reducing lr to 1.6674856200213739e-06
Epoch 81: reducing lr to 1.2488085698541816e-06
Epoch 84: reducing lr to 8.822523521255567e-07
Epoch 87: reducing lr to 5.735962783119056e-07
Epoch 90: reducing lr to 3.277098671155342e-07
Epoch 93: reducing lr to 1.484696620479998e-07
Epoch 96: reducing lr to 3.870291630569603e-08
Epoch 99: reducing lr to 1.4039219515930586e-10
[I 2024-06-20 21:25:28,737] Trial 22 finished with value: 1.1071219444274902 and parameters: {'hidden_size': 62, 'n_layers': 6, 'rnn_dropout': 0.15634358488745503, 'bidirectional': False, 'fc_dropout': 0.6774630411920283, 'learning_rate_model': 9.107329229641331e-05}. Best is trial 9 with value: 0.9844352006912231.
Epoch 68: reducing lr to 3.243242218494229e-05
Epoch 73: reducing lr to 2.384656501070863e-05
Epoch 77: reducing lr to 1.7598090740195766e-05
Epoch 86: reducing lr to 6.484666732560441e-06
Epoch 89: reducing lr to 3.896615564682874e-06
Epoch 92: reducing lr to 1.9424579724641864e-06
Epoch 95: reducing lr to 6.530179459615605e-07
Epoch 98: reducing lr to 4.86272927985071e-08
[I 2024-06-20 21:26:40,960] Trial 23 finished with value: 0.9788602590560913 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.663184719640143, 'bidirectional': True, 'fc_dropout': 0.056017714975377865, 'learning_rate_model': 0.0008818210135856344}. Best is trial 23 with value: 0.9788602590560913.
Epoch 36: reducing lr to 0.0001501700983652299
Epoch 39: reducing lr to 0.0001448305371145061
Epoch 45: reducing lr to 0.00013118169983307622
Epoch 48: reducing lr to 0.00012308767734047862
Epoch 51: reducing lr to 0.00011431319942490105
Epoch 54: reducing lr to 0.00010499665189430199
Epoch 57: reducing lr to 9.528493365247611e-05
Epoch 60: reducing lr to 8.533124736341399e-05
Epoch 63: reducing lr to 7.529253204158418e-05
Epoch 66: reducing lr to 6.532713380118921e-05
Epoch 69: reducing lr to 5.559216917617759e-05
Epoch 72: reducing lr to 4.624122203004525e-05
Epoch 75: reducing lr to 3.742171999533508e-05
Epoch 78: reducing lr to 2.9272777872260637e-05
Epoch 81: reducing lr to 2.1922885229947807e-05
Epoch 84: reducing lr to 1.548797592072779e-05
Epoch 87: reducing lr to 1.0069505992600148e-05
Epoch 90: reducing lr to 5.7529600444159196e-06
Epoch 93: reducing lr to 2.606390955170565e-06
Epoch 96: reducing lr to 6.794312697046279e-07
Epoch 99: reducing lr to 2.464590695438508e-09
[I 2024-06-20 21:27:50,112] Trial 24 finished with value: 1.0585122108459473 and parameters: {'hidden_size': 128, 'n_layers': 4, 'rnn_dropout': 0.2538899274577286, 'bidirectional': True, 'fc_dropout': 0.3041129380988403, 'learning_rate_model': 0.0015987953500037988}. Best is trial 23 with value: 0.9788602590560913.
Epoch 12: reducing lr to 3.765771395202411e-06
Epoch 15: reducing lr to 4.977324369722877e-06
Epoch 18: reducing lr to 5.9958984796389065e-06
Epoch 21: reducing lr to 6.678438006953322e-06
Epoch 24: reducing lr to 6.9290822958818664e-06
Epoch 27: reducing lr to 6.904070376379527e-06
[I 2024-06-20 21:28:37,821] Trial 25 finished with value: 1.0950953960418701 and parameters: {'hidden_size': 153, 'n_layers': 5, 'rnn_dropout': 0.2119356461302475, 'bidirectional': False, 'fc_dropout': 0.5037740056172516, 'learning_rate_model': 6.929406508731719e-05}. Best is trial 23 with value: 0.9788602590560913.
Epoch 27: reducing lr to 0.00012505193221403634
Epoch 33: reducing lr to 0.0001212108660143711
Epoch 36: reducing lr to 0.00011788860408496168
Epoch 40: reducing lr to 0.0001121174577532118
Epoch 44: reducing lr to 0.00010496379450217537
Epoch 47: reducing lr to 9.881050453744613e-05
Epoch 55: reducing lr to 7.991318025836414e-05
Epoch 61: reducing lr to 6.436227082698146e-05
Epoch 64: reducing lr to 5.648693429153812e-05
Epoch 67: reducing lr to 4.8710436450162726e-05
Epoch 72: reducing lr to 3.630092259143783e-05
Epoch 75: reducing lr to 2.937731532930663e-05
Epoch 78: reducing lr to 2.2980120267730914e-05
Epoch 81: reducing lr to 1.721020606237928e-05
Epoch 84: reducing lr to 1.2158584706759794e-05
Epoch 87: reducing lr to 7.904902628522518e-06
Epoch 90: reducing lr to 4.516268127781852e-06
Epoch 93: reducing lr to 2.0461050152436506e-06
Epoch 96: reducing lr to 5.333765165575592e-07
Epoch 99: reducing lr to 1.9347870174480503e-09
[I 2024-06-20 21:28:53,619] Trial 26 finished with value: 0.9790920615196228 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.20825207886283278, 'bidirectional': True, 'fc_dropout': 0.5115687047039521, 'learning_rate_model': 0.001255108400948586}. Best is trial 23 with value: 0.9788602590560913.
Epoch 8: reducing lr to 0.0016544886175935583
Epoch 14: reducing lr to 0.0035375843355066787
Epoch 17: reducing lr to 0.0043864143777569906
Epoch 20: reducing lr to 0.005009490592106666
Epoch 23: reducing lr to 0.005319303769021502
Epoch 26: reducing lr to 0.005335908953411023
Epoch 29: reducing lr to 0.005288434010517498
Epoch 32: reducing lr to 0.005199698882715715
Epoch 35: reducing lr to 0.005071102974680548
Epoch 38: reducing lr to 0.004904674286241499
Epoch 41: reducing lr to 0.004703037518388133
Epoch 44: reducing lr to 0.004469372604442557
Epoch 47: reducing lr to 0.004207364683273299
Epoch 50: reducing lr to 0.003921145693296363
Epoch 53: reducing lr to 0.0036152297111632877
Epoch 56: reducing lr to 0.0032944402707922706
Epoch 59: reducing lr to 0.002963837835802738
Epoch 62: reducing lr to 0.0026286349916156636
Epoch 65: reducing lr to 0.002294119074921555
Epoch 68: reducing lr to 0.00196556412903503
Epoch 71: reducing lr to 0.0016481536076103872
Epoch 74: reducing lr to 0.0013468918295044033
Epoch 77: reducing lr to 0.0010665307605206196
Epoch 80: reducing lr to 0.0008114910324918962
Epoch 83: reducing lr to 0.0005857955256978228
Epoch 86: reducing lr to 0.00039300266398805404
Epoch 89: reducing lr to 0.00023615404778913469
Epoch 92: reducing lr to 0.00011772249667514334
Epoch 95: reducing lr to 3.957609588574329e-05
Epoch 98: reducing lr to 2.947052855682003e-06
[I 2024-06-20 21:29:18,723] Trial 27 finished with value: 1.2885158061981201 and parameters: {'hidden_size': 187, 'n_layers': 2, 'rnn_dropout': 0.05276887254721903, 'bidirectional': False, 'fc_dropout': 0.7262526820028192, 'learning_rate_model': 0.05344268592242116}. Best is trial 23 with value: 0.9788602590560913.
Epoch 13: reducing lr to 0.002773291716272543
Epoch 16: reducing lr to 0.0035462451848059886
Epoch 19: reducing lr to 0.004156831685940056
Epoch 22: reducing lr to 0.004519295746519909
Epoch 25: reducing lr to 0.004594887373075616
Epoch 28: reducing lr to 0.004566055363559377
Epoch 31: reducing lr to 0.004501458917853642
Epoch 34: reducing lr to 0.004402116846302396
Epoch 37: reducing lr to 0.004269595712463763
Epoch 40: reducing lr to 0.00410598543240204
Epoch 43: reducing lr to 0.003913866358633388
Epoch 46: reducing lr to 0.003696268096365143
Epoch 49: reducing lr to 0.003456622622569963
Epoch 52: reducing lr to 0.0031987091349180443
Epoch 55: reducing lr to 0.0029265946675317103
Epoch 58: reducing lr to 0.0026445716489402883
Epoch 61: reducing lr to 0.002357086502920909
Epoch 64: reducing lr to 0.0020686745308891256
Epoch 67: reducing lr to 0.0017838822470498269
Epoch 70: reducing lr to 0.0015072028976511355
Epoch 73: reducing lr to 0.0012429990166088138
Epoch 76: reducing lr to 0.0009954368944976228
Epoch 79: reducing lr to 0.0007684214632539051
Epoch 82: reducing lr to 0.0005655322200080685
Epoch 85: reducing lr to 0.00038996944709221855
Epoch 88: reducing lr to 0.00024450116160795873
Epoch 91: reducing lr to 0.00013142234686148654
Epoch 94: reducing lr to 5.251574884692426e-05
Epoch 97: reducing lr to 9.02600423141331e-06
[I 2024-06-20 21:29:23,024] Trial 28 finished with value: 1.1275272369384766 and parameters: {'hidden_size': 18, 'n_layers': 2, 'rnn_dropout': 0.4934226856013261, 'bidirectional': False, 'fc_dropout': 0.44532255055612513, 'learning_rate_model': 0.04596480257092418}. Best is trial 23 with value: 0.9788602590560913.
Epoch 9: reducing lr to 0.0017282168523372024
Epoch 12: reducing lr to 0.0025709383057013913
Epoch 18: reducing lr to 0.004093473411062394
Epoch 21: reducing lr to 0.004559451515352931
Epoch 24: reducing lr to 0.004730569444692093
Epoch 27: reducing lr to 0.004713493500562928
Epoch 30: reducing lr to 0.004659191289742971
Epoch 33: reducing lr to 0.004568714925399721
Epoch 36: reducing lr to 0.004443491270441423
Epoch 39: reducing lr to 0.004285495144289374
Epoch 42: reducing lr to 0.004097218261192666
Epoch 45: reducing lr to 0.0038816298610410043
Epoch 48: reducing lr to 0.0036421299960203296
Epoch 51: reducing lr to 0.003382495645074353
Epoch 54: reducing lr to 0.003106821605611549
Epoch 57: reducing lr to 0.0028194545751686105
Epoch 60: reducing lr to 0.0025249277777858853
Epoch 63: reducing lr to 0.0022278849950708576
Epoch 66: reducing lr to 0.0019330116443191602
Epoch 69: reducing lr to 0.001644956759890111
Epoch 72: reducing lr to 0.0013682648454109588
Epoch 75: reducing lr to 0.0011072982433543877
Epoch 78: reducing lr to 0.0008661733218061074
Epoch 81: reducing lr to 0.0006486920512314018
Epoch 84: reducing lr to 0.0004582848819422197
Epoch 87: reducing lr to 0.0002979538700637642
Epoch 90: reducing lr to 0.00017022848100151018
Epoch 93: reducing lr to 7.712238043881756e-05
Epoch 96: reducing lr to 2.0104181515915447e-05
Epoch 99: reducing lr to 7.292655035545659e-08
[I 2024-06-20 21:30:20,041] Trial 29 finished with value: 1.178863763809204 and parameters: {'hidden_size': 134, 'n_layers': 3, 'rnn_dropout': 0.3887925336775278, 'bidirectional': True, 'fc_dropout': 0.740945141365163, 'learning_rate_model': 0.047307907887800385}. Best is trial 23 with value: 0.9788602590560913.
Epoch 69: reducing lr to 4.239551985153865e-07
Epoch 73: reducing lr to 3.297194806353444e-07
Epoch 76: reducing lr to 2.640508411297557e-07
Epoch 79: reducing lr to 2.0383244265499317e-07
Epoch 82: reducing lr to 1.500137871165323e-07
Epoch 85: reducing lr to 1.0344378542607041e-07
Epoch 88: reducing lr to 6.485668527723827e-08
Epoch 91: reducing lr to 3.486125682485934e-08
Epoch 94: reducing lr to 1.3930393510869048e-08
Epoch 97: reducing lr to 2.394249221140447e-09
[I 2024-06-20 21:30:51,364] Trial 30 finished with value: 1.1090885400772095 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.13916453334437148, 'bidirectional': False, 'fc_dropout': 0.40452973254151736, 'learning_rate_model': 1.2192681272217677e-05}. Best is trial 23 with value: 0.9788602590560913.
Epoch 3: reducing lr to 0.00016636839911127383
Epoch 6: reducing lr to 0.00036133188436759034
Epoch 13: reducing lr to 0.0010435972448605943
Epoch 17: reducing lr to 0.0014196593533967329
Epoch 23: reducing lr to 0.0017215882264893106
Epoch 26: reducing lr to 0.00172696248056184
Epoch 29: reducing lr to 0.0017115972549067951
Epoch 32: reducing lr to 0.001682878204833128
Epoch 35: reducing lr to 0.0016412582464962178
Epoch 38: reducing lr to 0.0015873937403487466
Epoch 41: reducing lr to 0.0015221341686759732
Epoch 44: reducing lr to 0.001446508714244304
Epoch 51: reducing lr to 0.0012367054426552953
Epoch 54: reducing lr to 0.0011359137134777915
Epoch 57: reducing lr to 0.0010308468019782913
Epoch 60: reducing lr to 0.0009231621420256713
Epoch 63: reducing lr to 0.0008145575894610297
Epoch 66: reducing lr to 0.0007067462229335755
Epoch 69: reducing lr to 0.0006014278187914718
Epoch 72: reducing lr to 0.0005002639349374366
Epoch 75: reducing lr to 0.00040484952765369107
Epoch 78: reducing lr to 0.00031668962025725896
Epoch 81: reducing lr to 0.00023717428625025396
Epoch 84: reducing lr to 0.00016755776422355868
Epoch 87: reducing lr to 0.00010893766361670075
Epoch 90: reducing lr to 6.22388056156338e-05
Epoch 93: reducing lr to 2.819742511068965e-05
Epoch 96: reducing lr to 7.350475302774634e-06
Epoch 99: reducing lr to 2.666334895963518e-08
[I 2024-06-20 21:32:17,168] Trial 31 finished with value: 1.1764564514160156 and parameters: {'hidden_size': 191, 'n_layers': 6, 'rnn_dropout': 0.012015184593697903, 'bidirectional': False, 'fc_dropout': 0.10479747584873352, 'learning_rate_model': 0.017296680706943544}. Best is trial 23 with value: 0.9788602590560913.
Epoch 22: reducing lr to 6.794639617945932e-05
Epoch 45: reducing lr to 5.670239994552223e-05
Epoch 50: reducing lr to 5.070441374032892e-05
Epoch 53: reducing lr to 4.674860802916322e-05
Epoch 60: reducing lr to 3.6883852869778184e-05
Epoch 65: reducing lr to 2.966529984929367e-05
Epoch 71: reducing lr to 2.131230741330595e-05
Epoch 79: reducing lr to 1.1553010049243878e-05
Epoch 82: reducing lr to 8.502624839834224e-06
Epoch 85: reducing lr to 5.863085762956898e-06
Epoch 88: reducing lr to 3.676009211334575e-06
Epoch 91: reducing lr to 1.975899641788476e-06
Epoch 94: reducing lr to 7.895601609082164e-07
Epoch 97: reducing lr to 1.3570354626544134e-07
[I 2024-06-20 21:33:25,348] Trial 32 finished with value: 1.02975332736969 and parameters: {'hidden_size': 79, 'n_layers': 7, 'rnn_dropout': 0.46561134395766407, 'bidirectional': True, 'fc_dropout': 0.7243138549669195, 'learning_rate_model': 0.0006910684453876749}. Best is trial 23 with value: 0.9788602590560913.
Epoch 4: reducing lr to 3.0224285644406105e-05
Epoch 7: reducing lr to 6.0833852973316245e-05
Epoch 10: reducing lr to 0.00010017542396429677
Epoch 25: reducing lr to 0.00023646948439721337
Epoch 29: reducing lr to 0.0002340800737237202
Epoch 32: reducing lr to 0.00023015242232135493
Epoch 35: reducing lr to 0.00022446042738039984
Epoch 38: reducing lr to 0.00021709385353602985
Epoch 42: reducing lr to 0.0002048712371183497
Epoch 48: reducing lr to 0.0001821156771407467
Epoch 51: reducing lr to 0.0001691333053738986
Epoch 54: reducing lr to 0.00015534890876483994
Epoch 57: reducing lr to 0.0001409798331430951
Epoch 60: reducing lr to 0.0001262527511333757
Epoch 63: reducing lr to 0.00011139986351733
Epoch 66: reducing lr to 9.665545296592623e-05
Epoch 69: reducing lr to 8.225198291164016e-05
Epoch 72: reducing lr to 6.841668998695042e-05
Epoch 75: reducing lr to 5.536770230760264e-05
Epoch 78: reducing lr to 4.3310848650194264e-05
Epoch 81: reducing lr to 3.243623711809084e-05
Epoch 84: reducing lr to 2.291539887084486e-05
Epoch 87: reducing lr to 1.4898444279215537e-05
Epoch 90: reducing lr to 8.511852987154542e-06
Epoch 93: reducing lr to 3.856313352809584e-06
Epoch 96: reducing lr to 1.0052597337635063e-06
Epoch 99: reducing lr to 3.646511276137257e-09
[I 2024-06-20 21:34:05,329] Trial 33 finished with value: 1.1003201007843018 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.22857508138731808, 'bidirectional': False, 'fc_dropout': 0.012426620440668579, 'learning_rate_model': 0.002365514599564762}. Best is trial 23 with value: 0.9788602590560913.
Epoch 5: reducing lr to 0.00023475976443330108
Epoch 18: reducing lr to 0.0012270864672124743
Epoch 23: reducing lr to 0.0014115083471821221
Epoch 26: reducing lr to 0.0014159146299195
Epoch 29: reducing lr to 0.001403316876325042
Epoch 32: reducing lr to 0.0013797704914936433
Epoch 35: reducing lr to 0.0013456468156354978
Epoch 38: reducing lr to 0.0013014839903593045
Epoch 41: reducing lr to 0.0012479784954143893
Epoch 44: reducing lr to 0.001185974144694926
Epoch 47: reducing lr to 0.001116448811339829
Epoch 50: reducing lr to 0.0010404989293595415
Epoch 53: reducing lr to 0.0009593223353789567
Epoch 56: reducing lr to 0.0008741989823174966
Epoch 59: reducing lr to 0.0007864716937756918
Epoch 62: reducing lr to 0.0006975236597632857
Epoch 65: reducing lr to 0.0006087579059763251
Epoch 68: reducing lr to 0.0005215739306358626
Epoch 71: reducing lr to 0.00043734719346707524
Epoch 74: reducing lr to 0.0003574056197295507
Epoch 77: reducing lr to 0.0002830101713251614
Epoch 80: reducing lr to 0.00021533388874996583
Epoch 83: reducing lr to 0.00015544426680044987
Epoch 86: reducing lr to 0.00010428555404460252
Epoch 89: reducing lr to 6.266485693418605e-05
Epoch 92: reducing lr to 3.1238352597157995e-05
Epoch 95: reducing lr to 1.050174837108099e-05
Epoch 98: reducing lr to 7.82017701190206e-07
[I 2024-06-20 21:35:13,638] Trial 34 finished with value: 1.2292972803115845 and parameters: {'hidden_size': 96, 'n_layers': 6, 'rnn_dropout': 0.2521958424762984, 'bidirectional': True, 'fc_dropout': 0.14720816130197206, 'learning_rate_model': 0.01418132908946585}. Best is trial 23 with value: 0.9788602590560913.
Epoch 5: reducing lr to 0.0013364211062329907
Epoch 10: reducing lr to 0.003418787854888584
Epoch 13: reducing lr to 0.004870872386362592
Epoch 18: reducing lr to 0.006985457060388002
Epoch 21: reducing lr to 0.007780642398542613
Epoch 24: reducing lr to 0.00807265283262309
Epoch 27: reducing lr to 0.008043512964715921
Epoch 30: reducing lr to 0.007950846975743857
Epoch 33: reducing lr to 0.007796450282609904
Epoch 36: reducing lr to 0.007582757807585617
Epoch 39: reducing lr to 0.00731313955332747
Epoch 42: reducing lr to 0.006991847596530711
Epoch 45: reducing lr to 0.00662394890494343
Epoch 48: reducing lr to 0.006215245621675629
Epoch 51: reducing lr to 0.005772183110255988
Epoch 54: reducing lr to 0.005301749087128586
Epoch 57: reducing lr to 0.004811361132902356
Epoch 60: reducing lr to 0.0043087551331441555
Epoch 63: reducing lr to 0.0038018556384151886
Epoch 66: reducing lr to 0.003298658250015854
Epoch 69: reducing lr to 0.0028070964822573775
Epoch 72: reducing lr to 0.0023349254691692432
Epoch 75: reducing lr to 0.00188958948923224
Epoch 78: reducing lr to 0.0014781130689596605
Epoch 81: reducing lr to 0.0011069842195740366
Epoch 84: reducing lr to 0.0007820569581766287
Epoch 87: reducing lr to 0.0005084542529779551
Epoch 90: reducing lr to 0.0002904926025114968
Epoch 93: reducing lr to 0.0001316083000550014
Epoch 96: reducing lr to 3.430751408672883e-05
Epoch 99: reducing lr to 1.2444817271833886e-07
[I 2024-06-20 21:35:20,770] Trial 35 finished with value: 1.3425078392028809 and parameters: {'hidden_size': 129, 'n_layers': 1, 'rnn_dropout': 0.3361549440009058, 'bidirectional': False, 'fc_dropout': 0.0003216199130859465, 'learning_rate_model': 0.08073030553318121}. Best is trial 23 with value: 0.9788602590560913.
Epoch 24: reducing lr to 1.3876655495667891e-05
Epoch 33: reducing lr to 1.34018713431686e-05
Epoch 40: reducing lr to 1.2396444258995186e-05
Epoch 46: reducing lr to 1.1159460299421294e-05
Epoch 49: reducing lr to 1.043594293514158e-05
Epoch 52: reducing lr to 9.657272327084976e-06
Epoch 55: reducing lr to 8.835727321006501e-06
Epoch 58: reducing lr to 7.984267254408826e-06
Epoch 61: reducing lr to 7.116316394234079e-06
Epoch 64: reducing lr to 6.2455673392801004e-06
Epoch 67: reducing lr to 5.385746541050799e-06
Epoch 70: reducing lr to 4.550419628936199e-06
Epoch 73: reducing lr to 3.7527575966977356e-06
Epoch 76: reducing lr to 3.0053389567843903e-06
Epoch 79: reducing lr to 2.3199531497290116e-06
Epoch 82: reducing lr to 1.7074070907978267e-06
Epoch 85: reducing lr to 1.1773628019819306e-06
Epoch 88: reducing lr to 7.381772466151933e-07
Epoch 91: reducing lr to 3.967792443680619e-07
Epoch 94: reducing lr to 1.5855111130275992e-07
Epoch 97: reducing lr to 2.7250549272092366e-08
[I 2024-06-20 21:36:33,687] Trial 36 finished with value: 1.076183557510376 and parameters: {'hidden_size': 85, 'n_layers': 7, 'rnn_dropout': 0.48377288077924246, 'bidirectional': True, 'fc_dropout': 0.5024609586458804, 'learning_rate_model': 0.00013877304786559896}. Best is trial 23 with value: 0.9788602590560913.
Epoch 24: reducing lr to 1.9530064028894475e-05
Epoch 27: reducing lr to 1.9459566325373683e-05
Epoch 30: reducing lr to 1.923538070319362e-05
Epoch 33: reducing lr to 1.8861850791121847e-05
[I 2024-06-20 21:38:21,058] Trial 37 finished with value: 1.0501960515975952 and parameters: {'hidden_size': 124, 'n_layers': 6, 'rnn_dropout': 0.6866510691432339, 'bidirectional': True, 'fc_dropout': 0.6915835440436799, 'learning_rate_model': 0.00019530977843660444}. Best is trial 23 with value: 0.9788602590560913.
Epoch 22: reducing lr to 0.00030221974546722956
Epoch 25: reducing lr to 0.0003072747990460338
Epoch 28: reducing lr to 0.0003053467104530231
Epoch 31: reducing lr to 0.00030102693974664517
Epoch 36: reducing lr to 0.00028871414407970143
Epoch 39: reducing lr to 0.0002784484062727346
Epoch 42: reducing lr to 0.0002662151878764577
Epoch 45: reducing lr to 0.0002522074141159129
Epoch 48: reducing lr to 0.0002366460020801515
Epoch 51: reducing lr to 0.00021977635953000194
Epoch 54: reducing lr to 0.0002018645443593624
Epoch 57: reducing lr to 0.00018319298157652011
Epoch 60: reducing lr to 0.0001640562156779249
Epoch 63: reducing lr to 0.00014475597459562333
Epoch 66: reducing lr to 0.0001255966915245591
Epoch 69: reducing lr to 0.00010688043569231833
Epoch 72: reducing lr to 8.890248448219103e-05
Epoch 75: reducing lr to 7.194627942619099e-05
Epoch 78: reducing lr to 5.627927996471097e-05
Epoch 81: reducing lr to 4.214851767312569e-05
Epoch 84: reducing lr to 2.9776884747085556e-05
Epoch 87: reducing lr to 1.9359438633970463e-05
Epoch 90: reducing lr to 1.1060530380080238e-05
Epoch 93: reducing lr to 5.010997142247198e-06
Epoch 96: reducing lr to 1.3062614969903443e-06
Epoch 99: reducing lr to 4.738374689014849e-09
[I 2024-06-20 21:39:45,681] Trial 38 finished with value: 1.2115222215652466 and parameters: {'hidden_size': 140, 'n_layers': 4, 'rnn_dropout': 0.3056822016252138, 'bidirectional': True, 'fc_dropout': 0.2539071567666215, 'learning_rate_model': 0.003073813202894127}. Best is trial 23 with value: 0.9788602590560913.
Epoch 3: reducing lr to 0.00014916374721123284
Epoch 6: reducing lr to 0.00032396547750102945
Epoch 9: reducing lr to 0.0005665258212770945
Epoch 13: reducing lr to 0.0009356757440372347
Epoch 16: reducing lr to 0.0011964610799370078
Epoch 22: reducing lr to 0.0015247568026607716
Epoch 25: reducing lr to 0.0015502605212221786
Epoch 37: reducing lr to 0.0014405109716936527
Epoch 44: reducing lr to 0.0012969209377681726
Epoch 47: reducing lr to 0.001220891573269057
Epoch 50: reducing lr to 0.0011378366495155595
Epoch 53: reducing lr to 0.0010490660596497849
Epoch 56: reducing lr to 0.0009559794950123757
Epoch 59: reducing lr to 0.0008600453991196163
Epoch 62: reducing lr to 0.0007627763581375473
Epoch 67: reducing lr to 0.0006018607198764401
Epoch 75: reducing lr to 0.00036298283162015477
Epoch 78: reducing lr to 0.0002839398029482753
Epoch 85: reducing lr to 0.0001315710678464888
Epoch 88: reducing lr to 8.249179304259408e-05
Epoch 91: reducing lr to 4.4340341645709647e-05
Epoch 94: reducing lr to 1.7718191017446207e-05
Epoch 97: reducing lr to 3.0452668124873393e-06
[I 2024-06-20 21:40:22,029] Trial 39 finished with value: 1.1016621589660645 and parameters: {'hidden_size': 95, 'n_layers': 7, 'rnn_dropout': 0.5422407131474489, 'bidirectional': False, 'fc_dropout': 0.2746769918195384, 'learning_rate_model': 0.015507979414037055}. Best is trial 23 with value: 0.9788602590560913.
Epoch 10: reducing lr to 5.46630854308891e-05
Epoch 23: reducing lr to 0.00012847691528247224
Epoch 53: reducing lr to 8.731848781278445e-05
Epoch 56: reducing lr to 7.957047424866338e-05
Epoch 59: reducing lr to 7.158544784733372e-05
Epoch 62: reducing lr to 6.348930796040475e-05
Epoch 65: reducing lr to 5.540975940368576e-05
Epoch 68: reducing lr to 4.7474185918649535e-05
Epoch 71: reducing lr to 3.980778323859715e-05
Epoch 74: reducing lr to 3.253142046176528e-05
Epoch 77: reducing lr to 2.5759871613943264e-05
Epoch 80: reducing lr to 1.9599908025768904e-05
Epoch 83: reducing lr to 1.4148694151711332e-05
Epoch 86: reducing lr to 9.492176450052081e-06
Epoch 89: reducing lr to 5.703818565150061e-06
Epoch 92: reducing lr to 2.843346401883683e-06
Epoch 95: reducing lr to 9.55879742746673e-07
Epoch 98: reducing lr to 7.118004094399267e-08
[I 2024-06-20 21:44:08,736] Trial 40 finished with value: 1.0067410469055176 and parameters: {'hidden_size': 178, 'n_layers': 7, 'rnn_dropout': 0.5301758499002098, 'bidirectional': True, 'fc_dropout': 0.6839183541579219, 'learning_rate_model': 0.001290798895846048}. Best is trial 23 with value: 0.9788602590560913.
Epoch 8: reducing lr to 2.257433789208368e-05
Epoch 22: reducing lr to 7.169434116624327e-05
Epoch 25: reducing lr to 7.289353063459561e-05
Epoch 28: reducing lr to 7.243613814631746e-05
Epoch 31: reducing lr to 7.141137679492326e-05
Epoch 34: reducing lr to 6.983540904033654e-05
Epoch 37: reducing lr to 6.773308692776388e-05
Epoch 40: reducing lr to 6.513756499360358e-05
Epoch 43: reducing lr to 6.208977808345968e-05
Epoch 46: reducing lr to 5.863778801083509e-05
Epoch 49: reducing lr to 5.4836039835702264e-05
Epoch 52: reducing lr to 5.074448694511499e-05
Epoch 55: reducing lr to 4.6427648978471746e-05
Epoch 58: reducing lr to 4.195362124368648e-05
Epoch 61: reducing lr to 3.739294203723882e-05
Epoch 64: reducing lr to 3.281755961505536e-05
Epoch 67: reducing lr to 2.8299600113331845e-05
Epoch 70: reducing lr to 2.3910344622646382e-05
Epoch 73: reducing lr to 1.9719000606384536e-05
Epoch 76: reducing lr to 1.5791662313433398e-05
Epoch 79: reducing lr to 1.2190277785739645e-05
Epoch 82: reducing lr to 8.971632350678418e-06
Epoch 85: reducing lr to 6.186495452476263e-06
Epoch 88: reducing lr to 3.878779057414487e-06
Epoch 91: reducing lr to 2.0848908992095422e-06
Epoch 94: reducing lr to 8.331125523996564e-07
Epoch 97: reducing lr to 1.4318899736384932e-07
[I 2024-06-20 21:45:01,897] Trial 41 finished with value: 1.0930531024932861 and parameters: {'hidden_size': 164, 'n_layers': 5, 'rnn_dropout': 0.5865140202300089, 'bidirectional': False, 'fc_dropout': 0.4550863925637725, 'learning_rate_model': 0.000729188002289183}. Best is trial 23 with value: 0.9788602590560913.
Epoch 6: reducing lr to 0.00036929915021775573
Epoch 9: reducing lr to 0.0006458018489744246
Epoch 15: reducing lr to 0.0012697981664823437
Epoch 18: reducing lr to 0.0015296533499350089
Epoch 22: reducing lr to 0.0017381215920130749
Epoch 25: reducing lr to 0.0017671941390781875
Epoch 28: reducing lr to 0.0017561053453607391
Epoch 31: reducing lr to 0.0017312615459402435
Epoch 34: reducing lr to 0.0016930545753759713
Epoch 37: reducing lr to 0.0016420869341676355
Epoch 40: reducing lr to 0.001579162404240695
Epoch 43: reducing lr to 0.0015052733894286004
Epoch 46: reducing lr to 0.0014215850761943529
Epoch 49: reducing lr to 0.001329417403221776
Epoch 52: reducing lr to 0.001230223850309378
Epoch 55: reducing lr to 0.001125568599183683
Epoch 58: reducing lr to 0.001017102518282488
Epoch 61: reducing lr to 0.0009065357026311561
Epoch 64: reducing lr to 0.0007956124296035972
Epoch 67: reducing lr to 0.0006860812890136104
Epoch 70: reducing lr to 0.0005796703838135438
Epoch 73: reducing lr to 0.00047805754497976406
Epoch 76: reducing lr to 0.00038284512828024047
Epoch 79: reducing lr to 0.0002955349709247027
Epoch 82: reducing lr to 0.00021750374786426527
Epoch 85: reducing lr to 0.00014998228800103126
Epoch 88: reducing lr to 9.403517098661266e-05
Epoch 91: reducing lr to 5.0545047628025586e-05
Epoch 94: reducing lr to 2.0197562211294456e-05
Epoch 97: reducing lr to 3.47140212195729e-06
[I 2024-06-20 21:45:07,179] Trial 42 finished with value: 1.2554529905319214 and parameters: {'hidden_size': 79, 'n_layers': 1, 'rnn_dropout': 0.3023393434624799, 'bidirectional': False, 'fc_dropout': 0.14529028106461103, 'learning_rate_model': 0.01767806762428918}. Best is trial 23 with value: 0.9788602590560913.
Epoch 25: reducing lr to 7.943452499824303e-06
Epoch 28: reducing lr to 7.893608906397238e-06
[I 2024-06-20 21:46:06,882] Trial 43 finished with value: 1.0916454792022705 and parameters: {'hidden_size': 177, 'n_layers': 5, 'rnn_dropout': 0.4555955301963006, 'bidirectional': False, 'fc_dropout': 0.276137640924172, 'learning_rate_model': 7.946206212265512e-05}. Best is trial 23 with value: 0.9788602590560913.
[I 2024-06-20 21:46:58,303] Trial 44 finished with value: 1.0584206581115723 and parameters: {'hidden_size': 125, 'n_layers': 3, 'rnn_dropout': 0.7330444427746806, 'bidirectional': True, 'fc_dropout': 0.08871304059522336, 'learning_rate_model': 5.91358615023068e-05}. Best is trial 23 with value: 0.9788602590560913.
Epoch 7: reducing lr to 7.53286110324208e-05
Epoch 10: reducing lr to 0.00012404401789451537
Epoch 13: reducing lr to 0.00017673005962973173
Epoch 22: reducing lr to 0.00028799545395114337
Epoch 25: reducing lr to 0.0002928125860942611
Epoch 28: reducing lr to 0.0002909752450272714
Epoch 31: reducing lr to 0.00028685878889160484
Epoch 34: reducing lr to 0.00028052814212769725
Epoch 37: reducing lr to 0.0002720831351534652
Epoch 41: reducing lr to 0.00025776885069344095
Epoch 46: reducing lr to 0.00023554741004890733
Epoch 49: reducing lr to 0.0002202758255180375
Epoch 52: reducing lr to 0.0002038400983334126
Epoch 55: reducing lr to 0.00018649940324348663
Epoch 58: reducing lr to 0.0001685272784215046
Epoch 61: reducing lr to 0.00015020707550143282
Epoch 64: reducing lr to 0.00013182780991028415
Epoch 67: reducing lr to 0.00011367921161834
Epoch 70: reducing lr to 9.60476160560574e-05
Epoch 73: reducing lr to 7.921102891412713e-05
Epoch 76: reducing lr to 6.343495013162587e-05
Epoch 79: reducing lr to 4.89682243756727e-05
Epoch 82: reducing lr to 3.603895774040469e-05
Epoch 85: reducing lr to 2.485109057730598e-05
Epoch 88: reducing lr to 1.5581016817297137e-05
Epoch 91: reducing lr to 8.374985963873752e-06
Epoch 94: reducing lr to 3.3466048200985095e-06
Epoch 97: reducing lr to 5.751887753733826e-07
[I 2024-06-20 21:47:33,620] Trial 45 finished with value: 1.1027613878250122 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.1665555507270332, 'bidirectional': False, 'fc_dropout': 0.3326789746140219, 'learning_rate_model': 0.0029291409380281584}. Best is trial 23 with value: 0.9788602590560913.
Epoch 45: reducing lr to 0.00012306111703976603
Epoch 49: reducing lr to 0.00011278907738438572
Epoch 52: reducing lr to 0.00010437339899146323
Epoch 60: reducing lr to 8.004895981909359e-05
Epoch 65: reducing lr to 6.438254712818329e-05
Epoch 72: reducing lr to 4.33787368477627e-05
Epoch 75: reducing lr to 3.510519127313623e-05
Epoch 78: reducing lr to 2.746069572509874e-05
Epoch 81: reducing lr to 2.0565785841812427e-05
Epoch 84: reducing lr to 1.4529218785204295e-05
Epoch 87: reducing lr to 9.446170137029659e-06
Epoch 90: reducing lr to 5.3968327156289846e-06
Epoch 93: reducing lr to 2.4450467008262e-06
Epoch 96: reducing lr to 6.37372218137075e-07
Epoch 99: reducing lr to 2.31202434801403e-09
[I 2024-06-20 21:47:40,078] Trial 46 finished with value: 0.9782123565673828 and parameters: {'hidden_size': 59, 'n_layers': 1, 'rnn_dropout': 0.41268561357482386, 'bidirectional': True, 'fc_dropout': 0.4974449853923324, 'learning_rate_model': 0.0014998246092237527}. Best is trial 46 with value: 0.9782123565673828.
[I 2024-06-20 21:47:56,204] Trial 47 finished with value: 1.1021238565444946 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6012222537081952, 'bidirectional': False, 'fc_dropout': 0.6282368225772952, 'learning_rate_model': 1.2283488171661822e-05}. Best is trial 46 with value: 0.9782123565673828.
Epoch 9: reducing lr to 0.0007384013528984573
Epoch 13: reducing lr to 0.001219545887094589
Epoch 18: reducing lr to 0.0017489855516074856
Epoch 21: reducing lr to 0.0019480802787326533
Epoch 24: reducing lr to 0.0020211924639068355
Epoch 27: reducing lr to 0.0020138965622206887
Epoch 30: reducing lr to 0.0019906952921482394
Epoch 33: reducing lr to 0.0019520381816438258
Epoch 36: reducing lr to 0.0018985348749777386
Epoch 39: reducing lr to 0.0018310291374045782
Epoch 42: reducing lr to 0.001750585583686131
Epoch 45: reducing lr to 0.0016584728571345324
Epoch 48: reducing lr to 0.0015561436707762929
Epoch 51: reducing lr to 0.0014452117841104776
Epoch 54: reducing lr to 0.0013274267483824498
Epoch 57: reducing lr to 0.0012046457421849266
Epoch 60: reducing lr to 0.0010788056397937596
Epoch 63: reducing lr to 0.0009518905525297555
Epoch 66: reducing lr to 0.0008259023810602463
Epoch 72: reducing lr to 0.0005846075459850447
Epoch 75: reducing lr to 0.0004731064390728625
Epoch 87: reducing lr to 0.0001273043602478926
Epoch 95: reducing lr to 1.4968306356896374e-05
Epoch 98: reducing lr to 1.114622071898136e-06
[I 2024-06-20 21:48:22,731] Trial 48 finished with value: 1.0979629755020142 and parameters: {'hidden_size': 76, 'n_layers': 7, 'rnn_dropout': 0.6757676860816557, 'bidirectional': False, 'fc_dropout': 0.7598447930829013, 'learning_rate_model': 0.020212870358199457}. Best is trial 46 with value: 0.9782123565673828.
[I 2024-06-20 21:48:41,519] Trial 49 finished with value: 1.091343641281128 and parameters: {'hidden_size': 174, 'n_layers': 1, 'rnn_dropout': 0.5210434658727928, 'bidirectional': True, 'fc_dropout': 0.6396922093888224, 'learning_rate_model': 1.3749435862460687e-05}. Best is trial 46 with value: 0.9782123565673828.
Epoch 16: reducing lr to 0.0011849446707331538
Epoch 23: reducing lr to 0.0015286963459704552
Epoch 27: reducing lr to 0.0015302552637857453
Epoch 30: reducing lr to 0.0015126258252530478
Epoch 33: reducing lr to 0.0014832522973659468
Epoch 36: reducing lr to 0.0014425979171005365
Epoch 39: reducing lr to 0.0013913038177932912
Epoch 42: reducing lr to 0.0013301789448357912
Epoch 45: reducing lr to 0.0012601872743044056
Epoch 48: reducing lr to 0.0011824326472788052
Epoch 51: reducing lr to 0.0010981412756778416
Epoch 54: reducing lr to 0.00100864255250645
Epoch 57: reducing lr to 0.0009153476511935986
Epoch 60: reducing lr to 0.000819728301773249
Epoch 63: reducing lr to 0.0007232921272532355
Epoch 66: reducing lr to 0.0006275602678406713
Epoch 69: reducing lr to 0.0005340420518711142
Epoch 72: reducing lr to 0.00044421287134331443
Epoch 75: reducing lr to 0.0003594889788797618
Epoch 78: reducing lr to 0.00028120677049544695
Epoch 81: reducing lr to 0.00021060057171061586
Epoch 84: reducing lr to 0.0001487840924829441
Epoch 87: reducing lr to 9.673196281610556e-05
Epoch 90: reducing lr to 5.526538417157077e-05
Epoch 93: reducing lr to 2.503810148631637e-05
Epoch 96: reducing lr to 6.526906122849077e-06
Epoch 99: reducing lr to 2.367590780354839e-08
[I 2024-06-20 21:49:43,718] Trial 50 finished with value: 1.2480340003967285 and parameters: {'hidden_size': 158, 'n_layers': 6, 'rnn_dropout': 0.20775871463868248, 'bidirectional': False, 'fc_dropout': 0.27623796927286526, 'learning_rate_model': 0.015358709003280025}. Best is trial 46 with value: 0.9782123565673828.
Epoch 16: reducing lr to 0.00011313080452181197
Epoch 34: reducing lr to 0.0001404344579881207
Epoch 47: reducing lr to 0.00011544081812097143
Epoch 60: reducing lr to 7.82623227559883e-05
Epoch 63: reducing lr to 6.905522450244303e-05
Epoch 72: reducing lr to 4.24105536361625e-05
Epoch 75: reducing lr to 3.432166783975583e-05
Epoch 78: reducing lr to 2.684779211121048e-05
Epoch 81: reducing lr to 2.0106771817146683e-05
Epoch 84: reducing lr to 1.4204936735340356e-05
Epoch 87: reducing lr to 9.235338194811342e-06
Epoch 90: reducing lr to 5.276379166014927e-06
Epoch 93: reducing lr to 2.390474960398928e-06
Epoch 96: reducing lr to 6.23146513886925e-07
Epoch 99: reducing lr to 2.2604215738986425e-09
[I 2024-06-20 21:50:57,128] Trial 51 finished with value: 1.0179805755615234 and parameters: {'hidden_size': 98, 'n_layers': 6, 'rnn_dropout': 0.7923774268990726, 'bidirectional': True, 'fc_dropout': 0.7210467490794191, 'learning_rate_model': 0.0014663495679358655}. Best is trial 46 with value: 0.9782123565673828.
Epoch 57: reducing lr to 1.57780033295483e-05
Epoch 73: reducing lr to 7.159225960909953e-06
Epoch 78: reducing lr to 4.847208987080555e-06
Epoch 85: reducing lr to 2.2460833454246807e-06
Epoch 88: reducing lr to 1.4082384943730217e-06
Epoch 91: reducing lr to 7.569453112371809e-07
Epoch 94: reducing lr to 3.0247176987096157e-07
Epoch 97: reducing lr to 5.1986528511595125e-08
[I 2024-06-20 21:54:01,476] Trial 52 finished with value: 1.0340096950531006 and parameters: {'hidden_size': 196, 'n_layers': 5, 'rnn_dropout': 0.7951304196883832, 'bidirectional': True, 'fc_dropout': 0.10834232245773592, 'learning_rate_model': 0.000264740682379336}. Best is trial 46 with value: 0.9782123565673828.
Epoch 12: reducing lr to 0.0003205300730977614
Epoch 15: reducing lr to 0.00042365347670627555
Epoch 22: reducing lr to 0.0005799041728296783
Epoch 25: reducing lr to 0.0005896038920180925
Epoch 28: reducing lr to 0.0005859042442040703
Epoch 31: reducing lr to 0.0005776153977740556
Epoch 34: reducing lr to 0.0005648680837983165
Epoch 37: reducing lr to 0.0005478633196024078
Epoch 40: reducing lr to 0.0005268692777323473
Epoch 43: reducing lr to 0.0005022170622528876
Epoch 46: reducing lr to 0.0004742954241554005
Epoch 49: reducing lr to 0.00044354474572047293
Epoch 52: reducing lr to 0.0004104499636775902
Epoch 55: reducing lr to 0.00037553294917457274
Epoch 58: reducing lr to 0.00033934449537817614
Epoch 61: reducing lr to 0.0003024551557213134
Epoch 64: reducing lr to 0.00026544688818227053
Epoch 67: reducing lr to 0.0002289030895350412
Epoch 70: reducing lr to 0.0001934003213491675
Epoch 73: reducing lr to 0.00015949837253065514
Epoch 76: reducing lr to 0.00012773185055488037
Epoch 79: reducing lr to 9.860182604246913e-05
Epoch 82: reducing lr to 7.256761067359939e-05
Epoch 85: reducing lr to 5.003985628048418e-05
Epoch 88: reducing lr to 3.137374755510132e-05
Epoch 91: reducing lr to 1.68637707339098e-05
Epoch 94: reducing lr to 6.738683105450214e-06
Epoch 97: reducing lr to 1.1581931812728631e-06
[I 2024-06-20 21:54:06,152] Trial 53 finished with value: 1.0944879055023193 and parameters: {'hidden_size': 20, 'n_layers': 2, 'rnn_dropout': 0.5965097541659968, 'bidirectional': False, 'fc_dropout': 0.6898770022704354, 'learning_rate_model': 0.005898082867158478}. Best is trial 46 with value: 0.9782123565673828.
Epoch 38: reducing lr to 8.987802114517948e-06
Epoch 41: reducing lr to 8.618303293044563e-06
Epoch 44: reducing lr to 8.190112982112e-06
Epoch 47: reducing lr to 7.709984188542289e-06
Epoch 50: reducing lr to 7.185488678097045e-06
Epoch 53: reducing lr to 6.624898483801499e-06
Epoch 56: reducing lr to 6.037052718269313e-06
Epoch 59: reducing lr to 5.431224667138786e-06
Epoch 62: reducing lr to 4.816966378830367e-06
Epoch 65: reducing lr to 4.203966883259953e-06
Epoch 68: reducing lr to 3.6018908502687515e-06
Epoch 71: reducing lr to 3.020236944395055e-06
Epoch 74: reducing lr to 2.468175566154315e-06
Epoch 77: reducing lr to 1.954414679787296e-06
Epoch 80: reducing lr to 1.487055080946489e-06
Epoch 83: reducing lr to 1.0734686866590442e-06
Epoch 86: reducing lr to 7.201762988240772e-07
Epoch 89: reducing lr to 4.327516418419815e-07
Epoch 92: reducing lr to 2.1572615076831023e-07
Epoch 95: reducing lr to 7.252308665716187e-08
Epoch 98: reducing lr to 5.4004662373199334e-09
[I 2024-06-20 21:54:24,010] Trial 54 finished with value: 1.0921462774276733 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.3535234245987387, 'bidirectional': False, 'fc_dropout': 0.1631249801864975, 'learning_rate_model': 9.793357468944974e-05}. Best is trial 46 with value: 0.9782123565673828.
Epoch 3: reducing lr to 0.00015049603156149657
Epoch 6: reducing lr to 0.0003268590367188005
Epoch 9: reducing lr to 0.0005715858542932867
Epoch 17: reducing lr to 0.0012842168344270046
Epoch 20: reducing lr to 0.0014666357521782429
Epoch 23: reducing lr to 0.001557340200745365
Epoch 26: reducing lr to 0.0015622017244171648
Epoch 31: reducing lr to 0.0015323037729186796
Epoch 35: reducing lr to 0.0014846740978739356
Epoch 38: reducing lr to 0.0014359485318378496
Epoch 41: reducing lr to 0.0013769150458476011
Epoch 44: reducing lr to 0.0013085046335469534
Epoch 47: reducing lr to 0.0012317961983327585
Epoch 50: reducing lr to 0.0011479994537468006
Epoch 53: reducing lr to 0.0010584359924906716
Epoch 56: reducing lr to 0.0009645180075141729
Epoch 59: reducing lr to 0.0008677270580158679
Epoch 62: reducing lr to 0.0007695892401125403
Epoch 65: reducing lr to 0.0006716525349574679
Epoch 68: reducing lr to 0.0005754610317831859
Epoch 71: reducing lr to 0.00048253229775732756
Epoch 74: reducing lr to 0.00039433145449569476
Epoch 77: reducing lr to 0.0003122497418484195
Epoch 80: reducing lr to 0.00023758139454338155
Epoch 83: reducing lr to 0.00017150419701520477
Epoch 86: reducing lr to 0.0001150599541227565
Epoch 89: reducing lr to 6.91391595893787e-05
Epoch 92: reducing lr to 3.446578435170634e-05
Epoch 95: reducing lr to 1.1586750407141869e-05
Epoch 98: reducing lr to 8.628128952897079e-07
[I 2024-06-20 21:54:41,552] Trial 55 finished with value: 1.133587121963501 and parameters: {'hidden_size': 64, 'n_layers': 6, 'rnn_dropout': 0.36558026194855425, 'bidirectional': False, 'fc_dropout': 0.16956813171494078, 'learning_rate_model': 0.015646491878786785}. Best is trial 46 with value: 0.9782123565673828.
Epoch 40: reducing lr to 9.898238598419478e-05
Epoch 45: reducing lr to 9.091724996244546e-05
Epoch 49: reducing lr to 8.332829238236285e-05
Epoch 55: reducing lr to 7.05509865463511e-05
Epoch 58: reducing lr to 6.375229918074272e-05
Epoch 61: reducing lr to 5.6821937113830596e-05
Epoch 72: reducing lr to 3.204810386833038e-05
Epoch 75: reducing lr to 2.5935628789456112e-05
Epoch 78: reducing lr to 2.0287894319817438e-05
Epoch 81: reducing lr to 1.5193951891806567e-05
Epoch 84: reducing lr to 1.0734151028603299e-05
Epoch 87: reducing lr to 6.978807215430954e-06
Epoch 90: reducing lr to 3.987166708829641e-06
Epoch 93: reducing lr to 1.8063944763075084e-06
Epoch 96: reducing lr to 4.7088902383975085e-07
Epoch 99: reducing lr to 1.708117889904874e-09
[I 2024-06-20 21:54:49,332] Trial 56 finished with value: 0.9808386564254761 and parameters: {'hidden_size': 71, 'n_layers': 1, 'rnn_dropout': 0.47474595957169685, 'bidirectional': True, 'fc_dropout': 0.5998866485809148, 'learning_rate_model': 0.001108066724703625}. Best is trial 46 with value: 0.9782123565673828.
Epoch 4: reducing lr to 0.0007881813926232419
Epoch 9: reducing lr to 0.002253513991268598
Epoch 16: reducing lr to 0.004759256652359398
Epoch 19: reducing lr to 0.0055786974174293815
Epoch 22: reducing lr to 0.006065144180599323
Epoch 25: reducing lr to 0.006166592313145071
Epoch 28: reducing lr to 0.0061278981659724585
Epoch 31: reducing lr to 0.0060412061726322925
Epoch 34: reducing lr to 0.005907883632804778
Epoch 37: reducing lr to 0.005730032961198091
Epoch 40: reducing lr to 0.005510458940452332
Epoch 43: reducing lr to 0.005252624545978954
Epoch 46: reducing lr to 0.004960595675082186
Epoch 49: reducing lr to 0.004638978230170542
Epoch 52: reducing lr to 0.004292844102981657
Epoch 55: reducing lr to 0.003927651477649266
Epoch 58: reducing lr to 0.0035491610300342814
Epoch 61: reducing lr to 0.0031633401061146905
Epoch 64: reducing lr to 0.0027762753305618252
Epoch 67: reducing lr to 0.0023940683762288134
Epoch 70: reducing lr to 0.0020227494274324873
Epoch 73: reducing lr to 0.0016681732453294345
Epoch 76: reducing lr to 0.0013359312216876446
Epoch 79: reducing lr to 0.00103126399056555
Epoch 82: reducing lr to 0.0007589754345607179
Epoch 85: reducing lr to 0.0005233605090935338
Epoch 88: reducing lr to 0.0003281340458008798
Epoch 91: reducing lr to 0.00017637603887319249
Epoch 94: reducing lr to 7.047903177259673e-05
Epoch 97: reducing lr to 1.2113395561770546e-05
[I 2024-06-20 21:55:34,890] Trial 57 finished with value: 1.1072418689727783 and parameters: {'hidden_size': 116, 'n_layers': 7, 'rnn_dropout': 0.6431687131989174, 'bidirectional': False, 'fc_dropout': 0.3720011852508049, 'learning_rate_model': 0.061687300513606574}. Best is trial 46 with value: 0.9782123565673828.
Epoch 15: reducing lr to 0.0011699110803861962
Epoch 18: reducing lr to 0.001409325080533351
Epoch 24: reducing lr to 0.0016286682467735896
Epoch 27: reducing lr to 0.0016227892403850338
Epoch 30: reducing lr to 0.0016040937561465976
Epoch 33: reducing lr to 0.0015729440217621443
Epoch 36: reducing lr to 0.001529831285978432
Epoch 39: reducing lr to 0.0014754354512305024
Epoch 42: reducing lr to 0.0014106143795421509
Epoch 45: reducing lr to 0.0013363903382708187
Epoch 48: reducing lr to 0.0012539339173628867
Epoch 51: reducing lr to 0.0011645454773238448
Epoch 54: reducing lr to 0.0010696347990678356
Epoch 57: reducing lr to 0.000970698389165392
Epoch 60: reducing lr to 0.0008692969726278137
Epoch 63: reducing lr to 0.0007670293378752872
Epoch 66: reducing lr to 0.0006655086079073797
Epoch 69: reducing lr to 0.0005663353795925454
Epoch 72: reducing lr to 0.00047107426134454527
Epoch 75: reducing lr to 0.00038122714606440985
Epoch 78: reducing lr to 0.00029821124114579636
Epoch 81: reducing lr to 0.00022333551132209997
Epoch 84: reducing lr to 0.0001577810121851533
Epoch 87: reducing lr to 0.00010258130925879326
Epoch 90: reducing lr to 5.860726175677296e-05
Epoch 93: reducing lr to 2.6552146333510067e-05
Epoch 96: reducing lr to 6.921585750967879e-06
Epoch 99: reducing lr to 2.5107581296673896e-08
[I 2024-06-20 21:55:46,460] Trial 58 finished with value: 1.257666826248169 and parameters: {'hidden_size': 56, 'n_layers': 2, 'rnn_dropout': 0.0651791719018096, 'bidirectional': True, 'fc_dropout': 0.5070294084895343, 'learning_rate_model': 0.01628744452416877}. Best is trial 46 with value: 0.9782123565673828.
Epoch 10: reducing lr to 0.00013577511860813062
Epoch 25: reducing lr to 0.0003205044812456016
Epoch 30: reducing lr to 0.00031576314091062545
Epoch 33: reducing lr to 0.00030963136841910045
Epoch 36: reducing lr to 0.0003011446993499449
Epoch 39: reducing lr to 0.0002904369713467372
Epoch 42: reducing lr to 0.00027767705309689883
Epoch 45: reducing lr to 0.00026306617619951766
Epoch 48: reducing lr to 0.00024683476930427277
Epoch 51: reducing lr to 0.00022923880617576253
Epoch 54: reducing lr to 0.00021055579980083073
Epoch 57: reducing lr to 0.00019108033496499492
Epoch 60: reducing lr to 0.00017111963774514604
Epoch 63: reducing lr to 0.00015098842693579036
Epoch 66: reducing lr to 0.00013100424306912342
Epoch 69: reducing lr to 0.00011148216092963232
Epoch 72: reducing lr to 9.273017103540947e-05
Epoch 75: reducing lr to 7.504391846201446e-05
Epoch 78: reducing lr to 5.8702378086213875e-05
Epoch 81: reducing lr to 4.396321740030629e-05
Epoch 84: reducing lr to 3.1058925198564543e-05
Epoch 87: reducing lr to 2.019295710500883e-05
Epoch 90: reducing lr to 1.1536740281905623e-05
Epoch 93: reducing lr to 5.2267450652811884e-06
Epoch 96: reducing lr to 1.3625024400431945e-06
Epoch 99: reducing lr to 4.942384882735893e-09
[I 2024-06-20 21:57:55,568] Trial 59 finished with value: 1.2261028289794922 and parameters: {'hidden_size': 144, 'n_layers': 6, 'rnn_dropout': 0.2739632958748159, 'bidirectional': True, 'fc_dropout': 0.6592078963298551, 'learning_rate_model': 0.003206155887492329}. Best is trial 46 with value: 0.9782123565673828.
[I 2024-06-20 21:57:59,409] Trial 60 finished with value: 1.1097674369812012 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.014666114330405834, 'bidirectional': False, 'fc_dropout': 0.09067353821407238, 'learning_rate_model': 1.2916159380130333e-05}. Best is trial 46 with value: 0.9782123565673828.
Epoch 36: reducing lr to 7.39275468237201e-05
Epoch 40: reducing lr to 7.030848038401998e-05
Epoch 43: reducing lr to 6.701874632338127e-05
Epoch 49: reducing lr to 5.9189173428641516e-05
Epoch 52: reducing lr to 5.4772814509233264e-05
Epoch 55: reducing lr to 5.011328636247903e-05
Epoch 61: reducing lr to 4.036136340043061e-05
Epoch 64: reducing lr to 3.5422766366429336e-05
Epoch 67: reducing lr to 3.054615074479969e-05
Epoch 72: reducing lr to 2.276418637282934e-05
Epoch 75: reducing lr to 1.8422415562723146e-05
Epoch 78: reducing lr to 1.4410756071749173e-05
Epoch 81: reducing lr to 1.0792462294366217e-05
Epoch 84: reducing lr to 7.62460754536844e-06
Epoch 87: reducing lr to 4.957137831455523e-06
Epoch 90: reducing lr to 2.8321365417512643e-06
Epoch 93: reducing lr to 1.2831055681315719e-06
Epoch 96: reducing lr to 3.344786182561286e-07
Epoch 99: reducing lr to 1.213298426398085e-09
[I 2024-06-20 21:59:10,207] Trial 61 finished with value: 0.9777320623397827 and parameters: {'hidden_size': 155, 'n_layers': 3, 'rnn_dropout': 0.5975507964860202, 'bidirectional': True, 'fc_dropout': 0.3824580053742648, 'learning_rate_model': 0.000787074253700552}. Best is trial 61 with value: 0.9777320623397827.
Epoch 10: reducing lr to 0.0001261507462649048
Epoch 13: reducing lr to 0.0001797315927696788
Epoch 16: reducing lr to 0.00022982518992760258
Epoch 22: reducing lr to 0.00029288668694794944
Epoch 27: reducing lr to 0.0002967996863682021
Epoch 30: reducing lr to 0.00029338037983080297
Epoch 35: reducing lr to 0.000282662646190236
Epoch 38: reducing lr to 0.00027338593189138753
Epoch 41: reducing lr to 0.0002621467236451245
Epoch 44: reducing lr to 0.0002491222705374984
Epoch 47: reducing lr to 0.0002345179817485943
Epoch 50: reducing lr to 0.00021856417100944757
Epoch 53: reducing lr to 0.00020151245238859522
Epoch 56: reducing lr to 0.00018363168906395207
Epoch 59: reducing lr to 0.0001652039506453759
Epoch 62: reducing lr to 0.00014651978599293546
Epoch 65: reducing lr to 0.00012787391059312347
Epoch 68: reducing lr to 0.0001095602989613216
Epoch 71: reducing lr to 9.1867876156564e-05
Epoch 74: reducing lr to 7.50755822866535e-05
Epoch 77: reducing lr to 5.944829133173618e-05
Epoch 80: reducing lr to 4.5232408757831575e-05
Epoch 83: reducing lr to 3.265216940907768e-05
Epoch 86: reducing lr to 2.190591938624033e-05
Epoch 89: reducing lr to 1.3163197117056595e-05
Epoch 92: reducing lr to 6.561837255614688e-06
Epoch 95: reducing lr to 2.2059666397618997e-06
Epoch 98: reducing lr to 1.642683579506851e-07
[I 2024-06-20 21:59:42,113] Trial 62 finished with value: 1.093623161315918 and parameters: {'hidden_size': 164, 'n_layers': 3, 'rnn_dropout': 0.7237489282748985, 'bidirectional': False, 'fc_dropout': 0.10051310491019105, 'learning_rate_model': 0.0029788886358192816}. Best is trial 61 with value: 0.9777320623397827.
Epoch 14: reducing lr to 0.0030333030727167634
Epoch 17: reducing lr to 0.0037611327245866454
Epoch 20: reducing lr to 0.004295389668386941
Epoch 23: reducing lr to 0.004561039098160695
Epoch 26: reducing lr to 0.004575277220013004
Epoch 29: reducing lr to 0.004534569811652277
Epoch 32: reducing lr to 0.004458483841597088
Epoch 35: reducing lr to 0.00434821922993349
Epoch 38: reducing lr to 0.004205514885908831
Epoch 41: reducing lr to 0.004032621360413646
Epoch 44: reducing lr to 0.0038322652885192645
Epoch 47: reducing lr to 0.0036076064939904798
Epoch 50: reducing lr to 0.003362187909038899
Epoch 53: reducing lr to 0.003099880130455715
Epoch 56: reducing lr to 0.0028248190992865093
Epoch 59: reducing lr to 0.002541343912041889
Epoch 62: reducing lr to 0.002253924102130722
Epoch 65: reducing lr to 0.001967093298467189
Epoch 68: reducing lr to 0.0016853737315551131
Epoch 71: reducing lr to 0.001413209955758627
Epoch 74: reducing lr to 0.0011548929262396369
Epoch 77: reducing lr to 0.000914497217935805
Epoch 80: reducing lr to 0.0006958133033419863
Epoch 83: reducing lr to 0.0005022906027280442
Epoch 86: reducing lr to 0.0003369802880162566
Epoch 89: reducing lr to 0.0002024903806825247
Epoch 92: reducing lr to 0.00010094120083824279
Epoch 95: reducing lr to 3.39345389031305e-05
Epoch 98: reducing lr to 2.52695162426958e-06
[I 2024-06-20 21:59:52,662] Trial 63 finished with value: 1.2953439950942993 and parameters: {'hidden_size': 17, 'n_layers': 4, 'rnn_dropout': 0.0024143652956770904, 'bidirectional': True, 'fc_dropout': 0.6367735535538155, 'learning_rate_model': 0.04582445195599803}. Best is trial 61 with value: 0.9777320623397827.
Epoch 10: reducing lr to 0.0002960229562480228
Epoch 13: reducing lr to 0.0004217547576858659
Epoch 16: reducing lr to 0.0005393034457344245
Epoch 19: reducing lr to 0.0006321598013500458
Epoch 22: reducing lr to 0.0006872823624361033
Epoch 27: reducing lr to 0.0006964645328986354
Epoch 30: reducing lr to 0.000688440852821519
Epoch 33: reducing lr to 0.000675072089541552
Epoch 36: reducing lr to 0.0006565690759385896
Epoch 39: reducing lr to 0.0006332236107995926
Epoch 42: reducing lr to 0.0006054038691523624
Epoch 45: reducing lr to 0.0005735485850850228
Epoch 48: reducing lr to 0.0005381601493948093
Epoch 51: reducing lr to 0.0004997966474753872
Epoch 54: reducing lr to 0.0004590631254913615
Epoch 57: reducing lr to 0.00041660185030258556
Epoch 60: reducing lr to 0.0003730826498749638
Epoch 63: reducing lr to 0.00032919168813080847
Epoch 66: reducing lr to 0.000285621281070523
Epoch 69: reducing lr to 0.00024305836876161916
Epoch 72: reducing lr to 0.00020217444583873036
Epoch 75: reducing lr to 0.00016361409085324726
Epoch 78: reducing lr to 0.00012798553724724634
Epoch 81: reducing lr to 9.58505631549039e-05
Epoch 84: reducing lr to 6.771605099238501e-05
Epoch 87: reducing lr to 4.402558376595134e-05
Epoch 90: reducing lr to 2.5152914604125754e-05
Epoch 93: reducing lr to 1.1395582207111843e-05
Epoch 96: reducing lr to 2.970588457822023e-06
Epoch 99: reducing lr to 1.0775607481739585e-08
[I 2024-06-20 21:59:58,254] Trial 64 finished with value: 1.1043106317520142 and parameters: {'hidden_size': 42, 'n_layers': 2, 'rnn_dropout': 0.1501053383219472, 'bidirectional': False, 'fc_dropout': 0.791964142687259, 'learning_rate_model': 0.006990203755569746}. Best is trial 61 with value: 0.9777320623397827.
Epoch 5: reducing lr to 0.0002946789284068248
Epoch 15: reducing lr to 0.0012786222480370751
Epoch 22: reducing lr to 0.0017502001467667415
Epoch 25: reducing lr to 0.001779474724779038
Epoch 28: reducing lr to 0.0017683088728150987
Epoch 31: reducing lr to 0.0017432924288610055
Epoch 34: reducing lr to 0.0017048199504127895
Epoch 37: reducing lr to 0.0016534981248667014
Epoch 40: reducing lr to 0.0015901363197896427
Epoch 43: reducing lr to 0.0015157338354278896
Epoch 46: reducing lr to 0.0014314639553583372
Epoch 49: reducing lr to 0.0013386557907828524
Epoch 52: reducing lr to 0.001238772922021915
Epoch 55: reducing lr to 0.001133390400613871
Epoch 58: reducing lr to 0.0010241705672116423
Epoch 61: reducing lr to 0.0009128354006331253
Epoch 64: reducing lr to 0.0008011412995847445
Epoch 67: reducing lr to 0.0006908490051808203
Epoch 70: reducing lr to 0.0005836986293069226
Epoch 73: reducing lr to 0.0004813796625226189
Epoch 76: reducing lr to 0.00038550559568675347
Epoch 79: reducing lr to 0.00029758870257635473
Epoch 82: reducing lr to 0.0002190152249322553
Epoch 85: reducing lr to 0.00015102454493289667
Epoch 88: reducing lr to 9.468864020692018e-05
Epoch 91: reducing lr to 5.089629527842421e-05
Epoch 94: reducing lr to 2.0337919112779766e-05
Epoch 97: reducing lr to 3.495525589955473e-06
[I 2024-06-20 22:02:53,725] Trial 65 finished with value: 1.2172961235046387 and parameters: {'hidden_size': 151, 'n_layers': 7, 'rnn_dropout': 0.3206989858968967, 'bidirectional': True, 'fc_dropout': 0.0955656402669832, 'learning_rate_model': 0.017800916053720233}. Best is trial 61 with value: 0.9777320623397827.
Epoch 17: reducing lr to 0.00011190120404160318
Epoch 20: reducing lr to 0.00012779641424996154
Epoch 23: reducing lr to 0.00013570001489939494
Epoch 26: reducing lr to 0.00013612362743722147
Epoch 29: reducing lr to 0.00013491250080528927
Epoch 32: reducing lr to 0.00013264879136366494
Epoch 35: reducing lr to 0.00012936819912939528
Epoch 38: reducing lr to 0.00012512246012264926
Epoch 41: reducing lr to 0.00011997853272348122
Epoch 44: reducing lr to 0.00011401753976636847
Epoch 47: reducing lr to 0.00010733349231386357
Epoch 50: reducing lr to 0.00010003179966931947
Epoch 53: reducing lr to 9.222761981119909e-05
Epoch 56: reducing lr to 8.404400523903804e-05
Epoch 59: reducing lr to 7.561005273286102e-05
Epoch 62: reducing lr to 6.705873983070784e-05
Epoch 65: reducing lr to 5.852495103980642e-05
Epoch 68: reducing lr to 5.014323174193058e-05
Epoch 71: reducing lr to 4.2045816298692576e-05
Epoch 74: reducing lr to 3.436036918892534e-05
Epoch 77: reducing lr to 2.7208117147995522e-05
Epoch 80: reducing lr to 2.0701834296658807e-05
Epoch 83: reducing lr to 1.4944147771394603e-05
Epoch 86: reducing lr to 1.0025836025621678e-05
Epoch 89: reducing lr to 6.02449290265537e-06
Epoch 92: reducing lr to 3.003202156990118e-06
Epoch 95: reducing lr to 1.0096202500469754e-06
Epoch 98: reducing lr to 7.518185345127462e-08
[I 2024-06-20 22:02:59,558] Trial 66 finished with value: 1.0922670364379883 and parameters: {'hidden_size': 43, 'n_layers': 2, 'rnn_dropout': 0.6552713429068246, 'bidirectional': False, 'fc_dropout': 0.21331096172130817, 'learning_rate_model': 0.0013633688901489999}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:03:24,971] Trial 67 finished with value: 1.1068003177642822 and parameters: {'hidden_size': 74, 'n_layers': 7, 'rnn_dropout': 0.29324531508098756, 'bidirectional': False, 'fc_dropout': 0.7511091818957707, 'learning_rate_model': 1.3298296907035845e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 10: reducing lr to 7.259417265851561e-05
Epoch 27: reducing lr to 0.00017079508695067177
Epoch 39: reducing lr to 0.00015528641669033174
Epoch 42: reducing lr to 0.0001484641379250246
Epoch 45: reducing lr to 0.00014065221677883832
Epoch 48: reducing lr to 0.00013197385533291823
Epoch 51: reducing lr to 0.00012256591374142068
Epoch 54: reducing lr to 0.00011257676842182503
Epoch 57: reducing lr to 0.00010216392348093432
Epoch 60: reducing lr to 9.149164187870495e-05
Epoch 63: reducing lr to 8.072819266723928e-05
Epoch 66: reducing lr to 7.004335358237423e-05
Epoch 69: reducing lr to 5.96055840115136e-05
Epoch 72: reducing lr to 4.957955563439356e-05
Epoch 75: reducing lr to 4.012333945754931e-05
Epoch 78: reducing lr to 3.138609351950059e-05
Epoch 81: reducing lr to 2.350558355093625e-05
Epoch 84: reducing lr to 1.6606113119738403e-05
Epoch 87: reducing lr to 1.079646278047315e-05
Epoch 90: reducing lr to 6.168288597546896e-06
Epoch 93: reducing lr to 2.794556451879578e-06
Epoch 96: reducing lr to 7.28482054695263e-07
Epoch 99: reducing lr to 2.6425190800729442e-09
[I 2024-06-20 22:06:09,266] Trial 68 finished with value: 1.1928761005401611 and parameters: {'hidden_size': 148, 'n_layers': 7, 'rnn_dropout': 0.021829778813975055, 'bidirectional': True, 'fc_dropout': 0.6875914566924687, 'learning_rate_model': 0.0017142186024413223}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:07:37,566] Trial 69 finished with value: 1.0222620964050293 and parameters: {'hidden_size': 143, 'n_layers': 4, 'rnn_dropout': 0.5026472300170858, 'bidirectional': True, 'fc_dropout': 0.4614047479896726, 'learning_rate_model': 0.00017610709636270444}. Best is trial 61 with value: 0.9777320623397827.
Epoch 7: reducing lr to 0.0014377825760740225
Epoch 13: reducing lr to 0.0033732123415203966
Epoch 17: reducing lr to 0.004588755360572483
Epoch 20: reducing lr to 0.005240573468123078
Epoch 28: reducing lr to 0.005553788018061583
Epoch 31: reducing lr to 0.005475218018881762
Epoch 34: reducing lr to 0.0053543861929967965
Epoch 37: reducing lr to 0.005193197984214497
Epoch 40: reducing lr to 0.004994195400870836
Epoch 43: reducing lr to 0.004760516979349097
Epoch 49: reducing lr to 0.004204361921977389
Epoch 52: reducing lr to 0.003890656387688575
Epoch 64: reducing lr to 0.002516171817497453
Epoch 67: reducing lr to 0.00216977304488377
Epoch 70: reducing lr to 0.0018332422030111733
Epoch 73: reducing lr to 0.0015118855325317363
Epoch 76: reducing lr to 0.001210770579244081
Epoch 79: reducing lr to 0.0009346469930040721
Epoch 82: reducing lr to 0.000687868590550815
Epoch 85: reducing lr to 0.0004743279417844269
Epoch 88: reducing lr to 0.00029739184342300465
Epoch 91: reducing lr to 0.00015985173134998656
Epoch 94: reducing lr to 6.387599656220988e-05
Epoch 97: reducing lr to 1.0978516500580182e-05
[I 2024-06-20 22:08:21,957] Trial 70 finished with value: 1.1072511672973633 and parameters: {'hidden_size': 111, 'n_layers': 7, 'rnn_dropout': 0.3411798308011168, 'bidirectional': False, 'fc_dropout': 0.7454888933092325, 'learning_rate_model': 0.05590794448273345}. Best is trial 61 with value: 0.9777320623397827.
Epoch 28: reducing lr to 4.96797360071721e-06
Epoch 31: reducing lr to 4.89769117717776e-06
Epoch 34: reducing lr to 4.7896047109369245e-06
Epoch 37: reducing lr to 4.645418659295573e-06
Epoch 40: reducing lr to 4.467406899158048e-06
Epoch 43: reducing lr to 4.25837691360551e-06
Epoch 46: reducing lr to 4.02162498301396e-06
Epoch 49: reducing lr to 3.760885177525105e-06
Epoch 52: reducing lr to 3.480269351411946e-06
Epoch 55: reducing lr to 3.184202531649445e-06
Epoch 58: reducing lr to 2.8773549795285316e-06
Epoch 61: reducing lr to 2.564564506723298e-06
Epoch 64: reducing lr to 2.250765625829361e-06
Epoch 67: reducing lr to 1.9409050492158777e-06
Epoch 70: reducing lr to 1.6398715324858444e-06
Epoch 73: reducing lr to 1.3524116132083634e-06
Epoch 76: reducing lr to 1.0830583116690679e-06
Epoch 79: reducing lr to 8.360602839239412e-07
Epoch 82: reducing lr to 6.153121054504589e-07
Epoch 85: reducing lr to 4.2429575727487397e-07
Epoch 88: reducing lr to 2.660229058778115e-07
Epoch 91: reducing lr to 1.429905460548793e-07
Epoch 94: reducing lr to 5.713834658588912e-08
Epoch 97: reducing lr to 9.820500885619586e-09
[I 2024-06-20 22:09:42,746] Trial 71 finished with value: 1.1069471836090088 and parameters: {'hidden_size': 172, 'n_layers': 7, 'rnn_dropout': 0.18232023196105357, 'bidirectional': False, 'fc_dropout': 0.25150129289124035, 'learning_rate_model': 5.001076586958479e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 12: reducing lr to 0.00010765501552503496
Epoch 15: reducing lr to 0.0001422906162010533
Epoch 18: reducing lr to 0.00017140938101936084
Epoch 21: reducing lr to 0.00019092166567453013
Epoch 24: reducing lr to 0.00019808702755768647
Epoch 27: reducing lr to 0.00019737199249587528
Epoch 30: reducing lr to 0.00019509815133216394
Epoch 33: reducing lr to 0.00019130956006709114
Epoch 36: reducing lr to 0.0001860659669055044
Epoch 39: reducing lr to 0.00017945006508628364
Epoch 42: reducing lr to 0.00017156619221083102
Epoch 45: reducing lr to 0.00016253868170469592
Epoch 48: reducing lr to 0.000152509906751261
Epoch 51: reducing lr to 0.00014163802389824325
Epoch 54: reducing lr to 0.00013009449796749526
Epoch 57: reducing lr to 0.0001180613418022493
Epoch 60: reducing lr to 0.00010572837882353407
Epoch 63: reducing lr to 9.329006192037456e-05
Epoch 66: reducing lr to 8.094258742723122e-05
Epoch 69: reducing lr to 6.888062818592952e-05
Epoch 72: reducing lr to 5.729447993692346e-05
Epoch 75: reducing lr to 4.636681063672559e-05
Epoch 78: reducing lr to 3.626998835390884e-05
Epoch 81: reducing lr to 2.716321612674065e-05
Epoch 84: reducing lr to 1.919014002435141e-05
Epoch 87: reducing lr to 1.2476467613526765e-05
Epoch 90: reducing lr to 7.128117280908895e-06
Epoch 93: reducing lr to 3.2294089068790307e-06
Epoch 96: reducing lr to 8.418389381084721e-07
Epoch 99: reducing lr to 3.0537134607013655e-09
[I 2024-06-20 22:10:04,200] Trial 72 finished with value: 1.0926707983016968 and parameters: {'hidden_size': 128, 'n_layers': 3, 'rnn_dropout': 0.6530812094617713, 'bidirectional': False, 'fc_dropout': 0.1922844998793111, 'learning_rate_model': 0.0019809629608084447}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:10:11,651] Trial 73 finished with value: 1.0729713439941406 and parameters: {'hidden_size': 80, 'n_layers': 1, 'rnn_dropout': 0.1830509367683007, 'bidirectional': True, 'fc_dropout': 0.41521278725894306, 'learning_rate_model': 4.9989708054979095e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 3: reducing lr to 2.320276831468045e-05
Epoch 6: reducing lr to 5.03935846138702e-05
Epoch 9: reducing lr to 8.812441106592555e-05
Epoch 12: reducing lr to 0.00013109606226230553
Epoch 15: reducing lr to 0.00017327329702067817
Epoch 18: reducing lr to 0.00020873244759536327
Epoch 21: reducing lr to 0.00023249338127373005
Epoch 24: reducing lr to 0.0002412189452707718
Epoch 27: reducing lr to 0.0002403482168562547
Epoch 30: reducing lr to 0.00023757926437114563
Epoch 33: reducing lr to 0.00023296573666925323
Epoch 36: reducing lr to 0.00022658039166477718
Epoch 39: reducing lr to 0.00021852392840959167
Epoch 42: reducing lr to 0.00020892340321057682
Epoch 45: reducing lr to 0.0001979302221347662
Epoch 48: reducing lr to 0.00018571775902472733
Epoch 51: reducing lr to 0.0001724786077928345
Epoch 54: reducing lr to 0.00015842156839932889
Epoch 57: reducing lr to 0.0001437682855758799
Epoch 60: reducing lr to 0.00012874991532484055
Epoch 63: reducing lr to 0.00011360325114739971
Epoch 66: reducing lr to 9.856720961193663e-05
Epoch 69: reducing lr to 8.38788520654612e-05
Epoch 72: reducing lr to 6.976990967365237e-05
Epoch 75: reducing lr to 5.646282492730835e-05
Epoch 78: reducing lr to 4.416749770837568e-05
Epoch 81: reducing lr to 3.307779628500033e-05
Epoch 84: reducing lr to 2.336864454652093e-05
Epoch 87: reducing lr to 1.5193121909830428e-05
Epoch 90: reducing lr to 8.68020967080478e-06
[I 2024-06-20 22:10:59,316] Trial 74 finished with value: 1.1071969270706177 and parameters: {'hidden_size': 121, 'n_layers': 7, 'rnn_dropout': 0.6534680870332976, 'bidirectional': False, 'fc_dropout': 0.7212500677354571, 'learning_rate_model': 0.002412302319431398}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:11:04,139] Trial 75 finished with value: 1.096245527267456 and parameters: {'hidden_size': 21, 'n_layers': 1, 'rnn_dropout': 0.05229737203378111, 'bidirectional': True, 'fc_dropout': 0.7802800274162293, 'learning_rate_model': 4.6982180906249337e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 21: reducing lr to 1.0877379653563202e-06
Epoch 24: reducing lr to 1.1285611800935775e-06
Epoch 27: reducing lr to 1.1244874109875674e-06
Epoch 30: reducing lr to 1.1115326562077968e-06
Epoch 33: reducing lr to 1.0899479159967924e-06
Epoch 36: reducing lr to 1.0600735937893596e-06
Epoch 39: reducing lr to 1.0223808177578314e-06
Epoch 42: reducing lr to 9.774640304965482e-07
Epoch 45: reducing lr to 9.260315968045232e-07
Epoch 48: reducing lr to 8.68894659389248e-07
Epoch 51: reducing lr to 8.06954283516488e-07
Epoch 54: reducing lr to 7.411873556794197e-07
Epoch 57: reducing lr to 6.72630857611188e-07
Epoch 60: reducing lr to 6.023662702481617e-07
Epoch 63: reducing lr to 5.315014499937427e-07
Epoch 66: reducing lr to 4.611541861826428e-07
Epoch 69: reducing lr to 3.924335883553073e-07
Epoch 72: reducing lr to 3.2642382839346655e-07
Epoch 75: reducing lr to 2.641656204070188e-07
Epoch 78: reducing lr to 2.0664099695648815e-07
Epoch 81: reducing lr to 1.5475698547803158e-07
Epoch 84: reducing lr to 1.0933198069084076e-07
Epoch 87: reducing lr to 7.108217628850325e-08
Epoch 90: reducing lr to 4.0611021072771964e-08
Epoch 93: reducing lr to 1.8398910680260133e-08
Epoch 96: reducing lr to 4.796208803545756e-09
Epoch 99: reducing lr to 1.7397921052089574e-11
[I 2024-06-20 22:12:29,153] Trial 76 finished with value: 1.1075935363769531 and parameters: {'hidden_size': 195, 'n_layers': 6, 'rnn_dropout': 0.6593902716995983, 'bidirectional': False, 'fc_dropout': 0.3815058668169547, 'learning_rate_model': 1.1286139856486e-05}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:13:06,432] Trial 77 finished with value: 1.1012319326400757 and parameters: {'hidden_size': 81, 'n_layers': 4, 'rnn_dropout': 0.5840729688720835, 'bidirectional': True, 'fc_dropout': 0.11013019275648253, 'learning_rate_model': 1.1054902973283413e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 7: reducing lr to 0.00026902058903565106
Epoch 10: reducing lr to 0.00044299761143835545
Epoch 13: reducing lr to 0.0006311549368862465
Epoch 16: reducing lr to 0.0008070662536747789
Epoch 19: reducing lr to 0.0009460255569192395
Epoch 22: reducing lr to 0.0010285163313071177
Epoch 25: reducing lr to 0.0010457197246638112
Epoch 28: reducing lr to 0.001039158040207845
Epoch 31: reducing lr to 0.001024456966616035
Epoch 34: reducing lr to 0.0010018483681291988
Epoch 37: reducing lr to 0.0009716887684833182
Epoch 40: reducing lr to 0.0009344537977154073
Epoch 43: reducing lr to 0.0008907307010185787
Epoch 46: reducing lr to 0.000841208966005047
Epoch 49: reducing lr to 0.0007866696533893646
Epoch 52: reducing lr to 0.0007279728455252975
Epoch 55: reducing lr to 0.0006660441315420844
Epoch 58: reducing lr to 0.0006018603965764819
Epoch 61: reducing lr to 0.0005364335725150483
Epoch 64: reducing lr to 0.00047079581831239265
Epoch 67: reducing lr to 0.00040598184476695054
Epoch 70: reducing lr to 0.0003430142397786921
Epoch 73: reducing lr to 0.0002828858433009879
Epoch 76: reducing lr to 0.00022654483357608147
Epoch 79: reducing lr to 0.0001748799079795013
Epoch 82: reducing lr to 0.00012870570035310828
Epoch 85: reducing lr to 8.87505415758669e-05
Epoch 88: reducing lr to 5.564438617034397e-05
Epoch 91: reducing lr to 2.9909534057344193e-05
Epoch 94: reducing lr to 1.1951708489420753e-05
Epoch 97: reducing lr to 2.0541680118201996e-06
[I 2024-06-20 22:13:36,101] Trial 78 finished with value: 1.0990912914276123 and parameters: {'hidden_size': 156, 'n_layers': 3, 'rnn_dropout': 0.7875067601548086, 'bidirectional': False, 'fc_dropout': 0.4191169085329054, 'learning_rate_model': 0.010460822384971712}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:13:42,128] Trial 79 finished with value: 1.0992084741592407 and parameters: {'hidden_size': 101, 'n_layers': 1, 'rnn_dropout': 0.25000110279363963, 'bidirectional': False, 'fc_dropout': 0.6161199650496738, 'learning_rate_model': 3.321567816135707e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 41: reducing lr to 5.514132287529281e-05
Epoch 47: reducing lr to 4.9329747752892694e-05
Epoch 50: reducing lr to 4.597393915522547e-05
Epoch 53: reducing lr to 4.238719082979566e-05
Epoch 58: reducing lr to 3.60509698814862e-05
Epoch 61: reducing lr to 3.213195398162493e-05
Epoch 64: reducing lr to 2.8200303530277065e-05
Epoch 67: reducing lr to 2.4317996899906755e-05
Epoch 70: reducing lr to 2.0546286310784186e-05
Epoch 73: reducing lr to 1.6944642104303632e-05
Epoch 76: reducing lr to 1.3569859420081926e-05
Epoch 79: reducing lr to 1.0475170540058814e-05
Epoch 82: reducing lr to 7.709371398082788e-06
Epoch 85: reducing lr to 5.316088447614913e-06
Epoch 88: reducing lr to 3.333055474843722e-06
Epoch 91: reducing lr to 1.7915578390005887e-06
Epoch 94: reducing lr to 7.158980475128444e-07
Epoch 97: reducing lr to 1.2304306704159557e-07
[I 2024-06-20 22:13:49,165] Trial 80 finished with value: 1.0936001539230347 and parameters: {'hidden_size': 20, 'n_layers': 4, 'rnn_dropout': 0.6479909685299331, 'bidirectional': False, 'fc_dropout': 0.5494465813255934, 'learning_rate_model': 0.0006265951288394312}. Best is trial 61 with value: 0.9777320623397827.
Epoch 3: reducing lr to 3.799486078329644e-05
Epoch 18: reducing lr to 0.00034180233064365573
Epoch 21: reducing lr to 0.000380711195092364
Epoch 27: reducing lr to 0.00039357359928423284
Epoch 30: reducing lr to 0.0003890394004868972
Epoch 34: reducing lr to 0.0003783144722266642
Epoch 37: reducing lr to 0.0003669257098295114
Epoch 40: reducing lr to 0.0003528651705677247
Epoch 50: reducing lr to 0.00028982876805350557
Epoch 53: reducing lr to 0.000267217200117867
Epoch 56: reducing lr to 0.00024350627081823526
Epoch 59: reducing lr to 0.00021907002081805876
Epoch 65: reducing lr to 0.00016956822246857236
Epoch 68: reducing lr to 0.00014528331120731145
Epoch 72: reducing lr to 0.00011424921237679895
Epoch 75: reducing lr to 9.245867318286262e-05
Epoch 78: reducing lr to 7.232490122803734e-05
Epoch 81: reducing lr to 5.416535853920738e-05
Epoch 84: reducing lr to 3.8266485455428714e-05
Epoch 87: reducing lr to 2.4878951683641277e-05
Epoch 90: reducing lr to 1.421396591730754e-05
Epoch 93: reducing lr to 6.439667913204572e-06
Epoch 96: reducing lr to 1.6786859001582076e-06
Epoch 99: reducing lr to 6.08931886797795e-09
[I 2024-06-20 22:14:53,521] Trial 81 finished with value: 1.1031112670898438 and parameters: {'hidden_size': 185, 'n_layers': 5, 'rnn_dropout': 0.004191870121172681, 'bidirectional': False, 'fc_dropout': 0.6643196560298716, 'learning_rate_model': 0.003950179114447677}. Best is trial 61 with value: 0.9777320623397827.
Epoch 10: reducing lr to 0.001122403533805753
Epoch 17: reducing lr to 0.0021753785836223236
Epoch 20: reducing lr to 0.002484384194112288
Epoch 23: reducing lr to 0.0026380315452157557
Epoch 26: reducing lr to 0.0026462666455476435
Epoch 29: reducing lr to 0.0026227221362662206
Epoch 35: reducing lr to 0.002514939961532899
Epoch 38: reducing lr to 0.0024324020676288137
Epoch 41: reducing lr to 0.002332403237449132
Epoch 44: reducing lr to 0.002216520512798506
Epoch 47: reducing lr to 0.0020865814848440916
Epoch 50: reducing lr to 0.0019446353285075748
Epoch 53: reducing lr to 0.0017929207346254576
Epoch 56: reducing lr to 0.0016338298648768171
Epoch 59: reducing lr to 0.0014698723827892774
Epoch 62: reducing lr to 0.0013036333944913391
Epoch 65: reducing lr to 0.001137735077919327
Epoch 68: reducing lr to 0.0009747930183526207
Epoch 71: reducing lr to 0.0008173778744426086
Epoch 74: reducing lr to 0.0006679714655362847
Epoch 77: reducing lr to 0.0005289304601443928
Epoch 80: reducing lr to 0.00040244720650107167
Epoch 83: reducing lr to 0.0002905167937271937
Epoch 86: reducing lr to 0.00019490397051436444
Epoch 89: reducing lr to 0.00011711717447426863
Epoch 92: reducing lr to 5.83827646052467e-05
Epoch 95: reducing lr to 1.9627190684445156e-05
Epoch 98: reducing lr to 1.4615481153723574e-06
[I 2024-06-20 22:15:35,949] Trial 82 finished with value: 1.1769990921020508 and parameters: {'hidden_size': 140, 'n_layers': 5, 'rnn_dropout': 0.21929582013348173, 'bidirectional': False, 'fc_dropout': 0.2819437041731271, 'learning_rate_model': 0.026504124871653878}. Best is trial 61 with value: 0.9777320623397827.
Epoch 24: reducing lr to 2.594891063817012e-05
Epoch 32: reducing lr to 2.5248138741008143e-05
[I 2024-06-20 22:16:10,475] Trial 83 finished with value: 1.0566580295562744 and parameters: {'hidden_size': 52, 'n_layers': 6, 'rnn_dropout': 0.23168219694950196, 'bidirectional': True, 'fc_dropout': 0.6461587272897676, 'learning_rate_model': 0.00025950124791777955}. Best is trial 61 with value: 0.9777320623397827.
Epoch 21: reducing lr to 6.925121646092627e-06
Epoch 24: reducing lr to 7.185024064730245e-06
Epoch 27: reducing lr to 7.159088271813452e-06
Epoch 30: reducing lr to 7.076611374249412e-06
Epoch 33: reducing lr to 6.93919137382537e-06
Epoch 36: reducing lr to 6.748995460866436e-06
Epoch 39: reducing lr to 6.509023089292781e-06
Epoch 42: reducing lr to 6.223058798587755e-06
Epoch 45: reducing lr to 5.8956124179190515e-06
Epoch 48: reducing lr to 5.531848115588807e-06
Epoch 65: reducing lr to 3.084439289004016e-06
Epoch 68: reducing lr to 2.6426977095161965e-06
Epoch 71: reducing lr to 2.215939790222535e-06
Epoch 74: reducing lr to 1.8108938295210054e-06
Epoch 87: reducing lr to 4.5254715137724725e-07
Epoch 98: reducing lr to 3.962307673654498e-09
[I 2024-06-20 22:16:28,636] Trial 84 finished with value: 1.1082875728607178 and parameters: {'hidden_size': 55, 'n_layers': 6, 'rnn_dropout': 0.2469135581045209, 'bidirectional': False, 'fc_dropout': 0.17711736072656087, 'learning_rate_model': 7.185360253135694e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 7: reducing lr to 0.00040674838628784076
Epoch 11: reducing lr to 0.0007642061018185676
Epoch 15: reducing lr to 0.0011360725130828177
Epoch 18: reducing lr to 0.0013685616905719324
Epoch 22: reducing lr to 0.0015550756153255624
Epoch 25: reducing lr to 0.0015810864589996232
Epoch 28: reducing lr to 0.001571165454167385
Epoch 31: reducing lr to 0.0015489380180384164
Epoch 34: reducing lr to 0.0015147547200844752
Epoch 37: reducing lr to 0.0014691546099552704
Epoch 40: reducing lr to 0.0014128568212707186
Epoch 43: reducing lr to 0.0013467492453089942
Epoch 46: reducing lr to 0.0012718743598025219
Epoch 49: reducing lr to 0.0011894130973571517
Epoch 52: reducing lr to 0.0011006658681412017
Epoch 55: reducing lr to 0.0010070321259512498
Epoch 58: reducing lr to 0.000909988882098544
Epoch 61: reducing lr to 0.0008110661371803106
Epoch 64: reducing lr to 0.0007118244743128263
Epoch 67: reducing lr to 0.0006138283348983157
Epoch 70: reducing lr to 0.000518623831000697
Epoch 73: reducing lr to 0.0004277120969767241
Epoch 76: reducing lr to 0.00034252674045965725
Epoch 79: reducing lr to 0.0002644114363878732
Epoch 82: reducing lr to 0.00019459787859484444
Epoch 85: reducing lr to 0.0001341872742809728
Epoch 88: reducing lr to 8.413208952481113e-05
Epoch 91: reducing lr to 4.522202094663364e-05
Epoch 94: reducing lr to 1.8070505900238952e-05
Epoch 97: reducing lr to 3.1058199930607267e-06
[I 2024-06-20 22:16:32,879] Trial 85 finished with value: 1.1832473278045654 and parameters: {'hidden_size': 52, 'n_layers': 1, 'rnn_dropout': 0.3016660793658673, 'bidirectional': False, 'fc_dropout': 0.5396512190107284, 'learning_rate_model': 0.015816345654373305}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:16:41,927] Trial 86 finished with value: 1.0448192358016968 and parameters: {'hidden_size': 30, 'n_layers': 2, 'rnn_dropout': 0.1661005294430515, 'bidirectional': True, 'fc_dropout': 0.44310768884113144, 'learning_rate_model': 0.00016522355094605626}. Best is trial 61 with value: 0.9777320623397827.
Epoch 12: reducing lr to 0.004358450464530885
Epoch 15: reducing lr to 0.005760684713622545
Epoch 18: reducing lr to 0.006939567958680504
Epoch 21: reducing lr to 0.007729529538311736
Epoch 24: reducing lr to 0.008019621687533639
Epoch 27: reducing lr to 0.00799067324623615
Epoch 30: reducing lr to 0.007898616001825249
Epoch 33: reducing lr to 0.007745233576690267
Epoch 36: reducing lr to 0.007532944897528692
Epoch 39: reducing lr to 0.00726509782866112
Epoch 42: reducing lr to 0.006945916513896197
Epoch 45: reducing lr to 0.006580434634885437
Epoch 48: reducing lr to 0.006174416219103319
Epoch 51: reducing lr to 0.005734264288977579
Epoch 54: reducing lr to 0.005266920657008171
Epoch 57: reducing lr to 0.00477975417598163
Epoch 60: reducing lr to 0.004280449912622683
Epoch 63: reducing lr to 0.0037768803592659133
Epoch 66: reducing lr to 0.0032769885922361726
Epoch 69: reducing lr to 0.002788656008733098
Epoch 72: reducing lr to 0.0023195867974963847
Epoch 75: reducing lr to 0.0018771763337998605
Epoch 78: reducing lr to 0.0014684029983987352
Epoch 81: reducing lr to 0.0010997121812519202
Epoch 84: reducing lr to 0.0007769194430527662
Epoch 87: reducing lr to 0.0005051140980350769
Epoch 90: reducing lr to 0.00028858428864359935
Epoch 93: reducing lr to 0.00013074373434160958
Epoch 96: reducing lr to 3.408214000030219e-05
Epoch 99: reducing lr to 1.2363064355452868e-07
[I 2024-06-20 22:18:36,374] Trial 87 finished with value: 1.107491135597229 and parameters: {'hidden_size': 170, 'n_layers': 4, 'rnn_dropout': 0.7387649720950202, 'bidirectional': True, 'fc_dropout': 0.10096808299948697, 'learning_rate_model': 0.08019996926893054}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:18:47,683] Trial 88 finished with value: 1.0615782737731934 and parameters: {'hidden_size': 45, 'n_layers': 2, 'rnn_dropout': 0.3449454041282968, 'bidirectional': True, 'fc_dropout': 0.5834551354888987, 'learning_rate_model': 5.6882708517701854e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 7: reducing lr to 6.291133523935825e-05
Epoch 10: reducing lr to 0.00010359642488084816
Epoch 13: reducing lr to 0.00014759762427389768
Epoch 22: reducing lr to 0.00024052187213612148
Epoch 25: reducing lr to 0.00024454494133910386
Epoch 28: reducing lr to 0.00024301047019686174
Epoch 31: reducing lr to 0.00023957257656787167
Epoch 34: reducing lr to 0.00023428548265511201
Epoch 37: reducing lr to 0.00022723256268787715
Epoch 42: reducing lr to 0.00021186761070202115
Epoch 50: reducing lr to 0.00017948737834715635
Epoch 53: reducing lr to 0.0001654843134466522
Epoch 56: reducing lr to 0.00015080042762417926
Epoch 59: reducing lr to 0.000135667359645373
Epoch 62: reducing lr to 0.00012032371153239761
Epoch 65: reducing lr to 0.00010501150698833569
Epoch 68: reducing lr to 8.997216122238203e-05
Epoch 71: reducing lr to 7.544294277285802e-05
Epoch 74: reducing lr to 6.165292042278592e-05
Epoch 77: reducing lr to 4.881961169147932e-05
Epoch 80: reducing lr to 3.71453675447986e-05
Epoch 83: reducing lr to 2.681433218223693e-05
Epoch 86: reducing lr to 1.7989389673344356e-05
Epoch 89: reducing lr to 1.0809766899558346e-05
Epoch 92: reducing lr to 5.388655243498573e-06
Epoch 95: reducing lr to 1.8115648464406275e-06
Epoch 98: reducing lr to 1.348990403037334e-07
[I 2024-06-20 22:19:37,301] Trial 89 finished with value: 1.0988965034484863 and parameters: {'hidden_size': 135, 'n_layers': 6, 'rnn_dropout': 0.1685859136666566, 'bidirectional': False, 'fc_dropout': 0.5105749684962667, 'learning_rate_model': 0.002446297163720527}. Best is trial 61 with value: 0.9777320623397827.
Epoch 4: reducing lr to 0.00028023141660064865
Epoch 7: reducing lr to 0.0005640350609624133
Epoch 10: reducing lr to 0.0009287994858294048
Epoch 19: reducing lr to 0.0019834600191074294
Epoch 22: reducing lr to 0.0021564121679652167
Epoch 25: reducing lr to 0.0021924812177561127
Epoch 28: reducing lr to 0.002178723831730738
Epoch 31: reducing lr to 0.002147901205963345
Epoch 34: reducing lr to 0.002100499472618291
Epoch 37: reducing lr to 0.0020372661279666965
Epoch 40: reducing lr to 0.00195919838942558
Epoch 43: reducing lr to 0.0018675274894425571
Epoch 46: reducing lr to 0.001763699024389199
Epoch 49: reducing lr to 0.0016493505850139589
Epoch 52: reducing lr to 0.001526285440741597
Epoch 55: reducing lr to 0.001396444204083623
Epoch 58: reducing lr to 0.0012618750359991633
Epoch 61: reducing lr to 0.0011246995773089832
Epoch 64: reducing lr to 0.0009870818773929945
Epoch 67: reducing lr to 0.0008511913358884739
Epoch 70: reducing lr to 0.0007191719352711373
Epoch 73: reducing lr to 0.0005931052877537621
Epoch 76: reducing lr to 0.00047497936672747135
Epoch 79: reducing lr to 0.00036665743656240195
Epoch 82: reducing lr to 0.00026984747823615535
Epoch 85: reducing lr to 0.0001860765278510243
Epoch 88: reducing lr to 0.00011666536326573399
Epoch 91: reducing lr to 6.270905110224052e-05
Epoch 94: reducing lr to 2.505824052575385e-05
Epoch 97: reducing lr to 4.306818239924529e-06
[I 2024-06-20 22:19:59,216] Trial 90 finished with value: 1.100550889968872 and parameters: {'hidden_size': 70, 'n_layers': 6, 'rnn_dropout': 0.7562467519999598, 'bidirectional': False, 'fc_dropout': 0.04491283172953225, 'learning_rate_model': 0.021932412730099944}. Best is trial 61 with value: 0.9777320623397827.
Epoch 16: reducing lr to 0.00010236066859872783
Epoch 19: reducing lr to 0.00011998495548143542
Epoch 22: reducing lr to 0.00013044730696884203
Epoch 25: reducing lr to 0.00013262922306078617
Epoch 28: reducing lr to 0.00013179700091670775
Epoch 31: reducing lr to 0.00012993245545327771
Epoch 34: reducing lr to 0.00012706499414306252
Epoch 37: reducing lr to 0.00012323983509277927
Epoch 40: reducing lr to 0.00011851730272854871
Epoch 43: reducing lr to 0.00011297187768975422
Epoch 46: reducing lr to 0.00010669100807951674
Epoch 49: reducing lr to 9.977375626922615e-05
Epoch 52: reducing lr to 9.232920698938711e-05
Epoch 55: reducing lr to 8.447475323182048e-05
Epoch 58: reducing lr to 7.63342938899409e-05
Epoch 61: reducing lr to 6.80361728562266e-05
Epoch 64: reducing lr to 5.971129943361637e-05
Epoch 67: reducing lr to 5.1490906576842104e-05
Epoch 70: reducing lr to 4.350468968657919e-05
Epoch 73: reducing lr to 3.587857121464099e-05
Epoch 76: reducing lr to 2.873280914280395e-05
Epoch 79: reducing lr to 2.2180117460937967e-05
Epoch 82: reducing lr to 1.6323816639123826e-05
Epoch 85: reducing lr to 1.1256281293934135e-05
Epoch 88: reducing lr to 7.057408913119321e-06
Epoch 91: reducing lr to 3.7934430905097294e-06
Epoch 94: reducing lr to 1.5158419352857268e-06
Epoch 97: reducing lr to 2.6053128866017523e-07
[I 2024-06-20 22:20:07,844] Trial 91 finished with value: 1.092646837234497 and parameters: {'hidden_size': 51, 'n_layers': 3, 'rnn_dropout': 0.24006483674028517, 'bidirectional': False, 'fc_dropout': 0.2961233420429906, 'learning_rate_model': 0.0013267520089493556}. Best is trial 61 with value: 0.9777320623397827.
Epoch 30: reducing lr to 2.6401173696403073e-05
Epoch 33: reducing lr to 2.5888491975070065e-05
Epoch 36: reducing lr to 2.5178915728923955e-05
Epoch 39: reducing lr to 2.428363521552628e-05
Epoch 42: reducing lr to 2.3216769662142277e-05
Epoch 45: reducing lr to 2.1995144181371844e-05
Epoch 48: reducing lr to 2.0638025071324585e-05
Epoch 51: reducing lr to 1.9166814474762935e-05
Epoch 54: reducing lr to 1.76047154436566e-05
Epoch 57: reducing lr to 1.597635841482079e-05
Epoch 60: reducing lr to 1.4307430772149217e-05
Epoch 63: reducing lr to 1.262424637081613e-05
Epoch 66: reducing lr to 1.0953354993427454e-05
Epoch 69: reducing lr to 9.321100259724908e-06
Epoch 72: reducing lr to 7.753233469057592e-06
Epoch 75: reducing lr to 6.274473709821411e-06
Epoch 78: reducing lr to 4.908146263609409e-06
Epoch 81: reducing lr to 3.675794886923609e-06
Epoch 84: reducing lr to 2.5968581279820304e-06
Epoch 87: reducing lr to 1.688347051640994e-06
Epoch 90: reducing lr to 9.645948010096932e-07
Epoch 93: reducing lr to 4.370117548785173e-07
Epoch 96: reducing lr to 1.1391976744851693e-07
Epoch 99: reducing lr to 4.132362041583352e-10
[I 2024-06-20 22:20:14,012] Trial 92 finished with value: 1.0922026634216309 and parameters: {'hidden_size': 107, 'n_layers': 1, 'rnn_dropout': 0.16516324711154642, 'bidirectional': False, 'fc_dropout': 0.2092396414663897, 'learning_rate_model': 0.0002680689020235863}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:20:38,451] Trial 93 finished with value: 1.1049978733062744 and parameters: {'hidden_size': 35, 'n_layers': 6, 'rnn_dropout': 0.08526710143104088, 'bidirectional': True, 'fc_dropout': 0.4579240899477195, 'learning_rate_model': 1.5114066835104806e-05}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:21:24,445] Trial 94 finished with value: 1.038432002067566 and parameters: {'hidden_size': 161, 'n_layers': 2, 'rnn_dropout': 0.4223231827265013, 'bidirectional': True, 'fc_dropout': 0.4408697393495774, 'learning_rate_model': 7.353072250353887e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 12: reducing lr to 9.349172160938012e-05
Epoch 30: reducing lr to 0.0001694306759596323
Epoch 33: reducing lr to 0.00016614051880236037
Epoch 39: reducing lr to 0.00015584128102169557
Epoch 42: reducing lr to 0.00014899462511364683
Epoch 45: reducing lr to 0.00014115479066701978
Epoch 48: reducing lr to 0.00013244541998459557
Epoch 51: reducing lr to 0.000123003862244745
Epoch 54: reducing lr to 0.00011297902403869615
Epoch 57: reducing lr to 0.00010252897226176103
Epoch 60: reducing lr to 9.18185568129174e-05
Epoch 63: reducing lr to 8.101664799772606e-05
Epoch 66: reducing lr to 7.029363019626163e-05
Epoch 69: reducing lr to 5.981856472948618e-05
Epoch 72: reducing lr to 4.9756711676581425e-05
Epoch 75: reducing lr to 4.026670685821896e-05
Epoch 78: reducing lr to 3.149824127953004e-05
Epoch 81: reducing lr to 2.358957293119425e-05
Epoch 84: reducing lr to 1.6665449538525844e-05
Epoch 87: reducing lr to 1.0835040347200895e-05
Epoch 90: reducing lr to 6.190328924069225e-06
Epoch 93: reducing lr to 2.8045418693435152e-06
Epoch 96: reducing lr to 7.310850428819118e-07
Epoch 99: reducing lr to 2.6519612425978874e-09
[I 2024-06-20 22:23:56,854] Trial 95 finished with value: 1.1640090942382812 and parameters: {'hidden_size': 156, 'n_layers': 6, 'rnn_dropout': 0.14123922742166642, 'bidirectional': True, 'fc_dropout': 0.6881895979384922, 'learning_rate_model': 0.0017203437921322648}. Best is trial 61 with value: 0.9777320623397827.
Epoch 12: reducing lr to 0.002956435073153523
Epoch 15: reducing lr to 0.003907602133219648
Epoch 18: reducing lr to 0.004707265178883634
Epoch 21: reducing lr to 0.005243113902982168
Epoch 24: reducing lr to 0.005439889938728263
Epoch 27: reducing lr to 0.005420253559271503
Epoch 30: reducing lr to 0.005357809057876088
Epoch 33: reducing lr to 0.005253766305763906
Epoch 36: reducing lr to 0.005109766115371867
Epoch 39: reducing lr to 0.004928079418440074
Epoch 42: reducing lr to 0.0047115715468133085
Epoch 45: reducing lr to 0.004463656960080531
Epoch 48: reducing lr to 0.004188245527844903
Epoch 51: reducing lr to 0.003889680564372267
Epoch 54: reducing lr to 0.0035726708573643835
Epoch 57: reducing lr to 0.0032422148655634835
Epoch 60: reducing lr to 0.002903525542745091
Epoch 63: reducing lr to 0.002561942977695462
Epoch 66: reducing lr to 0.0022228551379105295
Epoch 69: reducing lr to 0.0018916081525469925
Epoch 72: reducing lr to 0.0015734279462736283
Epoch 75: reducing lr to 0.0012733309686329068
Epoch 78: reducing lr to 0.0009960508123974011
Epoch 81: reducing lr to 0.0007459595306831795
Epoch 84: reducing lr to 0.0005270019492359488
Epoch 87: reducing lr to 0.0003426302644777085
Epoch 90: reducing lr to 0.00019575321996893048
Epoch 93: reducing lr to 8.868641847561004e-05
Epoch 96: reducing lr to 2.3118682863330214e-05
Epoch 99: reducing lr to 8.38614488567202e-08
[I 2024-06-20 22:25:01,875] Trial 96 finished with value: 1.1072428226470947 and parameters: {'hidden_size': 90, 'n_layers': 6, 'rnn_dropout': 0.573543201353976, 'bidirectional': True, 'fc_dropout': 0.0030349379274740686, 'learning_rate_model': 0.054401444720336284}. Best is trial 61 with value: 0.9777320623397827.
Epoch 3: reducing lr to 0.0001159840235056967
Epoch 6: reducing lr to 0.0002519031618607933
Epoch 9: reducing lr to 0.0004405088059267961
Epoch 19: reducing lr to 0.0010905036966712184
Epoch 28: reducing lr to 0.001197859482742297
Epoch 35: reducing lr to 0.001144206087558826
Epoch 38: reducing lr to 0.0011066543518896466
Epoch 44: reducing lr to 0.0010084361069189222
Epoch 47: reducing lr to 0.0009493185816216996
Epoch 54: reducing lr to 0.0007919042531408453
Epoch 57: reducing lr to 0.000718656670077439
Epoch 60: reducing lr to 0.0006435841190529262
Epoch 63: reducing lr to 0.0005678702632679817
Epoch 66: reducing lr to 0.0004927093785308623
Epoch 69: reducing lr to 0.00041928646692713774
Epoch 72: reducing lr to 0.0003487598864855827
Epoch 75: reducing lr to 0.0002822415637975202
Epoch 78: reducing lr to 0.00022078072852862502
Epoch 81: reducing lr to 0.00016534647287792646
Epoch 84: reducing lr to 0.00011681319149599423
Epoch 87: reducing lr to 7.594608474367961e-05
Epoch 90: reducing lr to 4.3389893345442516e-05
Epoch 93: reducing lr to 1.9657884756412934e-05
Epoch 96: reducing lr to 5.1243968496978225e-06
Epoch 99: reducing lr to 1.858840085605662e-08
[I 2024-06-20 22:26:13,936] Trial 97 finished with value: 1.11668860912323 and parameters: {'hidden_size': 174, 'n_layers': 6, 'rnn_dropout': 0.41335102633723075, 'bidirectional': False, 'fc_dropout': 0.2999007548294875, 'learning_rate_model': 0.01205841152767771}. Best is trial 61 with value: 0.9777320623397827.
Epoch 12: reducing lr to 0.002093265532149013
Epoch 15: reducing lr to 0.002766727039973757
Epoch 18: reducing lr to 0.003332918094200505
Epoch 21: reducing lr to 0.003712318837611737
Epoch 24: reducing lr to 0.003851643559105066
Epoch 27: reducing lr to 0.0038377402751580275
Epoch 30: reducing lr to 0.0037935272553524454
Epoch 33: reducing lr to 0.0037198611333245923
Epoch 36: reducing lr to 0.003617903665052162
Epoch 39: reducing lr to 0.003489262754317852
Epoch 42: reducing lr to 0.003335967162193844
Epoch 45: reducing lr to 0.003160434452533779
Epoch 48: reducing lr to 0.002965432957830332
Epoch 51: reducing lr to 0.0027540379054512656
Epoch 54: reducing lr to 0.0025295832914933687
Epoch 57: reducing lr to 0.0022956082098789113
Epoch 60: reducing lr to 0.002055803624958194
Epoch 63: reducing lr to 0.001813950517377939
Epoch 66: reducing lr to 0.0015738637676846515
Epoch 69: reducing lr to 0.0013393286333310502
Epoch 72: reducing lr to 0.0011140452625402757
Epoch 75: reducing lr to 0.0009015654873875085
Epoch 78: reducing lr to 0.0007052408668784024
Epoch 81: reducing lr to 0.0005281669765511095
Epoch 84: reducing lr to 0.00037313689914193136
Epoch 87: reducing lr to 0.00024259491757999614
Epoch 90: reducing lr to 0.00013860053004007485
Epoch 93: reducing lr to 6.279326904572238e-05
Epoch 96: reducing lr to 1.6368883736341684e-05
Epoch 99: reducing lr to 5.937701184829946e-08
[I 2024-06-20 22:28:16,103] Trial 98 finished with value: 1.0130914449691772 and parameters: {'hidden_size': 154, 'n_layers': 5, 'rnn_dropout': 0.3214927391742699, 'bidirectional': True, 'fc_dropout': 0.6199173173796783, 'learning_rate_model': 0.03851823778112661}. Best is trial 61 with value: 0.9777320623397827.
Epoch 60: reducing lr to 7.484483169291273e-06
Epoch 63: reducing lr to 6.603978100057329e-06
Epoch 66: reducing lr to 5.729903740311126e-06
Epoch 69: reducing lr to 4.876040927556968e-06
Epoch 72: reducing lr to 4.055860645484041e-06
Epoch 75: reducing lr to 3.2822939090317584e-06
Epoch 78: reducing lr to 2.5675426068747815e-06
Epoch 81: reducing lr to 1.9228766787745224e-06
Epoch 84: reducing lr to 1.358464790880069e-06
Epoch 87: reducing lr to 8.832057476404156e-07
Epoch 90: reducing lr to 5.045974828266411e-07
Epoch 93: reducing lr to 2.2860897782833843e-07
Epoch 96: reducing lr to 5.959354937280185e-08
Epoch 99: reducing lr to 2.1617154499819005e-10
[I 2024-06-20 22:28:20,558] Trial 99 finished with value: 1.0920474529266357 and parameters: {'hidden_size': 60, 'n_layers': 1, 'rnn_dropout': 0.1762271902349821, 'bidirectional': False, 'fc_dropout': 0.4345331441904756, 'learning_rate_model': 0.00014023182899556565}. Best is trial 61 with value: 0.9777320623397827.
Epoch 13: reducing lr to 0.0012391149542637738
Epoch 16: reducing lr to 0.0015844728537555564
Epoch 19: reducing lr to 0.001857284711227122
Epoch 22: reducing lr to 0.0020192347272360136
Epoch 25: reducing lr to 0.0020530092899092092
Epoch 30: reducing lr to 0.002022638370549526
Epoch 33: reducing lr to 0.0019833609606369203
Epoch 36: reducing lr to 0.001928999129652067
Epoch 39: reducing lr to 0.0018604101820686447
Epoch 42: reducing lr to 0.0017786758156610623
Epoch 45: reducing lr to 0.0016850850306353222
Epoch 48: reducing lr to 0.001581113850529735
Epoch 51: reducing lr to 0.0014684019295377502
Epoch 54: reducing lr to 0.0013487268925395136
Epoch 57: reducing lr to 0.0012239757187715815
Epoch 60: reducing lr to 0.0010961163619658582
Epoch 63: reducing lr to 0.0009671647708738845
Epoch 66: reducing lr to 0.0008391549690449935
Epoch 69: reducing lr to 0.0007141051855443583
Epoch 72: reducing lr to 0.0005939882707745389
Epoch 75: reducing lr to 0.0004806979957189577
Epoch 78: reducing lr to 0.00037602134947501244
Epoch 81: reducing lr to 0.00028160883550318734
Epoch 84: reducing lr to 0.00019894967371263047
Epoch 87: reducing lr to 0.00012934711042481006
Epoch 90: reducing lr to 7.389923186712742e-05
Epoch 93: reducing lr to 3.348020637123847e-05
Epoch 96: reducing lr to 8.727585199625257e-06
Epoch 99: reducing lr to 3.16587213979036e-08
[I 2024-06-20 22:28:27,848] Trial 100 finished with value: 1.1146557331085205 and parameters: {'hidden_size': 41, 'n_layers': 3, 'rnn_dropout': 0.4910968717930021, 'bidirectional': False, 'fc_dropout': 0.35529368561329155, 'learning_rate_model': 0.020537209952065802}. Best is trial 61 with value: 0.9777320623397827.
Epoch 11: reducing lr to 0.00015542710649914357
Epoch 14: reducing lr to 0.00021293200036258847
Epoch 17: reducing lr to 0.0002640242321576312
Epoch 20: reducing lr to 0.00030152803478593603
Epoch 23: reducing lr to 0.00032017611020763927
Epoch 26: reducing lr to 0.00032117559878320817
Epoch 29: reducing lr to 0.0003183180175643084
Epoch 32: reducing lr to 0.00031297692984079623
Epoch 35: reducing lr to 0.0003052365676785329
Epoch 38: reducing lr to 0.00029521899913851013
Epoch 41: reducing lr to 0.00028308220853404966
Epoch 44: reducing lr to 0.0002690176003658158
Epoch 47: reducing lr to 0.00025324698814169113
Epoch 50: reducing lr to 0.0002360190788404646
Epoch 53: reducing lr to 0.00021760558085974388
Epoch 56: reducing lr to 0.00019829682924983692
Epoch 59: reducing lr to 0.00017839742018119602
Epoch 62: reducing lr to 0.00015822110624188186
Epoch 65: reducing lr to 0.00013808613940027872
Epoch 68: reducing lr to 0.00011830997147844186
Epoch 71: reducing lr to 9.920460158387478e-05
Epoch 74: reducing lr to 8.10712464575977e-05
Epoch 77: reducing lr to 6.419593336800594e-05
Epoch 80: reducing lr to 4.884474614230026e-05
Epoch 83: reducing lr to 3.525982740208685e-05
Epoch 86: reducing lr to 2.365536350635634e-05
Epoch 89: reducing lr to 1.4214432511122166e-05
Epoch 92: reducing lr to 7.085876781260173e-06
Epoch 95: reducing lr to 2.382138901654146e-06
Epoch 98: reducing lr to 1.7738710945663234e-07
[I 2024-06-20 22:28:43,808] Trial 101 finished with value: 1.0931965112686157 and parameters: {'hidden_size': 94, 'n_layers': 3, 'rnn_dropout': 0.5399772838472842, 'bidirectional': False, 'fc_dropout': 0.25181254985921314, 'learning_rate_model': 0.0032167877678542485}. Best is trial 61 with value: 0.9777320623397827.
Epoch 20: reducing lr to 5.056282336509937e-05
Epoch 23: reducing lr to 5.368989360357992e-05
Epoch 26: reducing lr to 5.385749647452956e-05
Epoch 29: reducing lr to 5.337831259192564e-05
Epoch 32: reducing lr to 5.2482672903452994e-05
Epoch 35: reducing lr to 5.1184702168923215e-05
Epoch 38: reducing lr to 4.950486981437437e-05
Epoch 41: reducing lr to 4.7469668013028786e-05
Epoch 44: reducing lr to 4.5111193123572454e-05
Epoch 47: reducing lr to 4.246664074948228e-05
Epoch 50: reducing lr to 3.957771622355459e-05
Epoch 53: reducing lr to 3.648998195501876e-05
Epoch 56: reducing lr to 3.325212383099587e-05
Epoch 59: reducing lr to 2.9915219166320638e-05
Epoch 62: reducing lr to 2.6531880702961545e-05
Epoch 65: reducing lr to 2.3155475677813952e-05
Epoch 68: reducing lr to 1.983923715233069e-05
Epoch 71: reducing lr to 1.6635483829726066e-05
Epoch 74: reducing lr to 1.3594726332939798e-05
Epoch 77: reducing lr to 1.0764928183041281e-05
Epoch 80: reducing lr to 8.190708612748345e-06
Epoch 83: reducing lr to 5.912672186787868e-06
Epoch 86: reducing lr to 3.9667355224805256e-06
Epoch 89: reducing lr to 2.3835987284075935e-06
Epoch 92: reducing lr to 1.1882209769717606e-06
Epoch 95: reducing lr to 3.994576112996721e-07
Epoch 98: reducing lr to 2.974580154402589e-08
[I 2024-06-20 22:29:31,751] Trial 102 finished with value: 1.0928411483764648 and parameters: {'hidden_size': 178, 'n_layers': 4, 'rnn_dropout': 0.6275659350335385, 'bidirectional': False, 'fc_dropout': 0.10547278002200172, 'learning_rate_model': 0.0005394187370487681}. Best is trial 61 with value: 0.9777320623397827.
Epoch 67: reducing lr to 7.24717121661439e-07
[I 2024-06-20 22:32:16,833] Trial 103 finished with value: 1.0739424228668213 and parameters: {'hidden_size': 184, 'n_layers': 5, 'rnn_dropout': 0.6134196697896818, 'bidirectional': True, 'fc_dropout': 0.67431987215428, 'learning_rate_model': 1.8673586483652047e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 3: reducing lr to 6.109592085974535e-05
Epoch 6: reducing lr to 0.00013269289317774648
Epoch 9: reducing lr to 0.0002320430894829457
Epoch 18: reducing lr to 0.0005496198094206968
Epoch 23: reducing lr to 0.000632223538848326
Epoch 26: reducing lr to 0.0006341971408259216
Epoch 29: reducing lr to 0.0006285545271106519
Epoch 32: reducing lr to 0.000618007952040862
Epoch 35: reducing lr to 0.0006027237412513054
Epoch 40: reducing lr to 0.0005674089098029805
Epoch 43: reducing lr to 0.0005408598447870198
Epoch 46: reducing lr to 0.00051078979344336
Epoch 49: reducing lr to 0.0004776730229959322
Epoch 52: reducing lr to 0.00044203178333219377
Epoch 55: reducing lr to 0.0004044281006540083
Epoch 58: reducing lr to 0.0003654551485691077
Epoch 61: reducing lr to 0.00032572738139289073
Epoch 64: reducing lr to 0.0002858715355018481
Epoch 67: reducing lr to 0.0002465158967754282
Epoch 70: reducing lr to 0.00020828138995803106
Epoch 73: reducing lr to 0.00017177087656825496
Epoch 76: reducing lr to 0.00013756009912439847
Epoch 79: reducing lr to 0.00010618868281737662
Epoch 82: reducing lr to 7.815128078170379e-05
Epoch 85: reducing lr to 5.389014220189775e-05
Epoch 88: reducing lr to 3.378778123729843e-05
Epoch 91: reducing lr to 1.8161343186451785e-05
Epoch 94: reducing lr to 7.2571869265712805e-06
Epoch 97: reducing lr to 1.2473096422622316e-06
[I 2024-06-20 22:33:05,284] Trial 104 finished with value: 1.1074563264846802 and parameters: {'hidden_size': 122, 'n_layers': 7, 'rnn_dropout': 0.41428803853785756, 'bidirectional': False, 'fc_dropout': 0.20131131388468226, 'learning_rate_model': 0.006351907220679005}. Best is trial 61 with value: 0.9777320623397827.
[I 2024-06-20 22:34:56,699] Trial 105 finished with value: 1.0773119926452637 and parameters: {'hidden_size': 115, 'n_layers': 7, 'rnn_dropout': 0.49946936192214075, 'bidirectional': True, 'fc_dropout': 0.7914720449333105, 'learning_rate_model': 2.0457769512926938e-05}. Best is trial 61 with value: 0.9777320623397827.
Epoch 24: reducing lr to 1.0542908927769123e-05
Epoch 27: reducing lr to 1.0504852172464228e-05
Epoch 30: reducing lr to 1.0383830111601408e-05
Epoch 33: reducing lr to 1.0182187565066796e-05
Epoch 36: reducing lr to 9.903104548685105e-06
Epoch 39: reducing lr to 9.550982296081794e-06
Epoch 42: reducing lr to 9.131374032235237e-06
Epoch 45: reducing lr to 8.650897232294643e-06
Epoch 48: reducing lr to 8.117129512647453e-06
Epoch 51: reducing lr to 7.538488537486364e-06
Epoch 54: reducing lr to 6.924100285546063e-06
Epoch 57: reducing lr to 6.283652139455999e-06
Epoch 60: reducing lr to 5.627247189079947e-06
Epoch 63: reducing lr to 4.965234921332869e-06
Epoch 66: reducing lr to 4.3080576156092416e-06
Epoch 69: reducing lr to 3.66607646550857e-06
Epoch 72: reducing lr to 3.0494196994448257e-06
Epoch 75: reducing lr to 2.467809567548564e-06
Epoch 78: reducing lr to 1.930420122615796e-06
Epoch 81: reducing lr to 1.4457247268559144e-06
Epoch 84: reducing lr to 1.021368744245278e-06
Epoch 87: reducing lr to 6.64042786705795e-07
Epoch 90: reducing lr to 3.793842143307152e-07
Epoch 93: reducing lr to 1.7188083649665982e-07
Epoch 96: reducing lr to 4.4805716789013864e-08
Epoch 99: reducing lr to 1.6252968861578009e-10
[I 2024-06-20 22:35:23,007] Trial 106 finished with value: 1.1070210933685303 and parameters: {'hidden_size': 75, 'n_layers': 7, 'rnn_dropout': 0.007151281896996054, 'bidirectional': False, 'fc_dropout': 0.351864651366858, 'learning_rate_model': 0.00010543402232135162}. Best is trial 61 with value: 0.9777320623397827.
Epoch 13: reducing lr to 0.001612056063395877
Epoch 16: reducing lr to 0.002061357634651773
Epoch 19: reducing lr to 0.0024162787075433768
Epoch 22: reducing lr to 0.00262697143171384
Epoch 25: reducing lr to 0.0026709112521142954
Epoch 28: reducing lr to 0.002654151812244403
Epoch 31: reducing lr to 0.0026166032588907317
Epoch 34: reducing lr to 0.0025588578050479835
Epoch 37: reducing lr to 0.0024818260611174175
Epoch 40: reducing lr to 0.002386722851289246
Epoch 43: reducing lr to 0.0022750480801335387
Epoch 46: reducing lr to 0.0021485627933475527
Epoch 49: reducing lr to 0.0020092619268609452
Epoch 52: reducing lr to 0.0018593422486816614
Epoch 55: reducing lr to 0.0017011678400848378
Epoch 58: reducing lr to 0.001537233799367128
Epoch 61: reducing lr to 0.0013701247390192747
Epoch 64: reducing lr to 0.0012024769342312885
Epoch 67: reducing lr to 0.0010369331779514555
Epoch 70: reducing lr to 0.0008761052995867283
Epoch 73: reducing lr to 0.0007225291482183295
Epoch 76: reducing lr to 0.0005786265008066515
Epoch 79: reducing lr to 0.0004466672120403265
Epoch 82: reducing lr to 0.0003287319682096307
Epoch 85: reducing lr to 0.00022668102602963516
Epoch 88: reducing lr to 0.00014212337554132397
Epoch 91: reducing lr to 7.639304220348197e-05
Epoch 94: reducing lr to 3.052629871416641e-05
Epoch 97: reducing lr to 5.246626153358053e-06
[I 2024-06-20 22:37:09,896] Trial 107 finished with value: 1.1218664646148682 and parameters: {'hidden_size': 164, 'n_layers': 4, 'rnn_dropout': 0.10747088692455692, 'bidirectional': True, 'fc_dropout': 0.3932361200135483, 'learning_rate_model': 0.026718371620438223}. Best is trial 61 with value: 0.9777320623397827.
Epoch 24: reducing lr to 1.5025967070486122e-05
[I 2024-06-20 22:38:09,333] Trial 108 finished with value: 1.052877426147461 and parameters: {'hidden_size': 93, 'n_layers': 5, 'rnn_dropout': 0.31839251152633796, 'bidirectional': True, 'fc_dropout': 0.7719911392887311, 'learning_rate_model': 0.0001502667013784738}. Best is trial 61 with value: 0.9777320623397827.
Epoch 31: reducing lr to 0.0001045315968951531
Epoch 35: reducing lr to 0.0001012823678062363
Epoch 38: reducing lr to 9.795837858334832e-05
Epoch 41: reducing lr to 9.39311976353472e-05
Epoch 44: reducing lr to 8.926433603229738e-05
Epoch 52: reducing lr to 7.427951248203935e-05
Epoch 55: reducing lr to 6.796054782341477e-05
Epoch 58: reducing lr to 6.141148961083657e-05
Epoch 61: reducing lr to 5.4735591430836993e-05
Epoch 64: reducing lr to 4.803817076115384e-05
Epoch 67: reducing lr to 4.142480545972524e-05
Epoch 70: reducing lr to 3.4999836411168985e-05
Epoch 73: reducing lr to 2.8864569135549924e-05
Epoch 76: reducing lr to 2.311575204596186e-05
Epoch 79: reducing lr to 1.7844064359636683e-05
Epoch 82: reducing lr to 1.3132628139432577e-05
Epoch 85: reducing lr to 9.055759430168556e-06
Epoch 88: reducing lr to 5.677736336597797e-06
Epoch 91: reducing lr to 3.0518523074048697e-06
Epoch 94: reducing lr to 1.2195057623076668e-06
Epoch 97: reducing lr to 2.0959929949598313e-07
[I 2024-06-20 22:38:18,457] Trial 109 finished with value: 0.9788683652877808 and parameters: {'hidden_size': 35, 'n_layers': 2, 'rnn_dropout': 0.011273835353954365, 'bidirectional': True, 'fc_dropout': 0.6356626809478018, 'learning_rate_model': 0.0010673815537119522}. Best is trial 61 with value: 0.9777320623397827.
Epoch 13: reducing lr to 1.1690955540001709e-05
Epoch 16: reducing lr to 1.494938110774554e-05
Epoch 47: reducing lr to 1.5254631952587287e-05
Epoch 50: reducing lr to 1.4216888453123724e-05
Epoch 53: reducing lr to 1.3107729616855761e-05
Epoch 56: reducing lr to 1.1944644119040255e-05
Epoch 59: reducing lr to 1.074597967037856e-05
Epoch 62: reducing lr to 9.530635529219849e-06
Epoch 65: reducing lr to 8.317781979410387e-06
Epoch 68: reducing lr to 7.126541107035645e-06
Epoch 71: reducing lr to 5.975706547468841e-06
Epoch 74: reducing lr to 4.883422447481237e-06
Epoch 77: reducing lr to 3.866917997989537e-06
Epoch 80: reducing lr to 2.942221073134587e-06
Epoch 83: reducing lr to 2.1239174202128977e-06
Epoch 86: reducing lr to 1.4249088079666884e-06
Epoch 89: reducing lr to 8.562231597034482e-07
Epoch 92: reducing lr to 4.2682617137002523e-07
Epoch 95: reducing lr to 1.434909551001004e-07
Epoch 98: reducing lr to 1.0685122408567302e-08
[I 2024-06-20 22:39:10,617] Trial 110 finished with value: 1.0931025743484497 and parameters: {'hidden_size': 162, 'n_layers': 5, 'rnn_dropout': 0.6222787814873403, 'bidirectional': False, 'fc_dropout': 0.14383543815437952, 'learning_rate_model': 0.0001937670170464033}. Best is trial 61 with value: 0.9777320623397827.
Epoch 4: reducing lr to 0.0010874121774134312
Epoch 9: reducing lr to 0.003109054056606459
Epoch 14: reducing lr to 0.005633551972739072
Epoch 17: reducing lr to 0.00698530155819582
Epoch 20: reducing lr to 0.007977541432531893
Epoch 23: reducing lr to 0.0084709144431683
Epoch 26: reducing lr to 0.008497357959535209
Epoch 29: reducing lr to 0.008421754798499844
Epoch 32: reducing lr to 0.008280445388781591
Epoch 35: reducing lr to 0.008075658262118241
Epoch 38: reducing lr to 0.007810622978954626
Epoch 41: reducing lr to 0.00748951933771681
Epoch 44: reducing lr to 0.007117411336303052
Epoch 47: reducing lr to 0.006700167505149292
Epoch 50: reducing lr to 0.006244367896519181
Epoch 53: reducing lr to 0.0057572011122985125
Epoch 56: reducing lr to 0.005246348560601772
Epoch 59: reducing lr to 0.004719868956671464
Epoch 62: reducing lr to 0.004186063267522465
Epoch 65: reducing lr to 0.0036533515004870086
Epoch 68: reducing lr to 0.00313013249338817
Epoch 71: reducing lr to 0.00262466082132305
Epoch 74: reducing lr to 0.002144905789810349
Epoch 77: reducing lr to 0.0016984348357753794
Epoch 80: reducing lr to 0.0012922877515795018
Epoch 83: reducing lr to 0.0009328709159788923
Epoch 86: reducing lr to 0.0006258510675716547
Epoch 89: reducing lr to 0.00037607191111734966
Epoch 92: reducing lr to 0.00018747137608099985
Epoch 95: reducing lr to 6.302436123222649e-05
Epoch 98: reducing lr to 4.693139118198964e-06
[I 2024-06-20 22:39:19,678] Trial 111 finished with value: 1.1690843105316162 and parameters: {'hidden_size': 47, 'n_layers': 3, 'rnn_dropout': 0.19313499837247905, 'bidirectional': False, 'fc_dropout': 0.256255460657625, 'learning_rate_model': 0.08510670563663286}. Best is trial 61 with value: 0.9777320623397827.
Epoch 50: reducing lr to 3.956204542852132e-05
Epoch 57: reducing lr to 3.213552462802512e-05
Epoch 65: reducing lr to 2.314630726821712e-05
Epoch 68: reducing lr to 1.9831381807235117e-05
Epoch 73: reducing lr to 1.4581406619021478e-05
Epoch 76: reducing lr to 1.1677298154141521e-05
Epoch 79: reducing lr to 9.014219368457395e-06
Epoch 82: reducing lr to 6.6341607241116205e-06
Epoch 85: reducing lr to 4.5746641799928024e-06
Epoch 88: reducing lr to 2.8682008662849443e-06
Epoch 91: reducing lr to 1.5416928354791345e-06
Epoch 94: reducing lr to 6.160531726956365e-07
Epoch 97: reducing lr to 1.0588249554879976e-07
[I 2024-06-20 22:40:16,443] Trial 112 finished with value: 0.976586103439331 and parameters: {'hidden_size': 133, 'n_layers': 3, 'rnn_dropout': 0.6859875992668774, 'bidirectional': True, 'fc_dropout': 0.6342845423815101, 'learning_rate_model': 0.0005392051542230778}. Best is trial 112 with value: 0.976586103439331.
Epoch 14: reducing lr to 9.938115956594626e-05
Epoch 17: reducing lr to 0.00012322729463233893
Epoch 20: reducing lr to 0.00014073133999417532
Epoch 23: reducing lr to 0.0001494349043054403
Epoch 26: reducing lr to 0.00014990139282498557
Epoch 29: reducing lr to 0.0001485676819003498
Epoch 32: reducing lr to 0.0001460748509007715
Epoch 35: reducing lr to 0.00014246221322378446
Epoch 38: reducing lr to 0.00013778674135556584
Epoch 41: reducing lr to 0.00013212217087472472
Epoch 44: reducing lr to 0.00012555783547084175
Epoch 47: reducing lr to 0.00011819726154475485
Epoch 50: reducing lr to 0.00011015652741208637
Epoch 53: reducing lr to 0.00010156244677661061
Epoch 56: reducing lr to 9.255052690784612e-05
Epoch 59: reducing lr to 8.326293112820254e-05
Epoch 62: reducing lr to 7.384609630938165e-05
Epoch 65: reducing lr to 6.444855930633392e-05
Epoch 68: reducing lr to 5.52184835239419e-05
Epoch 71: reducing lr to 4.6301487436809864e-05
Epoch 74: reducing lr to 3.7838157095659614e-05
Epoch 77: reducing lr to 2.9961989210953652e-05
Epoch 80: reducing lr to 2.279717234638331e-05
Epoch 83: reducing lr to 1.6456721053423156e-05
Epoch 86: reducing lr to 1.1040601934948574e-05
Epoch 89: reducing lr to 6.634262502215257e-06
Epoch 92: reducing lr to 3.307171537692347e-06
Epoch 95: reducing lr to 1.111809055897705e-06
Epoch 98: reducing lr to 8.279139161720267e-08
[I 2024-06-20 22:40:29,433] Trial 113 finished with value: 1.0924338102340698 and parameters: {'hidden_size': 82, 'n_layers': 3, 'rnn_dropout': 0.5578951004905116, 'bidirectional': False, 'fc_dropout': 0.23623103551987823, 'learning_rate_model': 0.001501362396927331}. Best is trial 112 with value: 0.976586103439331.
Epoch 3: reducing lr to 5.855674738568371e-06
Epoch 6: reducing lr to 1.2717811788977106e-05
Epoch 9: reducing lr to 2.2239927612579876e-05
Epoch 12: reducing lr to 3.3084668592302016e-05
Epoch 15: reducing lr to 4.372892296607896e-05
Epoch 18: reducing lr to 5.267773672206096e-05
Epoch 21: reducing lr to 5.8674275463396364e-05
Epoch 24: reducing lr to 6.087634307810041e-05
Epoch 27: reducing lr to 6.0656597644629196e-05
Epoch 30: reducing lr to 5.995779804884603e-05
Epoch 33: reducing lr to 5.8793483633718066e-05
Epoch 36: reducing lr to 5.718201628927636e-05
Epoch 58: reducing lr to 3.502666710709751e-05
Epoch 61: reducing lr to 3.1219000745745134e-05
Epoch 64: reducing lr to 2.7399058813709816e-05
Epoch 67: reducing lr to 2.362705871505249e-05
Epoch 70: reducing lr to 1.9962512333531817e-05
Epoch 73: reducing lr to 1.646320030189113e-05
Epoch 76: reducing lr to 1.3184304060607599e-05
Epoch 79: reducing lr to 1.017754342262897e-05
Epoch 82: reducing lr to 7.4903279011170045e-06
Epoch 85: reducing lr to 5.1650444073661075e-06
Epoch 88: reducing lr to 3.2383546115577326e-06
Epoch 91: reducing lr to 1.740654973668562e-06
Epoch 94: reducing lr to 6.955575030376834e-07
Epoch 97: reducing lr to 1.1954709022447743e-07
[I 2024-06-20 22:41:03,777] Trial 114 finished with value: 1.092958927154541 and parameters: {'hidden_size': 106, 'n_layers': 6, 'rnn_dropout': 0.7107091475163088, 'bidirectional': False, 'fc_dropout': 0.7185640368137097, 'learning_rate_model': 0.0006087919149176257}. Best is trial 112 with value: 0.976586103439331.
Epoch 40: reducing lr to 4.1484690526303704e-05
Epoch 56: reducing lr to 2.8627881493251465e-05
Epoch 61: reducing lr to 2.3814746965674785e-05
Epoch 65: reducing lr to 1.9935334566702516e-05
Epoch 72: reducing lr to 1.3431739977906917e-05
Epoch 75: reducing lr to 1.0869929263045662e-05
Epoch 78: reducing lr to 8.502896842902649e-06
Epoch 86: reducing lr to 3.415097183861359e-06
Epoch 89: reducing lr to 2.0521210095070696e-06
Epoch 92: reducing lr to 1.0229797497877253e-06
Epoch 95: reducing lr to 3.4390660927360937e-07
Epoch 98: reducing lr to 2.5609169683482332e-08
[I 2024-06-20 22:42:01,330] Trial 115 finished with value: 0.9733659029006958 and parameters: {'hidden_size': 186, 'n_layers': 2, 'rnn_dropout': 0.4870649393550366, 'bidirectional': True, 'fc_dropout': 0.01101401004617344, 'learning_rate_model': 0.00046440389064944195}. Best is trial 115 with value: 0.9733659029006958.
Epoch 7: reducing lr to 0.00018936641073891407
Epoch 10: reducing lr to 0.00031183065929900393
Epoch 13: reducing lr to 0.0004442765717179194
Epoch 16: reducing lr to 0.0005681023903588337
Epoch 19: reducing lr to 0.0006659172995764201
Epoch 22: reducing lr to 0.0007239834197975602
Epoch 25: reducing lr to 0.0007360930685949429
Epoch 28: reducing lr to 0.0007314742301696691
Epoch 31: reducing lr to 0.0007211259904677601
Epoch 34: reducing lr to 0.0007052115611572137
Epoch 37: reducing lr to 0.0006839819030305407
Epoch 40: reducing lr to 0.000657771817053243
Epoch 43: reducing lr to 0.0006269946712684212
Epoch 46: reducing lr to 0.0005921358032290186
Epoch 49: reducing lr to 0.0005537450097539857
Epoch 52: reducing lr to 0.0005124277118219042
Epoch 55: reducing lr to 0.0004688354413168249
Epoch 58: reducing lr to 0.0004236558379198405
Epoch 61: reducing lr to 0.00037760121108635963
Epoch 65: reducing lr to 0.00031609013048295245
Epoch 68: reducing lr to 0.000270820912833628
Epoch 71: reducing lr to 0.00022708720509781298
Epoch 74: reducing lr to 0.00018557851629781992
Epoch 77: reducing lr to 0.0001469495855478089
Epoch 80: reducing lr to 0.00011180949984249625
Epoch 83: reducing lr to 8.071254285721245e-05
Epoch 86: reducing lr to 5.4149004163778906e-05
Epoch 89: reducing lr to 3.2537963959999425e-05
Epoch 92: reducing lr to 1.622013422999738e-05
Epoch 95: reducing lr to 5.452904973103026e-06
Epoch 98: reducing lr to 4.0605316954812203e-07
[I 2024-06-20 22:42:41,414] Trial 116 finished with value: 1.0984649658203125 and parameters: {'hidden_size': 189, 'n_layers': 3, 'rnn_dropout': 0.62379543638608, 'bidirectional': False, 'fc_dropout': 0.1156781766704774, 'learning_rate_model': 0.007363482458797472}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:44:34,650] Trial 117 finished with value: 1.0061578750610352 and parameters: {'hidden_size': 145, 'n_layers': 5, 'rnn_dropout': 0.20271682758903975, 'bidirectional': True, 'fc_dropout': 0.3397192927450718, 'learning_rate_model': 0.0003073891164855588}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:44:42,585] Trial 118 finished with value: 1.0753103494644165 and parameters: {'hidden_size': 81, 'n_layers': 1, 'rnn_dropout': 0.5053173014927518, 'bidirectional': True, 'fc_dropout': 0.5186502049304996, 'learning_rate_model': 4.7836881141741806e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 24: reducing lr to 9.752959560002735e-05
Epoch 27: reducing lr to 9.717754286200555e-05
Epoch 30: reducing lr to 9.605800054825721e-05
Epoch 33: reducing lr to 9.419265995260038e-05
Epoch 36: reducing lr to 9.161093854032015e-05
Epoch 39: reducing lr to 8.835355093188537e-05
Epoch 42: reducing lr to 8.447186850782559e-05
Epoch 45: reducing lr to 8.002710773881483e-05
Epoch 48: reducing lr to 7.50893671021275e-05
Epoch 51: reducing lr to 6.973651612980976e-05
Epoch 54: reducing lr to 6.405297677993276e-05
Epoch 57: reducing lr to 5.812836440597685e-05
Epoch 60: reducing lr to 5.205613995647866e-05
Epoch 63: reducing lr to 4.5932043732374066e-05
Epoch 66: reducing lr to 3.985267443269983e-05
Epoch 69: reducing lr to 3.3913880653760526e-05
Epoch 72: reducing lr to 2.8209355894013438e-05
Epoch 75: reducing lr to 2.282903805674992e-05
Epoch 78: reducing lr to 1.7857793820164613e-05
Epoch 81: reducing lr to 1.3374007963573781e-05
Epoch 84: reducing lr to 9.448405678851675e-06
Epoch 87: reducing lr to 6.142880005151992e-06
Epoch 90: reducing lr to 3.5095806341692703e-06
Epoch 93: reducing lr to 1.5900230752027193e-06
Epoch 96: reducing lr to 4.1448555317517585e-07
Epoch 99: reducing lr to 1.5035181383325075e-09
[I 2024-06-20 22:44:52,304] Trial 119 finished with value: 1.0919917821884155 and parameters: {'hidden_size': 43, 'n_layers': 4, 'rnn_dropout': 0.7002661810836313, 'bidirectional': False, 'fc_dropout': 0.3431458672239224, 'learning_rate_model': 0.0009753415902513679}. Best is trial 115 with value: 0.9733659029006958.
Epoch 27: reducing lr to 0.0002669752569629701
Epoch 30: reducing lr to 0.00026389954535212685
Epoch 33: reducing lr to 0.00025877490677635927
Epoch 36: reducing lr to 0.0002516821596544335
Epoch 39: reducing lr to 0.00024273315900903963
Epoch 42: reducing lr to 0.00023206903711326795
Epoch 45: reducing lr to 0.00021985797359491335
Epoch 48: reducing lr to 0.0002062925495630663
Epoch 51: reducing lr to 0.00019158669549708792
Epoch 54: reducing lr to 0.00017597234331546036
Epoch 57: reducing lr to 0.00015969569271945836
Epoch 60: reducing lr to 0.00014301350838965226
Epoch 63: reducing lr to 0.00012618881705723204
Epoch 66: reducing lr to 0.0001094870037251325
Epoch 69: reducing lr to 9.317139264373405e-05
Epoch 72: reducing lr to 7.749938737655257e-05
Epoch 75: reducing lr to 6.271807376394541e-05
Epoch 78: reducing lr to 4.906060550121417e-05
Epoch 81: reducing lr to 3.674232860332921e-05
Epoch 84: reducing lr to 2.5957545948489432e-05
Epoch 87: reducing lr to 1.6876295896851162e-05
Epoch 90: reducing lr to 9.641848970910099e-06
Epoch 93: reducing lr to 4.368260470241463e-06
Epoch 96: reducing lr to 1.1387135731916448e-06
Epoch 99: reducing lr to 4.130605997087339e-09
[I 2024-06-20 22:45:08,960] Trial 120 finished with value: 1.0275288820266724 and parameters: {'hidden_size': 45, 'n_layers': 3, 'rnn_dropout': 0.20950432328281618, 'bidirectional': True, 'fc_dropout': 0.3413113266145149, 'learning_rate_model': 0.002679549863220907}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:45:27,776] Trial 121 finished with value: 1.0778253078460693 and parameters: {'hidden_size': 42, 'n_layers': 4, 'rnn_dropout': 0.23755766862952366, 'bidirectional': True, 'fc_dropout': 0.5067792100979368, 'learning_rate_model': 3.155042401877456e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 16: reducing lr to 0.00014125109077049752
Epoch 30: reducing lr to 0.00018031225678446272
Epoch 39: reducing lr to 0.00016585009132524844
Epoch 46: reducing lr to 0.0001472266786934899
Epoch 56: reducing lr to 0.00011286045880559428
Epoch 59: reducing lr to 0.0001015347283542141
Epoch 62: reducing lr to 9.005139774922622e-05
Epoch 65: reducing lr to 7.859159980704291e-05
Epoch 68: reducing lr to 6.73360119415258e-05
Epoch 71: reducing lr to 5.646220815904194e-05
Epoch 74: reducing lr to 4.614162569194693e-05
Epoch 77: reducing lr to 3.653705669815932e-05
Epoch 80: reducing lr to 2.7799942544302294e-05
Epoch 83: reducing lr to 2.0068098481755515e-05
Epoch 86: reducing lr to 1.3463428480628035e-05
Epoch 89: reducing lr to 8.090131248872324e-06
Epoch 92: reducing lr to 4.03292028217637e-06
Epoch 95: reducing lr to 1.3557921747735842e-06
Epoch 98: reducing lr to 1.0095971093037909e-07
[I 2024-06-20 22:47:05,322] Trial 122 finished with value: 1.0228769779205322 and parameters: {'hidden_size': 103, 'n_layers': 7, 'rnn_dropout': 0.7586393392581321, 'bidirectional': True, 'fc_dropout': 0.511301478975924, 'learning_rate_model': 0.0018308318127610833}. Best is trial 115 with value: 0.9733659029006958.
Epoch 7: reducing lr to 0.0012853944404174157
Epoch 12: reducing lr to 0.0027162806427774825
Epoch 15: reducing lr to 0.00359018337000677
Epoch 18: reducing lr to 0.004324888918390304
Epoch 21: reducing lr to 0.004817210068935076
Epoch 24: reducing lr to 0.004998001773685581
Epoch 27: reducing lr to 0.004979960478648535
Epoch 30: reducing lr to 0.004922588411888641
Epoch 33: reducing lr to 0.004826997165475045
Epoch 36: reducing lr to 0.004694694266869209
Epoch 39: reducing lr to 0.0045277661775607585
Epoch 42: reducing lr to 0.0043288454753782355
Epoch 45: reducing lr to 0.004101069259651594
Epoch 48: reducing lr to 0.0038480297970317407
Epoch 51: reducing lr to 0.0035737175896517768
Epoch 54: reducing lr to 0.003282458925276785
Epoch 57: reducing lr to 0.002978846232419245
Epoch 60: reducing lr to 0.0026676690109605277
Epoch 63: reducing lr to 0.0023538335684778018
Epoch 66: reducing lr to 0.0020422902020183495
Epoch 69: reducing lr to 0.0017379507688639357
Epoch 72: reducing lr to 0.0014456166861494444
Epoch 75: reducing lr to 0.0011698969117753602
Epoch 78: reducing lr to 0.000915140523634747
Epoch 81: reducing lr to 0.0006853644282229357
Epoch 84: reducing lr to 0.00048419300880796765
Epoch 87: reducing lr to 0.00031479803614892427
Epoch 90: reducing lr to 0.0001798519734092459
Epoch 93: reducing lr to 8.14823244285244e-05
Epoch 96: reducing lr to 2.124072715765507e-05
Epoch 99: reducing lr to 7.704929232965619e-08
[I 2024-06-20 22:47:44,695] Trial 123 finished with value: 1.1096078157424927 and parameters: {'hidden_size': 102, 'n_layers': 3, 'rnn_dropout': 0.47941580452642407, 'bidirectional': True, 'fc_dropout': 0.46348843223778646, 'learning_rate_model': 0.0499823563097424}. Best is trial 115 with value: 0.9733659029006958.
Epoch 10: reducing lr to 0.00013228700995751482
Epoch 13: reducing lr to 0.00018847415260211614
Epoch 16: reducing lr to 0.00024100441803647584
Epoch 19: reducing lr to 0.0002825001513256547
Epoch 22: reducing lr to 0.00030713337193698264
Epoch 25: reducing lr to 0.00031227061288257327
Epoch 28: reducing lr to 0.00031031117654578105
Epoch 35: reducing lr to 0.00029641201022040955
Epoch 38: reducing lr to 0.00028668405510988003
Epoch 41: reducing lr to 0.00027489814581318963
Epoch 44: reducing lr to 0.00026124015322136093
Epoch 47: reducing lr to 0.00024592547809147117
Epoch 50: reducing lr to 0.0002291956371464323
Epoch 53: reducing lr to 0.0002113144835442758
Epoch 56: reducing lr to 0.00019256395858893442
Epoch 59: reducing lr to 0.0001732398524076391
Epoch 62: reducing lr to 0.0001536468468281481
Epoch 65: reducing lr to 0.00013409399297897731
Epoch 68: reducing lr to 0.00011488956497498524
Epoch 71: reducing lr to 9.633654185746244e-05
Epoch 74: reducing lr to 7.872743202537607e-05
Epoch 77: reducing lr to 6.23399935410962e-05
Epoch 80: reducing lr to 4.74326176016789e-05
Epoch 83: reducing lr to 3.4240446352038756e-05
Epoch 86: reducing lr to 2.2971473905440383e-05
Epoch 89: reducing lr to 1.3803485430361177e-05
Epoch 92: reducing lr to 6.881020176846813e-06
Epoch 95: reducing lr to 2.3132699526591528e-06
Epoch 98: reducing lr to 1.7225875032318357e-07
[I 2024-06-20 22:47:56,620] Trial 124 finished with value: 1.0977712869644165 and parameters: {'hidden_size': 27, 'n_layers': 7, 'rnn_dropout': 0.042395634134391624, 'bidirectional': False, 'fc_dropout': 0.08600710094813416, 'learning_rate_model': 0.0031237886599691338}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:48:03,243] Trial 125 finished with value: 1.0782908201217651 and parameters: {'hidden_size': 24, 'n_layers': 2, 'rnn_dropout': 0.04882934489573687, 'bidirectional': True, 'fc_dropout': 0.3028644746004818, 'learning_rate_model': 5.659486653989453e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.0001379089883739877
Epoch 12: reducing lr to 0.00020515683574762032
Epoch 15: reducing lr to 0.0002711614729143637
Epoch 18: reducing lr to 0.000326652744007199
Epoch 21: reducing lr to 0.00036383706429676756
Epoch 24: reducing lr to 0.00037749200609177487
Epoch 27: reducing lr to 0.00037612937259054555
Epoch 30: reducing lr to 0.00037179614152031053
Epoch 33: reducing lr to 0.00036457626985810583
Epoch 36: reducing lr to 0.0003545836190212405
Epoch 39: reducing lr to 0.000341975776495477
Epoch 42: reducing lr to 0.00032695157716137156
Epoch 45: reducing lr to 0.00030974796169501743
Epoch 48: reducing lr to 0.00029063624891658926
Epoch 51: reducing lr to 0.00026991783580907126
Epoch 54: reducing lr to 0.00024791948076938316
Epoch 57: reducing lr to 0.0002249880434287351
Epoch 60: reducing lr to 0.00020148526794014327
Epoch 63: reducing lr to 0.00017778172077670502
Epoch 66: reducing lr to 0.0001542512908739881
Epoch 69: reducing lr to 0.00013126496386642975
Epoch 72: reducing lr to 0.00010918538400035137
Epoch 75: reducing lr to 8.83606593482644e-05
Epoch 78: reducing lr to 6.911926961322653e-05
Epoch 81: reducing lr to 5.176460606236147e-05
Epoch 84: reducing lr to 3.657041323851893e-05
Epoch 87: reducing lr to 2.3776250501803854e-05
Epoch 90: reducing lr to 1.3583965215713733e-05
Epoch 93: reducing lr to 6.15424473666454e-06
Epoch 96: reducing lr to 1.6042820848537544e-06
Epoch 99: reducing lr to 5.819424091137865e-09
[I 2024-06-20 22:48:32,226] Trial 126 finished with value: 1.0935221910476685 and parameters: {'hidden_size': 154, 'n_layers': 3, 'rnn_dropout': 0.6362409339245143, 'bidirectional': False, 'fc_dropout': 0.022792679593253776, 'learning_rate_model': 0.0037750966900208034}. Best is trial 115 with value: 0.9733659029006958.
Epoch 25: reducing lr to 2.86656473048065e-05
Epoch 28: reducing lr to 2.8485776037292034e-05
Epoch 31: reducing lr to 2.8082784890959484e-05
Epoch 34: reducing lr to 2.7463029812237996e-05
Epoch 37: reducing lr to 2.66362839587246e-05
Epoch 40: reducing lr to 2.5615585473018148e-05
Epoch 43: reducing lr to 2.441703213274509e-05
Epoch 46: reducing lr to 2.3059524421702953e-05
Epoch 49: reducing lr to 2.1564473058690404e-05
Epoch 52: reducing lr to 1.9955454932260514e-05
Epoch 55: reducing lr to 1.8257842626387886e-05
Epoch 58: reducing lr to 1.6498414869757746e-05
Epoch 61: reducing lr to 1.4704911105236482e-05
Epoch 64: reducing lr to 1.2905625247395555e-05
Epoch 67: reducing lr to 1.1128921162872318e-05
Epoch 70: reducing lr to 9.402830401026852e-06
Epoch 73: reducing lr to 7.75456904974789e-06
Epoch 76: reducing lr to 6.210128913945666e-06
Epoch 79: reducing lr to 4.793871287499187e-06
Epoch 82: reducing lr to 3.528127207915512e-06
Epoch 85: reducing lr to 2.4328619446690286e-06
Epoch 88: reducing lr to 1.5253440389721392e-06
Epoch 91: reducing lr to 8.198909651575749e-07
Epoch 94: reducing lr to 3.276245557649166e-07
Epoch 97: reducing lr to 5.630959648444977e-08
[I 2024-06-20 22:48:40,596] Trial 127 finished with value: 1.0929245948791504 and parameters: {'hidden_size': 80, 'n_layers': 2, 'rnn_dropout': 0.34710670096936047, 'bidirectional': False, 'fc_dropout': 0.7835866833438452, 'learning_rate_model': 0.00028675584665119325}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:50:27,910] Trial 128 finished with value: 1.0747781991958618 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.7113048712859863, 'bidirectional': True, 'fc_dropout': 0.7288153917255746, 'learning_rate_model': 1.4932655812821317e-05}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:52:52,458] Trial 129 finished with value: 1.0728662014007568 and parameters: {'hidden_size': 191, 'n_layers': 4, 'rnn_dropout': 0.30109057334967004, 'bidirectional': True, 'fc_dropout': 0.41336508781408976, 'learning_rate_model': 1.3316445065440956e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 18: reducing lr to 0.0005902034060447992
Epoch 24: reducing lr to 0.0006820609097505071
Epoch 27: reducing lr to 0.000679598873388129
Epoch 30: reducing lr to 0.0006717694956046823
Epoch 33: reducing lr to 0.0006587244717240759
Epoch 36: reducing lr to 0.0006406695290746292
Epoch 39: reducing lr to 0.0006178894002127145
Epoch 42: reducing lr to 0.0005907433444003388
Epoch 45: reducing lr to 0.0005596594712940918
Epoch 48: reducing lr to 0.0005251280057420079
Epoch 51: reducing lr to 0.00048769351848225657
Epoch 54: reducing lr to 0.0004479464038169024
Epoch 57: reducing lr to 0.0004065133754029269
Epoch 60: reducing lr to 0.00036404804058067484
Epoch 63: reducing lr to 0.00032121994705363417
Epoch 66: reducing lr to 0.00027870464562400386
Epoch 69: reducing lr to 0.0002371724413452561
Epoch 72: reducing lr to 0.0001972785678662369
Epoch 75: reducing lr to 0.00015965199455529608
Epoch 78: reducing lr to 0.00012488622580851799
Epoch 81: reducing lr to 9.352943568078425e-05
Epoch 84: reducing lr to 6.607623186953545e-05
Epoch 87: reducing lr to 4.295945552757934e-05
Epoch 90: reducing lr to 2.4543808937761874e-05
Epoch 93: reducing lr to 1.1119625571345652e-05
Epoch 96: reducing lr to 2.898652370471062e-06
Epoch 99: reducing lr to 1.051466422012301e-08
[I 2024-06-20 22:53:22,719] Trial 130 finished with value: 1.2248486280441284 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.5047673602716244, 'bidirectional': True, 'fc_dropout': 0.36146899953669653, 'learning_rate_model': 0.006820928234876919}. Best is trial 115 with value: 0.9733659029006958.
Epoch 8: reducing lr to 0.0002502364211943445
Epoch 11: reducing lr to 0.00039055245021268604
Epoch 14: reducing lr to 0.0005350489778998474
Epoch 17: reducing lr to 0.0006634319656800283
Epoch 20: reducing lr to 0.0007576703668102593
Epoch 23: reducing lr to 0.0008045286768678893
Epoch 26: reducing lr to 0.0008070401610030597
Epoch 29: reducing lr to 0.0007998597188532908
Epoch 32: reducing lr to 0.0007864387979843171
Epoch 35: reducing lr to 0.0007669890541391392
Epoch 38: reducing lr to 0.0007418172161849857
Epoch 41: reducing lr to 0.0007113202622426793
Epoch 44: reducing lr to 0.0006759791476513491
Epoch 47: reducing lr to 0.0006363512385676787
Epoch 50: reducing lr to 0.0005930614782344449
Epoch 53: reducing lr to 0.0005467926071518035
Epoch 56: reducing lr to 0.0004982741702996135
Epoch 61: reducing lr to 0.00041450057308690887
Epoch 64: reducing lr to 0.0003637824821113945
Epoch 69: reducing lr to 0.00028105787425751076
Epoch 72: reducing lr to 0.00023378219917353677
Epoch 75: reducing lr to 0.00018919335634515417
Epoch 78: reducing lr to 0.00014799466983050322
Epoch 81: reducing lr to 0.00011083574560283374
Epoch 84: reducing lr to 7.830271157500775e-05
Epoch 87: reducing lr to 5.090850008271008e-05
Epoch 90: reducing lr to 2.908529644971682e-05
Epoch 93: reducing lr to 1.3177156282978032e-05
Epoch 96: reducing lr to 3.4350073256198684e-06
Epoch 99: reducing lr to 1.2460255320936864e-08
[I 2024-06-20 22:54:05,094] Trial 131 finished with value: 1.0989677906036377 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.4228267921420521, 'bidirectional': False, 'fc_dropout': 0.11377700245822933, 'learning_rate_model': 0.008083045312026036}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 22:54:55,362] Trial 132 finished with value: 1.0756254196166992 and parameters: {'hidden_size': 104, 'n_layers': 4, 'rnn_dropout': 0.7087983769322811, 'bidirectional': True, 'fc_dropout': 0.2151136177701167, 'learning_rate_model': 1.941101032854524e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 10: reducing lr to 0.00020844897946068955
Epoch 15: reducing lr to 0.0003535608111714274
Epoch 22: reducing lr to 0.00048396012548129665
Epoch 27: reducing lr to 0.0004904258877532783
Epoch 30: reducing lr to 0.0004847758937636477
Epoch 33: reducing lr to 0.000475362079721382
Epoch 36: reducing lr to 0.0004623329067431443
Epoch 39: reducing lr to 0.00044589384929659355
Epoch 42: reducing lr to 0.00042630416331843415
Epoch 46: reducing lr to 0.0003958241443054246
Epoch 49: reducing lr to 0.00037016110739126616
Epoch 52: reducing lr to 0.0003425417943725377
Epoch 55: reducing lr to 0.00031340173380381376
Epoch 58: reducing lr to 0.0002832005911653349
Epoch 61: reducing lr to 0.00025241452290487946
Epoch 64: reducing lr to 0.00022152920315516097
Epoch 67: reducing lr to 0.00019103150679857555
Epoch 70: reducing lr to 0.0001614026043847016
Epoch 73: reducing lr to 0.00013310966880500426
Epoch 76: reducing lr to 0.00010659885774033607
Epoch 79: reducing lr to 8.228834062591548e-05
Epoch 82: reducing lr to 6.056143689414e-05
Epoch 85: reducing lr to 4.176085680915069e-05
Epoch 88: reducing lr to 2.6183020428178354e-05
Epoch 91: reducing lr to 1.407369179746848e-05
Epoch 94: reducing lr to 5.6237807453236e-06
Epoch 97: reducing lr to 9.66572312465549e-07
[I 2024-06-20 22:55:01,181] Trial 133 finished with value: 1.0998613834381104 and parameters: {'hidden_size': 93, 'n_layers': 1, 'rnn_dropout': 0.23292315244194245, 'bidirectional': False, 'fc_dropout': 0.08750633135486768, 'learning_rate_model': 0.004922256224784005}. Best is trial 115 with value: 0.9733659029006958.
Epoch 28: reducing lr to 5.805220625328731e-06
Epoch 31: reducing lr to 5.72309358369743e-06
Epoch 34: reducing lr to 5.59679142640545e-06
Epoch 37: reducing lr to 5.428305861032829e-06
Epoch 40: reducing lr to 5.220293978410861e-06
Epoch 43: reducing lr to 4.976036403598753e-06
Epoch 46: reducing lr to 4.699384935410991e-06
Epoch 49: reducing lr to 4.394702942646458e-06
Epoch 52: reducing lr to 4.066795245771702e-06
Epoch 55: reducing lr to 3.7208326165997952e-06
Epoch 58: reducing lr to 3.3622723903242772e-06
Epoch 61: reducing lr to 2.996767689600199e-06
Epoch 64: reducing lr to 2.6300846349020884e-06
Epoch 67: reducing lr to 2.2680036024921823e-06
Epoch 70: reducing lr to 1.9162372444776906e-06
Epoch 73: reducing lr to 1.5803320270860265e-06
Epoch 76: reducing lr to 1.2655849154325814e-06
Epoch 79: reducing lr to 9.769605868180889e-07
Epoch 82: reducing lr to 7.190099651616044e-07
Epoch 85: reducing lr to 4.958018458503838e-07
Epoch 88: reducing lr to 3.108554481426402e-07
Epoch 91: reducing lr to 1.6708858256915152e-07
Epoch 94: reducing lr to 6.676780811590854e-08
Epoch 97: reducing lr to 1.1475538896589592e-08
[I 2024-06-20 22:55:36,765] Trial 134 finished with value: 1.1069185733795166 and parameters: {'hidden_size': 108, 'n_layers': 6, 'rnn_dropout': 0.11488559829936743, 'bidirectional': False, 'fc_dropout': 0.6375698015839202, 'learning_rate_model': 5.8439024207513246e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.00015850546406807067
Epoch 15: reducing lr to 0.00031165898327900263
Epoch 18: reducing lr to 0.00037543778247115157
Epoch 21: reducing lr to 0.0004181755185175538
Epoch 27: reducing lr to 0.00043230366240103527
Epoch 30: reducing lr to 0.0004273232705513092
Epoch 33: reducing lr to 0.0004190251231874386
Epoch 36: reducing lr to 0.0004075400867380936
Epoch 39: reducing lr to 0.0003930492841152515
Epoch 42: reducing lr to 0.0003757812458547897
Epoch 46: reducing lr to 0.00034891352908367575
Epoch 49: reducing lr to 0.0003262919156587643
Epoch 52: reducing lr to 0.00030194587180350273
Epoch 55: reducing lr to 0.0002762593099375337
Epoch 58: reducing lr to 0.0002496374188478882
Epoch 61: reducing lr to 0.0002224999238822502
Epoch 64: reducing lr to 0.00019527494009642822
Epoch 67: reducing lr to 0.000168391640990531
Epoch 70: reducing lr to 0.00014227417177388896
Epoch 73: reducing lr to 0.00011733433891308186
Epoch 76: reducing lr to 9.396542425610605e-05
Epoch 79: reducing lr to 7.253603839808592e-05
Epoch 82: reducing lr to 5.3384072136862655e-05
Epoch 85: reducing lr to 3.681161984801923e-05
Epoch 88: reducing lr to 2.307997172758744e-05
Epoch 91: reducing lr to 1.2405765395911983e-05
Epoch 94: reducing lr to 4.957285236065895e-06
Epoch 97: reducing lr to 8.520201748902882e-07
[I 2024-06-20 22:56:07,704] Trial 135 finished with value: 1.0952446460723877 and parameters: {'hidden_size': 158, 'n_layers': 3, 'rnn_dropout': 0.17351319008943628, 'bidirectional': False, 'fc_dropout': 0.3048651930347619, 'learning_rate_model': 0.004338901037624098}. Best is trial 115 with value: 0.9733659029006958.
Epoch 58: reducing lr to 1.1067134688158814e-05
Epoch 61: reducing lr to 9.864052581037081e-06
Epoch 64: reducing lr to 8.65709184641971e-06
Epoch 67: reducing lr to 7.465278962597542e-06
Epoch 70: reducing lr to 6.30741748947222e-06
Epoch 73: reducing lr to 5.201763975489589e-06
Epoch 76: reducing lr to 4.165753720222412e-06
Epoch 79: reducing lr to 3.215728276642e-06
Epoch 82: reducing lr to 2.366667301992268e-06
Epoch 85: reducing lr to 1.6319634966085397e-06
Epoch 88: reducing lr to 1.0232005958359503e-06
Epoch 91: reducing lr to 5.499827597156694e-07
Epoch 94: reducing lr to 2.1977051216265126e-07
Epoch 97: reducing lr to 3.777247047361085e-08
[I 2024-06-20 22:56:12,994] Trial 136 finished with value: 1.0921385288238525 and parameters: {'hidden_size': 29, 'n_layers': 2, 'rnn_dropout': 0.013260706320747318, 'bidirectional': False, 'fc_dropout': 0.3221254817399354, 'learning_rate_model': 0.00019235578705946124}. Best is trial 115 with value: 0.9733659029006958.
Epoch 58: reducing lr to 1.0996733623924407e-06
Epoch 65: reducing lr to 8.204676189851584e-07
Epoch 68: reducing lr to 7.029633895385975e-07
Epoch 71: reducing lr to 5.89444846581964e-07
Epoch 74: reducing lr to 4.817017322528599e-07
Epoch 77: reducing lr to 3.8143353726689374e-07
Epoch 80: reducing lr to 2.902212542211655e-07
Epoch 83: reducing lr to 2.095036240426563e-07
Epoch 86: reducing lr to 1.4055327968890572e-07
Epoch 89: reducing lr to 8.445801764229841e-08
Epoch 92: reducing lr to 4.210221588055368e-08
Epoch 95: reducing lr to 1.4153975491099774e-08
Epoch 98: reducing lr to 1.0539825355872516e-09
[I 2024-06-20 22:57:01,826] Trial 137 finished with value: 1.1068862676620483 and parameters: {'hidden_size': 119, 'n_layers': 7, 'rnn_dropout': 0.6675892112144237, 'bidirectional': False, 'fc_dropout': 0.03178296534646785, 'learning_rate_model': 1.9113215939952844e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 18: reducing lr to 5.2929575942945504e-05
Epoch 23: reducing lr to 6.0884493678750056e-05
Epoch 26: reducing lr to 6.107455581618356e-05
Epoch 29: reducing lr to 6.0531159915890467e-05
Epoch 32: reducing lr to 5.9515501934634666e-05
Epoch 35: reducing lr to 5.8043599390644316e-05
Epoch 38: reducing lr to 5.613866467188619e-05
Epoch 41: reducing lr to 5.383073997894651e-05
Epoch 44: reducing lr to 5.11562226748366e-05
Epoch 47: reducing lr to 4.81572926808185e-05
Epoch 50: reducing lr to 4.4881243964165756e-05
Epoch 53: reducing lr to 4.1379744427914214e-05
Epoch 56: reducing lr to 3.7708004008007385e-05
Epoch 59: reducing lr to 3.392394452629025e-05
Epoch 62: reducing lr to 3.008722898339107e-05
Epoch 65: reducing lr to 2.6258375979353985e-05
Epoch 68: reducing lr to 2.2497751958885622e-05
Epoch 71: reducing lr to 1.8864686481821808e-05
Epoch 74: reducing lr to 1.5416458739769607e-05
Epoch 77: reducing lr to 1.2207459503493464e-05
Epoch 80: reducing lr to 9.288287111153986e-06
Epoch 83: reducing lr to 6.704987255869762e-06
Epoch 86: reducing lr to 4.498289484925236e-06
Epoch 89: reducing lr to 2.703007809699441e-06
Epoch 92: reducing lr to 1.347446003442459e-06
Epoch 95: reducing lr to 4.529860794598656e-07
Epoch 98: reducing lr to 3.373182445561354e-08
[I 2024-06-20 22:57:19,727] Trial 138 finished with value: 1.0921627283096313 and parameters: {'hidden_size': 103, 'n_layers': 3, 'rnn_dropout': 0.7498016497027592, 'bidirectional': False, 'fc_dropout': 0.7162082925849766, 'learning_rate_model': 0.0006117023983794077}. Best is trial 115 with value: 0.9733659029006958.
Epoch 11: reducing lr to 0.00023432630142562402
Epoch 15: reducing lr to 0.0003483506210019933
Epoch 18: reducing lr to 0.0004196381034663031
Epoch 21: reducing lr to 0.00046740735669092704
Epoch 27: reducing lr to 0.00048319880811528856
Epoch 30: reducing lr to 0.0004776320743236551
Epoch 33: reducing lr to 0.0004683569853884435
Epoch 36: reducing lr to 0.00045551981465372977
Epoch 39: reducing lr to 0.00043932300864680835
Epoch 46: reducing lr to 0.0003899911475469551
Epoch 49: reducing lr to 0.00036470628971380103
Epoch 52: reducing lr to 0.00033749398411396584
Epoch 55: reducing lr to 0.0003087833412078184
Epoch 58: reducing lr to 0.0002790272526915974
Epoch 61: reducing lr to 0.00024869485821267534
Epoch 64: reducing lr to 0.0002182646748475767
Epoch 67: reducing lr to 0.0001882164027278602
Epoch 70: reducing lr to 0.00015902412171321997
Epoch 73: reducing lr to 0.00013114812027940093
Epoch 76: reducing lr to 0.00010502798137869574
Epoch 79: reducing lr to 8.107571216189637e-05
Epoch 82: reducing lr to 5.966898333825193e-05
Epoch 85: reducing lr to 4.114545487901803e-05
Epoch 88: reducing lr to 2.5797178696485967e-05
Epoch 91: reducing lr to 1.3866297175853421e-05
Epoch 94: reducing lr to 5.5409068344474365e-06
Epoch 97: reducing lr to 9.523285801249851e-07
[I 2024-06-20 22:57:42,982] Trial 139 finished with value: 1.0966203212738037 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.21904776332648687, 'bidirectional': False, 'fc_dropout': 0.176102939221585, 'learning_rate_model': 0.004849720213485595}. Best is trial 115 with value: 0.9733659029006958.
Epoch 19: reducing lr to 0.00011469511484826889
Epoch 24: reducing lr to 0.00012681994472858098
Epoch 29: reducing lr to 0.00012550085732213658
Epoch 32: reducing lr to 0.00012339506672485095
Epoch 35: reducing lr to 0.0001203433321897437
Epoch 40: reducing lr to 0.00011329216728393226
Epoch 45: reducing lr to 0.00010406106287824947
Epoch 48: reducing lr to 9.764040676072757e-05
Epoch 51: reducing lr to 9.06799732607913e-05
Epoch 54: reducing lr to 8.328953816486321e-05
Epoch 58: reducing lr to 7.296890322156195e-05
Epoch 63: reducing lr to 5.9726478014934056e-05
Epoch 66: reducing lr to 5.182133625949031e-05
Epoch 69: reducing lr to 4.4098988041332504e-05
Epoch 72: reducing lr to 3.6681265141087596e-05
Epoch 75: reducing lr to 2.9685115853826876e-05
Epoch 78: reducing lr to 2.3220894245633846e-05
Epoch 81: reducing lr to 1.7390525822497636e-05
Epoch 84: reducing lr to 1.2285976155168664e-05
Epoch 87: reducing lr to 7.987726165937896e-06
Epoch 90: reducing lr to 4.563587281456133e-06
Epoch 93: reducing lr to 2.067543059866019e-06
Epoch 96: reducing lr to 5.389649636202863e-07
Epoch 99: reducing lr to 1.955058728862584e-09
[I 2024-06-20 22:57:46,804] Trial 140 finished with value: 1.0921127796173096 and parameters: {'hidden_size': 42, 'n_layers': 1, 'rnn_dropout': 0.6961533969643021, 'bidirectional': False, 'fc_dropout': 0.45723367418026584, 'learning_rate_model': 0.0012682587865373047}. Best is trial 115 with value: 0.9733659029006958.
Epoch 32: reducing lr to 2.2169546711493985e-05
Epoch 35: reducing lr to 2.1621262463810084e-05
Epoch 38: reducing lr to 2.0911673569201795e-05
Epoch 41: reducing lr to 2.005197075861421e-05
Epoch 49: reducing lr to 1.7135369150442058e-05
Epoch 62: reducing lr to 1.1207504039859837e-05
Epoch 65: reducing lr to 9.781254865020122e-06
Epoch 68: reducing lr to 8.380421012056809e-06
Epoch 74: reducing lr to 5.742636641668775e-06
Epoch 83: reducing lr to 2.4976102584539023e-06
Epoch 86: reducing lr to 1.6756145141378368e-06
Epoch 89: reducing lr to 1.0068714192224959e-06
Epoch 92: reducing lr to 5.019241398206126e-07
Epoch 95: reducing lr to 1.68737483878932e-07
Epoch 98: reducing lr to 1.2565117215240308e-08
[I 2024-06-20 22:58:14,903] Trial 141 finished with value: 1.0773828029632568 and parameters: {'hidden_size': 30, 'n_layers': 7, 'rnn_dropout': 0.7609084947349537, 'bidirectional': True, 'fc_dropout': 0.4222774368943676, 'learning_rate_model': 0.0002278593719884827}. Best is trial 115 with value: 0.9733659029006958.
Epoch 24: reducing lr to 2.0101087137139053e-05
Epoch 27: reducing lr to 2.0028528210586475e-05
Epoch 33: reducing lr to 1.9413336574786303e-05
Epoch 65: reducing lr to 8.629140606756298e-06
Epoch 71: reducing lr to 6.1993945201336134e-06
Epoch 76: reducing lr to 4.3533962679855404e-06
Epoch 84: reducing lr to 1.947339417221819e-06
Epoch 87: reducing lr to 1.266062526937358e-06
Epoch 90: reducing lr to 7.233331145098311e-07
Epoch 93: reducing lr to 3.2770762749582083e-07
Epoch 96: reducing lr to 8.542648177921005e-08
Epoch 99: reducing lr to 3.098787493667704e-10
[I 2024-06-20 23:00:12,600] Trial 142 finished with value: 1.0477745532989502 and parameters: {'hidden_size': 118, 'n_layers': 7, 'rnn_dropout': 0.24936233417518514, 'bidirectional': True, 'fc_dropout': 0.6195818128120637, 'learning_rate_model': 0.00020102027670166017}. Best is trial 115 with value: 0.9733659029006958.
Epoch 24: reducing lr to 1.9377372128192638e-05
Epoch 27: reducing lr to 1.930742559687127e-05
Epoch 76: reducing lr to 4.196657570345086e-06
Epoch 84: reducing lr to 1.8772278479250255e-06
Epoch 87: reducing lr to 1.2204794971858857e-06
Epoch 90: reducing lr to 6.972903921502292e-07
Epoch 93: reducing lr to 3.159089159660965e-07
Epoch 96: reducing lr to 8.235080599096562e-08
Epoch 99: reducing lr to 2.987219447470697e-10
[I 2024-06-20 23:03:31,243] Trial 143 finished with value: 1.0408239364624023 and parameters: {'hidden_size': 180, 'n_layers': 6, 'rnn_dropout': 0.6101056763560886, 'bidirectional': True, 'fc_dropout': 0.0331800636611991, 'learning_rate_model': 0.0001937827879848057}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:03:40,529] Trial 144 finished with value: 1.1085680723190308 and parameters: {'hidden_size': 22, 'n_layers': 5, 'rnn_dropout': 0.5362800255489347, 'bidirectional': False, 'fc_dropout': 0.428806795791651, 'learning_rate_model': 1.4435761775167338e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 13: reducing lr to 0.00017799762328930097
Epoch 16: reducing lr to 0.00022760793997718805
Epoch 22: reducing lr to 0.0002900610480686038
Epoch 25: reducing lr to 0.0002949127302009014
Epoch 29: reducing lr to 0.0002919327785716783
Epoch 32: reducing lr to 0.0002870344112356069
Epoch 35: reducing lr to 0.00027993564425260345
Epoch 38: reducing lr to 0.00027074842751633984
Epoch 41: reducing lr to 0.00025961765009062604
Epoch 44: reducing lr to 0.00024671885104214007
Epoch 47: reducing lr to 0.00023225545785568596
Epoch 50: reducing lr to 0.00021645556229912404
Epoch 53: reducing lr to 0.00019956835098175084
Epoch 56: reducing lr to 0.00018186009321060004
Epoch 59: reducing lr to 0.00016361013731493927
Epoch 62: reducing lr to 0.00014510622907025906
Epoch 65: reducing lr to 0.00012664024068073822
Epoch 68: reducing lr to 0.00010850331052800011
Epoch 71: reducing lr to 9.098157625220191e-05
Epoch 74: reducing lr to 7.435128687258308e-05
Epoch 77: reducing lr to 5.887476098439242e-05
Epoch 80: reducing lr to 4.4796026844660856e-05
Epoch 83: reducing lr to 3.233715598071497e-05
Epoch 86: reducing lr to 2.1694580939448554e-05
Epoch 89: reducing lr to 1.303620451818489e-05
Epoch 92: reducing lr to 6.498531604331526e-06
Epoch 95: reducing lr to 2.1846844668888063e-06
Epoch 98: reducing lr to 1.6268357079731074e-07
[I 2024-06-20 23:03:45,727] Trial 145 finished with value: 1.0926499366760254 and parameters: {'hidden_size': 40, 'n_layers': 2, 'rnn_dropout': 0.2685310409317198, 'bidirectional': False, 'fc_dropout': 0.40964882201913505, 'learning_rate_model': 0.0029501496595471795}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.0007156104201777097
Epoch 17: reducing lr to 0.0016078056193672859
Epoch 23: reducing lr to 0.0019497488733218264
Epoch 26: reducing lr to 0.001955835372788791
Epoch 30: reducing lr to 0.0019292520102626239
Epoch 33: reducing lr to 0.0018917880606336966
Epoch 36: reducing lr to 0.0018399361462054186
Epoch 39: reducing lr to 0.0017745139892178779
Epoch 44: reducing lr to 0.0016382133035373354
Epoch 47: reducing lr to 0.0015421763650048464
Epoch 50: reducing lr to 0.001437265050016438
Epoch 53: reducing lr to 0.0013251339577917832
Epoch 56: reducing lr to 0.0012075511166727124
Epoch 59: reducing lr to 0.0010863713997157193
Epoch 62: reducing lr to 0.0009635054390247312
Epoch 65: reducing lr to 0.0008408912661923842
Epoch 68: reducing lr to 0.0007204620402291517
Epoch 71: reducing lr to 0.0006041177152194833
Epoch 74: reducing lr to 0.0004936925847996195
Epoch 77: reducing lr to 0.00039092844458301566
Epoch 80: reducing lr to 0.0002974456423275276
Epoch 83: reducing lr to 0.00021471873309397393
Epoch 86: reducing lr to 0.00014405202909931574
Epoch 89: reducing lr to 8.656040500803209e-05
Epoch 92: reducing lr to 4.315025334588399e-05
Epoch 95: reducing lr to 1.4506306034463983e-05
Epoch 98: reducing lr to 1.0802189975404191e-06
[I 2024-06-20 23:04:17,372] Trial 146 finished with value: 1.1305310726165771 and parameters: {'hidden_size': 95, 'n_layers': 6, 'rnn_dropout': 0.36324724484732107, 'bidirectional': False, 'fc_dropout': 0.5249612381952792, 'learning_rate_model': 0.01958899532517217}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:04:28,306] Trial 147 finished with value: 1.0970633029937744 and parameters: {'hidden_size': 192, 'n_layers': 1, 'rnn_dropout': 0.18805749127281166, 'bidirectional': False, 'fc_dropout': 0.6910633164011616, 'learning_rate_model': 2.1207815222486866e-05}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:04:39,000] Trial 148 finished with value: 1.0531294345855713 and parameters: {'hidden_size': 103, 'n_layers': 1, 'rnn_dropout': 0.6183084817336942, 'bidirectional': True, 'fc_dropout': 0.38915259453631584, 'learning_rate_model': 9.070770449331516e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 49: reducing lr to 4.11724264609275e-05
Epoch 52: reducing lr to 3.810037455850291e-05
Epoch 55: reducing lr to 3.485917234445024e-05
Epoch 59: reducing lr to 3.036309613977172e-05
Epoch 62: reducing lr to 2.6929103881008185e-05
Epoch 65: reducing lr to 2.3502148864720612e-05
Epoch 68: reducing lr to 2.0136261133400753e-05
Epoch 71: reducing lr to 1.688454268194869e-05
Epoch 74: reducing lr to 1.379826035523931e-05
Epoch 77: reducing lr to 1.0926095762233431e-05
Epoch 80: reducing lr to 8.313336154385323e-06
Epoch 83: reducing lr to 6.0011940093860454e-06
Epoch 86: reducing lr to 4.026123671716929e-06
Epoch 89: reducing lr to 2.419284877937884e-06
Epoch 92: reducing lr to 1.2060104777605689e-06
Epoch 95: reducing lr to 4.0543810788156105e-07
Epoch 98: reducing lr to 3.019114207435331e-08
[I 2024-06-20 23:08:54,898] Trial 149 finished with value: 1.029960036277771 and parameters: {'hidden_size': 192, 'n_layers': 7, 'rnn_dropout': 0.49114720190776073, 'bidirectional': True, 'fc_dropout': 0.7182805741341455, 'learning_rate_model': 0.0005474946675652392}. Best is trial 115 with value: 0.9733659029006958.
Epoch 8: reducing lr to 8.137350050972245e-05
Epoch 11: reducing lr to 0.00012700237581232486
Epoch 14: reducing lr to 0.00017399069275389595
Epoch 17: reducing lr to 0.00021573910440280086
Epoch 20: reducing lr to 0.0002463841581715749
Epoch 23: reducing lr to 0.0002616218469906534
Epoch 26: reducing lr to 0.00026243854767146567
Epoch 29: reducing lr to 0.000260103564977318
Epoch 32: reducing lr to 0.00025573926298658493
Epoch 35: reducing lr to 0.0002494144692849105
Epoch 38: reducing lr to 0.00024122892795237148
Epoch 47: reducing lr to 0.00020693281812775465
Epoch 50: reducing lr to 0.00019285557342560824
Epoch 53: reducing lr to 0.0001778095621908828
Epoch 56: reducing lr to 0.00016203202258622098
Epoch 63: reducing lr to 0.00012378457156948515
Epoch 68: reducing lr to 9.667327532815715e-05
Epoch 71: reducing lr to 8.10619227009585e-05
Epoch 74: reducing lr to 6.624482139631267e-05
Epoch 77: reducing lr to 5.245568960824333e-05
Epoch 80: reducing lr to 3.991194937452052e-05
Epoch 83: reducing lr to 2.8811459929109713e-05
Epoch 86: reducing lr to 1.93292369244999e-05
Epoch 89: reducing lr to 1.1614876840973813e-05
Epoch 92: reducing lr to 5.790001539650313e-06
Epoch 95: reducing lr to 1.9464899452831946e-06
Epoch 98: reducing lr to 1.449463021406961e-07
[I 2024-06-20 23:09:15,693] Trial 150 finished with value: 1.094542145729065 and parameters: {'hidden_size': 59, 'n_layers': 7, 'rnn_dropout': 0.547374440786582, 'bidirectional': False, 'fc_dropout': 0.561104235706677, 'learning_rate_model': 0.002628497037637164}. Best is trial 115 with value: 0.9733659029006958.
Epoch 12: reducing lr to 0.001273330530936036
Epoch 16: reducing lr to 0.0018077010066505546
Epoch 19: reducing lr to 0.0021189479101293056
Epoch 22: reducing lr to 0.002303714222958488
Epoch 25: reducing lr to 0.0023422471083902616
Epoch 28: reducing lr to 0.002327549971020975
Epoch 31: reducing lr to 0.002294621886852228
Epoch 34: reducing lr to 0.0022439821951819065
Epoch 37: reducing lr to 0.0021764294528986124
Epoch 40: reducing lr to 0.0020930289962034178
Epoch 43: reducing lr to 0.0019950961616277494
Epoch 46: reducing lr to 0.001884175292582075
Epoch 49: reducing lr to 0.0017620158417705838
Epoch 52: reducing lr to 0.0016305442578951345
Epoch 55: reducing lr to 0.0014918337144938982
Epoch 58: reducing lr to 0.0013480722800644192
Epoch 61: reducing lr to 0.0012015265222913999
Epoch 64: reducing lr to 0.0010545083142989794
Epoch 67: reducing lr to 0.0009093352449386397
Epoch 70: reducing lr to 0.0007682977496829928
Epoch 73: reducing lr to 0.0006336196333002087
Epoch 76: reducing lr to 0.0005074246653757247
Epoch 79: reducing lr to 0.0003917033877430476
Epoch 82: reducing lr to 0.00028828045161176233
Epoch 85: reducing lr to 0.00019878720317814956
Epoch 88: reducing lr to 0.00012463464112961973
Epoch 91: reducing lr to 6.699263483973816e-05
Epoch 94: reducing lr to 2.676994034770501e-05
Epoch 97: reducing lr to 4.601012080344968e-06
[I 2024-06-20 23:09:58,874] Trial 151 finished with value: 1.2363544702529907 and parameters: {'hidden_size': 151, 'n_layers': 2, 'rnn_dropout': 0.4803911642196355, 'bidirectional': True, 'fc_dropout': 0.10440853707141279, 'learning_rate_model': 0.023430590821514066}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:10:29,313] Trial 152 finished with value: 1.105417013168335 and parameters: {'hidden_size': 130, 'n_layers': 4, 'rnn_dropout': 0.19863258319767887, 'bidirectional': False, 'fc_dropout': 0.3373327478797674, 'learning_rate_model': 1.8101769159892324e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 27: reducing lr to 0.00012807931493326042
Epoch 31: reducing lr to 0.00012589187962874342
Epoch 40: reducing lr to 0.00011483170972929851
Epoch 47: reducing lr to 0.0001012026084307509
Epoch 50: reducing lr to 9.431798811646297e-05
Epoch 53: reducing lr to 8.695958263390425e-05
Epoch 56: reducing lr to 7.924341572979588e-05
Epoch 59: reducing lr to 7.129121018233941e-05
Epoch 62: reducing lr to 6.322834785904151e-05
Epoch 65: reducing lr to 5.518200867060931e-05
Epoch 68: reducing lr to 4.727905277312534e-05
Epoch 71: reducing lr to 3.964416130787034e-05
Epoch 74: reducing lr to 3.239770656482871e-05
Epoch 77: reducing lr to 2.5653990814113616e-05
Epoch 80: reducing lr to 1.951934652416451e-05
Epoch 83: reducing lr to 1.4090538774395021e-05
Epoch 86: reducing lr to 9.45316075736083e-06
Epoch 89: reducing lr to 5.680374159804731e-06
Epoch 92: reducing lr to 2.831659395219365e-06
Epoch 95: reducing lr to 9.519507902573232e-07
Epoch 98: reducing lr to 7.088746962298151e-08
[I 2024-06-20 23:11:25,737] Trial 153 finished with value: 0.9809659719467163 and parameters: {'hidden_size': 181, 'n_layers': 2, 'rnn_dropout': 0.6609820283939132, 'bidirectional': True, 'fc_dropout': 0.4787717544837091, 'learning_rate_model': 0.0012854933251677597}. Best is trial 115 with value: 0.9733659029006958.
Epoch 33: reducing lr to 7.813848354628584e-06
Epoch 41: reducing lr to 7.120243194997541e-06
Epoch 47: reducing lr to 6.369810922796333e-06
Epoch 65: reducing lr to 3.473220374674498e-06
Epoch 74: reducing lr to 2.039149665706548e-06
Epoch 87: reducing lr to 5.095888877656923e-07
[I 2024-06-20 23:13:26,484] Trial 154 finished with value: 1.0706068277359009 and parameters: {'hidden_size': 131, 'n_layers': 6, 'rnn_dropout': 0.22571155872388848, 'bidirectional': True, 'fc_dropout': 0.09208243277352271, 'learning_rate_model': 8.091045824612664e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 6: reducing lr to 0.0014935228319368297
Epoch 9: reducing lr to 0.0026117574486201724
Epoch 12: reducing lr to 0.003885315237366055
Epoch 15: reducing lr to 0.005135328777427883
Epoch 18: reducing lr to 0.006186237368078167
Epoch 21: reducing lr to 0.006890444009234829
Epoch 24: reducing lr to 0.007149044963125316
Epoch 27: reducing lr to 0.007123239044029425
Epoch 30: reducing lr to 0.007041175150604382
Epoch 33: reducing lr to 0.00690444328262266
Epoch 36: reducing lr to 0.00671519977817559
Epoch 39: reducing lr to 0.00647642907137876
Epoch 42: reducing lr to 0.006191896750584755
Epoch 45: reducing lr to 0.005866090061932961
Epoch 48: reducing lr to 0.005504147314085469
Epoch 51: reducing lr to 0.005111776444027253
Epoch 54: reducing lr to 0.004695165690009615
Epoch 57: reducing lr to 0.004260883972855775
Epoch 60: reducing lr to 0.0038157821004592162
Epoch 63: reducing lr to 0.0033668779601797967
Epoch 66: reducing lr to 0.0029212523610636803
Epoch 69: reducing lr to 0.002485931128660722
Epoch 72: reducing lr to 0.0020677821170730823
Epoch 75: reducing lr to 0.0016733978904405339
Epoch 78: reducing lr to 0.0013089992855721676
Epoch 81: reducing lr to 0.0009803320077414342
Epoch 84: reducing lr to 0.0006925803045977174
Epoch 87: reducing lr to 0.0004502810156212997
Epoch 90: reducing lr to 0.00025725678037552405
Epoch 93: reducing lr to 0.00011655073915868702
Epoch 96: reducing lr to 3.0382324852111507e-05
Epoch 99: reducing lr to 1.1020981587755996e-07
[I 2024-06-20 23:13:41,666] Trial 155 finished with value: 1.2502963542938232 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.10513252842996845, 'bidirectional': False, 'fc_dropout': 0.48174692311416867, 'learning_rate_model': 0.07149379468062945}. Best is trial 115 with value: 0.9733659029006958.
Epoch 18: reducing lr to 0.00537027127152276
Epoch 23: reducing lr to 0.006177382710125015
Epoch 26: reducing lr to 0.006196666545641922
Epoch 29: reducing lr to 0.006141533222912248
Epoch 32: reducing lr to 0.0060384838638108205
Epoch 35: reducing lr to 0.005889143616781576
Epoch 38: reducing lr to 0.0056958676267131825
Epoch 41: reducing lr to 0.005461703995992014
Epoch 44: reducing lr to 0.00519034562616615
Epoch 47: reducing lr to 0.004886072121131015
Epoch 50: reducing lr to 0.004553681959416635
Epoch 53: reducing lr to 0.0041984174020914195
Epoch 56: reducing lr to 0.0038258800873249175
Epoch 59: reducing lr to 0.0034419468031001584
Epoch 62: reducing lr to 0.003052670998600114
Epoch 65: reducing lr to 0.0026641929327144475
Epoch 68: reducing lr to 0.002282637426547363
Epoch 71: reducing lr to 0.0019140240981491163
Epoch 74: reducing lr to 0.0015641645337956859
Epoch 77: reducing lr to 0.0012385772585927172
Epoch 80: reducing lr to 0.0009423960148188852
Epoch 83: reducing lr to 0.000680292630247735
Epoch 86: reducing lr to 0.0004563995528308523
Epoch 89: reducing lr to 0.0002742490361679402
Epoch 92: reducing lr to 0.00013671280060915876
Epoch 95: reducing lr to 4.59602799679576e-05
Epoch 98: reducing lr to 3.42245416825801e-06
[I 2024-06-20 23:14:30,527] Trial 156 finished with value: 1.0437301397323608 and parameters: {'hidden_size': 79, 'n_layers': 5, 'rnn_dropout': 0.4791882311787358, 'bidirectional': True, 'fc_dropout': 0.6002537476597487, 'learning_rate_model': 0.06206374712466089}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.00016209485726464482
Epoch 22: reducing lr to 0.00022187823136211315
Epoch 25: reducing lr to 0.0002255894592495293
Epoch 28: reducing lr to 0.00022417393000849546
Epoch 31: reducing lr to 0.0002210025188131772
Epoch 34: reducing lr to 0.00021612524492540118
Epoch 37: reducing lr to 0.0002096190199639434
Epoch 40: reducing lr to 0.00020158644993337852
Epoch 43: reducing lr to 0.00019215421918558094
Epoch 46: reducing lr to 0.00018147106847194908
Epoch 49: reducing lr to 0.0001697054932890115
Epoch 52: reducing lr to 0.0001570430361951804
Epoch 55: reducing lr to 0.0001436833712964589
Epoch 58: reducing lr to 0.00012983723860716657
Epoch 61: reducing lr to 0.00011572293865439675
Epoch 64: reducing lr to 0.000101563135479898
Epoch 67: reducing lr to 8.758104362576354e-05
Epoch 70: reducing lr to 7.399726240359533e-05
Epoch 73: reducing lr to 6.1025973704505926e-05
Epoch 76: reducing lr to 4.887172470485141e-05
Epoch 79: reducing lr to 3.7726230981620106e-05
Epoch 82: reducing lr to 2.776523064468733e-05
Epoch 85: reducing lr to 1.914584396754998e-05
Epoch 88: reducing lr to 1.2003968836368133e-05
Epoch 91: reducing lr to 6.452279186539012e-06
Epoch 94: reducing lr to 2.5783002764944234e-06
Epoch 97: reducing lr to 4.4313848162626755e-07
[I 2024-06-20 23:14:43,476] Trial 157 finished with value: 1.0926249027252197 and parameters: {'hidden_size': 114, 'n_layers': 2, 'rnn_dropout': 0.6023407319839538, 'bidirectional': False, 'fc_dropout': 0.7340850781155146, 'learning_rate_model': 0.002256676630910648}. Best is trial 115 with value: 0.9733659029006958.
Epoch 19: reducing lr to 0.0006943051994079462
Epoch 24: reducing lr to 0.0007677026796665796
Epoch 30: reducing lr to 0.0007561190423339573
Epoch 33: reducing lr to 0.0007414360431380067
Epoch 36: reducing lr to 0.0007211140635976797
Epoch 39: reducing lr to 0.0006954735882084083
Epoch 42: reducing lr to 0.0006649189859850343
Epoch 45: reducing lr to 0.0006299321214148148
Epoch 48: reducing lr to 0.0005910647735604331
Epoch 51: reducing lr to 0.0005489298912201331
Epoch 54: reducing lr to 0.0005041920005106818
Epoch 57: reducing lr to 0.0004575565072792257
Epoch 60: reducing lr to 0.0004097590879139867
Epoch 63: reducing lr to 0.0003615533606897905
Epoch 66: reducing lr to 0.0003136997007486296
Epoch 69: reducing lr to 0.0002669525788106234
Epoch 72: reducing lr to 0.0002220495017770407
Epoch 75: reducing lr to 0.00017969841444079887
Epoch 78: reducing lr to 0.00014056734352613074
Epoch 81: reducing lr to 0.00010527329359207226
Epoch 84: reducing lr to 7.437297687543495e-05
Epoch 87: reducing lr to 4.835358346163502e-05
Epoch 90: reducing lr to 2.7625608829623046e-05
Epoch 93: reducing lr to 1.2515841658677745e-05
Epoch 96: reducing lr to 3.2626165206366826e-06
Epoch 99: reducing lr to 1.1834919406983336e-08
[I 2024-06-20 23:17:22,017] Trial 158 finished with value: 1.1231578588485718 and parameters: {'hidden_size': 174, 'n_layers': 5, 'rnn_dropout': 0.6580625220190969, 'bidirectional': True, 'fc_dropout': 0.0983638172417428, 'learning_rate_model': 0.007677386005956413}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:17:27,688] Trial 159 finished with value: 1.1024302244186401 and parameters: {'hidden_size': 97, 'n_layers': 1, 'rnn_dropout': 0.4718589184879114, 'bidirectional': False, 'fc_dropout': 0.6306409517343752, 'learning_rate_model': 1.1178590317786991e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 27: reducing lr to 0.00010638658400252796
Epoch 40: reducing lr to 9.538271921297927e-05
Epoch 49: reducing lr to 8.029791397504888e-05
Epoch 55: reducing lr to 6.798527710802817e-05
Epoch 61: reducing lr to 5.475550845714602e-05
Epoch 72: reducing lr to 3.0882618782996976e-05
Epoch 75: reducing lr to 2.499243450073799e-05
Epoch 78: reducing lr to 1.9550089726456348e-05
Epoch 81: reducing lr to 1.464139738218789e-05
Epoch 84: reducing lr to 1.0343784940832466e-05
Epoch 87: reducing lr to 6.725010742590653e-06
Epoch 90: reducing lr to 3.842166450749749e-06
Epoch 93: reducing lr to 1.7407017966714588e-06
Epoch 96: reducing lr to 4.5376432478150264e-07
Epoch 99: reducing lr to 1.6459992093929496e-09
[I 2024-06-20 23:17:34,650] Trial 160 finished with value: 0.9802416563034058 and parameters: {'hidden_size': 72, 'n_layers': 1, 'rnn_dropout': 0.7979879941826636, 'bidirectional': True, 'fc_dropout': 0.7431595695144926, 'learning_rate_model': 0.001067769949377941}. Best is trial 115 with value: 0.9733659029006958.
Epoch 7: reducing lr to 5.602846901682245e-05
Epoch 16: reducing lr to 0.00016808634146045924
Epoch 19: reducing lr to 0.00019702716358987524
Epoch 22: reducing lr to 0.00021420737947421567
Epoch 25: reducing lr to 0.00021779030149191294
Epoch 28: reducing lr to 0.00021642371042333657
Epoch 31: reducing lr to 0.00021336194236608354
Epoch 34: reducing lr to 0.00020865328729855037
Epoch 37: reducing lr to 0.00020237199782410401
Epoch 40: reducing lr to 0.0001946171326166097
Epoch 43: reducing lr to 0.00018551099625218023
Epoch 46: reducing lr to 0.0001751971871648867
Epoch 49: reducing lr to 0.00016383837556596626
Epoch 52: reducing lr to 0.0001516136893715485
Epoch 55: reducing lr to 0.0001387158994845436
Epoch 58: reducing lr to 0.00012534846014172201
Epoch 61: reducing lr to 0.00011172212470793358
Epoch 64: reducing lr to 9.80518592057272e-05
Epoch 67: reducing lr to 8.455316112590585e-05
Epoch 70: reducing lr to 7.143900314344519e-05
Epoch 73: reducing lr to 5.891616237813924e-05
Epoch 76: reducing lr to 4.718211432975703e-05
Epoch 79: reducing lr to 3.642194651724505e-05
Epoch 82: reducing lr to 2.680532137102308e-05
Epoch 85: reducing lr to 1.8483927147489424e-05
Epoch 88: reducing lr to 1.158896342350977e-05
Epoch 91: reducing lr to 6.229208731742915e-06
Epoch 94: reducing lr to 2.489162376745997e-06
Epoch 97: reducing lr to 4.278181429093458e-07
[I 2024-06-20 23:18:01,650] Trial 161 finished with value: 1.0937323570251465 and parameters: {'hidden_size': 100, 'n_layers': 5, 'rnn_dropout': 0.2113101153684557, 'bidirectional': False, 'fc_dropout': 0.1955583994972887, 'learning_rate_model': 0.0021786580164286193}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:18:09,439] Trial 162 finished with value: 1.0982009172439575 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.7768666557003711, 'bidirectional': False, 'fc_dropout': 0.6013123232650325, 'learning_rate_model': 2.0904698538829833e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 16: reducing lr to 0.0004741737140249165
Epoch 19: reducing lr to 0.0005558161425340043
Epoch 22: reducing lr to 0.0006042817507615708
Epoch 25: reducing lr to 0.0006143892194912225
Epoch 28: reducing lr to 0.0006105340486492028
Epoch 31: reducing lr to 0.0006018967618918369
Epoch 34: reducing lr to 0.0005886135858643564
Epoch 37: reducing lr to 0.0005708939881083157
Epoch 40: reducing lr to 0.0005490174144066674
Epoch 43: reducing lr to 0.0005233288875292196
Epoch 46: reducing lr to 0.0004942335004907887
Epoch 49: reducing lr to 0.00046219014803293055
Epoch 52: reducing lr to 0.0004277041522927007
Epoch 55: reducing lr to 0.0003913193224469468
Epoch 58: reducing lr to 0.0003536096054936536
Epoch 61: reducing lr to 0.00031516953936425467
Epoch 64: reducing lr to 0.0002766055459513054
Epoch 67: reducing lr to 0.00023852554642608747
Epoch 70: reducing lr to 0.00020153033942221744
Epoch 73: reducing lr to 0.00016620324583308958
Epoch 76: reducing lr to 0.00013310134656332003
Epoch 79: reducing lr to 0.00010274677586555505
Epoch 82: reducing lr to 7.561815362088471e-05
Epoch 85: reducing lr to 5.2143394336135426e-05
Epoch 88: reducing lr to 3.269261369173906e-05
Epoch 91: reducing lr to 1.757267731632914e-05
Epoch 94: reducing lr to 7.021958826264243e-06
Epoch 97: reducing lr to 1.206880440064121e-06
[I 2024-06-20 23:18:32,332] Trial 163 finished with value: 1.0984783172607422 and parameters: {'hidden_size': 105, 'n_layers': 4, 'rnn_dropout': 0.537978120419944, 'bidirectional': False, 'fc_dropout': 0.5453162700820984, 'learning_rate_model': 0.006146022063804241}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:18:47,621] Trial 164 finished with value: 1.0907890796661377 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6086419650446049, 'bidirectional': False, 'fc_dropout': 0.18701495408328475, 'learning_rate_model': 5.0899740979609974e-05}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:19:01,116] Trial 165 finished with value: 1.1051242351531982 and parameters: {'hidden_size': 88, 'n_layers': 3, 'rnn_dropout': 0.652381543204501, 'bidirectional': False, 'fc_dropout': 0.3421320797575714, 'learning_rate_model': 2.4989769577785342e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 45: reducing lr to 7.559972624434666e-05
Epoch 48: reducing lr to 7.093515881280255e-05
Epoch 51: reducing lr to 6.587844641161539e-05
Epoch 55: reducing lr to 5.866472282626074e-05
Epoch 58: reducing lr to 5.3011462263791046e-05
Epoch 61: reducing lr to 4.724871124295421e-05
Epoch 65: reducing lr to 3.955191578694659e-05
Epoch 68: reducing lr to 3.388744191846051e-05
Epoch 71: reducing lr to 2.8415104257126354e-05
Epoch 74: reducing lr to 2.322118010221687e-05
Epoch 77: reducing lr to 1.8387596043044136e-05
Epoch 80: reducing lr to 1.3990566283085938e-05
Epoch 83: reducing lr to 1.0099447563140392e-05
Epoch 86: reducing lr to 6.775589131367431e-06
Epoch 89: reducing lr to 4.071429906584855e-06
Epoch 92: reducing lr to 2.0296027026773124e-06
Epoch 95: reducing lr to 6.823143701477461e-07
Epoch 98: reducing lr to 5.0808864998251256e-08
[I 2024-06-20 23:19:15,510] Trial 166 finished with value: 0.9778228998184204 and parameters: {'hidden_size': 37, 'n_layers': 3, 'rnn_dropout': 0.15507821400760574, 'bidirectional': True, 'fc_dropout': 0.551076207214932, 'learning_rate_model': 0.0009213822578516836}. Best is trial 115 with value: 0.9733659029006958.
Epoch 18: reducing lr to 0.0006867919880447172
Epoch 22: reducing lr to 0.0007803911805853443
Epoch 25: reducing lr to 0.0007934443291285883
Epoch 28: reducing lr to 0.000788465622886061
Epoch 31: reducing lr to 0.0007773111202039263
Epoch 34: reducing lr to 0.0007601567490700238
Epoch 37: reducing lr to 0.0007372730233991651
Epoch 40: reducing lr to 0.0007090208295232527
Epoch 43: reducing lr to 0.0006758457422529112
Epoch 46: reducing lr to 0.0006382709132730642
Epoch 49: reducing lr to 0.0005968889757530497
Epoch 52: reducing lr to 0.0005523525208701058
Epoch 55: reducing lr to 0.0005053638433485033
Epoch 58: reducing lr to 0.0004566641589783708
Epoch 61: reducing lr to 0.00040702127542166267
Epoch 64: reducing lr to 0.0003572183477150285
Epoch 67: reducing lr to 0.0003080404671175725
Epoch 70: reducing lr to 0.0002602635265871596
Epoch 73: reducing lr to 0.00021464084769949918
Epoch 76: reducing lr to 0.0001718918647653029
Epoch 79: reducing lr to 0.00013269088073238162
Epoch 82: reducing lr to 9.765600252450816e-05
Epoch 85: reducing lr to 6.733985432196137e-05
Epoch 88: reducing lr to 4.222041682238973e-05
Epoch 91: reducing lr to 2.2693987332320435e-05
Epoch 94: reducing lr to 9.068410110919107e-06
Epoch 97: reducing lr to 1.5586087950861596e-06
[I 2024-06-20 23:19:23,130] Trial 167 finished with value: 1.0832905769348145 and parameters: {'hidden_size': 32, 'n_layers': 2, 'rnn_dropout': 0.36928166471329765, 'bidirectional': True, 'fc_dropout': 0.5835181586049222, 'learning_rate_model': 0.00793719388055494}. Best is trial 115 with value: 0.9733659029006958.
Epoch 41: reducing lr to 5.589849545090254e-06
Epoch 53: reducing lr to 4.296923015674546e-06
Epoch 56: reducing lr to 3.915645022395373e-06
Epoch 59: reducing lr to 3.5227036810587352e-06
Epoch 65: reducing lr to 2.7267017150499377e-06
Epoch 68: reducing lr to 2.3361939405275717e-06
Epoch 71: reducing lr to 1.9589320003760717e-06
Epoch 74: reducing lr to 1.6008638355539523e-06
Epoch 77: reducing lr to 1.267637449884561e-06
Epoch 80: reducing lr to 9.645070363747178e-07
Epoch 83: reducing lr to 6.962540358300503e-07
Epoch 86: reducing lr to 4.671078540036396e-07
Epoch 89: reducing lr to 2.806836202016396e-07
Epoch 92: reducing lr to 1.3992043268070114e-07
Epoch 95: reducing lr to 4.7038625721870156e-08
Epoch 98: reducing lr to 3.5027536991318682e-09
[I 2024-06-20 23:20:18,791] Trial 168 finished with value: 1.0752862691879272 and parameters: {'hidden_size': 77, 'n_layers': 6, 'rnn_dropout': 0.3427539807608588, 'bidirectional': True, 'fc_dropout': 0.46482852593447216, 'learning_rate_model': 6.351992141755928e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 5: reducing lr to 8.937255938150266e-05
Epoch 8: reducing lr to 0.00016713714225688597
Epoch 11: reducing lr to 0.0002608565936102364
Epoch 14: reducing lr to 0.0003573682708009784
Epoch 17: reducing lr to 0.0004431174418831339
Epoch 20: reducing lr to 0.0005060608654686725
Epoch 23: reducing lr to 0.0005373583240745759
Epoch 35: reducing lr to 0.0005122849745024443
Epoch 40: reducing lr to 0.0004822691375777128
Epoch 43: reducing lr to 0.00045970376282322077
Epoch 46: reducing lr to 0.00043414572614477
Epoch 49: reducing lr to 0.000405998130914752
Epoch 52: reducing lr to 0.0003757048633648997
Epoch 55: reducing lr to 0.00034374361760079725
Epoch 58: reducing lr to 0.0003106185614620616
Epoch 61: reducing lr to 0.00027685195032334155
Epoch 64: reducing lr to 0.0002429764786956974
Epoch 67: reducing lr to 0.00020952615809005006
Epoch 70: reducing lr to 0.00017702874342143314
Epoch 73: reducing lr to 0.00014599663676818935
Epoch 76: reducing lr to 0.0001169191904174781
Epoch 79: reducing lr to 9.025505873820616e-05
Epoch 82: reducing lr to 6.642467210511896e-05
Epoch 85: reducing lr to 4.5803920161693967e-05
Epoch 88: reducing lr to 2.8717920773634574e-05
Epoch 91: reducing lr to 1.543623155093607e-05
Epoch 94: reducing lr to 6.168245192929908e-06
Epoch 97: reducing lr to 1.060150686874213e-06
[I 2024-06-20 23:21:02,023] Trial 169 finished with value: 1.1147019863128662 and parameters: {'hidden_size': 113, 'n_layers': 7, 'rnn_dropout': 0.24399518066817283, 'bidirectional': False, 'fc_dropout': 0.2638756501455059, 'learning_rate_model': 0.005398802811105274}. Best is trial 115 with value: 0.9733659029006958.
Epoch 25: reducing lr to 2.38345113992789e-05
Epoch 28: reducing lr to 2.368495455409796e-05
Epoch 31: reducing lr to 2.334988111344131e-05
Epoch 34: reducing lr to 2.2834575830728527e-05
Epoch 37: reducing lr to 2.2147164754315554e-05
Epoch 40: reducing lr to 2.1298488656611738e-05
Epoch 43: reducing lr to 2.0301932292556005e-05
Epoch 46: reducing lr to 1.9173210772005597e-05
Epoch 49: reducing lr to 1.7930126379899258e-05
Epoch 52: reducing lr to 1.659228249769922e-05
Epoch 55: reducing lr to 1.518077556657567e-05
Epoch 58: reducing lr to 1.371787118923139e-05
Epoch 61: reducing lr to 1.2226633769556446e-05
Epoch 64: reducing lr to 1.0730588735817395e-05
Epoch 67: reducing lr to 9.25331967904595e-06
Epoch 70: reducing lr to 7.818133879753097e-06
Epoch 73: reducing lr to 6.447660589953632e-06
Epoch 76: reducing lr to 5.163511111978659e-06
Epoch 79: reducing lr to 3.985941033657272e-06
Epoch 82: reducing lr to 2.933517853652946e-06
Epoch 85: reducing lr to 2.0228420149215723e-06
Epoch 88: reducing lr to 1.268271722529971e-06
Epoch 91: reducing lr to 6.817114697402047e-07
Epoch 94: reducing lr to 2.724086822819912e-07
Epoch 97: reducing lr to 4.681951553462522e-08
[I 2024-06-20 23:21:12,939] Trial 170 finished with value: 1.0924397706985474 and parameters: {'hidden_size': 182, 'n_layers': 1, 'rnn_dropout': 0.7028702905638164, 'bidirectional': False, 'fc_dropout': 0.5336950309593632, 'learning_rate_model': 0.0002384277397661184}. Best is trial 115 with value: 0.9733659029006958.
Epoch 57: reducing lr to 0.00013643439447530117
Epoch 60: reducing lr to 0.00012218213958474
Epoch 65: reducing lr to 9.82698260891139e-05
Epoch 68: reducing lr to 8.419599803636078e-05
Epoch 72: reducing lr to 6.621081513764834e-05
Epoch 75: reducing lr to 5.3582549854198654e-05
Epoch 78: reducing lr to 4.191443044057883e-05
Epoch 81: reducing lr to 3.1390435579337027e-05
Epoch 84: reducing lr to 2.2176565962667627e-05
Epoch 87: reducing lr to 1.441809213801261e-05
Epoch 90: reducing lr to 8.237415822350122e-06
Epoch 93: reducing lr to 3.731979003433576e-06
Epoch 96: reducing lr to 9.728483855362107e-07
Epoch 99: reducing lr to 3.5289413160329713e-09
[I 2024-06-20 23:21:38,064] Trial 171 finished with value: 0.9885504245758057 and parameters: {'hidden_size': 44, 'n_layers': 5, 'rnn_dropout': 0.6782304349717934, 'bidirectional': True, 'fc_dropout': 0.7650412078041857, 'learning_rate_model': 0.0022892462334419333}. Best is trial 115 with value: 0.9733659029006958.
Epoch 43: reducing lr to 0.0001012505978356223
Epoch 46: reducing lr to 9.562139332943029e-05
Epoch 49: reducing lr to 8.942183379750104e-05
Epoch 52: reducing lr to 8.274968599740461e-05
Epoch 55: reducing lr to 7.571016293300223e-05
Epoch 58: reducing lr to 6.841430849668497e-05
Epoch 61: reducing lr to 6.097715039364563e-05
Epoch 64: reducing lr to 5.351601556803924e-05
Epoch 67: reducing lr to 4.6148521035658296e-05
Epoch 70: reducing lr to 3.89909057855632e-05
Epoch 73: reducing lr to 3.215602731634347e-05
Epoch 76: reducing lr to 2.575166636775818e-05
Epoch 79: reducing lr to 1.9878842406706043e-05
Epoch 82: reducing lr to 1.463015599518767e-05
Epoch 85: reducing lr to 1.0088397517359549e-05
Epoch 88: reducing lr to 6.3251747801000435e-06
Epoch 91: reducing lr to 3.3998583419522474e-06
Epoch 94: reducing lr to 1.3585673294151755e-06
Epoch 97: reducing lr to 2.3350013535377353e-07
[I 2024-06-20 23:22:01,608] Trial 172 finished with value: 0.9865332841873169 and parameters: {'hidden_size': 52, 'n_layers': 4, 'rnn_dropout': 0.2693967989609338, 'bidirectional': True, 'fc_dropout': 0.7941606989318544, 'learning_rate_model': 0.0011890962320255246}. Best is trial 115 with value: 0.9733659029006958.
Epoch 6: reducing lr to 0.0019213480659017773
Epoch 12: reducing lr to 0.0049982784039874105
Epoch 15: reducing lr to 0.00660636302525446
Epoch 18: reducing lr to 0.00795830833530171
Epoch 21: reducing lr to 0.00886423761810143
Epoch 24: reducing lr to 0.009196915788112063
Epoch 27: reducing lr to 0.009163717666407148
Epoch 30: reducing lr to 0.009058146261979334
Epoch 33: reducing lr to 0.008882247036017633
Epoch 36: reducing lr to 0.008638794017771954
Epoch 39: reducing lr to 0.00833162654373799
Epoch 42: reducing lr to 0.007965588869218375
Epoch 45: reducing lr to 0.007546453628890518
Epoch 48: reducing lr to 0.007080831019263503
Epoch 51: reducing lr to 0.006576064037345388
Epoch 54: reducing lr to 0.006040113565515222
Epoch 57: reducing lr to 0.005481430216678862
Epoch 60: reducing lr to 0.004908827238424184
Epoch 63: reducing lr to 0.004331332818347155
Epoch 66: reducing lr to 0.0037580560898837997
Epoch 69: reducing lr to 0.003198035452744468
Epoch 72: reducing lr to 0.002660106083676321
Epoch 75: reducing lr to 0.0021527490116187405
Epoch 78: reducing lr to 0.0016839670554880865
Epoch 81: reducing lr to 0.001261151799449209
Epoch 84: reducing lr to 0.0008909725383942247
Epoch 87: reducing lr to 0.0005792657065405119
Epoch 90: reducing lr to 0.00033094895293541565
Epoch 93: reducing lr to 0.00014993713686422966
Epoch 96: reducing lr to 3.908545610682296e-05
Epoch 99: reducing lr to 1.417798322550512e-07
[I 2024-06-20 23:22:05,913] Trial 173 finished with value: 1.2693488597869873 and parameters: {'hidden_size': 47, 'n_layers': 1, 'rnn_dropout': 0.2962445550191101, 'bidirectional': False, 'fc_dropout': 0.3311358765835271, 'learning_rate_model': 0.0919734611324751}. Best is trial 115 with value: 0.9733659029006958.
Epoch 14: reducing lr to 5.882936513706286e-05
Epoch 17: reducing lr to 7.29452498083183e-05
Epoch 20: reducing lr to 8.330689059078344e-05
Epoch 23: reducing lr to 8.845902571476087e-05
Epoch 26: reducing lr to 8.873516682207373e-05
Epoch 29: reducing lr to 8.794566741076306e-05
Epoch 32: reducing lr to 8.647001884981154e-05
Epoch 35: reducing lr to 8.433149297694372e-05
Epoch 38: reducing lr to 8.156381504961002e-05
Epoch 41: reducing lr to 7.821063335382894e-05
Epoch 44: reducing lr to 7.43248295853498e-05
Epoch 47: reducing lr to 6.996768691356629e-05
Epoch 50: reducing lr to 6.520791870068978e-05
Epoch 53: reducing lr to 6.0120593196238665e-05
Epoch 56: reducing lr to 5.478592486613434e-05
Epoch 59: reducing lr to 4.9288068272867956e-05
Epoch 62: reducing lr to 4.371370773600772e-05
Epoch 65: reducing lr to 3.8150770674738076e-05
Epoch 68: reducing lr to 3.268696344189147e-05
Epoch 71: reducing lr to 2.7408485901210773e-05
Epoch 74: reducing lr to 2.2398558938296595e-05
Epoch 77: reducing lr to 1.7736206854723736e-05
Epoch 80: reducing lr to 1.349494393017214e-05
Epoch 83: reducing lr to 9.741669910464203e-06
Epoch 86: reducing lr to 6.535560717955361e-06
Epoch 89: reducing lr to 3.927197598242574e-06
Epoch 92: reducing lr to 1.9577030778424627e-06
Epoch 95: reducing lr to 6.581430645181594e-07
Epoch 98: reducing lr to 4.900893719621052e-08
[I 2024-06-20 23:22:31,414] Trial 174 finished with value: 1.092660903930664 and parameters: {'hidden_size': 188, 'n_layers': 2, 'rnn_dropout': 0.3159058100294404, 'bidirectional': False, 'fc_dropout': 0.6242908186314149, 'learning_rate_model': 0.0008887418605061119}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.00020215853882730276
Epoch 12: reducing lr to 0.0003007360624870969
Epoch 15: reducing lr to 0.00039749118456275086
Epoch 18: reducing lr to 0.00047883493462620404
Epoch 21: reducing lr to 0.0005333428238193315
Epoch 24: reducing lr to 0.0005533593804890433
Epoch 27: reducing lr to 0.0005513619182437352
Epoch 30: reducing lr to 0.0005450099054279773
Epoch 33: reducing lr to 0.0005344264132063257
Epoch 36: reducing lr to 0.0005197783491750392
Epoch 39: reducing lr to 0.0005012967182616054
Epoch 42: reducing lr to 0.00047927298927741247
Epoch 45: reducing lr to 0.0004540544896985923
Epoch 48: reducing lr to 0.0004260389413624921
Epoch 51: reducing lr to 0.0003956681571951978
Epoch 54: reducing lr to 0.0003634211270062184
Epoch 57: reducing lr to 0.0003298063066768594
Epoch 60: reducing lr to 0.00029535397106639785
Epoch 63: reducing lr to 0.0002606073275293579
Epoch 66: reducing lr to 0.0002261144537638596
Epoch 69: reducing lr to 0.0001924191715662052
Epoch 72: reducing lr to 0.0001600530752277808
Epoch 75: reducing lr to 0.00012952645069965184
Epoch 78: reducing lr to 0.00010132081102594646
Epoch 81: reducing lr to 7.588089252137369e-05
Epoch 84: reducing lr to 5.360797285062311e-05
Epoch 87: reducing lr to 3.485321817604751e-05
Epoch 90: reducing lr to 1.991251325869029e-05
Epoch 93: reducing lr to 9.021407075917448e-06
Epoch 96: reducing lr to 2.35169096637379e-06
Epoch 99: reducing lr to 8.530598947543987e-09
[I 2024-06-20 23:22:42,283] Trial 175 finished with value: 1.0943844318389893 and parameters: {'hidden_size': 93, 'n_layers': 2, 'rnn_dropout': 0.7286449112694068, 'bidirectional': False, 'fc_dropout': 0.1507747197331348, 'learning_rate_model': 0.005533852722614425}. Best is trial 115 with value: 0.9733659029006958.
Epoch 24: reducing lr to 1.725699679334633e-05
Epoch 27: reducing lr to 1.719470418427978e-05
Epoch 30: reducing lr to 1.6996611102897565e-05
Epoch 33: reducing lr to 1.6666555631225545e-05
Epoch 36: reducing lr to 1.6209742928794723e-05
Epoch 39: reducing lr to 1.56333770865331e-05
Epoch 42: reducing lr to 1.4946547814529337e-05
Epoch 45: reducing lr to 1.4160086824241085e-05
Epoch 48: reducing lr to 1.3286397419405032e-05
Epoch 51: reducing lr to 1.2339257922965632e-05
Epoch 54: reducing lr to 1.133360605152839e-05
Epoch 57: reducing lr to 1.0285298446947811e-05
Epoch 60: reducing lr to 9.210872195010805e-06
Epoch 63: reducing lr to 8.127267692692046e-06
Epoch 66: reducing lr to 7.051577222895645e-06
Epoch 69: reducing lr to 6.000760344501125e-06
Epoch 72: reducing lr to 4.9913952909409865e-06
Epoch 75: reducing lr to 4.03939577639758e-06
Epoch 78: reducing lr to 3.1597782067573154e-06
Epoch 81: reducing lr to 2.366412073398532e-06
Epoch 84: reducing lr to 1.6718115716470054e-06
Epoch 87: reducing lr to 1.0869281257512907e-06
Epoch 90: reducing lr to 6.20989161056589e-07
Epoch 93: reducing lr to 2.813404786650435e-07
Epoch 96: reducing lr to 7.33395418900959e-08
Epoch 99: reducing lr to 2.660341974383522e-10
[I 2024-06-20 23:23:10,240] Trial 176 finished with value: 1.1070547103881836 and parameters: {'hidden_size': 81, 'n_layers': 7, 'rnn_dropout': 0.09821770355953596, 'bidirectional': False, 'fc_dropout': 0.46672505168658235, 'learning_rate_model': 0.0001725780425093903}. Best is trial 115 with value: 0.9733659029006958.
Epoch 16: reducing lr to 0.0004880913493904647
Epoch 22: reducing lr to 0.0006220182317524017
Epoch 25: reducing lr to 0.000632422368264529
Epoch 28: reducing lr to 0.000628454042980447
Epoch 31: reducing lr to 0.0006195632402560817
Epoch 34: reducing lr to 0.0006058901851716673
Epoch 37: reducing lr to 0.0005876504934224371
Epoch 40: reducing lr to 0.0005651318129003944
Epoch 43: reducing lr to 0.0005386892932570388
Epoch 46: reducing lr to 0.0005087399175312488
Epoch 49: reducing lr to 0.00047575604964158243
Epoch 52: reducing lr to 0.00044025784360851225
Epoch 55: reducing lr to 0.00040280507013861194
Epoch 58: reducing lr to 0.00036398852234512086
Epoch 61: reducing lr to 0.0003244201886462792
Epoch 64: reducing lr to 0.00028472429023166924
Epoch 67: reducing lr to 0.00024552659157544383
Epoch 70: reducing lr to 0.00020744552555804408
Epoch 73: reducing lr to 0.0001710815343245389
Epoch 76: reducing lr to 0.0001370080498523058
Epoch 79: reducing lr to 0.00010576253173558069
Epoch 82: reducing lr to 7.783764799179403e-05
Epoch 85: reducing lr to 5.367387299327628e-05
Epoch 88: reducing lr to 3.3652185812779214e-05
Epoch 91: reducing lr to 1.8088458997285536e-05
Epoch 94: reducing lr to 7.22806274895179e-06
Epoch 97: reducing lr to 1.2423040019314372e-06
[I 2024-06-20 23:23:14,156] Trial 177 finished with value: 1.0950827598571777 and parameters: {'hidden_size': 16, 'n_layers': 2, 'rnn_dropout': 0.11632290733698857, 'bidirectional': False, 'fc_dropout': 0.6984755026157125, 'learning_rate_model': 0.006326416066049896}. Best is trial 115 with value: 0.9733659029006958.
Epoch 50: reducing lr to 2.5910848761230028e-05
Epoch 53: reducing lr to 2.3889362347133104e-05
Epoch 61: reducing lr to 1.8109524990008516e-05
Epoch 64: reducing lr to 1.5893652212978542e-05
Epoch 67: reducing lr to 1.370558954546157e-05
Epoch 71: reducing lr to 1.0890964580856426e-05
Epoch 76: reducing lr to 7.647954071495547e-06
Epoch 83: reducing lr to 3.87092458648267e-06
Epoch 86: reducing lr to 2.5969533870582812e-06
Epoch 89: reducing lr to 1.5605010104770072e-06
Epoch 92: reducing lr to 7.779077967847153e-07
Epoch 95: reducing lr to 2.61518014188695e-07
Epoch 98: reducing lr to 1.947406365580156e-08
[I 2024-06-20 23:24:37,494] Trial 178 finished with value: 1.0343188047409058 and parameters: {'hidden_size': 103, 'n_layers': 6, 'rnn_dropout': 0.3966611176681699, 'bidirectional': True, 'fc_dropout': 0.7397926508306973, 'learning_rate_model': 0.00035314815123986564}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:25:03,624] Trial 179 finished with value: 1.0501258373260498 and parameters: {'hidden_size': 107, 'n_layers': 2, 'rnn_dropout': 0.7079732150704507, 'bidirectional': True, 'fc_dropout': 0.5740759697558036, 'learning_rate_model': 5.800691551121672e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 12: reducing lr to 0.0001943476060971096
Epoch 51: reducing lr to 0.00025569650185547945
Epoch 54: reducing lr to 0.00023485718824227332
Epoch 57: reducing lr to 0.00021313395423313062
Epoch 63: reducing lr to 0.00016841482134810208
Epoch 66: reducing lr to 0.00014612415428178702
Epoch 69: reducing lr to 0.00012434892261278273
Epoch 72: reducing lr to 0.00010343266371765571
Epoch 75: reducing lr to 8.370514467586566e-05
Epoch 78: reducing lr to 6.5477538369896e-05
Epoch 81: reducing lr to 4.903725109679448e-05
Epoch 84: reducing lr to 3.4643604445291474e-05
Epoch 87: reducing lr to 2.25235359579988e-05
Epoch 90: reducing lr to 1.286825813704819e-05
Epoch 93: reducing lr to 5.829992101154562e-06
Epoch 96: reducing lr to 1.5197562467738831e-06
Epoch 99: reducing lr to 5.512812365502176e-09
[I 2024-06-20 23:25:30,926] Trial 180 finished with value: 0.991034984588623 and parameters: {'hidden_size': 37, 'n_layers': 6, 'rnn_dropout': 0.5953087370956281, 'bidirectional': True, 'fc_dropout': 0.49859771615006726, 'learning_rate_model': 0.003576195751981789}. Best is trial 115 with value: 0.9733659029006958.
Epoch 13: reducing lr to 0.00013999705001984893
Epoch 16: reducing lr to 0.00017901609903021932
Epoch 22: reducing lr to 0.00022813614195999762
Epoch 25: reducing lr to 0.00023195204227149637
Epoch 29: reducing lr to 0.0002296082781830566
Epoch 32: reducing lr to 0.00022575565945539525
Epoch 35: reducing lr to 0.00022017240267907558
Epoch 38: reducing lr to 0.00021294655765259765
Epoch 42: reducing lr to 0.00020095743843406966
Epoch 47: reducing lr to 0.0001826714219546537
Epoch 50: reducing lr to 0.00017024463373318792
Epoch 53: reducing lr to 0.0001569626599415965
Epoch 57: reducing lr to 0.0001382865966828363
Epoch 60: reducing lr to 0.00012384085643198773
Epoch 63: reducing lr to 0.00010927171392739396
Epoch 66: reducing lr to 9.480897617412552e-05
Epoch 69: reducing lr to 8.068066569294717e-05
Epoch 72: reducing lr to 6.71096780558464e-05
Epoch 75: reducing lr to 5.430997432445037e-05
Epoch 78: reducing lr to 4.248345118412506e-05
Epoch 81: reducing lr to 3.18165849700324e-05
Epoch 84: reducing lr to 2.2477629961885434e-05
Epoch 87: reducing lr to 1.4613828866930382e-05
Epoch 90: reducing lr to 8.349245099925022e-06
Epoch 93: reducing lr to 3.782643498814048e-06
Epoch 96: reducing lr to 9.860555532318488e-07
Epoch 99: reducing lr to 3.5768494180844073e-09
[I 2024-06-20 23:25:35,911] Trial 181 finished with value: 1.0928140878677368 and parameters: {'hidden_size': 76, 'n_layers': 1, 'rnn_dropout': 0.7044308780514614, 'bidirectional': False, 'fc_dropout': 0.20311089675419175, 'learning_rate_model': 0.00232032451794255}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.0006654556330357249
Epoch 18: reducing lr to 0.0008016364058282585
Epoch 22: reducing lr to 0.000910887127448223
Epoch 25: reducing lr to 0.0009261230056545764
Epoch 28: reducing lr to 0.000920311766957257
Epoch 31: reducing lr to 0.0009072920235785297
Epoch 34: reducing lr to 0.0008872691219439663
Epoch 37: reducing lr to 0.0008605588109355732
Epoch 40: reducing lr to 0.0008275823238045455
Epoch 43: reducing lr to 0.0007888597437724909
Epoch 46: reducing lr to 0.0007450017032342329
Epoch 49: reducing lr to 0.0006966999346678908
Epoch 52: reducing lr to 0.0006447161546556362
Epoch 55: reducing lr to 0.0005898700946858919
Epoch 58: reducing lr to 0.0005330269156403875
Epoch 61: reducing lr to 0.00047508281692915
Epoch 64: reducing lr to 0.00041695191170392567
Epoch 67: reducing lr to 0.0003595505731113895
Epoch 70: reducing lr to 0.00030378443786961245
Epoch 73: reducing lr to 0.0002505327969588282
Epoch 76: reducing lr to 0.00020063538751212488
Epoch 79: reducing lr to 0.00015487926849485416
Epoch 82: reducing lr to 0.00011398590582597894
Epoch 85: reducing lr to 7.860033274607863e-05
Epoch 88: reducing lr to 4.928045723193216e-05
Epoch 91: reducing lr to 2.6488844884149653e-05
Epoch 94: reducing lr to 1.0584817258264822e-05
Epoch 97: reducing lr to 1.8192372280612795e-06
[I 2024-06-20 23:26:12,307] Trial 182 finished with value: 1.2349392175674438 and parameters: {'hidden_size': 95, 'n_layers': 3, 'rnn_dropout': 0.10137070707753822, 'bidirectional': True, 'fc_dropout': 0.24228664377406287, 'learning_rate_model': 0.009264440595593887}. Best is trial 115 with value: 0.9733659029006958.
Epoch 34: reducing lr to 4.795370892321194e-06
Epoch 41: reducing lr to 4.406321712278476e-06
Epoch 53: reducing lr to 3.3871439700172034e-06
Epoch 65: reducing lr to 2.149382532215813e-06
Epoch 68: reducing lr to 1.8415561995370027e-06
[I 2024-06-20 23:27:44,361] Trial 183 finished with value: 1.0722248554229736 and parameters: {'hidden_size': 129, 'n_layers': 5, 'rnn_dropout': 0.006706463620025272, 'bidirectional': True, 'fc_dropout': 0.18472676038734337, 'learning_rate_model': 5.007097358286679e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.0001101839322822353
Epoch 20: reducing lr to 0.0001437883338514764
Epoch 23: reducing lr to 0.0001526809586992023
Epoch 55: reducing lr to 9.766873002741089e-05
Epoch 62: reducing lr to 7.545019574318786e-05
Epoch 65: reducing lr to 6.584852359232395e-05
Epoch 76: reducing lr to 3.322054071464927e-05
Epoch 83: reducing lr to 1.6814197186129013e-05
Epoch 86: reducing lr to 1.1280428062500824e-05
Epoch 98: reducing lr to 8.458980251574607e-08
[I 2024-06-20 23:28:16,922] Trial 184 finished with value: 1.0305562019348145 and parameters: {'hidden_size': 50, 'n_layers': 6, 'rnn_dropout': 0.7860718314440683, 'bidirectional': True, 'fc_dropout': 0.624179663753091, 'learning_rate_model': 0.001533975286317709}. Best is trial 115 with value: 0.9733659029006958.
Epoch 73: reducing lr to 7.87007321613219e-06
Epoch 76: reducing lr to 6.302628672312979e-06
Epoch 79: reducing lr to 4.865275914018599e-06
Epoch 82: reducing lr to 3.580678599157726e-06
Epoch 85: reducing lr to 2.4690993795341222e-06
Epoch 88: reducing lr to 1.548064010970646e-06
Epoch 91: reducing lr to 8.321032263224471e-07
Epoch 94: reducing lr to 3.3250451762454886e-07
Epoch 97: reducing lr to 5.71483269103004e-08
[I 2024-06-20 23:30:12,804] Trial 185 finished with value: 0.9772704839706421 and parameters: {'hidden_size': 167, 'n_layers': 4, 'rnn_dropout': 0.471961038639523, 'bidirectional': True, 'fc_dropout': 0.4296056934724326, 'learning_rate_model': 0.00029102706982436836}. Best is trial 115 with value: 0.9733659029006958.
Epoch 14: reducing lr to 2.4908900441256784e-05
Epoch 17: reducing lr to 3.08856973197777e-05
Epoch 20: reducing lr to 3.5272912413076195e-05
Epoch 23: reducing lr to 3.7454374350733664e-05
Epoch 26: reducing lr to 3.757129506428835e-05
Epoch 29: reducing lr to 3.7237013669461955e-05
Epoch 32: reducing lr to 3.6612210341984466e-05
Epoch 35: reducing lr to 3.5706738594427616e-05
Epoch 38: reducing lr to 3.453487801451483e-05
Epoch 41: reducing lr to 3.311510969257123e-05
Epoch 44: reducing lr to 3.14698242304929e-05
Epoch 47: reducing lr to 2.962496949226912e-05
Epoch 50: reducing lr to 2.760963935464529e-05
Epoch 53: reducing lr to 2.545561841276698e-05
Epoch 56: reducing lr to 2.319687021767591e-05
Epoch 59: reducing lr to 2.0869026593952985e-05
Epoch 62: reducing lr to 1.850879048885782e-05
Epoch 65: reducing lr to 1.6153391189591112e-05
Epoch 68: reducing lr to 1.3839964381803709e-05
Epoch 71: reducing lr to 1.1605007889652281e-05
Epoch 74: reducing lr to 9.483758210237034e-06
Epoch 77: reducing lr to 7.5096749679441925e-06
Epoch 80: reducing lr to 5.713884792634414e-06
Epoch 83: reducing lr to 4.124713659003372e-06
Epoch 86: reducing lr to 2.767217203042329e-06
Epoch 89: reducing lr to 1.6628119946536459e-06
Epoch 92: reducing lr to 8.289096940942173e-07
Epoch 95: reducing lr to 2.786638956920912e-07
Epoch 98: reducing lr to 2.0750839899565196e-08
[I 2024-06-20 23:31:06,376] Trial 186 finished with value: 1.107484221458435 and parameters: {'hidden_size': 127, 'n_layers': 7, 'rnn_dropout': 0.31425835599757, 'bidirectional': False, 'fc_dropout': 0.5160627192683411, 'learning_rate_model': 0.00037630157098835073}. Best is trial 115 with value: 0.9733659029006958.
Epoch 12: reducing lr to 0.002743197770252285
Epoch 15: reducing lr to 0.0036257604830290725
Epoch 18: reducing lr to 0.0043677466351141675
Epoch 21: reducing lr to 0.004864946468280704
Epoch 24: reducing lr to 0.005047529738043498
Epoch 27: reducing lr to 0.005029309661833891
Epoch 30: reducing lr to 0.004971369063527531
Epoch 33: reducing lr to 0.00487483055057429
Epoch 36: reducing lr to 0.004741216589359174
Epoch 39: reducing lr to 0.00457263431727289
Epoch 42: reducing lr to 0.00437174239981397
Epoch 45: reducing lr to 0.004141709023565002
Epoch 48: reducing lr to 0.003886162052934293
Epoch 51: reducing lr to 0.003609131534148041
Epoch 54: reducing lr to 0.0033149866265499983
Epoch 57: reducing lr to 0.003008365267567202
Epoch 60: reducing lr to 0.0026941044188847916
Epoch 63: reducing lr to 0.002377159007395777
Epoch 66: reducing lr to 0.0020625283853792763
Epoch 69: reducing lr to 0.0017551730844279877
Epoch 72: reducing lr to 0.001459942101575222
Epoch 75: reducing lr to 0.0011814900674348705
Epoch 78: reducing lr to 0.0009242091573186527
Epoch 81: reducing lr to 0.0006921560834704237
Epoch 84: reducing lr to 0.000488991145176957
Epoch 87: reducing lr to 0.00031791754402833575
Epoch 90: reducing lr to 0.00018163422610383492
Epoch 93: reducing lr to 8.22897778554807e-05
Epoch 96: reducing lr to 2.1451213272953573e-05
Epoch 99: reducing lr to 7.781281638891136e-08
[I 2024-06-20 23:32:10,741] Trial 187 finished with value: 1.1836795806884766 and parameters: {'hidden_size': 143, 'n_layers': 3, 'rnn_dropout': 0.10889052339849768, 'bidirectional': True, 'fc_dropout': 0.17437514404893434, 'learning_rate_model': 0.05047765912753394}. Best is trial 115 with value: 0.9733659029006958.
Epoch 8: reducing lr to 0.0008734585189615238
Epoch 14: reducing lr to 0.0018676061844942974
Epoch 17: reducing lr to 0.0023157312569002907
Epoch 20: reducing lr to 0.0026446735183330715
Epoch 23: reducing lr to 0.002808234002089312
Epoch 26: reducing lr to 0.00281700042067311
Epoch 29: reducing lr to 0.0027919368494483806
Epoch 32: reducing lr to 0.002745090680495982
Epoch 35: reducing lr to 0.0026772007051993965
Epoch 38: reducing lr to 0.002589337570832163
Epoch 41: reducing lr to 0.0024828869426776123
Epoch 44: reducing lr to 0.0023595276112818894
Epoch 48: reducing lr to 0.002172143325927067
Epoch 51: reducing lr to 0.0020172990388739034
Epoch 54: reducing lr to 0.0018528887828960054
Epoch 57: reducing lr to 0.0016815049009505062
Epoch 60: reducing lr to 0.001505850979223219
Epoch 63: reducing lr to 0.0013286965397347145
Epoch 66: reducing lr to 0.0011528359357670034
Epoch 69: reducing lr to 0.0009810418220486752
Epoch 72: reducing lr to 0.0008160245118399269
Epoch 75: reducing lr to 0.0006603856786389053
Epoch 78: reducing lr to 0.000516580298372939
Epoch 81: reducing lr to 0.000386875842214272
Epoch 84: reducing lr to 0.0002733182090622198
Epoch 87: reducing lr to 0.0001776978062288617
Epoch 90: reducing lr to 0.00010152319090591527
Epoch 93: reducing lr to 4.5995300588620376e-05
Epoch 96: reducing lr to 1.1990006878045966e-05
Epoch 99: reducing lr to 4.3492934027888894e-08
[I 2024-06-20 23:32:56,131] Trial 188 finished with value: 1.1167820692062378 and parameters: {'hidden_size': 143, 'n_layers': 5, 'rnn_dropout': 0.597325977698404, 'bidirectional': False, 'fc_dropout': 0.7444478553998761, 'learning_rate_model': 0.02821413746745477}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:33:07,849] Trial 189 finished with value: 1.093096375465393 and parameters: {'hidden_size': 46, 'n_layers': 2, 'rnn_dropout': 0.05728822355344363, 'bidirectional': True, 'fc_dropout': 0.528478684742879, 'learning_rate_model': 2.283088758720842e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.00012019897085991536
Epoch 18: reducing lr to 0.00014479683723591725
Epoch 21: reducing lr to 0.00016127969884193583
Epoch 24: reducing lr to 0.00016733258656699356
Epoch 27: reducing lr to 0.00016672856586026603
Epoch 30: reducing lr to 0.00016480775495176004
Epoch 33: reducing lr to 0.00016160736982989674
Epoch 36: reducing lr to 0.0001571778823594071
Epoch 39: reducing lr to 0.0001515891470568841
Epoch 42: reducing lr to 0.0001449293023579138
Epoch 45: reducing lr to 0.00013730337802618343
Epoch 48: reducing lr to 0.00012883164278058366
Epoch 51: reducing lr to 0.0001196476982230885
Epoch 54: reducing lr to 0.00010989638802417759
Epoch 57: reducing lr to 9.973146621924619e-05
Epoch 60: reducing lr to 8.93132847728998e-05
Epoch 63: reducing lr to 7.880610635941427e-05
Epoch 66: reducing lr to 6.837566641601132e-05
Epoch 69: reducing lr to 5.818641342050721e-05
Epoch 72: reducing lr to 4.839909832593226e-05
Epoch 75: reducing lr to 3.916802856989623e-05
Epoch 78: reducing lr to 3.063881083402525e-05
Epoch 81: reducing lr to 2.2945930735631423e-05
Epoch 84: reducing lr to 1.6210732254651932e-05
Epoch 87: reducing lr to 1.0539405950663679e-05
Epoch 90: reducing lr to 6.021425616172752e-06
Epoch 93: reducing lr to 2.72801986143786e-06
Epoch 96: reducing lr to 7.11137365850341e-07
Epoch 99: reducing lr to 2.57960240160858e-09
[I 2024-06-20 23:33:20,136] Trial 190 finished with value: 1.0932039022445679 and parameters: {'hidden_size': 54, 'n_layers': 4, 'rnn_dropout': 0.08396293466394243, 'bidirectional': False, 'fc_dropout': 0.27136334790808, 'learning_rate_model': 0.0016734041608502312}. Best is trial 115 with value: 0.9733659029006958.
Epoch 24: reducing lr to 2.7058226995475602e-05
Epoch 27: reducing lr to 2.696055486999239e-05
Epoch 30: reducing lr to 2.6649953458491876e-05
Epoch 33: reducing lr to 2.6132440708124813e-05
Epoch 36: reducing lr to 2.5416178084633132e-05
Epoch 39: reducing lr to 2.4512461292012148e-05
Epoch 42: reducing lr to 2.343554260380908e-05
Epoch 45: reducing lr to 2.2202405676617267e-05
Epoch 48: reducing lr to 2.0832498355969745e-05
Epoch 51: reducing lr to 1.9347424458238078e-05
Epoch 54: reducing lr to 1.7770605679071046e-05
Epoch 57: reducing lr to 1.6126904549291543e-05
Epoch 60: reducing lr to 1.4442250506473461e-05
Epoch 63: reducing lr to 1.2743205362745718e-05
Epoch 66: reducing lr to 1.1056569080826544e-05
Epoch 69: reducing lr to 9.408933517885554e-06
Epoch 72: reducing lr to 7.826292629230986e-06
Epoch 75: reducing lr to 6.333598432635326e-06
Epoch 78: reducing lr to 4.954396005147382e-06
Epoch 81: reducing lr to 3.7104321113126392e-06
Epoch 84: reducing lr to 2.6213284698951127e-06
Epoch 87: reducing lr to 1.70425644198328e-06
Epoch 90: reducing lr to 9.73684232709462e-07
Epoch 93: reducing lr to 4.4112974151270946e-07
Epoch 96: reducing lr to 1.1499323989974056e-07
Epoch 99: reducing lr to 4.1713015242438033e-10
[I 2024-06-20 23:33:40,082] Trial 191 finished with value: 1.0924701690673828 and parameters: {'hidden_size': 115, 'n_layers': 3, 'rnn_dropout': 0.5570160664808824, 'bidirectional': False, 'fc_dropout': 0.4491930070394994, 'learning_rate_model': 0.0002705949305417018}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:35:54,340] Trial 192 finished with value: 1.0816025733947754 and parameters: {'hidden_size': 129, 'n_layers': 7, 'rnn_dropout': 0.4775108199567785, 'bidirectional': True, 'fc_dropout': 0.5916895700060374, 'learning_rate_model': 1.413889647537952e-05}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:36:35,470] Trial 193 finished with value: 1.105550765991211 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.00425322636888259, 'bidirectional': False, 'fc_dropout': 0.6143703026250493, 'learning_rate_model': 1.6306861470410482e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.003413893131075093
Epoch 18: reducing lr to 0.004112522133132048
Epoch 21: reducing lr to 0.004580668637338215
Epoch 24: reducing lr to 0.0047525828532413265
Epoch 27: reducing lr to 0.004735427447275934
Epoch 30: reducing lr to 0.0046808725445198615
Epoch 33: reducing lr to 0.004589975154083321
Epoch 36: reducing lr to 0.004464168778691711
Epoch 39: reducing lr to 0.004305437427464786
Epoch 42: reducing lr to 0.004116284409687794
Epoch 45: reducing lr to 0.003899692782422251
Epoch 48: reducing lr to 0.0036590784197839377
Epoch 51: reducing lr to 0.003398235876651457
Epoch 54: reducing lr to 0.003121279005316494
Epoch 57: reducing lr to 0.0028325747304004155
Epoch 60: reducing lr to 0.0025366773710176426
Epoch 63: reducing lr to 0.0022382523183224453
Epoch 66: reducing lr to 0.001942006792906308
Epoch 69: reducing lr to 0.001652611463118687
Epoch 72: reducing lr to 0.0013746319801497548
Epoch 75: reducing lr to 0.0011124509863596004
Epoch 78: reducing lr to 0.0008702040050949364
Epoch 81: reducing lr to 0.0006517106990524206
Epoch 84: reducing lr to 0.00046041748192961684
Epoch 87: reducing lr to 0.0002993403797318335
Epoch 90: reducing lr to 0.00017102062857334357
Epoch 93: reducing lr to 7.748126460461192e-05
Epoch 96: reducing lr to 2.019773506510905e-05
Epoch 99: reducing lr to 7.326590948930623e-08
[I 2024-06-20 23:38:14,272] Trial 194 finished with value: 1.107243537902832 and parameters: {'hidden_size': 117, 'n_layers': 6, 'rnn_dropout': 0.5196664439027832, 'bidirectional': True, 'fc_dropout': 0.2214451700804693, 'learning_rate_model': 0.04752805227340119}. Best is trial 115 with value: 0.9733659029006958.
Epoch 34: reducing lr to 3.4660394509758837e-06
Epoch 38: reducing lr to 3.321385073796624e-06
Epoch 58: reducing lr to 2.0822231635806347e-06
Epoch 68: reducing lr to 1.3310558415836795e-06
[I 2024-06-20 23:39:05,913] Trial 195 finished with value: 1.1067686080932617 and parameters: {'hidden_size': 157, 'n_layers': 5, 'rnn_dropout': 0.6663745150696564, 'bidirectional': False, 'fc_dropout': 0.7988164056718448, 'learning_rate_model': 3.619072928538003e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 23: reducing lr to 0.000547846010113048
Epoch 31: reducing lr to 0.0005390386171710511
Epoch 34: reducing lr to 0.0005271426488076602
Epoch 37: reducing lr to 0.000511273569463851
Epoch 40: reducing lr to 0.0004916816414403268
Epoch 43: reducing lr to 0.00046867585559483125
Epoch 46: reducing lr to 0.000442618999688252
Epoch 49: reducing lr to 0.00041392204450922996
Epoch 52: reducing lr to 0.0003830375396696429
Epoch 55: reducing lr to 0.0003504525024874962
Epoch 58: reducing lr to 0.00031668094070583014
Epoch 61: reducing lr to 0.0002822553026192798
Epoch 64: reducing lr to 0.00024771867940075307
Epoch 67: reducing lr to 0.00021361550492706073
Epoch 70: reducing lr to 0.00018048383436840347
Epoch 73: reducing lr to 0.0001488460704151596
Epoch 76: reducing lr to 0.00011920111610102006
Epoch 79: reducing lr to 9.201657740651806e-05
Epoch 82: reducing lr to 6.772109029580513e-05
Epoch 85: reducing lr to 4.6697880695039e-05
Epoch 88: reducing lr to 2.927841183380433e-05
Epoch 91: reducing lr to 1.5737502309888517e-05
Epoch 94: reducing lr to 6.288631564729634e-06
Epoch 97: reducing lr to 1.080841773360222e-06
[I 2024-06-20 23:39:11,133] Trial 196 finished with value: 0.986052393913269 and parameters: {'hidden_size': 29, 'n_layers': 1, 'rnn_dropout': 0.011795032719957544, 'bidirectional': True, 'fc_dropout': 0.6733894609486798, 'learning_rate_model': 0.005504171884830901}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:39:28,809] Trial 197 finished with value: 1.1063706874847412 and parameters: {'hidden_size': 106, 'n_layers': 3, 'rnn_dropout': 0.2829182296536041, 'bidirectional': False, 'fc_dropout': 0.5805140363958952, 'learning_rate_model': 1.269705997902816e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 17: reducing lr to 0.0012602471668112713
Epoch 20: reducing lr to 0.0014392612694969372
Epoch 23: reducing lr to 0.0015282727364544601
Epoch 26: reducing lr to 0.0015330435206939498
Epoch 29: reducing lr to 0.0015194036414842966
Epoch 32: reducing lr to 0.0014939094259865766
Epoch 35: reducing lr to 0.001456962932835669
Epoch 38: reducing lr to 0.0014091468203988225
Epoch 41: reducing lr to 0.001351215183410589
Epoch 44: reducing lr to 0.0012840816386920683
Epoch 47: reducing lr to 0.0012088049521095036
Epoch 50: reducing lr to 0.00112657225812712
Epoch 53: reducing lr to 0.0010386804821653065
Epoch 56: reducing lr to 0.0009465154588559323
Epoch 59: reducing lr to 0.0008515310943715361
Epoch 62: reducing lr to 0.0007552250005296096
Epoch 65: reducing lr to 0.0006591162657040141
Epoch 68: reducing lr to 0.000564720158989843
Epoch 71: reducing lr to 0.0004735259224466817
Epoch 74: reducing lr to 0.0003869713314687233
Epoch 77: reducing lr to 0.00030642165867386385
Epoch 80: reducing lr to 0.00023314698214025475
Epoch 83: reducing lr to 0.00016830310317578946
Epoch 86: reducing lr to 0.00011291238154601605
Epoch 89: reducing lr to 6.784869007507126e-05
Epoch 92: reducing lr to 3.382248691713042e-05
Epoch 95: reducing lr to 1.1370485872555166e-05
Epoch 98: reducing lr to 8.467086535759311e-07
[I 2024-06-20 23:39:53,440] Trial 198 finished with value: 1.2148374319076538 and parameters: {'hidden_size': 101, 'n_layers': 2, 'rnn_dropout': 0.023172948942318784, 'bidirectional': True, 'fc_dropout': 0.2637880633954088, 'learning_rate_model': 0.015354453027065824}. Best is trial 115 with value: 0.9733659029006958.
Epoch 14: reducing lr to 0.002359415896255995
Epoch 18: reducing lr to 0.003084211889154874
Epoch 21: reducing lr to 0.003435301310049813
Epoch 24: reducing lr to 0.003564229459598593
Epoch 27: reducing lr to 0.003551363654788578
Epoch 30: reducing lr to 0.0035104498616842423
Epoch 33: reducing lr to 0.0034422807909286228
Epoch 36: reducing lr to 0.0033479315069239726
Epoch 39: reducing lr to 0.003228889929812056
Epoch 42: reducing lr to 0.0030870334321661656
Epoch 45: reducing lr to 0.002924599176427574
Epoch 48: reducing lr to 0.002744148918914837
Epoch 51: reducing lr to 0.002548528409970877
Epoch 54: reducing lr to 0.0023408228590456387
Epoch 57: reducing lr to 0.002124307268777467
Epoch 60: reducing lr to 0.001902397179485656
Epoch 64: reducing lr to 0.001604178694896505
Epoch 67: reducing lr to 0.0013833330725503156
Epoch 70: reducing lr to 0.0011687787233785097
Epoch 73: reducing lr to 0.0009638986270905259
Epoch 76: reducing lr to 0.0007719235841225781
Epoch 79: reducing lr to 0.0005958817211924094
Epoch 82: reducing lr to 0.00043854880265990493
Epoch 85: reducing lr to 0.00030240652618129804
Epoch 88: reducing lr to 0.0001896013841096382
Epoch 91: reducing lr to 0.00010191304901785714
Epoch 94: reducing lr to 4.072397285742324e-05
Epoch 97: reducing lr to 6.99932419134866e-06
[I 2024-06-20 23:40:19,728] Trial 199 finished with value: 1.0153597593307495 and parameters: {'hidden_size': 76, 'n_layers': 3, 'rnn_dropout': 0.742487262800581, 'bidirectional': True, 'fc_dropout': 0.76154813405434, 'learning_rate_model': 0.03564396230455292}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.00045533338409405115
Epoch 14: reducing lr to 0.0008250561867093624
Epoch 17: reducing lr to 0.001023025312362188
Epoch 20: reducing lr to 0.0011683428049462962
Epoch 23: reducing lr to 0.0012405992528766198
Epoch 26: reducing lr to 0.0012444720114634657
Epoch 29: reducing lr to 0.0012333996265722186
Epoch 32: reducing lr to 0.001212704299131828
Epoch 35: reducing lr to 0.0011827124065160082
Epoch 38: reducing lr to 0.0011438969307507086
Epoch 41: reducing lr to 0.001096870020009464
Epoch 44: reducing lr to 0.0010423733170099874
Epoch 48: reducing lr to 0.0009595921797404235
Epoch 51: reducing lr to 0.000891186303774444
Epoch 54: reducing lr to 0.0008185544502395589
Epoch 57: reducing lr to 0.0007428418437621453
Epoch 61: reducing lr to 0.0006391681441244521
Epoch 64: reducing lr to 0.0005609598370986434
Epoch 67: reducing lr to 0.0004837330763085933
Epoch 70: reducing lr to 0.0004087062896151156
Epoch 73: reducing lr to 0.0003370624597823821
Epoch 76: reducing lr to 0.00026993135451779517
Epoch 79: reducing lr to 0.00020837186923974103
Epoch 82: reducing lr to 0.00015335465162487818
Epoch 85: reducing lr to 0.00010574751815611901
Epoch 88: reducing lr to 6.630106850451703e-05
Epoch 91: reducing lr to 3.5637630369459056e-05
Epoch 94: reducing lr to 1.4240628710994984e-05
Epoch 97: reducing lr to 2.4475700685158697e-06
[I 2024-06-20 23:40:39,335] Trial 200 finished with value: 1.1060611009597778 and parameters: {'hidden_size': 111, 'n_layers': 3, 'rnn_dropout': 0.5682836626556875, 'bidirectional': False, 'fc_dropout': 0.7923249575119574, 'learning_rate_model': 0.012464216955083157}. Best is trial 115 with value: 0.9733659029006958.
Epoch 11: reducing lr to 0.0026116423723258333
Epoch 16: reducing lr to 0.004170159947735878
Epoch 19: reducing lr to 0.004888171038048231
Epoch 22: reducing lr to 0.005314405838281454
Epoch 25: reducing lr to 0.005403296808030933
Epoch 28: reducing lr to 0.005369392189192814
Epoch 31: reducing lr to 0.005293430856399947
Epoch 34: reducing lr to 0.005176610866151365
Epoch 37: reducing lr to 0.005020774398066699
Epoch 40: reducing lr to 0.004828379061197578
Epoch 43: reducing lr to 0.004602459186830522
Epoch 46: reducing lr to 0.004346577398990291
Epoch 49: reducing lr to 0.00406476948543749
Epoch 52: reducing lr to 0.0037614795435025406
Epoch 55: reducing lr to 0.0034414901479706424
Epoch 58: reducing lr to 0.0031098489231877425
Epoch 61: reducing lr to 0.002771784582167113
Epoch 64: reducing lr to 0.0024326303524010557
Epoch 67: reducing lr to 0.0020977326469126395
Epoch 70: reducing lr to 0.0017723752389783188
Epoch 73: reducing lr to 0.0014616881924425494
Epoch 76: reducing lr to 0.0011705708013981207
Epoch 79: reducing lr to 0.0009036150187165762
Epoch 82: reducing lr to 0.000665030106529655
Epoch 85: reducing lr to 0.0004585793943612061
Epoch 88: reducing lr to 0.0002875178951757082
Epoch 91: reducing lr to 0.00015454436412557505
Epoch 94: reducing lr to 6.175519769617416e-05
Epoch 97: reducing lr to 1.0614009853352618e-05
[I 2024-06-20 23:40:47,530] Trial 201 finished with value: 1.381685733795166 and parameters: {'hidden_size': 77, 'n_layers': 1, 'rnn_dropout': 0.1479244722227267, 'bidirectional': True, 'fc_dropout': 0.6405277265033917, 'learning_rate_model': 0.05405169938844537}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:41:20,318] Trial 202 finished with value: 1.1066770553588867 and parameters: {'hidden_size': 48, 'n_layers': 7, 'rnn_dropout': 0.7659159410613351, 'bidirectional': True, 'fc_dropout': 0.04606680448653897, 'learning_rate_model': 1.0548472419055645e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 65: reducing lr to 5.567772079788182e-06
Epoch 68: reducing lr to 4.770377090844175e-06
Epoch 71: reducing lr to 4.0000293533015775e-06
Epoch 74: reducing lr to 3.268874229235841e-06
Epoch 77: reducing lr to 2.58844462590287e-06
Epoch 80: reducing lr to 1.969469310942962e-06
Epoch 83: reducing lr to 1.4217117184977437e-06
Epoch 86: reducing lr to 9.538080580712163e-07
Epoch 89: reducing lr to 5.731402210908625e-07
Epoch 92: reducing lr to 2.8570968147031895e-07
Epoch 95: reducing lr to 9.605023736930106e-08
Epoch 98: reducing lr to 7.152426736211433e-09
[I 2024-06-20 23:41:25,026] Trial 203 finished with value: 1.091509461402893 and parameters: {'hidden_size': 73, 'n_layers': 1, 'rnn_dropout': 0.23508998837643535, 'bidirectional': False, 'fc_dropout': 0.38156073798942236, 'learning_rate_model': 0.00012970411945941392}. Best is trial 115 with value: 0.9733659029006958.
Epoch 10: reducing lr to 0.0015447666539833717
Epoch 13: reducing lr to 0.0022008856816026555
Epoch 16: reducing lr to 0.0028143019376201985
Epoch 19: reducing lr to 0.0032988636877746384
Epoch 22: reducing lr to 0.0035865153460350836
Epoch 25: reducing lr to 0.003646504898363579
Epoch 28: reducing lr to 0.003623623801310648
Epoch 31: reducing lr to 0.0035723600299583645
Epoch 35: reducing lr to 0.003461317852567093
Epoch 38: reducing lr to 0.003347720752813918
Epoch 41: reducing lr to 0.0032100921249218284
Epoch 44: reducing lr to 0.003050602455278643
Epoch 47: reducing lr to 0.0028717670619558965
Epoch 50: reducing lr to 0.002676406229273673
Epoch 53: reducing lr to 0.002467601072756513
Epoch 56: reducing lr to 0.0022486439302147254
Epoch 59: reducing lr to 0.0020229888575323367
Epoch 62: reducing lr to 0.001794193742424449
Epoch 65: reducing lr to 0.001565867494623475
Epoch 68: reducing lr to 0.0013416099503723883
Epoch 71: reducing lr to 0.0011249591132891684
Epoch 74: reducing lr to 0.0009193307172457935
Epoch 77: reducing lr to 0.0007279682507205509
Epoch 80: reducing lr to 0.0005538890478040913
Epoch 83: reducing lr to 0.00039983895440015777
Epoch 86: reducing lr to 0.0002682467983316728
Epoch 89: reducing lr to 0.00016118864587245097
Epoch 92: reducing lr to 8.035233782964432e-05
Epoch 95: reducing lr to 2.7012949235735023e-05
Epoch 98: reducing lr to 2.0115321484819036e-06
[I 2024-06-20 23:41:55,923] Trial 204 finished with value: 1.1908302307128906 and parameters: {'hidden_size': 109, 'n_layers': 5, 'rnn_dropout': 0.08899653805394801, 'bidirectional': False, 'fc_dropout': 0.6753998876555427, 'learning_rate_model': 0.03647769011909392}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.0015568582133687731
Epoch 13: reducing lr to 0.0025713116903842066
Epoch 17: reducing lr to 0.0034978883949046157
Epoch 20: reducing lr to 0.003994752318743316
Epoch 23: reducing lr to 0.004241808757736908
Epoch 26: reducing lr to 0.004255050343407859
Epoch 29: reducing lr to 0.0042171920752428396
Epoch 32: reducing lr to 0.004146431415845914
Epoch 35: reducing lr to 0.0040438843020507865
Epoch 38: reducing lr to 0.003911167935621259
Epoch 41: reducing lr to 0.0037503753498051825
Epoch 44: reducing lr to 0.0035640423405639164
Epoch 47: reducing lr to 0.003355107573370381
Epoch 50: reducing lr to 0.0031268660081141602
Epoch 53: reducing lr to 0.0028829173357896114
Epoch 56: reducing lr to 0.002627108020014135
Epoch 59: reducing lr to 0.0023634734608760114
Epoch 62: reducing lr to 0.002096170365991357
Epoch 65: reducing lr to 0.0018294150523920209
Epoch 68: reducing lr to 0.0015674132364822662
Epoch 71: reducing lr to 0.0013142983951344178
Epoch 74: reducing lr to 0.0010740611565349708
Epoch 77: reducing lr to 0.0008504909132505463
Epoch 80: reducing lr to 0.0006471128399351198
Epoch 83: reducing lr to 0.00046713492950322885
Epoch 86: reducing lr to 0.00031339480020429816
Epoch 89: reducing lr to 0.0001883179362533828
Epoch 92: reducing lr to 9.387625506319456e-05
Epoch 95: reducing lr to 3.155943661327307e-05
Epoch 98: reducing lr to 2.3500884994663573e-06
[I 2024-06-20 23:42:14,828] Trial 205 finished with value: 1.2455395460128784 and parameters: {'hidden_size': 86, 'n_layers': 4, 'rnn_dropout': 0.08126518075693295, 'bidirectional': False, 'fc_dropout': 0.22769917517498836, 'learning_rate_model': 0.042617166273324125}. Best is trial 115 with value: 0.9733659029006958.
Epoch 20: reducing lr to 2.2509660646325403e-05
Epoch 23: reducing lr to 2.3901776141482007e-05
Epoch 26: reducing lr to 2.3976389928793385e-05
Epoch 61: reducing lr to 1.2314414878297945e-05
Epoch 65: reducing lr to 1.0308401804389814e-05
Epoch 68: reducing lr to 8.832071986098383e-06
Epoch 71: reducing lr to 7.405818559432642e-06
Epoch 74: reducing lr to 6.0521279463472695e-06
Epoch 83: reducing lr to 2.6322154416999486e-06
Epoch 86: reducing lr to 1.7659193957589085e-06
Epoch 89: reducing lr to 1.0611353346716359e-06
Epoch 92: reducing lr to 5.28974633622631e-07
Epoch 95: reducing lr to 1.7783135265254024e-07
Epoch 98: reducing lr to 1.3242296490728459e-08
[I 2024-06-20 23:46:10,868] Trial 206 finished with value: 1.0370901823043823 and parameters: {'hidden_size': 180, 'n_layers': 7, 'rnn_dropout': 0.1249196515526358, 'bidirectional': True, 'fc_dropout': 0.16850254273504195, 'learning_rate_model': 0.0002401395315598264}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.00014174618579402087
Epoch 18: reducing lr to 0.00017075353679316333
Epoch 21: reducing lr to 0.00019019116381200658
Epoch 24: reducing lr to 0.00019732910968565557
Epoch 27: reducing lr to 0.00019661681048120544
Epoch 30: reducing lr to 0.0001943516694574143
Epoch 33: reducing lr to 0.00019057757404835504
Epoch 36: reducing lr to 0.0001853540438511119
Epoch 39: reducing lr to 0.0001787634557048268
Epoch 42: reducing lr to 0.00017090974799580027
Epoch 45: reducing lr to 0.00016191677842673156
Epoch 48: reducing lr to 0.000151926375434679
Epoch 51: reducing lr to 0.00014109609043094255
Epoch 54: reducing lr to 0.00012959673218102173
Epoch 57: reducing lr to 0.00011760961711310063
Epoch 60: reducing lr to 0.00010532384234843411
Epoch 63: reducing lr to 9.29331167630662e-05
Epoch 66: reducing lr to 8.063288600772932e-05
Epoch 69: reducing lr to 6.861707806968766e-05
Epoch 72: reducing lr to 5.707526058243914e-05
Epoch 75: reducing lr to 4.6189402580862805e-05
Epoch 78: reducing lr to 3.6131212621188156e-05
Epoch 81: reducing lr to 2.7059284601197928e-05
Epoch 84: reducing lr to 1.911671497340003e-05
Epoch 87: reducing lr to 1.2428730324009649e-05
Epoch 90: reducing lr to 7.1008437761885345e-06
Epoch 93: reducing lr to 3.217052586746414e-06
Epoch 96: reducing lr to 8.386179054924951e-07
Epoch 99: reducing lr to 3.042029384078479e-09
[I 2024-06-20 23:46:21,595] Trial 207 finished with value: 1.093424677848816 and parameters: {'hidden_size': 45, 'n_layers': 4, 'rnn_dropout': 0.06701996307636841, 'bidirectional': False, 'fc_dropout': 0.5528218732453634, 'learning_rate_model': 0.0019733834274571717}. Best is trial 115 with value: 0.9733659029006958.
Epoch 5: reducing lr to 0.00143025312483944
Epoch 8: reducing lr to 0.0026747406770485835
Epoch 11: reducing lr to 0.004174558284197809
Epoch 14: reducing lr to 0.005719060633026405
Epoch 17: reducing lr to 0.007091327699577796
Epoch 20: reducing lr to 0.008098628250153072
Epoch 23: reducing lr to 0.008599489904786305
Epoch 26: reducing lr to 0.008626334793088495
Epoch 29: reducing lr to 0.00854958409226919
Epoch 32: reducing lr to 0.008406129822901202
Epoch 35: reducing lr to 0.0081982343424089
Epoch 38: reducing lr to 0.007929176231000892
Epoch 41: reducing lr to 0.00760319873001908
Epoch 44: reducing lr to 0.007225442701066559
Epoch 47: reducing lr to 0.0068018657498514655
Epoch 50: reducing lr to 0.006339147803717406
Epoch 53: reducing lr to 0.005844586576478057
Epoch 56: reducing lr to 0.005325980068216907
Epoch 59: reducing lr to 0.004791509313087758
Epoch 62: reducing lr to 0.004249601274026349
Epoch 65: reducing lr to 0.0037088037611348267
Epoch 68: reducing lr to 0.003177643093685576
Epoch 71: reducing lr to 0.0026644991385385503
Epoch 74: reducing lr to 0.0021774621630215553
Epoch 77: reducing lr to 0.001724214466121432
Epoch 80: reducing lr to 0.0013119026934275593
Epoch 83: reducing lr to 0.0009470304626791575
Epoch 86: reducing lr to 0.0006353505248565802
Epoch 89: reducing lr to 0.00038178010471296293
Epoch 92: reducing lr to 0.00019031690343008223
Epoch 95: reducing lr to 6.398097416852599e-05
Epoch 98: reducing lr to 4.764373756750706e-06
[I 2024-06-20 23:46:44,762] Trial 208 finished with value: 1.1473805904388428 and parameters: {'hidden_size': 177, 'n_layers': 2, 'rnn_dropout': 0.5036863653751306, 'bidirectional': False, 'fc_dropout': 0.6905811853710797, 'learning_rate_model': 0.08639849462086026}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-20 23:46:51,153] Trial 209 finished with value: 1.1075758934020996 and parameters: {'hidden_size': 17, 'n_layers': 2, 'rnn_dropout': 0.17907973626202028, 'bidirectional': True, 'fc_dropout': 0.39215624979538544, 'learning_rate_model': 1.5202975046828169e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 6: reducing lr to 0.00013490736298390303
Epoch 14: reducing lr to 0.00042747494118741577
Epoch 17: reducing lr to 0.0005300459438762021
Epoch 20: reducing lr to 0.0006053372847528165
Epoch 23: reducing lr to 0.0006427745179097717
Epoch 26: reducing lr to 0.0006447810567077508
Epoch 29: reducing lr to 0.00063904427519343
Epoch 32: reducing lr to 0.0006283216916616108
Epoch 35: reducing lr to 0.0006127824075030604
Epoch 38: reducing lr to 0.0005926714823476346
Epoch 41: reducing lr to 0.0005683060800548317
Epoch 44: reducing lr to 0.0005400704576997986
Epoch 47: reducing lr to 0.0005084099204319501
Epoch 50: reducing lr to 0.00047382376380549943
Epoch 53: reducing lr to 0.00043685746022989957
Epoch 56: reducing lr to 0.00039809387634024394
Epoch 59: reducing lr to 0.0003581445089046475
Epoch 62: reducing lr to 0.0003176392367994664
Epoch 65: reducing lr to 0.00027721697170183797
Epoch 68: reducing lr to 0.00023751501894272146
Epoch 71: reducing lr to 0.0001991597371713733
Epoch 74: reducing lr to 0.00016275583872991695
Epoch 77: reducing lr to 0.00012887754209902738
Epoch 80: reducing lr to 9.805902799456629e-05
Epoch 83: reducing lr to 7.078641359363159e-05
Epoch 86: reducing lr to 4.7489692044537255e-05
Epoch 89: reducing lr to 2.8536404539277696e-05
Epoch 92: reducing lr to 1.4225361876902057e-05
Epoch 95: reducing lr to 4.7822999133567e-06
Epoch 98: reducing lr to 3.5611624393354356e-07
[I 2024-06-20 23:47:08,346] Trial 210 finished with value: 1.0980643033981323 and parameters: {'hidden_size': 67, 'n_layers': 5, 'rnn_dropout': 0.2357536332487234, 'bidirectional': False, 'fc_dropout': 0.30442726176771207, 'learning_rate_model': 0.006457912195134263}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.001190263973778878
Epoch 19: reducing lr to 0.0014985816999217865
Epoch 22: reducing lr to 0.0016292538197243637
Epoch 25: reducing lr to 0.001656505398999719
Epoch 28: reducing lr to 0.0016461111552348948
Epoch 31: reducing lr to 0.0016228234547148016
Epoch 34: reducing lr to 0.0015870095893224267
Epoch 37: reducing lr to 0.0015392343217562221
Epoch 40: reducing lr to 0.0014802510888173286
Epoch 43: reducing lr to 0.0014109901348245482
Epoch 46: reducing lr to 0.0013325436644339075
Epoch 49: reducing lr to 0.0012461489415700384
Epoch 52: reducing lr to 0.0011531684068840971
Epoch 55: reducing lr to 0.0010550682691064701
Epoch 58: reducing lr to 0.0009533959940304038
Epoch 61: reducing lr to 0.0008497545643614621
Epoch 64: reducing lr to 0.0007457790041320028
Epoch 67: reducing lr to 0.000643108379703295
Epoch 70: reducing lr to 0.0005433625537759271
Epoch 73: reducing lr to 0.0004481142658749465
Epoch 76: reducing lr to 0.0003588655077295385
Epoch 79: reducing lr to 0.0002770240485209843
Epoch 82: reducing lr to 0.00020388033474792334
Epoch 85: reducing lr to 0.00014058810197143095
Epoch 88: reducing lr to 8.814524957424276e-05
Epoch 91: reducing lr to 4.737914326277508e-05
Epoch 94: reducing lr to 1.8932481785557607e-05
Epoch 97: reducing lr to 3.253969798768658e-06
[I 2024-06-20 23:48:18,981] Trial 211 finished with value: 1.1796278953552246 and parameters: {'hidden_size': 107, 'n_layers': 5, 'rnn_dropout': 0.28271547760130356, 'bidirectional': True, 'fc_dropout': 0.7496643078391715, 'learning_rate_model': 0.0165707965050135}. Best is trial 115 with value: 0.9733659029006958.
Epoch 27: reducing lr to 0.00016000570390787267
Epoch 31: reducing lr to 0.00015727300561201938
Epoch 35: reducing lr to 0.00015238437825039488
Epoch 39: reducing lr to 0.0001454767397205276
Epoch 44: reducing lr to 0.00013430264952177004
Epoch 47: reducing lr to 0.00012642942857488696
Epoch 50: reducing lr to 0.00011782867582960498
Epoch 53: reducing lr to 0.00010863603727208403
Epoch 56: reducing lr to 9.89964579410591e-05
Epoch 59: reducing lr to 8.906200250690969e-05
Epoch 62: reducing lr to 7.898930683217256e-05
Epoch 65: reducing lr to 6.893725302162974e-05
Epoch 68: reducing lr to 5.9064323720058195e-05
Epoch 71: reducing lr to 4.9526279393424344e-05
Epoch 74: reducing lr to 4.0473497587079596e-05
Epoch 77: reducing lr to 3.204877274989538e-05
Epoch 80: reducing lr to 2.4384942892988356e-05
Epoch 83: reducing lr to 1.760289377104381e-05
Epoch 86: reducing lr to 1.1809554430580478e-05
Epoch 89: reducing lr to 7.096323605207053e-06
Epoch 92: reducing lr to 3.5375084180884517e-06
Epoch 95: reducing lr to 1.18924399587978e-06
Epoch 98: reducing lr to 8.855762135503466e-08
[I 2024-06-20 23:49:19,251] Trial 212 finished with value: 0.9843335151672363 and parameters: {'hidden_size': 189, 'n_layers': 2, 'rnn_dropout': 0.7465246102585805, 'bidirectional': True, 'fc_dropout': 0.2197073987390904, 'learning_rate_model': 0.001605928829877941}. Best is trial 115 with value: 0.9733659029006958.
Epoch 22: reducing lr to 0.00037360446150522285
Epoch 25: reducing lr to 0.0003798535256332779
Epoch 28: reducing lr to 0.00037747001988512613
Epoch 31: reducing lr to 0.00037212991344672214
Epoch 34: reducing lr to 0.00036391743008019396
Epoch 37: reducing lr to 0.0003529620756128591
Epoch 40: reducing lr to 0.00033943662076156984
Epoch 43: reducing lr to 0.00032355438000414847
Epoch 46: reducing lr to 0.0003055658069699977
Epoch 49: reducing lr to 0.00028575461885327477
Epoch 52: reducing lr to 0.00026443323714389464
Epoch 55: reducing lr to 0.00024193787840709634
Epoch 58: reducing lr to 0.00021862339227857462
Epoch 61: reducing lr to 0.00019485735898632357
Epoch 64: reducing lr to 0.00017101470616025694
Epoch 67: reducing lr to 0.0001474712883773962
Epoch 70: reducing lr to 0.00012459855662017217
Epoch 73: reducing lr to 0.00010275715604787794
Epoch 76: reducing lr to 8.22915086310954e-05
Epoch 79: reducing lr to 6.352443015244162e-05
Epoch 82: reducing lr to 4.675183311087091e-05
Epoch 85: reducing lr to 3.223828079774813e-05
Epoch 88: reducing lr to 2.021260168473919e-05
Epoch 91: reducing lr to 1.0864519138130717e-05
Epoch 94: reducing lr to 4.341410513708293e-06
Epoch 97: reducing lr to 7.461683500173399e-07
[I 2024-06-20 23:49:41,326] Trial 213 finished with value: 1.145073652267456 and parameters: {'hidden_size': 50, 'n_layers': 4, 'rnn_dropout': 0.14825411675014752, 'bidirectional': True, 'fc_dropout': 0.7711342139391983, 'learning_rate_model': 0.003799852073396136}. Best is trial 115 with value: 0.9733659029006958.
Epoch 31: reducing lr to 1.691830278607033e-06
Epoch 34: reducing lr to 1.6544935112040592e-06
Epoch 37: reducing lr to 1.604686710592306e-06
Epoch 40: reducing lr to 1.5431953517348577e-06
Epoch 43: reducing lr to 1.470989235444292e-06
Epoch 46: reducing lr to 1.3892070098601417e-06
Epoch 49: reducing lr to 1.2991385507013205e-06
Epoch 52: reducing lr to 1.2022042332648053e-06
Epoch 55: reducing lr to 1.0999326134250006e-06
Epoch 58: reducing lr to 9.93936959388325e-07
Epoch 61: reducing lr to 8.858884170021465e-07
Epoch 64: reducing lr to 7.77491535924135e-07
Epoch 67: reducing lr to 6.704550800315834e-07
Epoch 70: reducing lr to 5.664677929497336e-07
Epoch 73: reducing lr to 4.6716929132394933e-07
Epoch 76: reducing lr to 3.741254356169133e-07
Epoch 79: reducing lr to 2.8880385714690797e-07
Epoch 82: reducing lr to 2.1254987567313235e-07
Epoch 85: reducing lr to 1.4656628669996666e-07
Epoch 88: reducing lr to 9.189342297944874e-08
Epoch 91: reducing lr to 4.9393832036098226e-08
Epoch 94: reducing lr to 1.9737541900152114e-08
Epoch 97: reducing lr to 3.3923373582230735e-09
[I 2024-06-20 23:50:20,659] Trial 214 finished with value: 1.1079710721969604 and parameters: {'hidden_size': 105, 'n_layers': 7, 'rnn_dropout': 0.11455182536701028, 'bidirectional': False, 'fc_dropout': 0.3470718617221808, 'learning_rate_model': 1.727543140097415e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 10: reducing lr to 0.00023583552041676176
Epoch 13: reducing lr to 0.00033600351144306154
Epoch 16: reducing lr to 0.00042965218103141744
Epoch 19: reducing lr to 0.0005036289672515576
Epoch 22: reducing lr to 0.0005475440002111745
Epoch 25: reducing lr to 0.0005567024496484909
Epoch 28: reducing lr to 0.0005532092518782841
Epoch 31: reducing lr to 0.0005453829448019263
Epoch 34: reducing lr to 0.0005333469643533531
Epoch 37: reducing lr to 0.0005172911105645406
Epoch 40: reducing lr to 0.0004974685912506298
Epoch 43: reducing lr to 0.00047419203400182427
Epoch 46: reducing lr to 0.00044782849648536437
Epoch 49: reducing lr to 0.00041879378649645557
Epoch 52: reducing lr to 0.0003875457800241877
Epoch 55: reducing lr to 0.0003545772264386472
Epoch 58: reducing lr to 0.0003204081832044024
Epoch 61: reducing lr to 0.00028557736537754096
Epoch 64: reducing lr to 0.0002506342561560032
Epoch 67: reducing lr to 0.00021612969724486646
Epoch 70: reducing lr to 0.00018260807656707772
Epoch 73: reducing lr to 0.0001505979453406298
Epoch 76: reducing lr to 0.0001206040785427089
Epoch 79: reducing lr to 9.309958574013708e-05
Epoch 82: reducing lr to 6.851814781760482e-05
Epoch 85: reducing lr to 4.724750115887829e-05
Epoch 88: reducing lr to 2.962301021927835e-05
Epoch 91: reducing lr to 1.5922728131499523e-05
Epoch 94: reducing lr to 6.362646927869815e-06
Epoch 97: reducing lr to 1.0935629664415338e-06
[I 2024-06-20 23:50:44,605] Trial 215 finished with value: 1.0979833602905273 and parameters: {'hidden_size': 107, 'n_layers': 4, 'rnn_dropout': 0.6453974486198909, 'bidirectional': False, 'fc_dropout': 0.1921377657771018, 'learning_rate_model': 0.005568954385864473}. Best is trial 115 with value: 0.9733659029006958.
Epoch 6: reducing lr to 0.001149624417915987
Epoch 10: reducing lr to 0.0023304941575967423
Epoch 13: reducing lr to 0.003320340460021688
Epoch 18: reducing lr to 0.004761794986518019
Epoch 21: reducing lr to 0.005303851078745568
Epoch 24: reducing lr to 0.005502906603530103
Epoch 27: reducing lr to 0.005483042752717104
Epoch 30: reducing lr to 0.005419874882970955
Epoch 33: reducing lr to 0.005314626880879682
Epoch 36: reducing lr to 0.00516895856634698
Epoch 39: reducing lr to 0.00498516717799524
Epoch 42: reducing lr to 0.004766151240189389
Epoch 45: reducing lr to 0.004515364766233231
Epoch 48: reducing lr to 0.0042367629183635985
Epoch 51: reducing lr to 0.0039347393245810405
Epoch 54: reducing lr to 0.003614057320032131
Epoch 57: reducing lr to 0.003279773266505419
Epoch 60: reducing lr to 0.0029371605055718425
Epoch 63: reducing lr to 0.002591620986567936
Epoch 66: reducing lr to 0.002248605092175505
Epoch 69: reducing lr to 0.001913520882074264
Epoch 72: reducing lr to 0.0015916548189855695
Epoch 75: reducing lr to 0.0012880814639068788
Epoch 78: reducing lr to 0.0010075892444019873
Epoch 81: reducing lr to 0.0007546008602376863
Epoch 84: reducing lr to 0.0005331068347316066
Epoch 87: reducing lr to 0.0003465993551708568
Epoch 90: reducing lr to 0.0001980208605251922
Epoch 93: reducing lr to 8.971377792010293e-05
Epoch 96: reducing lr to 2.3386493849410218e-05
Epoch 99: reducing lr to 8.483291498370682e-08
[I 2024-06-20 23:51:50,958] Trial 216 finished with value: 1.1486375331878662 and parameters: {'hidden_size': 199, 'n_layers': 2, 'rnn_dropout': 0.6571573921810379, 'bidirectional': True, 'fc_dropout': 0.11105321443653891, 'learning_rate_model': 0.055031640853951194}. Best is trial 115 with value: 0.9733659029006958.
Epoch 18: reducing lr to 0.0002861345610033671
Epoch 22: reducing lr to 0.0003251303040144783
Epoch 27: reducing lr to 0.00032947408182279275
Epoch 30: reducing lr to 0.0003256783470777005
Epoch 33: reducing lr to 0.00031935403220061483
Epoch 36: reducing lr to 0.00031060087517707124
Epoch 39: reducing lr to 0.00029955691625588304
Epoch 42: reducing lr to 0.00028639632673150233
Epoch 45: reducing lr to 0.0002713266570304344
Epoch 48: reducing lr to 0.0002545855714396841
Epoch 51: reducing lr to 0.00023643708149748594
Epoch 54: reducing lr to 0.00021716741431251153
Epoch 57: reducing lr to 0.0001970804048597517
Epoch 60: reducing lr to 0.00017649292635186963
Epoch 63: reducing lr to 0.00015572957999625617
Epoch 66: reducing lr to 0.00013511787734273115
Epoch 69: reducing lr to 0.00011498278676702607
Epoch 72: reducing lr to 9.564196992704913e-05
Epoch 75: reducing lr to 7.740035538175821e-05
Epoch 78: reducing lr to 6.05456780342187e-05
Epoch 81: reducing lr to 4.534369633472177e-05
Epoch 84: reducing lr to 3.203419940499479e-05
Epoch 87: reducing lr to 2.0827031532573904e-05
Epoch 90: reducing lr to 1.1899002824839562e-05
Epoch 93: reducing lr to 5.390868891626309e-06
Epoch 96: reducing lr to 1.4052860675344004e-06
Epoch 99: reducing lr to 5.097579580020176e-09
[I 2024-06-20 23:53:20,292] Trial 217 finished with value: 1.0820434093475342 and parameters: {'hidden_size': 180, 'n_layers': 3, 'rnn_dropout': 0.7582112528812855, 'bidirectional': True, 'fc_dropout': 0.3744396438931783, 'learning_rate_model': 0.003306831655210467}. Best is trial 115 with value: 0.9733659029006958.
Epoch 53: reducing lr to 1.3087003035264048e-05
Epoch 56: reducing lr to 1.1925756664984218e-05
Epoch 59: reducing lr to 1.0728987603031168e-05
Epoch 62: reducing lr to 9.51556522332467e-06
Epoch 65: reducing lr to 8.304629496722868e-06
Epoch 68: reducing lr to 7.1152722725357505e-06
Epoch 71: reducing lr to 5.9662574687233494e-06
Epoch 74: reducing lr to 4.875700541646808e-06
Epoch 77: reducing lr to 3.860803438585539e-06
Epoch 80: reducing lr to 2.93756868962381e-06
Epoch 83: reducing lr to 2.12055897836287e-06
Epoch 86: reducing lr to 1.422655672638729e-06
Epoch 89: reducing lr to 8.548692578684954e-07
Epoch 92: reducing lr to 4.26151253003155e-07
Epoch 95: reducing lr to 1.4326406020102356e-07
Epoch 98: reducing lr to 1.0668226571691991e-08
[I 2024-06-20 23:53:24,436] Trial 218 finished with value: 1.0925781726837158 and parameters: {'hidden_size': 45, 'n_layers': 1, 'rnn_dropout': 0.2124096139779087, 'bidirectional': False, 'fc_dropout': 0.4411750686502212, 'learning_rate_model': 0.00019346062318522451}. Best is trial 115 with value: 0.9733659029006958.
Epoch 7: reducing lr to 0.0007764695195867614
Epoch 16: reducing lr to 0.0023294215082641746
Epoch 19: reducing lr to 0.0027304973657630914
Epoch 22: reducing lr to 0.0029685890753563945
Epoch 25: reducing lr to 0.0030182429350212535
Epoch 28: reducing lr to 0.0029993040575343384
Epoch 31: reducing lr to 0.0029568726005586433
Epoch 34: reducing lr to 0.0028916177898821367
Epoch 37: reducing lr to 0.0028045686538590844
Epoch 40: reducing lr to 0.002697097995320918
Epoch 43: reducing lr to 0.0025709007700128847
Epoch 46: reducing lr to 0.0024279670342237476
Epoch 49: reducing lr to 0.0022705511501195045
Epoch 52: reducing lr to 0.002101135558670298
Epoch 55: reducing lr to 0.0019223917719306185
Epoch 58: reducing lr to 0.001737139356743092
Epoch 61: reducing lr to 0.0015482990347841785
Epoch 64: reducing lr to 0.0013588499087704969
Epoch 67: reducing lr to 0.0011717784467618053
Epoch 70: reducing lr to 0.0009900361267036085
Epoch 73: reducing lr to 0.0008164885655525245
Epoch 76: reducing lr to 0.0006538724739331067
Epoch 79: reducing lr to 0.0005047528838628239
Epoch 82: reducing lr to 0.00037148105905013826
Epoch 85: reducing lr to 0.0002561591684395053
Epoch 88: reducing lr to 0.00016060543898244704
Epoch 91: reducing lr to 8.632737599683221e-05
Epoch 94: reducing lr to 3.4496011559143755e-05
Epoch 97: reducing lr to 5.928909958178435e-06
[I 2024-06-20 23:54:08,865] Trial 219 finished with value: 1.1854342222213745 and parameters: {'hidden_size': 160, 'n_layers': 2, 'rnn_dropout': 0.1959045110095069, 'bidirectional': True, 'fc_dropout': 0.07035094701648266, 'learning_rate_model': 0.030192892524910118}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 0.0019866738444177696
Epoch 18: reducing lr to 0.0023932325479414365
Epoch 21: reducing lr to 0.0026656647476480263
Epoch 24: reducing lr to 0.002765708147691643
Epoch 27: reducing lr to 0.002755724766544887
Epoch 30: reducing lr to 0.002723977200282872
Epoch 33: reducing lr to 0.0026710805625813766
Epoch 36: reducing lr to 0.0025978690630249135
Epoch 39: reducing lr to 0.0025054972717403062
Epoch 42: reducing lr to 0.0023954219593089834
Epoch 45: reducing lr to 0.0022693790797321325
Epoch 48: reducing lr to 0.0021293564596642765
Epoch 51: reducing lr to 0.001977562294452807
Epoch 54: reducing lr to 0.001816390590715358
Epoch 57: reducing lr to 0.001648382627452974
Epoch 60: reducing lr to 0.0014761887356269233
Epoch 63: reducing lr to 0.0013025238832295558
Epoch 66: reducing lr to 0.0011301273803886198
Epoch 69: reducing lr to 0.0009617172660964172
Epoch 72: reducing lr to 0.0007999504658787321
Epoch 75: reducing lr to 0.000647377405484681
Epoch 78: reducing lr to 0.0005064047027404355
Epoch 81: reducing lr to 0.00037925516418463
Epoch 84: reducing lr to 0.000267934388612278
Epoch 87: reducing lr to 0.0001741975159029182
Epoch 90: reducing lr to 9.95233876977135e-05
Epoch 93: reducing lr to 4.508928543229436e-05
Epoch 96: reducing lr to 1.1753827794162841e-05
Epoch 99: reducing lr to 4.263621047329025e-08
[I 2024-06-20 23:54:17,523] Trial 220 finished with value: 1.1649038791656494 and parameters: {'hidden_size': 32, 'n_layers': 5, 'rnn_dropout': 0.09632070148617872, 'bidirectional': False, 'fc_dropout': 0.7256872441762966, 'learning_rate_model': 0.02765837555610633}. Best is trial 115 with value: 0.9733659029006958.
Epoch 6: reducing lr to 0.0003556796079810656
Epoch 10: reducing lr to 0.0007210261329337461
Epoch 15: reducing lr to 0.0012229687336220714
Epoch 18: reducing lr to 0.001473240605972153
Epoch 22: reducing lr to 0.0016740206580657126
Epoch 25: reducing lr to 0.0017020210261603406
Epoch 28: reducing lr to 0.0016913411808368965
Epoch 31: reducing lr to 0.001667413606583255
Epoch 34: reducing lr to 0.0016306156873234043
Epoch 37: reducing lr to 0.0015815277036819275
Epoch 40: reducing lr to 0.0015209237945648605
Epoch 43: reducing lr to 0.0014497597645177383
Epoch 46: reducing lr to 0.00136915782859072
Epoch 49: reducing lr to 0.0012803892468810593
Epoch 52: reducing lr to 0.0011848538956804745
Epoch 55: reducing lr to 0.0010840582705846727
Epoch 58: reducing lr to 0.0009795923569441145
Epoch 61: reducing lr to 0.0008731031824540247
Epoch 64: reducing lr to 0.0007662706965326369
Epoch 67: reducing lr to 0.0006607790019977482
Epoch 70: reducing lr to 0.0005582924703494812
Epoch 73: reducing lr to 0.00046042705511380785
Epoch 76: reducing lr to 0.0003687260180909825
Epoch 79: reducing lr to 0.00028463580959017294
Epoch 82: reducing lr to 0.0002094823335747141
Epoch 85: reducing lr to 0.00014445102667811453
Epoch 88: reducing lr to 9.056720746102274e-05
Epoch 91: reducing lr to 4.868097507161814e-05
Epoch 94: reducing lr to 1.9452687625331684e-05
Epoch 97: reducing lr to 3.3433787896729877e-06
[I 2024-06-20 23:54:22,704] Trial 221 finished with value: 1.2486010789871216 and parameters: {'hidden_size': 78, 'n_layers': 1, 'rnn_dropout': 0.6406874841040265, 'bidirectional': False, 'fc_dropout': 0.5357172799004265, 'learning_rate_model': 0.017026110563109636}. Best is trial 115 with value: 0.9733659029006958.
Epoch 15: reducing lr to 8.660728975363162e-05
Epoch 18: reducing lr to 0.0001043308569797629
Epoch 22: reducing lr to 0.0001185495493063566
Epoch 27: reducing lr to 0.00012013338475664946
Epoch 30: reducing lr to 0.00011874937767468543
Epoch 33: reducing lr to 0.00011644339552201409
Epoch 36: reducing lr to 0.000113251804927916
Epoch 39: reducing lr to 0.00010922493835627069
Epoch 42: reducing lr to 0.00010442630243258941
Epoch 45: reducing lr to 9.893157453673108e-05
Epoch 48: reducing lr to 9.282741221418665e-05
Epoch 51: reducing lr to 8.62100797887763e-05
Epoch 54: reducing lr to 7.918394186236363e-05
Epoch 57: reducing lr to 7.185978324615797e-05
Epoch 60: reducing lr to 6.435314277515765e-05
Epoch 63: reducing lr to 5.678237707858282e-05
Epoch 66: reducing lr to 4.926690395952464e-05
Epoch 69: reducing lr to 4.192521392473107e-05
Epoch 72: reducing lr to 3.48731332934099e-05
Epoch 75: reducing lr to 2.8221845621165687e-05
Epoch 78: reducing lr to 2.2076265284348217e-05
Epoch 81: reducing lr to 1.653329356874156e-05
Epoch 84: reducing lr to 1.1680362780588363e-05
Epoch 87: reducing lr to 7.593986691151276e-06
Epoch 90: reducing lr to 4.3386340942768015e-06
Epoch 93: reducing lr to 1.965627533272022e-06
Epoch 96: reducing lr to 5.123977306811973e-07
Epoch 99: reducing lr to 1.8586878992952333e-09
[I 2024-06-20 23:55:13,854] Trial 222 finished with value: 1.0926882028579712 and parameters: {'hidden_size': 182, 'n_layers': 4, 'rnn_dropout': 0.4462602429719284, 'bidirectional': False, 'fc_dropout': 0.5463254941038999, 'learning_rate_model': 0.0012057424285486997}. Best is trial 115 with value: 0.9733659029006958.
Epoch 14: reducing lr to 0.0006145774586013375
Epoch 17: reducing lr to 0.0007620430058999986
Epoch 23: reducing lr to 0.0009241120159543906
Epoch 26: reducing lr to 0.0009269968014616334
Epoch 29: reducing lr to 0.0009187490744865061
Epoch 32: reducing lr to 0.0009033332980240988
Epoch 35: reducing lr to 0.0008809925878526014
Epoch 38: reducing lr to 0.0008520792643304994
Epoch 41: reducing lr to 0.0008170493115166224
Epoch 44: reducing lr to 0.0007764551728735916
Epoch 47: reducing lr to 0.0007309370602142196
Epoch 50: reducing lr to 0.0006812128069440081
Epoch 53: reducing lr to 0.000628066634580615
Epoch 56: reducing lr to 0.0005723365260343468
Epoch 59: reducing lr to 0.0005149016255391253
Epoch 62: reducing lr to 0.00045666750514551874
Epoch 65: reducing lr to 0.000398552723292801
Epoch 68: reducing lr to 0.00034147352898861226
Epoch 71: reducing lr to 0.00028633043327990095
Epoch 74: reducing lr to 0.00023399282648314958
Epoch 77: reducing lr to 0.00018528625812309682
Epoch 80: reducing lr to 0.00014097871573575202
Epoch 83: reducing lr to 0.0001017690862744728
Epoch 86: reducing lr to 6.827556760501657e-05
Epoch 89: reducing lr to 4.102657089244434e-05
Epoch 92: reducing lr to 2.045169414072838e-05
Epoch 95: reducing lr to 6.87547606616698e-06
Epoch 98: reducing lr to 5.119856044788485e-07
[I 2024-06-20 23:57:06,194] Trial 223 finished with value: 1.1631786823272705 and parameters: {'hidden_size': 169, 'n_layers': 4, 'rnn_dropout': 0.6598868499081856, 'bidirectional': True, 'fc_dropout': 0.7758964562757713, 'learning_rate_model': 0.00928449104813407}. Best is trial 115 with value: 0.9733659029006958.
Epoch 20: reducing lr to 7.411040794764217e-06
Epoch 23: reducing lr to 7.869378434221841e-06
Epoch 26: reducing lr to 7.893944145376034e-06
Epoch 29: reducing lr to 7.82370971094723e-06
Epoch 32: reducing lr to 7.692434955565151e-06
Epoch 35: reducing lr to 7.502190158621119e-06
Epoch 38: reducing lr to 7.255975543230015e-06
Epoch 41: reducing lr to 6.957674092252089e-06
Epoch 44: reducing lr to 6.611990199306101e-06
Epoch 47: reducing lr to 6.2243756591378215e-06
Epoch 50: reducing lr to 5.800943261780378e-06
Epoch 58: reducing lr to 4.548873441289286e-06
Epoch 65: reducing lr to 3.3939199484833316e-06
Epoch 68: reducing lr to 2.907855734464595e-06
Epoch 71: reducing lr to 2.4382785829131096e-06
Epoch 74: reducing lr to 1.9925918835579682e-06
Epoch 99: reducing lr to 1.2187814720128475e-10
[I 2024-06-20 23:57:21,304] Trial 224 finished with value: 1.1078438758850098 and parameters: {'hidden_size': 64, 'n_layers': 5, 'rnn_dropout': 0.7642098028305495, 'bidirectional': False, 'fc_dropout': 0.4953078651359258, 'learning_rate_model': 7.906311395751618e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 55: reducing lr to 2.7724404233418027e-05
Epoch 58: reducing lr to 2.505272569272287e-05
Epoch 61: reducing lr to 2.2329302976290912e-05
Epoch 64: reducing lr to 1.9597100192257864e-05
Epoch 67: reducing lr to 1.689918767047422e-05
Epoch 72: reducing lr to 1.25939356776594e-05
Epoch 75: reducing lr to 1.0191917814421336e-05
Epoch 78: reducing lr to 7.972528956741785e-06
Epoch 81: reducing lr to 5.97076362461353e-06
Epoch 84: reducing lr to 4.2181967508567696e-06
Epoch 87: reducing lr to 2.742460194806579e-06
Epoch 90: reducing lr to 1.5668359436617555e-06
Epoch 93: reducing lr to 7.098584033727228e-07
Epoch 96: reducing lr to 1.850451465683757e-07
Epoch 99: reducing lr to 6.712386768268317e-10
[I 2024-06-20 23:59:03,846] Trial 225 finished with value: 0.977317750453949 and parameters: {'hidden_size': 157, 'n_layers': 4, 'rnn_dropout': 0.5329504721787962, 'bidirectional': True, 'fc_dropout': 0.5000968546443917, 'learning_rate_model': 0.0004354367145964692}. Best is trial 115 with value: 0.9733659029006958.
Epoch 8: reducing lr to 0.002173029772709269
Epoch 11: reducing lr to 0.003391521098591789
Epoch 14: reducing lr to 0.004646315485510583
Epoch 17: reducing lr to 0.00576118139281604
Epoch 20: reducing lr to 0.006579538890142407
Epoch 23: reducing lr to 0.006986452089939933
Epoch 26: reducing lr to 0.0070082615842308295
Epoch 29: reducing lr to 0.006945907293443773
Epoch 32: reducing lr to 0.006829361266745266
Epoch 35: reducing lr to 0.006660461502892283
Epoch 38: reducing lr to 0.0064418716067967535
Epoch 41: reducing lr to 0.0061770388994823126
Epoch 44: reducing lr to 0.005870139952314081
Epoch 47: reducing lr to 0.005526014881079335
Epoch 50: reducing lr to 0.0051500906346863325
Epoch 53: reducing lr to 0.004748296068042735
Epoch 56: reducing lr to 0.004326966481798212
Epoch 59: reducing lr to 0.003892748363569418
Epoch 62: reducing lr to 0.0034524879999926657
Epoch 65: reducing lr to 0.0030131298571251
Epoch 68: reducing lr to 0.002581600941307746
Epoch 71: reducing lr to 0.002164709277084545
Epoch 74: reducing lr to 0.0017690276107121157
Epoch 77: reducing lr to 0.0014007972442219047
Epoch 80: reducing lr to 0.001065824300717353
Epoch 83: reducing lr to 0.000769392490540518
Epoch 86: reducing lr to 0.0005161755001024087
Epoch 89: reducing lr to 0.0003101682123011512
Epoch 92: reducing lr to 0.00015461846486731075
Epoch 95: reducing lr to 5.197982852997995e-05
Epoch 98: reducing lr to 3.870702722910679e-06
[I 2024-06-20 23:59:09,853] Trial 226 finished with value: 1.1218527555465698 and parameters: {'hidden_size': 32, 'n_layers': 3, 'rnn_dropout': 0.5377155277254789, 'bidirectional': False, 'fc_dropout': 0.7598771111040699, 'learning_rate_model': 0.07019241257270516}. Best is trial 115 with value: 0.9733659029006958.
Epoch 11: reducing lr to 0.00011574250754056996
Epoch 14: reducing lr to 0.00015856490037489913
Epoch 17: reducing lr to 0.00019661195122078787
Epoch 20: reducing lr to 0.0002245400537009723
Epoch 23: reducing lr to 0.00023842678851016398
Epoch 26: reducing lr to 0.00023917108155273656
Epoch 29: reducing lr to 0.00023704311543906435
Epoch 32: reducing lr to 0.00023306574688323356
Epoch 35: reducing lr to 0.00022730170130513846
Epoch 38: reducing lr to 0.00021984187960223528
Epoch 41: reducing lr to 0.00021080392856720893
Epoch 44: reducing lr to 0.00020033038213355681
Epoch 47: reducing lr to 0.00018858641902838114
Epoch 50: reducing lr to 0.00017575724484429855
Epoch 53: reducing lr to 0.00016204519372988382
Epoch 56: reducing lr to 0.0001476664706155797
Epoch 59: reducing lr to 0.0001328479003155996
Epoch 62: reducing lr to 0.0001178231261892483
Epoch 65: reducing lr to 0.0001028291421668653
Epoch 68: reducing lr to 8.810234633071308e-05
Epoch 71: reducing lr to 7.387507626891406e-05
Epoch 74: reducing lr to 6.037164022283082e-05
Epoch 77: reducing lr to 4.78050352302048e-05
Epoch 80: reducing lr to 3.6373406968903244e-05
Epoch 83: reducing lr to 2.6257072726163866e-05
Epoch 86: reducing lr to 1.7615531490476924e-05
Epoch 89: reducing lr to 1.0585116709436668e-05
Epoch 92: reducing lr to 5.2766674054443e-06
Epoch 95: reducing lr to 1.7739166352486162e-06
Epoch 98: reducing lr to 1.3209554830125557e-07
[I 2024-06-20 23:59:38,888] Trial 227 finished with value: 1.0925801992416382 and parameters: {'hidden_size': 151, 'n_layers': 3, 'rnn_dropout': 0.5949993570809757, 'bidirectional': False, 'fc_dropout': 0.4847170889742864, 'learning_rate_model': 0.002395457850714959}. Best is trial 115 with value: 0.9733659029006958.
Epoch 18: reducing lr to 0.00036755264074721407
Epoch 23: reducing lr to 0.0004227930421415786
Epoch 29: reducing lr to 0.00042033942797047366
Epoch 32: reducing lr to 0.00041328651348068144
Epoch 35: reducing lr to 0.0004030653534330484
Epoch 38: reducing lr to 0.00038983713888839897
Epoch 41: reducing lr to 0.0003738104883735701
Epoch 44: reducing lr to 0.00035523815182378636
Epoch 47: reducing lr to 0.0003344130343147107
Epoch 50: reducing lr to 0.0003116635537913792
Epoch 53: reducing lr to 0.00028734850160747955
Epoch 56: reducing lr to 0.00026185126564001645
Epoch 59: reducing lr to 0.00023557406559690823
Epoch 62: reducing lr to 0.0002089312122494986
Epoch 65: reducing lr to 0.00018234295780771522
Epoch 68: reducing lr to 0.00015622849788703687
Epoch 71: reducing lr to 0.00013099982778505548
Epoch 74: reducing lr to 0.00010705470466796743
Epoch 77: reducing lr to 8.477082798018739e-05
Epoch 80: reducing lr to 6.449956182159755e-05
Epoch 83: reducing lr to 4.65606558935564e-05
Epoch 86: reducing lr to 3.123694360997534e-05
Epoch 89: reducing lr to 1.8770179823210648e-05
Epoch 92: reducing lr to 9.356911103225364e-06
Epoch 95: reducing lr to 3.1456180549542348e-06
Epoch 98: reducing lr to 2.3423994874333804e-07
[I 2024-06-21 00:00:32,755] Trial 228 finished with value: 1.1661567687988281 and parameters: {'hidden_size': 178, 'n_layers': 2, 'rnn_dropout': 0.6129506921622574, 'bidirectional': True, 'fc_dropout': 0.3808712944016839, 'learning_rate_model': 0.004247773156507247}. Best is trial 115 with value: 0.9733659029006958.
[I 2024-06-21 00:00:55,208] Trial 229 finished with value: 1.096038818359375 and parameters: {'hidden_size': 172, 'n_layers': 2, 'rnn_dropout': 0.6849273903898975, 'bidirectional': False, 'fc_dropout': 0.022169392097891996, 'learning_rate_model': 2.195637079696368e-05}. Best is trial 115 with value: 0.9733659029006958.
Epoch 12: reducing lr to 9.585473401131708e-06
Epoch 15: reducing lr to 1.2669385724148257e-05
Epoch 36: reducing lr to 1.6567090422404044e-05
Epoch 39: reducing lr to 1.597801846320775e-05
Epoch 42: reducing lr to 1.5276047882673294e-05
Epoch 45: reducing lr to 1.4472249179817018e-05
Epoch 48: reducing lr to 1.3579299092045852e-05
Epoch 51: reducing lr to 1.2611279688587706e-05
Epoch 54: reducing lr to 1.1583457991430204e-05
Epoch 57: reducing lr to 1.0512040205727443e-05
Epoch 60: reducing lr to 9.413927981108185e-06
Epoch 63: reducing lr to 8.306435169476419e-06
Epoch 66: reducing lr to 7.207030857025653e-06
Epoch 69: reducing lr to 6.133048479993297e-06
Epoch 72: reducing lr to 5.101431742762973e-06
Epoch 75: reducing lr to 4.128445180989163e-06
Epoch 78: reducing lr to 3.2294362406635962e-06
Epoch 81: reducing lr to 2.4185801692770683e-06
Epoch 84: reducing lr to 1.7086670404560674e-06
Epoch 87: reducing lr to 1.1108897051037146e-06
Epoch 90: reducing lr to 6.346790092692914e-07
Epoch 93: reducing lr to 2.875426939218464e-07
Epoch 96: reducing lr to 7.495632887999653e-08
Epoch 99: reducing lr to 2.718989822219275e-10
[I 2024-06-21 00:02:07,649] Trial 230 finished with value: 1.0933549404144287 and parameters: {'hidden_size': 197, 'n_layers': 5, 'rnn_dropout': 0.3341058774513466, 'bidirectional': False, 'fc_dropout': 0.029580054687486257, 'learning_rate_model': 0.00017638256496317842}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 7.505163656586204e-05
Epoch 18: reducing lr to 0.0001777681303845604
Epoch 22: reducing lr to 0.00020199519440553497
Epoch 25: reducing lr to 0.00020537385031963595
Epoch 28: reducing lr to 0.0002040851700265129
Epoch 31: reducing lr to 0.00020119795654457065
Epoch 34: reducing lr to 0.00019675774679040825
Epoch 37: reducing lr to 0.00019083456014939456
Epoch 40: reducing lr to 0.0001835218078575666
Epoch 43: reducing lr to 0.0001749348217801908
Epoch 47: reducing lr to 0.000161740042910587
Epoch 50: reducing lr to 0.0001507371764595907
Epoch 53: reducing lr to 0.00013897711575604744
Epoch 56: reducing lr to 0.0001266452876981797
Epoch 59: reducing lr to 0.00011393622726561675
Epoch 62: reducing lr to 0.00010105031732343679
Epoch 65: reducing lr to 8.819081433443355e-05
Epoch 68: reducing lr to 7.556046373577113e-05
Epoch 71: reducing lr to 6.335852850548513e-05
Epoch 74: reducing lr to 5.177738529916981e-05
Epoch 77: reducing lr to 4.099970978457272e-05
Epoch 80: reducing lr to 3.119544044722165e-05
Epoch 83: reducing lr to 2.2519225357352082e-05
Epoch 86: reducing lr to 1.5107857892639426e-05
Epoch 89: reducing lr to 9.078263639653765e-06
Epoch 92: reducing lr to 4.525503039818669e-06
Epoch 95: reducing lr to 1.5213892611309118e-06
Epoch 98: reducing lr to 1.1329097694637996e-07
[I 2024-06-21 00:02:27,031] Trial 231 finished with value: 1.0937435626983643 and parameters: {'hidden_size': 75, 'n_layers': 5, 'rnn_dropout': 0.5206650053405577, 'bidirectional': False, 'fc_dropout': 0.5283661820888798, 'learning_rate_model': 0.00205445046128604}. Best is trial 115 with value: 0.9733659029006958.
Epoch 10: reducing lr to 0.0002265168898972053
Epoch 30: reducing lr to 0.0005267952284371246
Epoch 33: reducing lr to 0.0005165654451854066
Epoch 36: reducing lr to 0.000502406931439746
Epoch 39: reducing lr to 0.00048454297175393516
Epoch 42: reducing lr to 0.00046325529381319294
Epoch 45: reducing lr to 0.0004388796171251997
Epoch 48: reducing lr to 0.0004118003713380651
Epoch 51: reducing lr to 0.000382444603628374
Epoch 54: reducing lr to 0.0003512752955742709
Epoch 57: reducing lr to 0.00031878391004546656
Epoch 60: reducing lr to 0.00028548299968154657
Epoch 63: reducing lr to 0.00025189761740277035
Epoch 66: reducing lr to 0.00021855752370212507
Epoch 69: reducing lr to 0.00018598836540650128
Epoch 72: reducing lr to 0.00015470397049109278
Epoch 75: reducing lr to 0.00012519757073294172
Epoch 78: reducing lr to 9.79345866162459e-05
Epoch 81: reducing lr to 7.334489100417358e-05
Epoch 84: reducing lr to 5.1816350533517245e-05
Epoch 87: reducing lr to 3.368839510614402e-05
Epoch 90: reducing lr to 1.9247020772277054e-05
Epoch 93: reducing lr to 8.719904269722037e-06
Epoch 96: reducing lr to 2.27309553001891e-06
Epoch 99: reducing lr to 8.245499350575253e-09
[I 2024-06-21 00:03:42,277] Trial 232 finished with value: 1.2331706285476685 and parameters: {'hidden_size': 89, 'n_layers': 7, 'rnn_dropout': 0.12326333746588594, 'bidirectional': True, 'fc_dropout': 0.43879127173288324, 'learning_rate_model': 0.0053489068365791615}. Best is trial 115 with value: 0.9733659029006958.
Epoch 9: reducing lr to 0.0014643110966505353
Epoch 12: reducing lr to 0.002178345550068696
Epoch 16: reducing lr to 0.0030925178875568383
Epoch 19: reducing lr to 0.003624982389658419
Epoch 22: reducing lr to 0.003941070683762371
Epoch 25: reducing lr to 0.0040069906766254226
Epoch 28: reducing lr to 0.003981847602608657
Epoch 31: reducing lr to 0.003925516003013264
Epoch 34: reducing lr to 0.0038388843356442224
Epoch 37: reducing lr to 0.003723318728778911
Epoch 40: reducing lr to 0.0035806417024280834
Epoch 43: reducing lr to 0.0034131034637535592
Epoch 46: reducing lr to 0.0032233459925981494
Epoch 49: reducing lr to 0.003014362158778952
Epoch 52: reducing lr to 0.0027894476273689177
Epoch 55: reducing lr to 0.002552149072418245
Epoch 58: reducing lr to 0.0023062097241087847
Epoch 61: reducing lr to 0.002055507104819797
Epoch 64: reducing lr to 0.0018039962430454082
Epoch 67: reducing lr to 0.0015556419454393956
Epoch 70: reducing lr to 0.001314362566111919
Epoch 73: reducing lr to 0.0010839624709391368
Epoch 76: reducing lr to 0.0008680748909741118
Epoch 79: reducing lr to 0.0006701051383804161
Epoch 82: reducing lr to 0.0004931747285433011
Epoch 85: reducing lr to 0.00034007448100329616
Epoch 88: reducing lr to 0.00021321825660579747
Epoch 91: reducing lr to 0.00011460740510419107
Epoch 94: reducing lr to 4.579657756981631e-05
Epoch 97: reducing lr to 7.871164593583474e-06
[I 2024-06-21 00:03:55,797] Trial 233 finished with value: 1.2059111595153809 and parameters: {'hidden_size': 59, 'n_layers': 2, 'rnn_dropout': 0.4067856107436928, 'bidirectional': True, 'fc_dropout': 0.09517866973841188, 'learning_rate_model': 0.040083797577684495}. Best is trial 115 with value: 0.9733659029006958.
Epoch 5: reducing lr to 0.00011031033330268656
Epoch 8: reducing lr to 0.00020629322912096783
Epoch 11: reducing lr to 0.00032196882336687566
Epoch 14: reducing lr to 0.00044109079270722646
Epoch 17: reducing lr to 0.0005469288677043199
Epoch 20: reducing lr to 0.0006246183742260498
Epoch 23: reducing lr to 0.0006632480511004348
Epoch 26: reducing lr to 0.000665318501795252
Epoch 29: reducing lr to 0.0006593989933938682
Epoch 32: reducing lr to 0.0006483348761457744
Epoch 35: reducing lr to 0.0006323006376274684
Epoch 38: reducing lr to 0.0006115491430620986
Epoch 41: reducing lr to 0.0005864076585528323
Epoch 44: reducing lr to 0.0005572726804589864
Epoch 47: reducing lr to 0.0005246036977059383
Epoch 50: reducing lr to 0.0004889159093160945
Epoch 53: reducing lr to 0.00045077216198361955
Epoch 56: reducing lr to 0.00041077388770216895
Epoch 59: reducing lr to 0.0003695520605200374
Epoch 62: reducing lr to 0.0003277566220972197
Epoch 65: reducing lr to 0.0002860468346055653
Epoch 68: reducing lr to 0.0002450803027057088
Epoch 71: reducing lr to 0.0002055033357049328
Epoch 74: reducing lr to 0.000167939907129279
Epoch 77: reducing lr to 0.00013298252535859384
Epoch 80: reducing lr to 0.00010118238573254799
Epoch 83: reducing lr to 7.304108914124047e-05
Epoch 86: reducing lr to 4.900232479396541e-05
Epoch 89: reducing lr to 2.944534073571715e-05
Epoch 92: reducing lr to 1.4678465430980475e-05
Epoch 95: reducing lr to 4.934624831777913e-06
Epoch 98: reducing lr to 3.6745919163418475e-07
[I 2024-06-21 00:04:32,853] Trial 234 finished with value: 1.107452392578125 and parameters: {'hidden_size': 104, 'n_layers': 7, 'rnn_dropout': 0.4427469649986126, 'bidirectional': False, 'fc_dropout': 0.6833903783331062, 'learning_rate_model': 0.006663608401168412}. Best is trial 115 with value: 0.9733659029006958.
Epoch 40: reducing lr to 4.552787738167352e-05
Epoch 52: reducing lr to 3.5467840710041816e-05
Epoch 61: reducing lr to 2.613578324855419e-05
Epoch 65: reducing lr to 2.1878275002196053e-05
Epoch 72: reducing lr to 1.4740826145224106e-05
Epoch 75: reducing lr to 1.1929335867206777e-05
Epoch 78: reducing lr to 9.331607393991168e-06
Epoch 81: reducing lr to 6.988600767652463e-06
Epoch 84: reducing lr to 4.937273505456322e-06
Epoch 87: reducing lr to 3.2099678747410063e-06
Epoch 90: reducing lr to 1.8339347472273749e-06
Epoch 93: reducing lr to 8.308680923633615e-07
Epoch 96: reducing lr to 2.1658982580169494e-07
Epoch 99: reducing lr to 7.8566485412759e-10
[I 2024-06-21 00:06:07,758] Trial 235 finished with value: 0.9706074595451355 and parameters: {'hidden_size': 181, 'n_layers': 3, 'rnn_dropout': 0.42240111731010094, 'bidirectional': True, 'fc_dropout': 0.18517034842739824, 'learning_rate_model': 0.000509665689217419}. Best is trial 235 with value: 0.9706074595451355.
Epoch 6: reducing lr to 6.6326186945472e-05
Epoch 34: reducing lr to 0.0003040728748199387
Epoch 37: reducing lr to 0.0002949190782380605
Epoch 40: reducing lr to 0.00028361782251372624
Epoch 43: reducing lr to 0.000270347343535494
Epoch 46: reducing lr to 0.0002553169089800585
Epoch 49: reducing lr to 0.00023876358004793486
Epoch 52: reducing lr to 0.00022094840194537515
Epoch 55: reducing lr to 0.00020215230196275885
Epoch 58: reducing lr to 0.00018267177633779234
Epoch 61: reducing lr to 0.00016281395841286176
Epoch 64: reducing lr to 0.00014289212068566577
Epoch 67: reducing lr to 0.00012322031016880273
Epoch 70: reducing lr to 0.00010410889443124984
Epoch 73: reducing lr to 8.585921218698953e-05
Epoch 76: reducing lr to 6.875904679040216e-05
Epoch 79: reducing lr to 5.307812844659509e-05
Epoch 82: reducing lr to 3.906370819884018e-05
Epoch 85: reducing lr to 2.6936843116482592e-05
Epoch 88: reducing lr to 1.688873187666389e-05
Epoch 91: reducing lr to 9.077898706691922e-06
Epoch 94: reducing lr to 3.627485430928324e-06
Epoch 97: reducing lr to 6.234643810257203e-07
[I 2024-06-21 00:06:27,249] Trial 236 finished with value: 1.0972371101379395 and parameters: {'hidden_size': 61, 'n_layers': 6, 'rnn_dropout': 0.7893419559744661, 'bidirectional': False, 'fc_dropout': 0.26409244175925944, 'learning_rate_model': 0.0031749837967185394}. Best is trial 235 with value: 0.9706074595451355.
Epoch 16: reducing lr to 0.00021955506794238706
Epoch 19: reducing lr to 0.00025735768839163525
Epoch 22: reducing lr to 0.00027979855677497486
Epoch 25: reducing lr to 0.00028447858419536375
Epoch 28: reducing lr to 0.00028269353734203966
Epoch 31: reducing lr to 0.0002786942433601896
Epoch 34: reducing lr to 0.000272543778817461
Epoch 37: reducing lr to 0.000264339132768619
Epoch 40: reducing lr to 0.000254209696059355
Epoch 43: reducing lr to 0.00024231522342812534
Epoch 46: reducing lr to 0.0002288432837379023
Epoch 49: reducing lr to 0.00021400635748513824
Epoch 52: reducing lr to 0.00019803842228785073
Epoch 55: reducing lr to 0.00018119127628929177
Epoch 58: reducing lr to 0.0001637306722471761
Epoch 61: reducing lr to 0.00014593189707022363
Epoch 64: reducing lr to 0.00012807574025790208
Epoch 67: reducing lr to 0.00011044368551569008
Epoch 70: reducing lr to 9.33139186242876e-05
Epoch 73: reducing lr to 7.69565327048301e-05
Epoch 76: reducing lr to 6.162947106426349e-05
Epoch 79: reducing lr to 4.757449577822395e-05
Epoch 82: reducing lr to 3.5013220608510557e-05
Epoch 85: reducing lr to 2.4143781377167e-05
Epoch 88: reducing lr to 1.5137551509080122e-05
Epoch 91: reducing lr to 8.136617969324145e-06
Epoch 94: reducing lr to 3.2513540957440807e-06
Epoch 97: reducing lr to 5.58817811235077e-07
[I 2024-06-21 00:06:44,809] Trial 237 finished with value: 1.0968854427337646 and parameters: {'hidden_size': 63, 'n_layers': 5, 'rnn_dropout': 0.29691774757018363, 'bidirectional': False, 'fc_dropout': 0.753999099380966, 'learning_rate_model': 0.00284577202801892}. Best is trial 235 with value: 0.9706074595451355.
Epoch 34: reducing lr to 8.742120400917354e-06
Epoch 37: reducing lr to 8.47894798906815e-06
Epoch 40: reducing lr to 8.15403594855092e-06
Epoch 55: reducing lr to 5.811895467912214e-06
Epoch 65: reducing lr to 3.918395741682435e-06
Epoch 68: reducing lr to 3.357218113657838e-06
Epoch 71: reducing lr to 2.8150753586842105e-06
Epoch 74: reducing lr to 2.3005149414127e-06
Epoch 77: reducing lr to 1.821653303039741e-06
Epoch 83: reducing lr to 1.0005490641192899e-06
Epoch 87: reducing lr to 5.74904760547165e-07
Epoch 90: reducing lr to 3.284574356679355e-07
Epoch 93: reducing lr to 1.488083495928999e-07
Epoch 96: reducing lr to 3.87912050208667e-08
Epoch 99: reducing lr to 1.407124564653875e-10
[I 2024-06-21 00:07:30,913] Trial 238 finished with value: 1.0766514539718628 and parameters: {'hidden_size': 55, 'n_layers': 7, 'rnn_dropout': 0.6003628373786594, 'bidirectional': True, 'fc_dropout': 0.6641252209298395, 'learning_rate_model': 9.128104780247618e-05}. Best is trial 235 with value: 0.9706074595451355.
Epoch 3: reducing lr to 2.151731838622399e-05
Epoch 6: reducing lr to 4.673299280731525e-05
Epoch 16: reducing lr to 0.0001725931030498493
Epoch 19: reducing lr to 0.00020230989177122605
Epoch 22: reducing lr to 0.00021995074673172364
Epoch 25: reducing lr to 0.0002236297347068737
Epoch 28: reducing lr to 0.00022222650234976225
Epoch 31: reducing lr to 0.0002190826416099262
Epoch 34: reducing lr to 0.00021424773722545468
Epoch 37: reducing lr to 0.00020779803267403476
Epoch 40: reducing lr to 0.00019983524260873005
Epoch 43: reducing lr to 0.00019048495085821518
Epoch 46: reducing lr to 0.00017989460604391942
Epoch 49: reducing lr to 0.00016823123992039962
Epoch 52: reducing lr to 0.00015567878321408505
Epoch 55: reducing lr to 0.0001424351754364311
Epoch 58: reducing lr to 0.0001287093258762457
Epoch 61: reducing lr to 0.00011471763865596731
Epoch 64: reducing lr to 0.00010068084350627854
Epoch 67: reducing lr to 8.68202158759397e-05
Epoch 70: reducing lr to 7.335443870206215e-05
Epoch 73: reducing lr to 6.049583325022215e-05
Epoch 76: reducing lr to 4.8447169769241293e-05
Epoch 79: reducing lr to 3.73984983783215e-05
Epoch 82: reducing lr to 2.7524030527856064e-05
Epoch 85: reducing lr to 1.8979521567390492e-05
Epoch 88: reducing lr to 1.1899688820731945e-05
Epoch 91: reducing lr to 6.396227410361208e-06
Epoch 94: reducing lr to 2.555902251573459e-06
Epoch 97: reducing lr to 4.392888808464959e-07
[I 2024-06-21 00:08:09,861] Trial 239 finished with value: 1.0938940048217773 and parameters: {'hidden_size': 132, 'n_layers': 5, 'rnn_dropout': 0.5939067437656814, 'bidirectional': False, 'fc_dropout': 0.722348721697368, 'learning_rate_model': 0.0022370725918161565}. Best is trial 235 with value: 0.9706074595451355.
[I 2024-06-21 00:08:14,466] Trial 240 finished with value: 1.0962742567062378 and parameters: {'hidden_size': 32, 'n_layers': 1, 'rnn_dropout': 0.08421002133022953, 'bidirectional': True, 'fc_dropout': 0.6226407120541021, 'learning_rate_model': 1.4376264309585954e-05}. Best is trial 235 with value: 0.9706074595451355.
Epoch 5: reducing lr to 0.0010308555257875144
Epoch 8: reducing lr to 0.0019278204389825762
Epoch 11: reducing lr to 0.0030088145938995605
Epoch 14: reducing lr to 0.004122015294691939
Epoch 17: reducing lr to 0.005111077345208016
Epoch 20: reducing lr to 0.005837089629785893
Epoch 23: reducing lr to 0.006198085872595505
Epoch 26: reducing lr to 0.006217434336839252
Epoch 29: reducing lr to 0.006162116237774426
Epoch 32: reducing lr to 0.0060587215143457305
Epoch 35: reducing lr to 0.005908880761593802
Epoch 38: reducing lr to 0.0057149570175474215
Epoch 41: reducing lr to 0.005480008600142476
Epoch 44: reducing lr to 0.005207740787485909
Epoch 47: reducing lr to 0.004902447526333044
Epoch 50: reducing lr to 0.004568943377053948
Epoch 53: reducing lr to 0.0042124881698701365
Epoch 56: reducing lr to 0.003838702316537085
Epoch 59: reducing lr to 0.003453482300773399
Epoch 62: reducing lr to 0.0030629018595680416
Epoch 65: reducing lr to 0.002673121830554679
Epoch 68: reducing lr to 0.0022902875618426195
Epoch 71: reducing lr to 0.0019204388459049039
Epoch 74: reducing lr to 0.0015694067462331102
Epoch 77: reducing lr to 0.0012427282829698973
Epoch 80: reducing lr to 0.000945554403852214
Epoch 83: reducing lr to 0.0006825725940305214
Epoch 86: reducing lr to 0.0004579291511311527
Epoch 89: reducing lr to 0.00027516816690980814
Epoch 92: reducing lr to 0.00013717098613134844
Epoch 95: reducing lr to 4.611431334876209e-05
Epoch 98: reducing lr to 3.4339243374245655e-06
[I 2024-06-21 00:08:35,595] Trial 241 finished with value: 1.107245922088623 and parameters: {'hidden_size': 60, 'n_layers': 7, 'rnn_dropout': 0.7788150886690349, 'bidirectional': False, 'fc_dropout': 0.03476123337198143, 'learning_rate_model': 0.06227175040056983}. Best is trial 235 with value: 0.9706074595451355.
Epoch 28: reducing lr to 1.0682864297463442e-05
Epoch 33: reducing lr to 1.0385615868395378e-05
Epoch 41: reducing lr to 9.463724832719208e-06
Epoch 44: reducing lr to 8.993530742198785e-06
Epoch 47: reducing lr to 8.466303208877199e-06
Epoch 50: reducing lr to 7.890356758854217e-06
Epoch 53: reducing lr to 7.274774878948076e-06
Epoch 56: reducing lr to 6.629263763835015e-06
Epoch 61: reducing lr to 5.514702131964039e-06
Epoch 64: reducing lr to 4.839925828643534e-06
Epoch 67: reducing lr to 4.173618243873411e-06
Epoch 70: reducing lr to 3.526291895812528e-06
Epoch 73: reducing lr to 2.9081534845782615e-06
Epoch 76: reducing lr to 2.328950574159689e-06
Epoch 79: reducing lr to 1.7978192469398001e-06
Epoch 87: reducing lr to 6.773095917586304e-07
Epoch 90: reducing lr to 3.8696387111249914e-07
Epoch 93: reducing lr to 1.75314816348217e-07
Epoch 96: reducing lr to 4.570088306714092e-08
Epoch 99: reducing lr to 1.6577684337304531e-10
[I 2024-06-21 00:09:25,004] Trial 242 finished with value: 1.0753183364868164 and parameters: {'hidden_size': 73, 'n_layers': 6, 'rnn_dropout': 0.6485032827575212, 'bidirectional': True, 'fc_dropout': 0.7445155137323587, 'learning_rate_model': 0.00010754047185755203}. Best is trial 235 with value: 0.9706074595451355.
Epoch 7: reducing lr to 2.398368329190133e-05
Epoch 10: reducing lr to 3.949405675562727e-05
Epoch 23: reducing lr to 9.282451848366779e-05
Epoch 26: reducing lr to 9.311428727902527e-05
Epoch 29: reducing lr to 9.228582571610502e-05
Epoch 32: reducing lr to 9.073735323390601e-05
Epoch 35: reducing lr to 8.84932901458282e-05
Epoch 38: reducing lr to 8.558902606597006e-05
Epoch 41: reducing lr to 8.207036334292826e-05
Epoch 44: reducing lr to 7.799279340795993e-05
Epoch 47: reducing lr to 7.342062378247583e-05
Epoch 50: reducing lr to 6.842595886406714e-05
Epoch 53: reducing lr to 6.308757155418343e-05
Epoch 56: reducing lr to 5.7489634938775105e-05
Epoch 59: reducing lr to 5.1720456645903036e-05
Epoch 62: reducing lr to 4.5870998905356515e-05
Epoch 65: reducing lr to 4.003352839406715e-05
Epoch 68: reducing lr to 3.4300079812890455e-05
Epoch 71: reducing lr to 2.8761107027677192e-05
Epoch 74: reducing lr to 2.350393791222252e-05
Epoch 77: reducing lr to 1.861149665298356e-05
Epoch 80: reducing lr to 1.4160925492459913e-05
Epoch 83: reducing lr to 1.0222425709067952e-05
Epoch 86: reducing lr to 6.858093583589505e-06
Epoch 89: reducing lr to 4.121006568878066e-06
Epoch 92: reducing lr to 2.0543166066591287e-06
Epoch 95: reducing lr to 6.906227212388218e-07
Epoch 98: reducing lr to 5.142755032486483e-08
[I 2024-06-21 00:10:31,339] Trial 243 finished with value: 1.0928587913513184 and parameters: {'hidden_size': 188, 'n_layers': 5, 'rnn_dropout': 0.7883559319558645, 'bidirectional': False, 'fc_dropout': 0.45244338830430486, 'learning_rate_model': 0.0009326016716911777}. Best is trial 235 with value: 0.9706074595451355.
[I 2024-06-21 00:10:42,512] Trial 244 finished with value: 1.1033904552459717 and parameters: {'hidden_size': 63, 'n_layers': 3, 'rnn_dropout': 0.1995697653611102, 'bidirectional': False, 'fc_dropout': 0.4019160047186067, 'learning_rate_model': 2.15159018655157e-05}. Best is trial 235 with value: 0.9706074595451355.
[I 2024-06-21 00:12:27,753] Trial 245 finished with value: 1.0756511688232422 and parameters: {'hidden_size': 159, 'n_layers': 4, 'rnn_dropout': 0.5915102298771057, 'bidirectional': True, 'fc_dropout': 0.7889974259490361, 'learning_rate_model': 1.2970911036527296e-05}. Best is trial 235 with value: 0.9706074595451355.
Epoch 5: reducing lr to 0.0002702336996850883
Epoch 9: reducing lr to 0.0005963445718902614
Epoch 13: reducing lr to 0.0009849244819029517
Epoch 19: reducing lr to 0.0014762836778436951
Epoch 22: reducing lr to 0.0016050114726805392
Epoch 25: reducing lr to 0.001631857564342923
Epoch 28: reducing lr to 0.001621617980865866
Epoch 31: reducing lr to 0.0015986767877536588
Epoch 34: reducing lr to 0.001563395811800186
Epoch 37: reducing lr to 0.0015163314123641835
Epoch 40: reducing lr to 0.0014582258155464152
Epoch 43: reducing lr to 0.0013899954241725174
Epoch 46: reducing lr to 0.0013127161915299488
Epoch 49: reducing lr to 0.0012276069717774211
Epoch 52: reducing lr to 0.0011360099332434535
Epoch 55: reducing lr to 0.0010393694683272685
Epoch 58: reducing lr to 0.0009392100174332232
Epoch 61: reducing lr to 0.0008371107118187013
Epoch 64: reducing lr to 0.0007346822473116172
Epoch 67: reducing lr to 0.0006335393019212974
Epoch 70: reducing lr to 0.0005352776357356653
Epoch 73: reducing lr to 0.0004414465868324835
Epoch 76: reducing lr to 0.00035352579817961115
Epoch 79: reducing lr to 0.00027290209217358896
Epoch 82: reducing lr to 0.0002008467142214381
Epoch 85: reducing lr to 0.00013849623297166949
Epoch 88: reducing lr to 8.683369964594193e-05
Epoch 94: reducing lr to 1.8650777493513238e-05
Epoch 97: reducing lr to 3.2055526251045796e-06
[I 2024-06-21 00:14:01,167] Trial 246 finished with value: 1.107142448425293 and parameters: {'hidden_size': 188, 'n_layers': 7, 'rnn_dropout': 0.3127349692011665, 'bidirectional': False, 'fc_dropout': 0.14237786837613556, 'learning_rate_model': 0.016324232713169764}. Best is trial 235 with value: 0.9706074595451355.
[I 2024-06-21 00:14:23,384] Trial 247 finished with value: 1.0265496969223022 and parameters: {'hidden_size': 62, 'n_layers': 3, 'rnn_dropout': 0.7327326320079657, 'bidirectional': True, 'fc_dropout': 0.6902711844628926, 'learning_rate_model': 0.00025711669394827373}. Best is trial 235 with value: 0.9706074595451355.
Epoch 7: reducing lr to 0.0002231309154428434
Epoch 13: reducing lr to 0.0005234921957405841
Epoch 16: reducing lr to 0.0006693964675752562
Epoch 19: reducing lr to 0.0007846520197743749
Epoch 22: reducing lr to 0.0008530714744738705
Epoch 25: reducing lr to 0.0008673403039421372
Epoch 28: reducing lr to 0.0008618979150723663
Epoch 35: reducing lr to 0.0008232925944054378
Epoch 38: reducing lr to 0.0007962729287877996
Epoch 41: reducing lr to 0.0007635372382363803
Epoch 44: reducing lr to 0.0007256017843155505
Epoch 47: reducing lr to 0.0006830648486131548
Epoch 50: reducing lr to 0.0006365972505377953
Epoch 55: reducing lr to 0.0005524299732189798
Epoch 58: reducing lr to 0.0004991947335269062
Epoch 61: reducing lr to 0.00044492845153088075
Epoch 64: reducing lr to 0.00039048721996808017
Epoch 67: reducing lr to 0.0003367292481246454
Epoch 70: reducing lr to 0.00028450269031864974
Epoch 73: reducing lr to 0.00023463102734194572
Epoch 76: reducing lr to 0.0001879006966934371
Epoch 79: reducing lr to 0.00014504880128284612
Epoch 82: reducing lr to 0.00010675101428276029
Epoch 85: reducing lr to 7.361142750768046e-05
Epoch 88: reducing lr to 4.615253750633406e-05
Epoch 91: reducing lr to 2.4807550004284953e-05
Epoch 94: reducing lr to 9.912979768240092e-06
Epoch 97: reducing lr to 1.7037669517929941e-06
[I 2024-06-21 00:15:20,249] Trial 248 finished with value: 1.1280112266540527 and parameters: {'hidden_size': 200, 'n_layers': 4, 'rnn_dropout': 0.2358496094387963, 'bidirectional': False, 'fc_dropout': 0.37191010472945796, 'learning_rate_model': 0.008676409799751065}. Best is trial 235 with value: 0.9706074595451355.
Epoch 8: reducing lr to 0.0022204400988307506
Epoch 11: reducing lr to 0.0034655159988695025
Epoch 17: reducing lr to 0.005886876628154616
Epoch 20: reducing lr to 0.006723088733972532
Epoch 23: reducing lr to 0.007138879809143796
Epoch 26: reducing lr to 0.007161165134576014
Epoch 29: reducing lr to 0.007097450421903159
Epoch 32: reducing lr to 0.006978361638909576
Epoch 35: reducing lr to 0.006805776885100784
Epoch 38: reducing lr to 0.006582417878894155
Epoch 41: reducing lr to 0.00631180715363486
Epoch 44: reducing lr to 0.005998212403512469
Epoch 47: reducing lr to 0.005646579344095178
Epoch 50: reducing lr to 0.005262453327371049
Epoch 53: reducing lr to 0.004851892561719204
Epoch 56: reducing lr to 0.004421370568937345
Epoch 59: reducing lr to 0.003977678847147465
Epoch 62: reducing lr to 0.003527813052629234
Epoch 65: reducing lr to 0.0030788691631239173
Epoch 68: reducing lr to 0.0026379253157273047
Epoch 71: reducing lr to 0.0022119380698391055
Epoch 74: reducing lr to 0.001807623573360708
Epoch 77: reducing lr to 0.0014313592986459594
Epoch 80: reducing lr to 0.0010890780445545617
Epoch 83: reducing lr to 0.0007861787993845362
Epoch 86: reducing lr to 0.0005274372182358253
Epoch 89: reducing lr to 0.0003169353428220459
Epoch 92: reducing lr to 0.00015799187094569096
Epoch 95: reducing lr to 5.311390439644709e-05
Epoch 98: reducing lr to 3.9551522231964785e-06
[I 2024-06-21 00:16:09,254] Trial 249 finished with value: 1.1115397214889526 and parameters: {'hidden_size': 134, 'n_layers': 6, 'rnn_dropout': 0.04088872306089302, 'bidirectional': False, 'fc_dropout': 0.4415960217233577, 'learning_rate_model': 0.07172384357890647}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.221149701109651e-05
Epoch 38: reducing lr to 6.8622719598443e-05
Epoch 41: reducing lr to 6.580156113335334e-05
Epoch 44: reducing lr to 6.253228759266953e-05
Epoch 49: reducing lr to 5.623058472748909e-05
Epoch 52: reducing lr to 5.203497884182522e-05
Epoch 55: reducing lr to 4.760835861604929e-05
Epoch 58: reducing lr to 4.3020551101725755e-05
Epoch 61: reducing lr to 3.834388846705306e-05
Epoch 64: reducing lr to 3.3652148696601116e-05
Epoch 67: reducing lr to 2.9019292178913182e-05
Epoch 72: reducing lr to 2.1626311645202377e-05
Epoch 75: reducing lr to 1.7501565559681333e-05
Epoch 78: reducing lr to 1.3690430079355616e-05
Epoch 81: reducing lr to 1.0252997808682514e-05
Epoch 84: reducing lr to 7.243489235587617e-06
Epoch 87: reducing lr to 4.709353800548576e-06
Epoch 90: reducing lr to 2.6905713417802294e-06
Epoch 93: reducing lr to 1.2189691489799138e-06
Epoch 96: reducing lr to 3.1775960355418184e-07
Epoch 99: reducing lr to 1.1526513382978907e-09
[I 2024-06-21 00:16:27,379] Trial 250 finished with value: 0.9816610813140869 and parameters: {'hidden_size': 167, 'n_layers': 1, 'rnn_dropout': 0.4609653268247358, 'bidirectional': True, 'fc_dropout': 0.48332839868050753, 'learning_rate_model': 0.0007477321095367409}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 7.237210268026324e-05
Epoch 65: reducing lr to 3.4778181105436974e-05
Epoch 72: reducing lr to 2.3432337388158144e-05
Epoch 75: reducing lr to 1.8963131381046177e-05
Epoch 78: reducing lr to 1.4833725781419415e-05
Epoch 87: reducing lr to 5.1026346491746555e-06
Epoch 90: reducing lr to 2.9152625043896476e-06
Epoch 93: reducing lr to 1.3207659647774444e-06
Epoch 96: reducing lr to 3.4429589108694216e-07
Epoch 99: reducing lr to 1.2489099155245722e-09
[I 2024-06-21 00:16:33,179] Trial 251 finished with value: 0.9819461107254028 and parameters: {'hidden_size': 38, 'n_layers': 1, 'rnn_dropout': 0.45761144386578617, 'bidirectional': True, 'fc_dropout': 0.49362784911210733, 'learning_rate_model': 0.000810175648722377}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.463968360427151e-05
Epoch 38: reducing lr to 7.093022982344971e-05
Epoch 41: reducing lr to 6.80142069746304e-05
Epoch 45: reducing lr to 6.341468649888346e-05
Epoch 49: reducing lr to 5.812139654573318e-05
Epoch 55: reducing lr to 4.9209239125378626e-05
Epoch 61: reducing lr to 3.9633241544605076e-05
Epoch 64: reducing lr to 3.4783737151055416e-05
Epoch 67: reducing lr to 2.999509602080607e-05
Epoch 72: reducing lr to 2.2353518837550645e-05
Epoch 75: reducing lr to 1.8090073880524766e-05
Epoch 78: reducing lr to 1.4150785011041666e-05
Epoch 81: reducing lr to 1.0597765509801765e-05
Epoch 84: reducing lr to 7.487059084956035e-06
Epoch 87: reducing lr to 4.867710713703991e-06
Epoch 90: reducing lr to 2.7810445978475687e-06
Epoch 93: reducing lr to 1.259958252759221e-06
Epoch 96: reducing lr to 3.284446002809979e-07
Epoch 99: reducing lr to 1.191410436800211e-09
[I 2024-06-21 00:16:50,956] Trial 252 finished with value: 0.9809263944625854 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.4631435752586541, 'bidirectional': True, 'fc_dropout': 0.48372189143235567, 'learning_rate_model': 0.000772875378390239}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 7.003913989894331e-05
Epoch 65: reducing lr to 3.365708334654713e-05
Epoch 72: reducing lr to 2.2676980434562087e-05
Epoch 75: reducing lr to 1.8351842250416493e-05
Epoch 78: reducing lr to 1.4355550782010484e-05
Epoch 87: reducing lr to 4.938147833366763e-06
Epoch 90: reducing lr to 2.8212870819735445e-06
Epoch 93: reducing lr to 1.2781901969809293e-06
Epoch 96: reducing lr to 3.331972844426625e-07
Epoch 99: reducing lr to 1.2086504751793008e-09
[I 2024-06-21 00:16:56,747] Trial 253 finished with value: 0.9822982549667358 and parameters: {'hidden_size': 38, 'n_layers': 1, 'rnn_dropout': 0.46242342199421804, 'bidirectional': True, 'fc_dropout': 0.5023454156608191, 'learning_rate_model': 0.0007840590987700915}. Best is trial 235 with value: 0.9706074595451355.
Epoch 65: reducing lr to 1.9553865237447974e-05
Epoch 73: reducing lr to 1.2318287176299979e-05
Epoch 87: reducing lr to 2.8689318162845336e-06
Epoch 90: reducing lr to 1.6390923369396213e-06
[I 2024-06-21 00:17:01,784] Trial 254 finished with value: 0.9886606335639954 and parameters: {'hidden_size': 40, 'n_layers': 1, 'rnn_dropout': 0.4612015607007566, 'bidirectional': True, 'fc_dropout': 0.48841510379095565, 'learning_rate_model': 0.0004555173660678509}. Best is trial 235 with value: 0.9706074595451355.
Epoch 65: reducing lr to 3.242023394831651e-05
Epoch 72: reducing lr to 2.184363402378186e-05
Epoch 75: reducing lr to 1.7677438446315628e-05
Epoch 78: reducing lr to 1.3828004940822183e-05
Epoch 87: reducing lr to 4.756678003875426e-06
Epoch 90: reducing lr to 2.7176088400520696e-06
Epoch 93: reducing lr to 1.2312185458820501e-06
Epoch 96: reducing lr to 3.2095276353419963e-07
Epoch 99: reducing lr to 1.1642343088289625e-09
[I 2024-06-21 00:17:07,778] Trial 255 finished with value: 0.9840295314788818 and parameters: {'hidden_size': 39, 'n_layers': 1, 'rnn_dropout': 0.4385325860295944, 'bidirectional': True, 'fc_dropout': 0.4808635999169968, 'learning_rate_model': 0.0007552460547369541}. Best is trial 235 with value: 0.9706074595451355.
Epoch 65: reducing lr to 3.4571935077778356e-05
Epoch 72: reducing lr to 2.3293375937287192e-05
Epoch 75: reducing lr to 1.885067378852712e-05
Epoch 78: reducing lr to 1.4745756919318199e-05
Epoch 81: reducing lr to 1.1043350172696016e-05
Epoch 87: reducing lr to 5.072374351093069e-06
Epoch 90: reducing lr to 2.897974040990997e-06
Epoch 93: reducing lr to 1.312933389149742e-06
Epoch 96: reducing lr to 3.422541034597869e-07
Epoch 99: reducing lr to 1.2415034698421215e-09
[I 2024-06-21 00:17:13,383] Trial 256 finished with value: 0.9849701523780823 and parameters: {'hidden_size': 37, 'n_layers': 1, 'rnn_dropout': 0.4358161674355238, 'bidirectional': True, 'fc_dropout': 0.4832563366702162, 'learning_rate_model': 0.0008053710412373521}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.279483585208489e-05
Epoch 38: reducing lr to 6.917706758142244e-05
Epoch 41: reducing lr to 6.633311923691761e-05
Epoch 45: reducing lr to 6.18472837957403e-05
Epoch 49: reducing lr to 5.6684826579281625e-05
Epoch 55: reducing lr to 4.799294840972407e-05
Epoch 61: reducing lr to 3.865363802748553e-05
Epoch 64: reducing lr to 3.392399744963869e-05
Epoch 67: reducing lr to 2.9253715795187868e-05
Epoch 72: reducing lr to 2.180101329372279e-05
Epoch 75: reducing lr to 1.7642946688610114e-05
Epoch 78: reducing lr to 1.3801024097561547e-05
Epoch 81: reducing lr to 1.033582356505001e-05
Epoch 84: reducing lr to 7.3020035828909205e-06
Epoch 87: reducing lr to 4.747396897583297e-06
Epoch 90: reducing lr to 2.7123063124299724e-06
Epoch 93: reducing lr to 1.2288162243072222e-06
Epoch 96: reducing lr to 3.2032652885723764e-07
Epoch 99: reducing lr to 1.1619626851520383e-09
[I 2024-06-21 00:17:31,156] Trial 257 finished with value: 0.9811098575592041 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.4354431738726739, 'bidirectional': True, 'fc_dropout': 0.48393576374954256, 'learning_rate_model': 0.0007537724382960232}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.164709261244338e-05
Epoch 38: reducing lr to 6.80863650511477e-05
Epoch 41: reducing lr to 6.528725673475872e-05
Epoch 45: reducing lr to 6.0872148663751146e-05
Epoch 49: reducing lr to 5.579108699274255e-05
Epoch 55: reducing lr to 4.723625212154709e-05
Epoch 61: reducing lr to 3.8044192986305095e-05
Epoch 64: reducing lr to 3.338912381088834e-05
Epoch 67: reducing lr to 2.8792477657271803e-05
Epoch 72: reducing lr to 2.145728059163869e-05
Epoch 75: reducing lr to 1.736477348370923e-05
Epoch 78: reducing lr to 1.3583425803359855e-05
Epoch 81: reducing lr to 1.017286047180232e-05
Epoch 84: reducing lr to 7.186874190125603e-06
Epoch 87: reducing lr to 4.672545534415611e-06
Epoch 90: reducing lr to 2.6695418608381766e-06
Epoch 93: reducing lr to 1.209441697286155e-06
Epoch 96: reducing lr to 3.152759974057941e-07
Epoch 99: reducing lr to 1.143642225994064e-09
[I 2024-06-21 00:17:48,920] Trial 258 finished with value: 0.9812232851982117 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.4381087727212091, 'bidirectional': True, 'fc_dropout': 0.4694815584319639, 'learning_rate_model': 0.0007418878422232979}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.4573814583168e-05
Epoch 38: reducing lr to 7.086763437047474e-05
Epoch 41: reducing lr to 6.795418489229812e-05
Epoch 44: reducing lr to 6.457796075991173e-05
Epoch 49: reducing lr to 5.8070104802376736e-05
Epoch 52: reducing lr to 5.373724440850529e-05
Epoch 55: reducing lr to 4.916581230128298e-05
Epoch 58: reducing lr to 4.442791984540641e-05
Epoch 72: reducing lr to 2.23337920068227e-05
Epoch 75: reducing lr to 1.8074109511429666e-05
Epoch 78: reducing lr to 1.4138297038002211e-05
Epoch 81: reducing lr to 1.0588413052686383e-05
Epoch 84: reducing lr to 7.480451805435867e-06
Epoch 87: reducing lr to 4.863414991586172e-06
Epoch 90: reducing lr to 2.7785903446077505e-06
Epoch 93: reducing lr to 1.2588463480359895e-06
Epoch 96: reducing lr to 3.281547501200461e-07
Epoch 99: reducing lr to 1.1903590250755999e-09
[I 2024-06-21 00:18:06,047] Trial 259 finished with value: 0.9812608957290649 and parameters: {'hidden_size': 168, 'n_layers': 1, 'rnn_dropout': 0.43247318037273824, 'bidirectional': True, 'fc_dropout': 0.46884432263899795, 'learning_rate_model': 0.0007721933210428434}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.309356134570156e-05
Epoch 38: reducing lr to 6.946094697229301e-05
Epoch 41: reducing lr to 6.660532802144521e-05
Epoch 45: reducing lr to 6.210108422216442e-05
Epoch 49: reducing lr to 5.691744201968016e-05
Epoch 55: reducing lr to 4.818989530899239e-05
Epoch 61: reducing lr to 3.881225954183723e-05
Epoch 64: reducing lr to 3.4063210111704256e-05
Epoch 67: reducing lr to 2.9373763194000556e-05
Epoch 72: reducing lr to 2.189047731107073e-05
Epoch 75: reducing lr to 1.7715347400785868e-05
Epoch 78: reducing lr to 1.385765885314142e-05
Epoch 81: reducing lr to 1.037823830450595e-05
Epoch 84: reducing lr to 7.331968546739755e-06
Epoch 87: reducing lr to 4.76687861582642e-06
Epoch 90: reducing lr to 2.7234367042020802e-06
Epoch 93: reducing lr to 1.233858872303796e-06
Epoch 96: reducing lr to 3.2164104106585875e-07
Epoch 99: reducing lr to 1.1667309887425544e-09
[I 2024-06-21 00:18:23,783] Trial 260 finished with value: 0.981080174446106 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.4519493433291724, 'bidirectional': True, 'fc_dropout': 0.4786285070237736, 'learning_rate_model': 0.0007568656665596621}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.192792663058156e-05
Epoch 38: reducing lr to 6.835324213967336e-05
Epoch 41: reducing lr to 6.554316220103101e-05
Epoch 44: reducing lr to 6.228672690882996e-05
Epoch 49: reducing lr to 5.600977046704934e-05
Epoch 52: reducing lr to 5.183064048351647e-05
Epoch 55: reducing lr to 4.742140333985038e-05
Epoch 58: reducing lr to 4.2851611880814646e-05
Epoch 72: reducing lr to 2.1541386367702283e-05
Epoch 75: reducing lr to 1.7432837921967297e-05
Epoch 78: reducing lr to 1.363666855068353e-05
Epoch 81: reducing lr to 1.0212734878119254e-05
Epoch 84: reducing lr to 7.215044471473725e-06
Epoch 87: reducing lr to 4.690860440010767e-06
Epoch 90: reducing lr to 2.6800056234282856e-06
Epoch 93: reducing lr to 1.214182327494151e-06
Epoch 96: reducing lr to 3.1651177993298214e-07
Epoch 99: reducing lr to 1.148124943034512e-09
[I 2024-06-21 00:18:40,862] Trial 261 finished with value: 0.9816436767578125 and parameters: {'hidden_size': 168, 'n_layers': 1, 'rnn_dropout': 0.45263913423285973, 'bidirectional': True, 'fc_dropout': 0.47529155208974266, 'learning_rate_model': 0.0007447958087037585}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.147668415326843e-05
Epoch 38: reducing lr to 6.79244255762e-05
Epoch 41: reducing lr to 6.513197477678579e-05
Epoch 44: reducing lr to 6.18959688504416e-05
Epoch 49: reducing lr to 5.5658391124376256e-05
Epoch 52: reducing lr to 5.1505479065578916e-05
Epoch 55: reducing lr to 4.7123903432329885e-05
Epoch 58: reducing lr to 4.2582780727078344e-05
Epoch 72: reducing lr to 2.1406245692798014e-05
Epoch 75: reducing lr to 1.7323472375940806e-05
Epoch 78: reducing lr to 1.3551118411991044e-05
Epoch 81: reducing lr to 1.0148664912496398e-05
Epoch 84: reducing lr to 7.169780626208814e-06
Epoch 87: reducing lr to 4.661432155548292e-06
Epoch 90: reducing lr to 2.6631925101720107e-06
Epoch 93: reducing lr to 1.2065651102736085e-06
Epoch 96: reducing lr to 3.145261317103018e-07
Epoch 99: reducing lr to 1.1409221392188855e-09
[I 2024-06-21 00:18:57,920] Trial 262 finished with value: 0.9817124009132385 and parameters: {'hidden_size': 168, 'n_layers': 1, 'rnn_dropout': 0.45798558469048495, 'bidirectional': True, 'fc_dropout': 0.47179858081117, 'learning_rate_model': 0.0007401233049690404}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.019612580804333e-05
Epoch 38: reducing lr to 6.670750860465012e-05
Epoch 41: reducing lr to 6.396508665334464e-05
Epoch 44: reducing lr to 6.078705619750901e-05
Epoch 49: reducing lr to 5.4661229349449924e-05
Epoch 52: reducing lr to 5.0582719821448166e-05
Epoch 55: reducing lr to 4.6279643398240266e-05
Epoch 58: reducing lr to 4.181987830835364e-05
Epoch 61: reducing lr to 3.7273737980937164e-05
Epoch 64: reducing lr to 3.271294130981101e-05
Epoch 67: reducing lr to 2.8209384502004323e-05
Epoch 72: reducing lr to 2.102273676416512e-05
Epoch 75: reducing lr to 1.7013109389995368e-05
Epoch 78: reducing lr to 1.3308339973466983e-05
Epoch 81: reducing lr to 9.966843977452411e-06
Epoch 84: reducing lr to 7.0413286348624135e-06
Epoch 87: reducing lr to 4.577919106248346e-06
Epoch 90: reducing lr to 2.6154794211522525e-06
Epoch 93: reducing lr to 1.1849485923933769e-06
Epoch 96: reducing lr to 3.0889116042525287e-07
Epoch 99: reducing lr to 1.1204816643419457e-09
[I 2024-06-21 00:19:16,035] Trial 263 finished with value: 0.9819539785385132 and parameters: {'hidden_size': 167, 'n_layers': 1, 'rnn_dropout': 0.4569167623192835, 'bidirectional': True, 'fc_dropout': 0.47047359647876225, 'learning_rate_model': 0.0007268634414778724}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010887310729791727
Epoch 33: reducing lr to 0.00010552898613888715
Epoch 36: reducing lr to 0.00010263654798852502
Epoch 40: reducing lr to 9.761205438267526e-05
Epoch 44: reducing lr to 9.138390953985741e-05
Epoch 48: reducing lr to 8.412656340829307e-05
Epoch 55: reducing lr to 6.95742648253948e-05
Epoch 64: reducing lr to 4.9178832739081354e-05
Epoch 72: reducing lr to 3.160442362095217e-05
Epoch 75: reducing lr to 2.5576570848165975e-05
Epoch 78: reducing lr to 2.0007024724299983e-05
Epoch 81: reducing lr to 1.4983603836217822e-05
Epoch 84: reducing lr to 1.0585545332511808e-05
Epoch 87: reducing lr to 6.882191237010888e-06
Epoch 90: reducing lr to 3.931967589437747e-06
Epoch 93: reducing lr to 1.7813863962225532e-06
Epoch 96: reducing lr to 4.6436994366443745e-07
Epoch 99: reducing lr to 1.6844703701985805e-09
[I 2024-06-21 00:19:33,088] Trial 264 finished with value: 0.9783423542976379 and parameters: {'hidden_size': 168, 'n_layers': 1, 'rnn_dropout': 0.4618560620963362, 'bidirectional': True, 'fc_dropout': 0.47581795284775075, 'learning_rate_model': 0.0010927264312326627}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010399181773419636
Epoch 33: reducing lr to 0.00010079762913535967
Epoch 38: reducing lr to 9.578817399784304e-05
Epoch 41: reducing lr to 9.185021264173805e-05
Epoch 48: reducing lr to 8.035477691125733e-05
Epoch 55: reducing lr to 6.64549257964604e-05
Epoch 61: reducing lr to 5.352295976796729e-05
Epoch 64: reducing lr to 4.6973916125942244e-05
Epoch 72: reducing lr to 3.0187449796863297e-05
Epoch 75: reducing lr to 2.442985379879129e-05
Epoch 78: reducing lr to 1.9110016423429157e-05
Epoch 81: reducing lr to 1.431181894049953e-05
Epoch 84: reducing lr to 1.0110945927385132e-05
Epoch 87: reducing lr to 6.573630481333846e-06
Epoch 90: reducing lr to 3.755679130004958e-06
Epoch 93: reducing lr to 1.701518529486257e-06
Epoch 96: reducing lr to 4.435500716503749e-07
Epoch 99: reducing lr to 1.6089477012639839e-09
[I 2024-06-21 00:19:50,869] Trial 265 finished with value: 0.9789657592773438 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.45019292683850537, 'bidirectional': True, 'fc_dropout': 0.4668757260371255, 'learning_rate_model': 0.0010437344050366762}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010913043173912872
Epoch 33: reducing lr to 0.0001057784066621344
Epoch 36: reducing lr to 0.00010287913215843165
Epoch 40: reducing lr to 9.784276303031945e-05
Epoch 43: reducing lr to 9.326469978147983e-05
Epoch 48: reducing lr to 8.432539874474437e-05
Epoch 55: reducing lr to 6.973870542292365e-05
Epoch 61: reducing lr to 5.616772391039335e-05
Epoch 64: reducing lr to 4.92950681985818e-05
Epoch 72: reducing lr to 3.1679121504110924e-05
Epoch 75: reducing lr to 2.5637021743386583e-05
Epoch 78: reducing lr to 2.005431185135327e-05
Epoch 81: reducing lr to 1.501901797640524e-05
Epoch 84: reducing lr to 1.0610564546211237e-05
Epoch 87: reducing lr to 6.898457476290027e-06
Epoch 90: reducing lr to 3.941260897839826e-06
Epoch 93: reducing lr to 1.7855967496364054e-06
Epoch 96: reducing lr to 4.654674941912242e-07
Epoch 99: reducing lr to 1.6884516600189706e-09
[I 2024-06-21 00:20:08,509] Trial 266 finished with value: 0.979222297668457 and parameters: {'hidden_size': 165, 'n_layers': 1, 'rnn_dropout': 0.42717404720504876, 'bidirectional': True, 'fc_dropout': 0.48244228207034157, 'learning_rate_model': 0.0010953091187786744}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010593548044019892
Epoch 33: reducing lr to 0.00010268159074765346
Epoch 38: reducing lr to 9.757850621370613e-05
Epoch 41: reducing lr to 9.356694225316254e-05
Epoch 48: reducing lr to 8.18566506791606e-05
Epoch 55: reducing lr to 6.769700391101839e-05
Epoch 61: reducing lr to 5.4523332519232946e-05
Epoch 64: reducing lr to 4.785188374799356e-05
Epoch 72: reducing lr to 3.075166938296053e-05
Epoch 75: reducing lr to 2.4886460835540794e-05
Epoch 78: reducing lr to 1.9467192853677298e-05
Epoch 81: reducing lr to 1.4579314492897796e-05
Epoch 84: reducing lr to 1.0299924915825338e-05
Epoch 87: reducing lr to 6.696495151728085e-06
Epoch 90: reducing lr to 3.825874782122118e-06
Epoch 93: reducing lr to 1.733320821064514e-06
Epoch 96: reducing lr to 4.5184025977570117e-07
Epoch 99: reducing lr to 1.6390197945387318e-09
[I 2024-06-21 00:20:26,263] Trial 267 finished with value: 0.9788944721221924 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.424941901273458, 'bidirectional': True, 'fc_dropout': 0.4699963939784904, 'learning_rate_model': 0.0010632423594338853}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010829229269394634
Epoch 38: reducing lr to 9.974939568521469e-05
Epoch 48: reducing lr to 8.367776629188745e-05
Epoch 55: reducing lr to 6.920310109108019e-05
Epoch 61: reducing lr to 5.573634687157746e-05
Epoch 64: reducing lr to 4.89164742469797e-05
Epoch 72: reducing lr to 3.143582082045614e-05
Epoch 75: reducing lr to 2.5440125345352662e-05
Epoch 78: reducing lr to 1.9900291551799605e-05
Epoch 81: reducing lr to 1.4903669533393374e-05
Epoch 84: reducing lr to 1.0529073725585995e-05
Epoch 87: reducing lr to 6.845476227427855e-06
Epoch 90: reducing lr to 3.91099138828978e-06
Epoch 93: reducing lr to 1.771883083054402e-06
Epoch 96: reducing lr to 4.6189262992168707e-07
Epoch 99: reducing lr to 1.6754840831802523e-09
[I 2024-06-21 00:20:44,033] Trial 268 finished with value: 0.9788250923156738 and parameters: {'hidden_size': 166, 'n_layers': 1, 'rnn_dropout': 0.42816099308644273, 'bidirectional': True, 'fc_dropout': 0.47692017308813134, 'learning_rate_model': 0.0010868969708162507}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010317408041515498
Epoch 33: reducing lr to 0.00010000500924649923
Epoch 40: reducing lr to 9.250249394286623e-05
Epoch 43: reducing lr to 8.817430190464037e-05
Epoch 46: reducing lr to 8.327209699699381e-05
Epoch 55: reducing lr to 6.593235898262903e-05
Epoch 64: reducing lr to 4.660453779335183e-05
Epoch 72: reducing lr to 2.995007146457296e-05
Epoch 75: reducing lr to 2.4237750193091674e-05
Epoch 78: reducing lr to 1.89597452392397e-05
Epoch 81: reducing lr to 1.419927827426246e-05
Epoch 86: reducing lr to 7.6149780209673525e-06
Epoch 89: reducing lr to 4.5758160141413064e-06
Epoch 92: reducing lr to 2.2810385447714606e-06
Epoch 95: reducing lr to 7.668423854113871e-07
Epoch 98: reducing lr to 5.7103283969926753e-08
[I 2024-06-21 00:21:01,058] Trial 269 finished with value: 0.9779452085494995 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.38355241678284857, 'bidirectional': True, 'fc_dropout': 0.46880778750606716, 'learning_rate_model': 0.0010355270230256463}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011743562116870205
Epoch 36: reducing lr to 0.00011070857686335466
Epoch 40: reducing lr to 0.00010528892326564358
Epoch 43: reducing lr to 0.00010036245415148978
Epoch 46: reducing lr to 9.478262755057179e-05
Epoch 55: reducing lr to 7.504605324406238e-05
Epoch 61: reducing lr to 6.0442274825933564e-05
Epoch 64: reducing lr to 5.304658711781069e-05
Epoch 72: reducing lr to 3.409000819136461e-05
Epoch 75: reducing lr to 2.7588084509250957e-05
Epoch 78: reducing lr to 2.1580511795319127e-05
Epoch 81: reducing lr to 1.6162015281120284e-05
Epoch 84: reducing lr to 1.1418063857875776e-05
Epoch 87: reducing lr to 7.423453072838231e-06
Epoch 90: reducing lr to 4.241203982699774e-06
Epoch 93: reducing lr to 1.92148661110063e-06
Epoch 96: reducing lr to 5.008911212305619e-07
Epoch 99: reducing lr to 1.816948456555452e-09
[I 2024-06-21 00:21:18,083] Trial 270 finished with value: 0.9768081307411194 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.4274589282399127, 'bidirectional': True, 'fc_dropout': 0.5070607371400728, 'learning_rate_model': 0.0011786657918022108}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001073419761483325
Epoch 33: reducing lr to 0.0001040448848592276
Epoch 40: reducing lr to 9.623929245138116e-05
Epoch 43: reducing lr to 9.173625559693881e-05
Epoch 46: reducing lr to 8.663601762870641e-05
Epoch 55: reducing lr to 6.859581085519512e-05
Epoch 64: reducing lr to 4.848720884245663e-05
Epoch 72: reducing lr to 3.115995649154161e-05
Epoch 75: reducing lr to 2.5216876105385925e-05
Epoch 78: reducing lr to 1.9725656996987265e-05
Epoch 81: reducing lr to 1.4772882721187156e-05
Epoch 86: reducing lr to 7.922598251495421e-06
Epoch 89: reducing lr to 4.76066402988716e-06
Epoch 92: reducing lr to 2.373185048813102e-06
Epoch 95: reducing lr to 7.978203121669722e-07
Epoch 98: reducing lr to 5.9410070060500125e-08
[I 2024-06-21 00:21:35,102] Trial 271 finished with value: 0.9774974584579468 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.38297407182551846, 'bidirectional': True, 'fc_dropout': 0.5113794684941662, 'learning_rate_model': 0.0010773589312286745}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010630755483734227
Epoch 33: reducing lr to 0.00010304223659375145
Epoch 40: reducing lr to 9.531186425750516e-05
Epoch 43: reducing lr to 9.085222177172966e-05
Epoch 46: reducing lr to 8.580113321395874e-05
Epoch 55: reducing lr to 6.793477431441805e-05
Epoch 64: reducing lr to 4.801995265865242e-05
Epoch 72: reducing lr to 3.085967766119999e-05
Epoch 75: reducing lr to 2.4973868896314555e-05
Epoch 78: reducing lr to 1.953556696228573e-05
Epoch 81: reducing lr to 1.463052103510789e-05
Epoch 86: reducing lr to 7.846250630892302e-06
Epoch 89: reducing lr to 4.714786988084096e-06
Epoch 92: reducing lr to 2.3503154010061237e-06
Epoch 95: reducing lr to 7.901319654189448e-07
Epoch 98: reducing lr to 5.883755365299753e-08
[I 2024-06-21 00:21:52,164] Trial 272 finished with value: 0.9776028990745544 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.3958919829683727, 'bidirectional': True, 'fc_dropout': 0.5073237857161844, 'learning_rate_model': 0.001066976757562442}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010516325164734646
Epoch 40: reducing lr to 9.428591957764501e-05
Epoch 44: reducing lr to 8.82699990288806e-05
Epoch 48: reducing lr to 8.125994726800651e-05
Epoch 55: reducing lr to 6.720351886339599e-05
Epoch 64: reducing lr to 4.7503061972051455e-05
Epoch 72: reducing lr to 3.052750157414782e-05
Epoch 75: reducing lr to 2.4705048134814448e-05
Epoch 78: reducing lr to 1.9325284526314702e-05
Epoch 81: reducing lr to 1.4473036913519455e-05
Epoch 86: reducing lr to 7.761792949214134e-06
Epoch 89: reducing lr to 4.664036636438168e-06
Epoch 92: reducing lr to 2.325016414353844e-06
Epoch 95: reducing lr to 7.816269205053548e-07
Epoch 98: reducing lr to 5.82042214270764e-08
[I 2024-06-21 00:22:09,361] Trial 273 finished with value: 0.9773217439651489 and parameters: {'hidden_size': 164, 'n_layers': 1, 'rnn_dropout': 0.4157522152600995, 'bidirectional': True, 'fc_dropout': 0.5139821129942365, 'learning_rate_model': 0.001055491732728617}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010884362574558045
Epoch 33: reducing lr to 0.00010550041013508676
Epoch 40: reducing lr to 9.758562219054432e-05
Epoch 43: reducing lr to 9.301959056256138e-05
Epoch 48: reducing lr to 8.410378292793759e-05
Epoch 55: reducing lr to 6.955542493572274e-05
Epoch 61: reducing lr to 5.602010935774524e-05
Epoch 64: reducing lr to 4.916551569167409e-05
Epoch 72: reducing lr to 3.15958655160888e-05
Epoch 75: reducing lr to 2.556964501468174e-05
Epoch 78: reducing lr to 2.0001607058164114e-05
Epoch 81: reducing lr to 1.4979546453162864e-05
Epoch 86: reducing lr to 8.033430629474699e-06
Epoch 89: reducing lr to 4.827262852450612e-06
Epoch 92: reducing lr to 2.406384478343066e-06
Epoch 95: reducing lr to 8.089813378293507e-07
Epoch 98: reducing lr to 6.024118115962631e-08
[I 2024-06-21 00:22:26,167] Trial 274 finished with value: 0.977105438709259 and parameters: {'hidden_size': 162, 'n_layers': 1, 'rnn_dropout': 0.3859774119203427, 'bidirectional': True, 'fc_dropout': 0.5107605760874179, 'learning_rate_model': 0.0010924305338134398}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010158379918228902
Epoch 33: reducing lr to 9.846357472382299e-05
Epoch 38: reducing lr to 9.357011775970996e-05
Epoch 41: reducing lr to 8.972334323165362e-05
Epoch 44: reducing lr to 8.526554394913395e-05
Epoch 48: reducing lr to 7.849409404454124e-05
Epoch 55: reducing lr to 6.491610574628563e-05
Epoch 64: reducing lr to 4.58861953422155e-05
Epoch 72: reducing lr to 2.948843384801798e-05
Epoch 75: reducing lr to 2.386415985815611e-05
Epoch 78: reducing lr to 1.8667507819603306e-05
Epoch 81: reducing lr to 1.3980416660289846e-05
Epoch 86: reducing lr to 7.497604000411963e-06
Epoch 89: reducing lr to 4.505286339410444e-06
Epoch 92: reducing lr to 2.2458795903654988e-06
Epoch 95: reducing lr to 7.550226042301245e-07
Epoch 98: reducing lr to 5.6223118326910114e-08
[I 2024-06-21 00:22:42,874] Trial 275 finished with value: 0.9778475761413574 and parameters: {'hidden_size': 162, 'n_layers': 1, 'rnn_dropout': 0.38386734126149064, 'bidirectional': True, 'fc_dropout': 0.5118235988643999, 'learning_rate_model': 0.0010195658515355116}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011110595361579661
Epoch 33: reducing lr to 0.00010769324886618291
Epoch 36: reducing lr to 0.00010474149055830881
Epoch 40: reducing lr to 9.961395110095843e-05
Epoch 43: reducing lr to 9.495301395565634e-05
Epoch 48: reducing lr to 8.585189018552966e-05
Epoch 55: reducing lr to 7.100114282024666e-05
Epoch 61: reducing lr to 5.718449407779147e-05
Epoch 64: reducing lr to 5.018742685680579e-05
Epoch 72: reducing lr to 3.225258938623745e-05
Epoch 75: reducing lr to 2.6101113165912415e-05
Epoch 78: reducing lr to 2.041734286985572e-05
Epoch 81: reducing lr to 1.5290898130324016e-05
Epoch 86: reducing lr to 8.200406452652334e-06
Epoch 89: reducing lr to 4.927598092232914e-06
Epoch 92: reducing lr to 2.4564014695496694e-06
Epoch 95: reducing lr to 8.257961123696051e-07
Epoch 98: reducing lr to 6.14933013654527e-08
[I 2024-06-21 00:22:59,611] Trial 276 finished with value: 0.9769642353057861 and parameters: {'hidden_size': 162, 'n_layers': 1, 'rnn_dropout': 0.3926163924103288, 'bidirectional': True, 'fc_dropout': 0.5131694095155419, 'learning_rate_model': 0.0011151368340307645}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010239676123647936
Epoch 40: reducing lr to 9.180557508177451e-05
Epoch 43: reducing lr to 8.750999187967074e-05
Epoch 48: reducing lr to 7.912227216398477e-05
Epoch 55: reducing lr to 6.543562097511494e-05
Epoch 64: reducing lr to 4.625341665038342e-05
Epoch 72: reducing lr to 2.972442598405654e-05
Epoch 75: reducing lr to 2.4055141654229707e-05
Epoch 78: reducing lr to 1.8816901479082477e-05
Epoch 81: reducing lr to 1.4092300133226225e-05
Epoch 84: reducing lr to 9.95586132216395e-06
Epoch 87: reducing lr to 6.472802240792426e-06
Epoch 90: reducing lr to 3.698073440151859e-06
Epoch 93: reducing lr to 1.6754201474637438e-06
Epoch 96: reducing lr to 4.367467727056868e-07
Epoch 99: reducing lr to 1.584269197297053e-09
[I 2024-06-21 00:23:16,240] Trial 277 finished with value: 0.9780463576316833 and parameters: {'hidden_size': 161, 'n_layers': 1, 'rnn_dropout': 0.38404129886497734, 'bidirectional': True, 'fc_dropout': 0.5135135767531711, 'learning_rate_model': 0.001027725305658302}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010996220739432203
Epoch 40: reducing lr to 9.858850578079265e-05
Epoch 44: reducing lr to 9.229805837936249e-05
Epoch 48: reducing lr to 8.49681141878392e-05
Epoch 55: reducing lr to 7.027024329435943e-05
Epoch 64: reducing lr to 4.967078775723514e-05
Epoch 72: reducing lr to 3.1920574979783227e-05
Epoch 75: reducing lr to 2.583242324797135e-05
Epoch 78: reducing lr to 2.020716278499097e-05
Epoch 81: reducing lr to 1.5133490661233862e-05
Epoch 84: reducing lr to 1.0691436665351142e-05
Epoch 87: reducing lr to 6.951036476443551e-06
Epoch 90: reducing lr to 3.971300592665069e-06
Epoch 93: reducing lr to 1.7992062981616978e-06
Epoch 96: reducing lr to 4.690152170745959e-07
Epoch 99: reducing lr to 1.7013207833640673e-09
[I 2024-06-21 00:23:32,790] Trial 278 finished with value: 0.9775230884552002 and parameters: {'hidden_size': 161, 'n_layers': 1, 'rnn_dropout': 0.37960536907098924, 'bidirectional': True, 'fc_dropout': 0.5129527044937888, 'learning_rate_model': 0.0011036573993214395}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010191890867498386
Epoch 33: reducing lr to 9.878839107091994e-05
Epoch 38: reducing lr to 9.387879133705457e-05
Epoch 41: reducing lr to 9.001932688529992e-05
Epoch 44: reducing lr to 8.554682200141344e-05
Epoch 48: reducing lr to 7.87530341141837e-05
Epoch 55: reducing lr to 6.5130254099069e-05
Epoch 64: reducing lr to 4.603756691688199e-05
Epoch 72: reducing lr to 2.9585711703214576e-05
Epoch 75: reducing lr to 2.3942884089460998e-05
Epoch 78: reducing lr to 1.8729089086750824e-05
Epoch 81: reducing lr to 1.402653592706668e-05
Epoch 86: reducing lr to 7.5223374548922066e-06
Epoch 89: reducing lr to 4.520148593350516e-06
Epoch 92: reducing lr to 2.2532884053167063e-06
Epoch 95: reducing lr to 7.575133088888767e-07
Epoch 98: reducing lr to 5.640858983723893e-08
[I 2024-06-21 00:23:49,468] Trial 279 finished with value: 0.9778062701225281 and parameters: {'hidden_size': 162, 'n_layers': 1, 'rnn_dropout': 0.3836838229370212, 'bidirectional': True, 'fc_dropout': 0.5187076197341137, 'learning_rate_model': 0.001022929244104281}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.995666117978305e-05
Epoch 33: reducing lr to 9.688641551551043e-05
Epoch 40: reducing lr to 8.961786146410717e-05
Epoch 43: reducing lr to 8.542464139037218e-05
Epoch 46: reducing lr to 8.067530867991053e-05
Epoch 55: reducing lr to 6.387629956178831e-05
Epoch 64: reducing lr to 4.5151204400424173e-05
Epoch 72: reducing lr to 2.901609719852535e-05
Epoch 75: reducing lr to 2.3481911096880667e-05
Epoch 78: reducing lr to 1.8368497429857522e-05
Epoch 81: reducing lr to 1.3756482652879826e-05
Epoch 86: reducing lr to 7.377509689163392e-06
Epoch 89: reducing lr to 4.433122050675087e-06
Epoch 92: reducing lr to 2.209905782928176e-06
Epoch 95: reducing lr to 7.429288847395857e-07
Epoch 98: reducing lr to 5.532255373701719e-08
[I 2024-06-21 00:24:06,457] Trial 280 finished with value: 0.9783230423927307 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.3722493962405158, 'bidirectional': True, 'fc_dropout': 0.505528926069243, 'learning_rate_model': 0.0010032347598019388}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 4.422833865827475e-05
Epoch 50: reducing lr to 3.5359281683197145e-05
Epoch 55: reducing lr to 3.0684278037313394e-05
Epoch 61: reducing lr to 2.4713192577031238e-05
Epoch 64: reducing lr to 2.168929820679481e-05
Epoch 67: reducing lr to 1.8703354947499293e-05
Epoch 73: reducing lr to 1.3032391485186321e-05
Epoch 76: reducing lr to 1.0436792897297901e-05
Epoch 79: reducing lr to 8.056618871723733e-06
Epoch 82: reducing lr to 5.929399130772739e-06
Epoch 85: reducing lr to 4.088687467857329e-06
Epoch 88: reducing lr to 2.563505533054286e-06
Epoch 91: reducing lr to 1.3779153895661248e-06
Epoch 94: reducing lr to 5.506084791426929e-07
Epoch 97: reducing lr to 9.463436343791314e-08
[I 2024-06-21 00:24:23,482] Trial 281 finished with value: 0.9825778603553772 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.38714974785591305, 'bidirectional': True, 'fc_dropout': 0.5072151174124707, 'learning_rate_model': 0.0004819241959481815}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.864356093135834e-05
Epoch 33: reducing lr to 9.56136481503263e-05
Epoch 38: reducing lr to 9.08618272488629e-05
Epoch 41: reducing lr to 8.712639364033449e-05
Epoch 44: reducing lr to 8.27976207583919e-05
Epoch 47: reducing lr to 7.794377785634713e-05
Epoch 55: reducing lr to 6.303717604732665e-05
Epoch 64: reducing lr to 4.455806676442176e-05
Epoch 72: reducing lr to 2.8634921557102412e-05
Epoch 75: reducing lr to 2.317343637462736e-05
Epoch 78: reducing lr to 1.8127196067310477e-05
Epoch 81: reducing lr to 1.3575767925359491e-05
Epoch 86: reducing lr to 7.280593588812926e-06
Epoch 89: reducing lr to 4.374885474969846e-06
Epoch 92: reducing lr to 2.1808749229703783e-06
Epoch 95: reducing lr to 7.331692539996231e-07
Epoch 98: reducing lr to 5.4595798179178295e-08
[I 2024-06-21 00:24:40,209] Trial 282 finished with value: 0.9782292246818542 and parameters: {'hidden_size': 162, 'n_layers': 1, 'rnn_dropout': 0.37737198244716913, 'bidirectional': True, 'fc_dropout': 0.5178067395920326, 'learning_rate_model': 0.0009900555699733108}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.736877501451999e-05
Epoch 40: reducing lr to 8.729764767238908e-05
Epoch 43: reducing lr to 8.321299041066325e-05
Epoch 48: reducing lr to 7.523713273685113e-05
Epoch 55: reducing lr to 6.222253692132924e-05
Epoch 64: reducing lr to 4.3982236011187346e-05
Epoch 72: reducing lr to 2.826486805957948e-05
Epoch 75: reducing lr to 2.2873962490511587e-05
Epoch 78: reducing lr to 1.7892935523183773e-05
Epoch 81: reducing lr to 1.3400326187468887e-05
Epoch 84: reducing lr to 9.46699885277419e-06
Epoch 87: reducing lr to 6.154968355314241e-06
Epoch 90: reducing lr to 3.5164870102653193e-06
Epoch 93: reducing lr to 1.5931520237875843e-06
Epoch 96: reducing lr to 4.1530120422154363e-07
Epoch 99: reducing lr to 1.506476856997542e-09
[I 2024-06-21 00:24:56,806] Trial 283 finished with value: 0.9785388708114624 and parameters: {'hidden_size': 161, 'n_layers': 1, 'rnn_dropout': 0.3794837571972454, 'bidirectional': True, 'fc_dropout': 0.5185192472123316, 'learning_rate_model': 0.0009772609294962948}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013443497289093522
Epoch 36: reducing lr to 0.00012673415767128094
Epoch 40: reducing lr to 0.00012052998403779805
Epoch 46: reducing lr to 0.000108502853209999
Epoch 61: reducing lr to 6.919157489717771e-05
Epoch 64: reducing lr to 6.072532703595128e-05
Epoch 67: reducing lr to 5.236533404757924e-05
Epoch 72: reducing lr to 3.902469524535758e-05
Epoch 75: reducing lr to 3.1581587904968815e-05
Epoch 78: reducing lr to 2.470439113195946e-05
Epoch 81: reducing lr to 1.850154207520254e-05
Epoch 84: reducing lr to 1.3070881644977302e-05
Epoch 87: reducing lr to 8.49803239147092e-06
Epoch 90: reducing lr to 4.855137962101797e-06
Epoch 93: reducing lr to 2.1996307245016143e-06
Epoch 96: reducing lr to 5.7339743796481e-07
Epoch 99: reducing lr to 2.079960186420274e-09
[I 2024-06-21 00:25:13,289] Trial 284 finished with value: 0.9781901836395264 and parameters: {'hidden_size': 161, 'n_layers': 1, 'rnn_dropout': 0.3802064973165566, 'bidirectional': True, 'fc_dropout': 0.5117099660117091, 'learning_rate_model': 0.0013492831407667704}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.000135937892452593
Epoch 36: reducing lr to 0.00012815098575253412
Epoch 40: reducing lr to 0.00012187745238536609
Epoch 46: reducing lr to 0.00010971586391010368
Epoch 61: reducing lr to 6.996510405539193e-05
Epoch 64: reducing lr to 6.14042075380096e-05
Epoch 67: reducing lr to 5.295075377282217e-05
Epoch 72: reducing lr to 3.9460972923782546e-05
Epoch 75: reducing lr to 3.1934655155475874e-05
Epoch 78: reducing lr to 2.4980574567658067e-05
Epoch 81: reducing lr to 1.8708380585358785e-05
Epoch 84: reducing lr to 1.3217007934066428e-05
Epoch 87: reducing lr to 8.5930363836769e-06
Epoch 90: reducing lr to 4.909416113544653e-06
Epoch 93: reducing lr to 2.224221558071084e-06
Epoch 96: reducing lr to 5.798077507546172e-07
Epoch 99: reducing lr to 2.1032131598385656e-09
[I 2024-06-21 00:25:29,902] Trial 285 finished with value: 0.978335976600647 and parameters: {'hidden_size': 161, 'n_layers': 1, 'rnn_dropout': 0.3830194345375438, 'bidirectional': True, 'fc_dropout': 0.5197095585781993, 'learning_rate_model': 0.0013643674896000057}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014323293005171842
Epoch 36: reducing lr to 0.0001350281429789914
Epoch 40: reducing lr to 0.0001284179436464538
Epoch 46: reducing lr to 0.00011560370973443009
Epoch 52: reducing lr to 0.00010004215947473472
Epoch 61: reducing lr to 7.37197456457697e-05
Epoch 64: reducing lr to 6.469943298731173e-05
Epoch 67: reducing lr to 5.5792328941493256e-05
Epoch 70: reducing lr to 4.713896334042903e-05
Epoch 73: reducing lr to 3.8875777884600764e-05
Epoch 76: reducing lr to 3.1133076608704327e-05
Epoch 79: reducing lr to 2.403298934919439e-05
Epoch 82: reducing lr to 1.768746771143902e-05
Epoch 85: reducing lr to 1.2196603057899902e-05
Epoch 88: reducing lr to 7.646967313884264e-06
Epoch 91: reducing lr to 4.110337898415267e-06
Epoch 94: reducing lr to 1.642471603224946e-06
Epoch 97: reducing lr to 2.8229542501426436e-07
[I 2024-06-21 00:25:46,443] Trial 286 finished with value: 0.9791306257247925 and parameters: {'hidden_size': 161, 'n_layers': 1, 'rnn_dropout': 0.3811054383384467, 'bidirectional': True, 'fc_dropout': 0.5187543114084208, 'learning_rate_model': 0.0014375855743891859}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.643534496903995e-05
Epoch 33: reducing lr to 9.347325923829216e-05
Epoch 38: reducing lr to 8.88278116942544e-05
Epoch 41: reducing lr to 8.517599879084503e-05
Epoch 44: reducing lr to 8.094412899395842e-05
Epoch 47: reducing lr to 7.619894329440798e-05
Epoch 55: reducing lr to 6.162603783361146e-05
Epoch 64: reducing lr to 4.356059837063869e-05
Epoch 72: reducing lr to 2.7993905658394858e-05
Epoch 75: reducing lr to 2.265468024274127e-05
Epoch 78: reducing lr to 1.7721404109579308e-05
Epoch 81: reducing lr to 1.3271863370916554e-05
Epoch 84: reducing lr to 9.376243051765229e-06
Epoch 87: reducing lr to 6.095963480384141e-06
Epoch 90: reducing lr to 3.4827760528312496e-06
Epoch 93: reducing lr to 1.57787920181977e-06
Epoch 96: reducing lr to 4.1131990095583233e-07
Epoch 99: reducing lr to 1.4920349503317644e-09
[I 2024-06-21 00:26:02,193] Trial 287 finished with value: 0.9797464609146118 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.37081424462587204, 'bidirectional': True, 'fc_dropout': 0.5192009833806656, 'learning_rate_model': 0.0009678923746004409}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012721593220141523
Epoch 36: reducing lr to 0.00011992864403664614
Epoch 40: reducing lr to 0.0001140576291113611
Epoch 44: reducing lr to 0.00010678017307351306
Epoch 48: reducing lr to 9.830011701238733e-05
Epoch 55: reducing lr to 8.129606269775355e-05
Epoch 61: reducing lr to 6.547604772583718e-05
Epoch 64: reducing lr to 5.7464429984165566e-05
Epoch 72: reducing lr to 3.6929103177206455e-05
Epoch 75: reducing lr to 2.988568420355189e-05
Epoch 78: reducing lr to 2.3377786893818846e-05
Epoch 81: reducing lr to 1.7508025416645892e-05
Epoch 84: reducing lr to 1.2368986710840305e-05
Epoch 87: reducing lr to 8.041695470388196e-06
Epoch 90: reducing lr to 4.5944212918192645e-06
Epoch 93: reducing lr to 2.0815124747588327e-06
Epoch 96: reducing lr to 5.426064960921623e-07
Epoch 99: reducing lr to 1.9682681401183773e-09
[I 2024-06-21 00:26:18,216] Trial 288 finished with value: 0.9782118797302246 and parameters: {'hidden_size': 160, 'n_layers': 1, 'rnn_dropout': 0.40090691881380286, 'bidirectional': True, 'fc_dropout': 0.5085573917485556, 'learning_rate_model': 0.0012768278139614383}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 5.612172631670018e-05
Epoch 38: reducing lr to 5.333258065290726e-05
Epoch 41: reducing lr to 5.114001728243074e-05
Epoch 50: reducing lr to 4.263786068856015e-05
Epoch 55: reducing lr to 3.700052461489139e-05
Epoch 61: reducing lr to 2.980031301851211e-05
Epoch 64: reducing lr to 2.6153961035170455e-05
Epoch 67: reducing lr to 2.2553372260362634e-05
Epoch 70: reducing lr to 1.9055354174211832e-05
Epoch 73: reducing lr to 1.57150616792142e-05
Epoch 76: reducing lr to 1.2585168600916701e-05
Epoch 79: reducing lr to 9.715044444373469e-06
Epoch 82: reducing lr to 7.149944288175323e-06
Epoch 85: reducing lr to 4.930328851572975e-06
Epoch 88: reducing lr to 3.091193785326879e-06
Epoch 91: reducing lr to 1.6615542404771802e-06
Epoch 94: reducing lr to 6.639492237983462e-07
Epoch 97: reducing lr to 1.1411450155486905e-07
[I 2024-06-21 00:26:34,902] Trial 289 finished with value: 0.9833033084869385 and parameters: {'hidden_size': 162, 'n_layers': 1, 'rnn_dropout': 0.3980882769373458, 'bidirectional': True, 'fc_dropout': 0.505643459548431, 'learning_rate_model': 0.000581126531737479}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013459624004124557
Epoch 36: reducing lr to 0.0001268861869834095
Epoch 39: reducing lr to 0.00012237452604262145
Epoch 46: reducing lr to 0.00010863301239076451
Epoch 56: reducing lr to 8.327547512893547e-05
Epoch 61: reducing lr to 6.92745765735213e-05
Epoch 64: reducing lr to 6.079817266705573e-05
Epoch 67: reducing lr to 5.242815109596556e-05
Epoch 70: reducing lr to 4.429656799433561e-05
Epoch 73: reducing lr to 3.653163787165779e-05
Epoch 76: reducing lr to 2.9255807661929365e-05
Epoch 79: reducing lr to 2.258384299046918e-05
Epoch 82: reducing lr to 1.6620944980676006e-05
Epoch 85: reducing lr to 1.1461169664518721e-05
Epoch 88: reducing lr to 7.1858688347399e-06
Epoch 91: reducing lr to 3.862491860118808e-06
Epoch 94: reducing lr to 1.543433497372209e-06
Epoch 97: reducing lr to 2.652735147849403e-07
[I 2024-06-21 00:26:53,292] Trial 290 finished with value: 0.979671835899353 and parameters: {'hidden_size': 172, 'n_layers': 1, 'rnn_dropout': 0.40084743101254966, 'bidirectional': True, 'fc_dropout': 0.5576597140918693, 'learning_rate_model': 0.0013509017303524565}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012472654780299464
Epoch 36: reducing lr to 0.00011758185861266413
Epoch 40: reducing lr to 0.00011182572877059935
Epoch 44: reducing lr to 0.00010469067931034982
Epoch 48: reducing lr to 9.637656252185292e-05
Epoch 55: reducing lr to 7.970524662125462e-05
Epoch 61: reducing lr to 6.419480056710178e-05
Epoch 64: reducing lr to 5.633995561219597e-05
Epoch 67: reducing lr to 4.858369217364254e-05
Epoch 72: reducing lr to 3.620646779886838e-05
Epoch 75: reducing lr to 2.9300875723159218e-05
Epoch 78: reducing lr to 2.2920326126476187e-05
Epoch 81: reducing lr to 1.716542520482382e-05
Epoch 84: reducing lr to 1.2126948139024584e-05
Epoch 87: reducing lr to 7.884334117180184e-06
Epoch 90: reducing lr to 4.504516823992726e-06
Epoch 93: reducing lr to 2.0407810617186937e-06
Epoch 96: reducing lr to 5.319886739178553e-07
Epoch 99: reducing lr to 1.9297527127258225e-09
[I 2024-06-21 00:27:09,522] Trial 291 finished with value: 0.9776159524917603 and parameters: {'hidden_size': 156, 'n_layers': 1, 'rnn_dropout': 0.36229864634342, 'bidirectional': True, 'fc_dropout': 0.5369226233739942, 'learning_rate_model': 0.0012518426160814066}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.000124506656155985
Epoch 33: reducing lr to 0.00012068233852948439
Epoch 36: reducing lr to 0.00011737456298070588
Epoch 40: reducing lr to 0.00011162858113755284
Epoch 44: reducing lr to 0.00010450611069760772
Epoch 48: reducing lr to 9.62066516132344e-05
Epoch 55: reducing lr to 7.956472707458569e-05
Epoch 61: reducing lr to 6.408162578054027e-05
Epoch 64: reducing lr to 5.624062883814315e-05
Epoch 72: reducing lr to 3.6142636160961834e-05
Epoch 75: reducing lr to 2.924921857450016e-05
Epoch 78: reducing lr to 2.2879917822464513e-05
Epoch 81: reducing lr to 1.7135162733149612e-05
Epoch 84: reducing lr to 1.2105568451648788e-05
Epoch 87: reducing lr to 7.870434115575574e-06
Epoch 90: reducing lr to 4.4965754062203414e-06
Epoch 93: reducing lr to 2.037183185270167e-06
Epoch 96: reducing lr to 5.310507832461537e-07
Epoch 99: reducing lr to 1.926350578140487e-09
[I 2024-06-21 00:27:25,715] Trial 292 finished with value: 0.9776151776313782 and parameters: {'hidden_size': 156, 'n_layers': 1, 'rnn_dropout': 0.3576689544455275, 'bidirectional': True, 'fc_dropout': 0.5352084094988009, 'learning_rate_model': 0.0012496356301630455}. Best is trial 235 with value: 0.9706074595451355.
Epoch 55: reducing lr to 2.554918761819274e-05
Epoch 61: reducing lr to 2.0577378194373272e-05
Epoch 65: reducing lr to 1.72253318249261e-05
Epoch 68: reducing lr to 1.4758385785601817e-05
Epoch 73: reducing lr to 1.0851388283076462e-05
Epoch 76: reducing lr to 8.69016958916308e-06
Epoch 79: reducing lr to 6.708323620051662e-06
Epoch 82: reducing lr to 4.937099405468879e-06
Epoch 85: reducing lr to 3.404435427856878e-06
Epoch 88: reducing lr to 2.1344964918071027e-06
Epoch 91: reducing lr to 1.1473178142633746e-06
Epoch 94: reducing lr to 4.5846277760479106e-07
Epoch 97: reducing lr to 7.87970668126312e-08
[I 2024-06-21 00:27:41,838] Trial 293 finished with value: 0.9830365777015686 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.35922478030631727, 'bidirectional': True, 'fc_dropout': 0.5307555880603934, 'learning_rate_model': 0.00040127298041863375}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012069359630662421
Epoch 36: reducing lr to 0.00011377992597690354
Epoch 40: reducing lr to 0.00010820991683543351
Epoch 44: reducing lr to 0.00010130557454141487
Epoch 61: reducing lr to 6.211910359988338e-05
Epoch 64: reducing lr to 5.451823992861481e-05
Epoch 72: reducing lr to 3.503575529972689e-05
Epoch 75: reducing lr to 2.8353451035519125e-05
Epoch 78: reducing lr to 2.2179212344548424e-05
Epoch 81: reducing lr to 1.6610392387151565e-05
Epoch 84: reducing lr to 1.1734831187941029e-05
Epoch 87: reducing lr to 7.62939932073259e-06
Epoch 90: reducing lr to 4.358866213230667e-06
Epoch 93: reducing lr to 1.9747937383974127e-06
Epoch 96: reducing lr to 5.14787167451734e-07
Epoch 99: reducing lr to 1.867355418579998e-09
[I 2024-06-21 00:27:57,436] Trial 294 finished with value: 0.9763159155845642 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.4068271145568542, 'bidirectional': True, 'fc_dropout': 0.5404352469439596, 'learning_rate_model': 0.0012113651023469604}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012525274586471871
Epoch 36: reducing lr to 0.00011807791456214509
Epoch 40: reducing lr to 0.0001122975006809621
Epoch 43: reducing lr to 0.00010704310020328194
Epoch 48: reducing lr to 9.678315727884666e-05
Epoch 55: reducing lr to 8.004150820325228e-05
Epoch 61: reducing lr to 6.446562646765123e-05
Epoch 64: reducing lr to 5.657764338567289e-05
Epoch 67: reducing lr to 4.8788657717093226e-05
Epoch 72: reducing lr to 3.635921613924361e-05
Epoch 75: reducing lr to 2.9424490657460914e-05
Epoch 78: reducing lr to 2.3017022711078874e-05
Epoch 81: reducing lr to 1.7237842934894524e-05
Epoch 84: reducing lr to 1.2178109473301729e-05
Epoch 87: reducing lr to 7.91759665353289e-06
Epoch 90: reducing lr to 4.52352054103244e-06
Epoch 93: reducing lr to 2.049390736707656e-06
Epoch 96: reducing lr to 5.34233034994204e-07
Epoch 99: reducing lr to 1.937893979791702e-09
[I 2024-06-21 00:28:14,641] Trial 295 finished with value: 0.9781022071838379 and parameters: {'hidden_size': 159, 'n_layers': 1, 'rnn_dropout': 0.4089439121377312, 'bidirectional': True, 'fc_dropout': 0.5430099216002242, 'learning_rate_model': 0.0012571239067911122}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001248193982996651
Epoch 36: reducing lr to 0.0001176693903704489
Epoch 40: reducing lr to 0.00011190897547821517
Epoch 44: reducing lr to 0.00010476861445520838
Epoch 61: reducing lr to 6.424258926342431e-05
Epoch 64: reducing lr to 5.638189690659663e-05
Epoch 72: reducing lr to 3.623342107046151e-05
Epoch 75: reducing lr to 2.9322688247531098e-05
Epoch 78: reducing lr to 2.2937388762315763e-05
Epoch 81: reducing lr to 1.717820370534275e-05
Epoch 84: reducing lr to 1.2135975833429958e-05
Epoch 87: reducing lr to 7.890203471793041e-06
Epoch 90: reducing lr to 4.50787013274482e-06
Epoch 93: reducing lr to 2.042300285480677e-06
Epoch 96: reducing lr to 5.32384703580066e-07
Epoch 99: reducing lr to 1.9311892833582395e-09
[I 2024-06-21 00:28:30,203] Trial 296 finished with value: 0.9764212369918823 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.3932016316145152, 'bidirectional': True, 'fc_dropout': 0.5660785907076019, 'learning_rate_model': 0.0012527745284184658}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013567359987366702
Epoch 36: reducing lr to 0.00012790183259953628
Epoch 40: reducing lr to 0.00012164049633416728
Epoch 61: reducing lr to 6.982907680462317e-05
Epoch 72: reducing lr to 3.938425228237204e-05
Epoch 75: reducing lr to 3.187256729891227e-05
Epoch 78: reducing lr to 2.4932006943455394e-05
Epoch 81: reducing lr to 1.8672007458902074e-05
Epoch 84: reducing lr to 1.319131122029841e-05
Epoch 87: reducing lr to 8.57632966779604e-06
Epoch 90: reducing lr to 4.899871149868502e-06
Epoch 93: reducing lr to 2.2198971916925598e-06
Epoch 96: reducing lr to 5.786804794473713e-07
Epoch 99: reducing lr to 2.0991240598814894e-09
[I 2024-06-21 00:28:45,335] Trial 297 finished with value: 0.9772422313690186 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.40526425707392455, 'bidirectional': True, 'fc_dropout': 0.5703797044812856, 'learning_rate_model': 0.0013617148649644215}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001653674886823716
Epoch 33: reducing lr to 0.00016028809918350155
Epoch 36: reducing lr to 0.00015589477152926624
Epoch 40: reducing lr to 0.00014826306237609223
Epoch 46: reducing lr to 0.0001334685756567694
Epoch 61: reducing lr to 8.51120562802302e-05
Epoch 64: reducing lr to 7.469778596599176e-05
Epoch 67: reducing lr to 6.441421900301933e-05
Epoch 72: reducing lr to 4.8003995616194415e-05
Epoch 75: reducing lr to 3.8848283063092766e-05
Epoch 78: reducing lr to 3.0388693009470997e-05
Epoch 81: reducing lr to 2.2758613208555766e-05
Epoch 84: reducing lr to 1.607839705705142e-05
Epoch 87: reducing lr to 1.0453368235206826e-05
Epoch 90: reducing lr to 5.972270122377924e-06
Epoch 93: reducing lr to 2.705749858963554e-06
Epoch 96: reducing lr to 7.053320448844246e-07
Epoch 99: reducing lr to 2.5585439948540984e-09
[I 2024-06-21 00:29:01,386] Trial 298 finished with value: 0.9788635969161987 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.4096108604720495, 'bidirectional': True, 'fc_dropout': 0.5725057479520274, 'learning_rate_model': 0.0016597434410990897}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 4.8680875731844796e-05
Epoch 38: reducing lr to 4.6261526535583285e-05
Epoch 41: reducing lr to 4.4359662284828744e-05
Epoch 50: reducing lr to 3.698475677562782e-05
Epoch 55: reducing lr to 3.2094842033656446e-05
Epoch 61: reducing lr to 2.5849264269559316e-05
Epoch 64: reducing lr to 2.2686360712852436e-05
Epoch 67: reducing lr to 1.9563152889223236e-05
Epoch 73: reducing lr to 1.3631493806997414e-05
Epoch 76: reducing lr to 1.0916574897718928e-05
Epoch 79: reducing lr to 8.426983672189032e-06
Epoch 82: reducing lr to 6.201975103511774e-06
Epoch 85: reducing lr to 4.276645461441053e-06
Epoch 88: reducing lr to 2.6813505286235204e-06
Epoch 91: reducing lr to 1.4412585073727527e-06
Epoch 94: reducing lr to 5.759200897276066e-07
Epoch 97: reducing lr to 9.898472898081527e-08
[I 2024-06-21 00:29:16,478] Trial 299 finished with value: 0.9835561513900757 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.39393442554916774, 'bidirectional': True, 'fc_dropout': 0.5411483658245088, 'learning_rate_model': 0.0005040783727205479}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012104841526031847
Epoch 36: reducing lr to 0.00011411441989805549
Epoch 40: reducing lr to 0.00010852803586283697
Epoch 44: reducing lr to 0.0001016033959591383
Epoch 47: reducing lr to 9.564710255622764e-05
Epoch 55: reducing lr to 7.735477299247494e-05
Epoch 61: reducing lr to 6.230172335783426e-05
Epoch 64: reducing lr to 5.4678514420722735e-05
Epoch 72: reducing lr to 3.5138754550869873e-05
Epoch 75: reducing lr to 2.8436805431592352e-05
Epoch 78: reducing lr to 2.2244415513222437e-05
Epoch 81: reducing lr to 1.6659224158079045e-05
Epoch 84: reducing lr to 1.1769329625731434e-05
Epoch 87: reducing lr to 7.651828476604475e-06
Epoch 90: reducing lr to 4.371680549669394e-06
Epoch 93: reducing lr to 1.98059930115685e-06
Epoch 96: reducing lr to 5.163005554832504e-07
Epoch 99: reducing lr to 1.8728451306720266e-09
[I 2024-06-21 00:29:31,455] Trial 300 finished with value: 0.9773716926574707 and parameters: {'hidden_size': 152, 'n_layers': 1, 'rnn_dropout': 0.3535559923488275, 'bidirectional': True, 'fc_dropout': 0.5556570850042788, 'learning_rate_model': 0.0012149263128113873}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 5.822190160652153e-05
Epoch 40: reducing lr to 5.385401332671685e-05
Epoch 50: reducing lr to 4.4233445630542106e-05
Epoch 53: reducing lr to 4.078248537004189e-05
Epoch 56: reducing lr to 3.716374141626273e-05
Epoch 61: reducing lr to 3.091549398563423e-05
Epoch 64: reducing lr to 2.713268899494582e-05
Epoch 67: reducing lr to 2.3397359753834676e-05
Epoch 70: reducing lr to 1.9768439579846713e-05
Epoch 73: reducing lr to 1.6303147370492782e-05
Epoch 76: reducing lr to 1.3056128100001386e-05
Epoch 79: reducing lr to 1.0078598768530425e-05
Epoch 82: reducing lr to 7.4175079805836416e-06
Epoch 85: reducing lr to 5.114830567830514e-06
Epoch 88: reducing lr to 3.2068717808211314e-06
Epoch 91: reducing lr to 1.7237325693984286e-06
Epoch 94: reducing lr to 6.887953902481978e-07
Epoch 97: reducing lr to 1.1838487012877201e-07
[I 2024-06-21 00:29:46,549] Trial 301 finished with value: 0.9839774370193481 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.4164502338467392, 'bidirectional': True, 'fc_dropout': 0.5577791775114609, 'learning_rate_model': 0.0006028733250440038}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012350419655767276
Epoch 33: reducing lr to 0.00011971067024811892
Epoch 36: reducing lr to 0.00011642952710157744
Epoch 39: reducing lr to 0.0001122896710442172
Epoch 48: reducing lr to 9.543204819596534e-05
Epoch 55: reducing lr to 7.89241153450161e-05
Epoch 61: reducing lr to 6.356567552677492e-05
Epoch 64: reducing lr to 5.578781000953947e-05
Epoch 67: reducing lr to 4.8107559885233585e-05
Epoch 72: reducing lr to 3.5851635393240036e-05
Epoch 75: reducing lr to 2.9013719840469372e-05
Epoch 78: reducing lr to 2.2695701219610835e-05
Epoch 81: reducing lr to 1.699719976088112e-05
Epoch 84: reducing lr to 1.2008101025713105e-05
Epoch 87: reducing lr to 7.807065678372004e-06
Epoch 90: reducing lr to 4.460371436772669e-06
Epoch 93: reducing lr to 2.0207808988330486e-06
Epoch 96: reducing lr to 5.267750523632163e-07
Epoch 99: reducing lr to 1.910840655302247e-09
[I 2024-06-21 00:30:02,325] Trial 302 finished with value: 0.9782896041870117 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.35938532146908314, 'bidirectional': True, 'fc_dropout': 0.544766112333668, 'learning_rate_model': 0.0012395742465348442}. Best is trial 235 with value: 0.9706074595451355.
Epoch 55: reducing lr to 2.6459402498467053e-05
Epoch 61: reducing lr to 2.1310467485095523e-05
Epoch 64: reducing lr to 1.8702928922264157e-05
Epoch 67: reducing lr to 1.6128116034725802e-05
Epoch 73: reducing lr to 1.1237979639110705e-05
Epoch 76: reducing lr to 8.999765408426274e-06
Epoch 79: reducing lr to 6.947314231883045e-06
Epoch 82: reducing lr to 5.11298841655631e-06
Epoch 85: reducing lr to 3.5257217807412154e-06
Epoch 88: reducing lr to 2.2105400239056598e-06
Epoch 91: reducing lr to 1.18819213725761e-06
Epoch 94: reducing lr to 4.7479596394574035e-07
Epoch 97: reducing lr to 8.160428964126593e-08
[I 2024-06-21 00:30:17,328] Trial 303 finished with value: 0.983183741569519 and parameters: {'hidden_size': 152, 'n_layers': 1, 'rnn_dropout': 0.3508561141779256, 'bidirectional': True, 'fc_dropout': 0.5705992176500804, 'learning_rate_model': 0.00041556872411456963}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 3.0102115880931204e-05
Epoch 41: reducing lr to 2.886458347866649e-05
Epoch 55: reducing lr to 2.088393372264076e-05
Epoch 61: reducing lr to 1.681997129689569e-05
Epoch 65: reducing lr to 1.4080004951942088e-05
Epoch 68: reducing lr to 1.2063520578642716e-05
Epoch 73: reducing lr to 8.869936574462385e-06
Epoch 76: reducing lr to 7.103354065526548e-06
Epoch 79: reducing lr to 5.483391016762758e-06
Epoch 82: reducing lr to 4.0355904190269696e-06
Epoch 85: reducing lr to 2.7827892182272957e-06
Epoch 88: reducing lr to 1.7447397518959507e-06
Epoch 91: reducing lr to 9.378188281344716e-07
Epoch 94: reducing lr to 3.7474797261179506e-07
Epoch 97: reducing lr to 6.440880803903147e-08
[I 2024-06-21 00:30:34,085] Trial 304 finished with value: 0.9833984375 and parameters: {'hidden_size': 157, 'n_layers': 1, 'rnn_dropout': 0.3956890218847193, 'bidirectional': True, 'fc_dropout': 0.5420018601577673, 'learning_rate_model': 0.00032800097024541147}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.467049907430433e-05
Epoch 40: reducing lr to 7.831840011588692e-05
Epoch 43: reducing lr to 7.46538818809782e-05
Epoch 49: reducing lr to 6.593232198724308e-05
Epoch 52: reducing lr to 6.101282773823644e-05
Epoch 55: reducing lr to 5.582246111737463e-05
Epoch 64: reducing lr to 3.945831817648187e-05
Epoch 72: reducing lr to 2.5357604757236145e-05
Epoch 75: reducing lr to 2.052119609557687e-05
Epoch 78: reducing lr to 1.6052506807645606e-05
Epoch 81: reducing lr to 1.2021997568274963e-05
Epoch 84: reducing lr to 8.493243790837191e-06
Epoch 87: reducing lr to 5.521881599389159e-06
Epoch 90: reducing lr to 3.1547887487853216e-06
Epoch 93: reducing lr to 1.4292838463721256e-06
Epoch 96: reducing lr to 3.725842190260994e-07
Epoch 99: reducing lr to 1.3515239000974794e-09
[I 2024-06-21 00:30:50,275] Trial 305 finished with value: 0.9803885221481323 and parameters: {'hidden_size': 156, 'n_layers': 1, 'rnn_dropout': 0.4110710337294816, 'bidirectional': True, 'fc_dropout': 0.5649350485977027, 'learning_rate_model': 0.000876741980278147}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015838131853693855
Epoch 33: reducing lr to 0.0001535165146229182
Epoch 36: reducing lr to 0.00014930878895575832
Epoch 40: reducing lr to 0.00014199949153580557
Epoch 61: reducing lr to 8.151638393048889e-05
Epoch 64: reducing lr to 7.154207835741882e-05
Epoch 67: reducing lr to 6.169295439819346e-05
Epoch 72: reducing lr to 4.597600278817545e-05
Epoch 75: reducing lr to 3.720708552481485e-05
Epoch 78: reducing lr to 2.9104882137375887e-05
Epoch 81: reducing lr to 2.1797145235522334e-05
Epoch 84: reducing lr to 1.5399143726173662e-05
Epoch 87: reducing lr to 1.0011751750213884e-05
Epoch 90: reducing lr to 5.719963604561978e-06
Epoch 93: reducing lr to 2.5914418469333984e-06
Epoch 96: reducing lr to 6.755343517958219e-07
Epoch 99: reducing lr to 2.450454890910725e-09
[I 2024-06-21 00:31:04,401] Trial 306 finished with value: 0.9788910746574402 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.3919368785369738, 'bidirectional': True, 'fc_dropout': 0.535454099363963, 'learning_rate_model': 0.0015896253654744653}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011934192659897799
Epoch 36: reducing lr to 0.0001125056837305254
Epoch 40: reducing lr to 0.000106998054142389
Epoch 44: reducing lr to 0.00010017103484325682
Epoch 61: reducing lr to 6.142342037250829e-05
Epoch 64: reducing lr to 5.390768016669932e-05
Epoch 72: reducing lr to 3.4643383454224536e-05
Epoch 75: reducing lr to 2.8035915540308786e-05
Epoch 78: reducing lr to 2.1930823280149195e-05
Epoch 81: reducing lr to 1.6424369558195566e-05
Epoch 84: reducing lr to 1.1603410662524042e-05
Epoch 87: reducing lr to 7.5439562793041835e-06
Epoch 90: reducing lr to 4.310050471547592e-06
Epoch 93: reducing lr to 1.9526776613500467e-06
Epoch 96: reducing lr to 5.090219716052122e-07
Epoch 99: reducing lr to 1.8464425629530905e-09
[I 2024-06-21 00:31:19,982] Trial 307 finished with value: 0.9763127565383911 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.36057645557776624, 'bidirectional': True, 'fc_dropout': 0.5335189768103447, 'learning_rate_model': 0.0011977988025278516}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.622883829423553e-05
Epoch 40: reducing lr to 7.975983055360935e-05
Epoch 43: reducing lr to 7.602786778311804e-05
Epoch 46: reducing lr to 7.180096517642225e-05
Epoch 49: reducing lr to 6.714578977516395e-05
Epoch 52: reducing lr to 6.213575347297153e-05
Epoch 55: reducing lr to 5.6849859461111205e-05
Epoch 58: reducing lr to 5.13714892755867e-05
Epoch 64: reducing lr to 4.0184538589012744e-05
Epoch 72: reducing lr to 2.5824305089095977e-05
Epoch 75: reducing lr to 2.0898883543569493e-05
Epoch 78: reducing lr to 1.634794915427231e-05
Epoch 81: reducing lr to 1.2243259407019051e-05
Epoch 84: reducing lr to 8.649559804660184e-06
Epoch 87: reducing lr to 5.623510440109624e-06
Epoch 90: reducing lr to 3.2128518414985825e-06
Epoch 93: reducing lr to 1.4555894557468978e-06
Epoch 96: reducing lr to 3.7944153778037657e-07
Epoch 99: reducing lr to 1.3763983572356344e-09
[I 2024-06-21 00:31:35,110] Trial 308 finished with value: 0.9810261726379395 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.3630209401167558, 'bidirectional': True, 'fc_dropout': 0.5567371792927328, 'learning_rate_model': 0.000892878195708135}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011547854112228376
Epoch 33: reducing lr to 0.00011193152898709925
Epoch 36: reducing lr to 0.00010886360389356567
Epoch 40: reducing lr to 0.00010353426953467687
Epoch 44: reducing lr to 9.692825728613478e-05
Epoch 47: reducing lr to 9.124603442361141e-05
Epoch 55: reducing lr to 7.379540091298294e-05
Epoch 64: reducing lr to 5.2162558778319737e-05
Epoch 72: reducing lr to 3.3521893728737345e-05
Epoch 75: reducing lr to 2.712832545850778e-05
Epoch 78: reducing lr to 2.122086973266557e-05
Epoch 81: reducing lr to 1.589267317434035e-05
Epoch 84: reducing lr to 1.1227780324458959e-05
Epoch 87: reducing lr to 7.299740252658193e-06
Epoch 90: reducing lr to 4.170523761445541e-06
Epoch 93: reducing lr to 1.889464784429794e-06
Epoch 96: reducing lr to 4.925437049267678e-07
Epoch 99: reducing lr to 1.7866687719364158e-09
[I 2024-06-21 00:31:51,161] Trial 309 finished with value: 0.9776517152786255 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3493362093254765, 'bidirectional': True, 'fc_dropout': 0.5237115602029963, 'learning_rate_model': 0.001159023171619548}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012201301969382117
Epoch 33: reducing lr to 0.00011826529602760162
Epoch 36: reducing lr to 0.00011502376906321007
Epoch 40: reducing lr to 0.00010939286853600745
Epoch 44: reducing lr to 0.0001024130479152614
Epoch 48: reducing lr to 9.427981154080569e-05
Epoch 55: reducing lr to 7.797119375949342e-05
Epoch 61: reducing lr to 6.279819015119612e-05
Epoch 64: reducing lr to 5.5114233775777044e-05
Epoch 72: reducing lr to 3.541876646473625e-05
Epoch 75: reducing lr to 2.8663411195368594e-05
Epoch 78: reducing lr to 2.2421675676262063e-05
Epoch 81: reducing lr to 1.6791977333302706e-05
Epoch 84: reducing lr to 1.1863116458974322e-05
Epoch 87: reducing lr to 7.712803976837684e-06
Epoch 90: reducing lr to 4.406517374513398e-06
Epoch 93: reducing lr to 1.9963821997828105e-06
Epoch 96: reducing lr to 5.204148249990381e-07
Epoch 99: reducing lr to 1.887769363376722e-09
[I 2024-06-21 00:32:07,256] Trial 310 finished with value: 0.9772639870643616 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.33428363056401017, 'bidirectional': True, 'fc_dropout': 0.534471273177898, 'learning_rate_model': 0.0012246077556059643}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001706752561629774
Epoch 31: reducing lr to 0.000167760335192851
Epoch 34: reducing lr to 0.00016405805565941132
Epoch 40: reducing lr to 0.0001530218324785177
Epoch 46: reducing lr to 0.0001377524900537188
Epoch 61: reducing lr to 8.784388106714102e-05
Epoch 64: reducing lr to 7.709534598448534e-05
Epoch 67: reducing lr to 6.648170941263388e-05
Epoch 72: reducing lr to 4.9544770341026644e-05
Epoch 75: reducing lr to 4.00951886983093e-05
Epoch 78: reducing lr to 3.13640728608491e-05
Epoch 81: reducing lr to 2.3489091902128223e-05
Epoch 84: reducing lr to 1.6594462177950754e-05
Epoch 87: reducing lr to 1.0788887921837605e-05
Epoch 90: reducing lr to 6.163960891788101e-06
Epoch 93: reducing lr to 2.792595775452265e-06
Epoch 96: reducing lr to 7.27970947613692e-07
Epoch 99: reducing lr to 2.640665073909941e-09
[I 2024-06-21 00:32:23,333] Trial 311 finished with value: 0.9792175889015198 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3507726036443418, 'bidirectional': True, 'fc_dropout': 0.531370431052811, 'learning_rate_model': 0.001713015897087917}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013986396184072306
Epoch 36: reducing lr to 0.0001318521587893103
Epoch 40: reducing lr to 0.00012539743733055227
Epoch 46: reducing lr to 0.00011288460580331731
Epoch 61: reducing lr to 7.198579047558932e-05
Epoch 64: reducing lr to 6.317764373867335e-05
Epoch 67: reducing lr to 5.448004284531817e-05
Epoch 72: reducing lr to 4.060065896000619e-05
Epoch 75: reducing lr to 3.285697099960393e-05
Epoch 78: reducing lr to 2.570204720003821e-05
Epoch 81: reducing lr to 1.9248703809387556e-05
Epoch 84: reducing lr to 1.3598732921238105e-05
Epoch 87: reducing lr to 8.841214845828706e-06
Epoch 90: reducing lr to 5.051206661928361e-06
Epoch 93: reducing lr to 2.2884600718070914e-06
Epoch 96: reducing lr to 5.965533793660886e-07
Epoch 99: reducing lr to 2.1639567880865977e-09
[I 2024-06-21 00:32:38,911] Trial 312 finished with value: 0.9776841402053833 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.33097530412744286, 'bidirectional': True, 'fc_dropout': 0.5533189456790042, 'learning_rate_model': 0.0014037722599582522}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001459140325722965
Epoch 36: reducing lr to 0.00013755566436921384
Epoch 40: reducing lr to 0.00013082173216263958
Epoch 46: reducing lr to 0.00011776763528873694
Epoch 61: reducing lr to 7.509966711910604e-05
Epoch 64: reducing lr to 6.59105079321559e-05
Epoch 67: reducing lr to 5.68366764508261e-05
Epoch 72: reducing lr to 4.235691450449204e-05
Epoch 75: reducing lr to 3.427825919962819e-05
Epoch 78: reducing lr to 2.681383612307439e-05
Epoch 81: reducing lr to 2.008134159545657e-05
Epoch 84: reducing lr to 1.4186970913001533e-05
Epoch 87: reducing lr to 9.223657717218362e-06
Epoch 90: reducing lr to 5.269705817695608e-06
Epoch 93: reducing lr to 2.38745158555086e-06
Epoch 96: reducing lr to 6.223583836918994e-07
Epoch 99: reducing lr to 2.257562685230308e-09
[I 2024-06-21 00:32:53,703] Trial 313 finished with value: 0.9778149127960205 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.35341210793400823, 'bidirectional': True, 'fc_dropout': 0.5583101416343066, 'learning_rate_model': 0.00146449498904439}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014358560828987745
Epoch 36: reducing lr to 0.0001353606188108446
Epoch 40: reducing lr to 0.00012873414337857464
Epoch 46: reducing lr to 0.00011588835735463214
Epoch 61: reducing lr to 7.390126361096454e-05
Epoch 64: reducing lr to 6.485874050149589e-05
Epoch 67: reducing lr to 5.592970475490969e-05
Epoch 72: reducing lr to 4.1681003719751445e-05
Epoch 75: reducing lr to 3.3731263618240755e-05
Epoch 78: reducing lr to 2.6385954129594163e-05
Epoch 81: reducing lr to 1.9760893434507804e-05
Epoch 84: reducing lr to 1.3960582216962742e-05
Epoch 87: reducing lr to 9.076471129178233e-06
Epoch 90: reducing lr to 5.185614447106909e-06
Epoch 93: reducing lr to 2.349353808751061e-06
Epoch 96: reducing lr to 6.124270950597609e-07
Epoch 99: reducing lr to 2.2215376115509306e-09
[I 2024-06-21 00:33:08,501] Trial 314 finished with value: 0.9775139093399048 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.34050891697083296, 'bidirectional': True, 'fc_dropout': 0.5697094183443183, 'learning_rate_model': 0.0014411252991396002}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012857387813214357
Epoch 36: reducing lr to 0.00012120880298630888
Epoch 40: reducing lr to 0.00011527511886001205
Epoch 44: reducing lr to 0.00010791998079254994
Epoch 61: reducing lr to 6.617496122692842e-05
Epoch 64: reducing lr to 5.807782476505728e-05
Epoch 67: reducing lr to 5.008231067703386e-05
Epoch 72: reducing lr to 3.7323296927290686e-05
Epoch 75: reducing lr to 3.020469411488117e-05
Epoch 78: reducing lr to 2.3627329305940917e-05
Epoch 81: reducing lr to 1.7694912007485655e-05
Epoch 84: reducing lr to 1.2501017462653898e-05
Epoch 87: reducing lr to 8.127535250447202e-06
Epoch 90: reducing lr to 4.64346370018208e-06
Epoch 93: reducing lr to 2.1037312436342864e-06
Epoch 96: reducing lr to 5.483984615370927e-07
Epoch 99: reducing lr to 1.989278100621508e-09
[I 2024-06-21 00:33:24,048] Trial 315 finished with value: 0.9766277074813843 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.3264938367539949, 'bidirectional': True, 'fc_dropout': 0.5323516552359615, 'learning_rate_model': 0.0012904571063323386}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00018462896202606045
Epoch 31: reducing lr to 0.00018147572912516165
Epoch 34: reducing lr to 0.00017747076646826325
Epoch 40: reducing lr to 0.00016553226714279196
Epoch 46: reducing lr to 0.00014901456618197375
Epoch 49: reducing lr to 0.00013935329016409522
Epoch 52: reducing lr to 0.00012895554155037186
Epoch 55: reducing lr to 0.00011798528229096132
Epoch 58: reducing lr to 0.00010661556108214024
Epoch 61: reducing lr to 9.502563491851402e-05
Epoch 64: reducing lr to 8.339834388508838e-05
Epoch 67: reducing lr to 7.191698010900845e-05
Epoch 70: reducing lr to 6.076268822669045e-05
Epoch 73: reducing lr to 5.011134322392089e-05
Epoch 76: reducing lr to 4.013090856179089e-05
Epoch 79: reducing lr to 3.0978811061974025e-05
Epoch 82: reducing lr to 2.279935768439072e-05
Epoch 85: reducing lr to 1.5721567393830732e-05
Epoch 88: reducing lr to 9.857032438698784e-06
Epoch 91: reducing lr to 5.298274771637843e-06
Epoch 94: reducing lr to 2.1171655648683734e-06
Epoch 97: reducing lr to 3.6388218328194977e-07
[I 2024-06-21 00:33:38,858] Trial 316 finished with value: 0.9824527502059937 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.3291535923483935, 'bidirectional': True, 'fc_dropout': 0.5915808228477835, 'learning_rate_model': 0.001853065020224579}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001233959972926702
Epoch 36: reducing lr to 0.00011632752579629302
Epoch 40: reducing lr to 0.0001106328008566629
Epoch 44: reducing lr to 0.00010357386625622268
Epoch 48: reducing lr to 9.534844231243838e-05
Epoch 61: reducing lr to 6.350998705979901e-05
Epoch 64: reducing lr to 5.5738935556627144e-05
Epoch 67: reducing lr to 4.80654139277218e-05
Epoch 72: reducing lr to 3.582022657712847e-05
Epoch 75: reducing lr to 2.8988301569275148e-05
Epoch 78: reducing lr to 2.2675818023257004e-05
Epoch 81: reducing lr to 1.6982308894234604e-05
Epoch 84: reducing lr to 1.1997580996910268e-05
Epoch 87: reducing lr to 7.800226082700212e-06
Epoch 90: reducing lr to 4.456463805092566e-06
Epoch 93: reducing lr to 2.0190105378730488e-06
Epoch 96: reducing lr to 5.263135565187473e-07
Epoch 99: reducing lr to 1.909166610545264e-09
[I 2024-06-21 00:33:54,525] Trial 317 finished with value: 0.9769993424415588 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.33578412825823295, 'bidirectional': True, 'fc_dropout': 0.5782046245497935, 'learning_rate_model': 0.0012384882832548148}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012550195508845817
Epoch 33: reducing lr to 0.00012164706609036499
Epoch 36: reducing lr to 0.00011831284837717369
Epoch 40: reducing lr to 0.00011252093349099254
Epoch 44: reducing lr to 0.00010534152643862592
Epoch 47: reducing lr to 9.916609270380011e-05
Epoch 61: reducing lr to 6.459389055175549e-05
Epoch 64: reducing lr to 5.669021313807082e-05
Epoch 72: reducing lr to 3.643155828206276e-05
Epoch 75: reducing lr to 2.9483035118303267e-05
Epoch 78: reducing lr to 2.3062818548311984e-05
Epoch 81: reducing lr to 1.7272140222567444e-05
Epoch 84: reducing lr to 1.220233965833681e-05
Epoch 87: reducing lr to 7.933349905905017e-06
Epoch 90: reducing lr to 4.532520767213635e-06
Epoch 93: reducing lr to 2.053468308589276e-06
Epoch 96: reducing lr to 5.352959721699599e-07
Epoch 99: reducing lr to 1.9417497120616244e-09
[I 2024-06-21 00:34:09,122] Trial 318 finished with value: 0.9778985977172852 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.3279454280725468, 'bidirectional': True, 'fc_dropout': 0.5670446702229104, 'learning_rate_model': 0.0012596251443551501}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014990587273086385
Epoch 36: reducing lr to 0.00014131884064079946
Epoch 40: reducing lr to 0.0001344006850217583
Epoch 46: reducing lr to 0.0001209894609599051
Epoch 57: reducing lr to 8.966869237545368e-05
Epoch 61: reducing lr to 7.715420472468247e-05
Epoch 64: reducing lr to 6.771365330341955e-05
Epoch 67: reducing lr to 5.8391584663122214e-05
Epoch 70: reducing lr to 4.9335075646526804e-05
Epoch 73: reducing lr to 4.0686924506662174e-05
Epoch 76: reducing lr to 3.258350588890067e-05
Epoch 79: reducing lr to 2.515263942042996e-05
Epoch 82: reducing lr to 1.8511492313428593e-05
Epoch 85: reducing lr to 1.2764818991598047e-05
Epoch 88: reducing lr to 8.003224597292656e-06
Epoch 91: reducing lr to 4.301830519407783e-06
Epoch 94: reducing lr to 1.7189911497879047e-06
Epoch 97: reducing lr to 2.9544701794072347e-07
[I 2024-06-21 00:34:24,662] Trial 319 finished with value: 0.9788414835929871 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.33811192675088586, 'bidirectional': True, 'fc_dropout': 0.5768945088627818, 'learning_rate_model': 0.0015045598807222446}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.0001224506305606688
Epoch 27: reducing lr to 0.00012257550197151355
Epoch 33: reducing lr to 0.00011881050122987019
Epoch 36: reducing lr to 0.00011555403076621347
Epoch 39: reducing lr to 0.0001114453045167068
Epoch 44: reducing lr to 0.00010288517396050926
Epoch 48: reducing lr to 9.471444321592491e-05
Epoch 55: reducing lr to 7.833064240497641e-05
Epoch 61: reducing lr to 6.30876909693862e-05
Epoch 64: reducing lr to 5.536831141294447e-05
Epoch 67: reducing lr to 4.774581322670703e-05
Epoch 72: reducing lr to 3.5582047633288047e-05
Epoch 75: reducing lr to 2.879555005117318e-05
Epoch 78: reducing lr to 2.2525040015868696e-05
Epoch 81: reducing lr to 1.6869388659414437e-05
Epoch 84: reducing lr to 1.1917805645285102e-05
Epoch 87: reducing lr to 7.748360145836482e-06
Epoch 90: reducing lr to 4.426831501118324e-06
Epoch 93: reducing lr to 2.0055855586513906e-06
Epoch 96: reducing lr to 5.228139469685284e-07
Epoch 99: reducing lr to 1.896472015061948e-09
[I 2024-06-21 00:34:40,437] Trial 320 finished with value: 0.9783030152320862 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.3387119925327068, 'bidirectional': True, 'fc_dropout': 0.5424637655497996, 'learning_rate_model': 0.0012302532200111687}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00018014307398954535
Epoch 31: reducing lr to 0.0001770664544736283
Epoch 34: reducing lr to 0.00017315879948651323
Epoch 40: reducing lr to 0.0001615103671727956
Epoch 46: reducing lr to 0.00014539399304779857
Epoch 49: reducing lr to 0.00013596745486319645
Epoch 52: reducing lr to 0.0001258223379904586
Epoch 55: reducing lr to 0.00011511862063340851
Epoch 58: reducing lr to 0.00010402514696337803
Epoch 61: reducing lr to 9.27168186084107e-05
Epoch 64: reducing lr to 8.137203322941516e-05
Epoch 67: reducing lr to 7.01696295462741e-05
Epoch 70: reducing lr to 5.9286350965236665e-05
Epoch 73: reducing lr to 4.88937992774291e-05
Epoch 76: reducing lr to 3.915585697380434e-05
Epoch 79: reducing lr to 3.0226125912237012e-05
Epoch 82: reducing lr to 2.22454068591556e-05
Epoch 85: reducing lr to 1.5339584034809857e-05
Epoch 88: reducing lr to 9.61753835604204e-06
Epoch 91: reducing lr to 5.169543790585658e-06
Epoch 94: reducing lr to 2.065725273082378e-06
Epoch 97: reducing lr to 3.5504102036377183e-07
[I 2024-06-21 00:34:55,416] Trial 321 finished with value: 0.983093798160553 and parameters: {'hidden_size': 152, 'n_layers': 1, 'rnn_dropout': 0.3658291134461264, 'bidirectional': True, 'fc_dropout': 0.6053659553666249, 'learning_rate_model': 0.0018080415195024294}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.000148036219209122
Epoch 36: reducing lr to 0.00013955628615724757
Epoch 40: reducing lr to 0.00013272441504315254
Epoch 46: reducing lr to 0.00011948045822601317
Epoch 61: reducing lr to 7.619192334135284e-05
Epoch 64: reducing lr to 6.686911620782467e-05
Epoch 67: reducing lr to 5.766331411630051e-05
Epoch 72: reducing lr to 4.29729572274156e-05
Epoch 75: reducing lr to 3.477680524297086e-05
Epoch 78: reducing lr to 2.720381893486616e-05
Epoch 81: reducing lr to 2.0373406409457895e-05
Epoch 84: reducing lr to 1.4393307476783988e-05
Epoch 87: reducing lr to 9.357807413481718e-06
Epoch 90: reducing lr to 5.346348886694314e-06
Epoch 93: reducing lr to 2.4221748932520217e-06
Epoch 96: reducing lr to 6.314100192467923e-07
Epoch 99: reducing lr to 2.2903968772578984e-09
[I 2024-06-21 00:35:10,236] Trial 322 finished with value: 0.9780912399291992 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.327297658659157, 'bidirectional': True, 'fc_dropout': 0.5893932841818849, 'learning_rate_model': 0.0014857947341111164}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001170815143105022
Epoch 36: reducing lr to 0.00011037475424685446
Epoch 40: reducing lr to 0.00010497144267968675
Epoch 44: reducing lr to 9.82737314850674e-05
Epoch 47: reducing lr to 9.251263292140001e-05
Epoch 55: reducing lr to 7.481976481581471e-05
Epoch 61: reducing lr to 6.0260021572380135e-05
Epoch 64: reducing lr to 5.288663428479821e-05
Epoch 72: reducing lr to 3.398721565212892e-05
Epoch 75: reducing lr to 2.7504897399308482e-05
Epoch 78: reducing lr to 2.151543948459998e-05
Epoch 81: reducing lr to 1.6113281511958753e-05
Epoch 86: reducing lr to 8.641445162859795e-06
Epoch 89: reducing lr to 5.192616847043085e-06
Epoch 92: reducing lr to 2.588512986477173e-06
Epoch 95: reducing lr to 8.702095270456643e-07
Epoch 98: reducing lr to 6.480056747198201e-08
[I 2024-06-21 00:35:24,084] Trial 323 finished with value: 0.9767931699752808 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.34798081956707755, 'bidirectional': True, 'fc_dropout': 0.5301359170006927, 'learning_rate_model': 0.0011751117284247697}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011811058847979651
Epoch 40: reducing lr to 0.00010589407680183132
Epoch 44: reducing lr to 9.91374968641455e-05
Epoch 47: reducing lr to 9.332576180373016e-05
Epoch 55: reducing lr to 7.547738431944051e-05
Epoch 61: reducing lr to 6.078966992896697e-05
Epoch 64: reducing lr to 5.33514751229434e-05
Epoch 72: reducing lr to 3.428594227793157e-05
Epoch 75: reducing lr to 2.7746648452917632e-05
Epoch 78: reducing lr to 2.1704546903862588e-05
Epoch 81: reducing lr to 1.625490730048887e-05
Epoch 84: reducing lr to 1.1483689770893962e-05
Epoch 87: reducing lr to 7.466119753609746e-06
Epoch 90: reducing lr to 4.265580522113603e-06
Epoch 93: reducing lr to 1.932530454853412e-06
Epoch 96: reducing lr to 5.037700188757722e-07
Epoch 99: reducing lr to 1.8273914618484322e-09
[I 2024-06-21 00:35:37,721] Trial 324 finished with value: 0.9775627255439758 and parameters: {'hidden_size': 137, 'n_layers': 1, 'rnn_dropout': 0.34480610653813476, 'bidirectional': True, 'fc_dropout': 0.529496902280768, 'learning_rate_model': 0.0011854402344479299}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011913637801508709
Epoch 33: reducing lr to 0.00011547701260871291
Epoch 36: reducing lr to 0.00011231190955049062
Epoch 40: reducing lr to 0.00010681376602894202
Epoch 44: reducing lr to 9.999850524745008e-05
Epoch 47: reducing lr to 9.413629531358277e-05
Epoch 55: reducing lr to 7.61329048107234e-05
Epoch 64: reducing lr to 5.3814832266259914e-05
Epoch 72: reducing lr to 3.458371541790957e-05
Epoch 75: reducing lr to 2.798762787727515e-05
Epoch 78: reducing lr to 2.1893050723619727e-05
Epoch 81: reducing lr to 1.6396081043000642e-05
Epoch 84: reducing lr to 1.1583425526554176e-05
Epoch 87: reducing lr to 7.530962945156354e-06
Epoch 90: reducing lr to 4.302627055517921e-06
Epoch 93: reducing lr to 1.94931446670819e-06
Epoch 96: reducing lr to 5.081452575415608e-07
Epoch 99: reducing lr to 1.8432623423480024e-09
[I 2024-06-21 00:35:51,894] Trial 325 finished with value: 0.9785346984863281 and parameters: {'hidden_size': 135, 'n_layers': 1, 'rnn_dropout': 0.35869171474634415, 'bidirectional': True, 'fc_dropout': 0.5317829677395475, 'learning_rate_model': 0.0011957357735935764}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00017789939625633447
Epoch 33: reducing lr to 0.0001724350795856292
Epoch 36: reducing lr to 0.0001677088160167005
Epoch 40: reducing lr to 0.00015949875936305353
Epoch 46: reducing lr to 0.00014358311429726222
Epoch 61: reducing lr to 9.15620328217647e-05
Epoch 64: reducing lr to 8.035854647680466e-05
Epoch 67: reducing lr to 6.929566846703899e-05
Epoch 70: reducing lr to 5.854794086376507e-05
Epoch 73: reducing lr to 4.828482816185229e-05
Epoch 76: reducing lr to 3.8668171699698204e-05
Epoch 79: reducing lr to 2.984966022766425e-05
Epoch 82: reducing lr to 2.196834084195751e-05
Epoch 85: reducing lr to 1.5148529877836732e-05
Epoch 88: reducing lr to 9.497752142895666e-06
Epoch 91: reducing lr to 5.105157244731187e-06
Epoch 94: reducing lr to 2.0399967135796515e-06
Epoch 97: reducing lr to 3.5061899283795725e-07
[I 2024-06-21 00:36:06,023] Trial 326 finished with value: 0.980664849281311 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.36491408441996104, 'bidirectional': True, 'fc_dropout': 0.5307064805780755, 'learning_rate_model': 0.0017855224050664018}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012124645130821392
Epoch 33: reducing lr to 0.00011752227337906182
Epoch 36: reducing lr to 0.00011430111188139038
Epoch 39: reducing lr to 0.000110236935360495
Epoch 44: reducing lr to 0.00010176961982043572
Epoch 47: reducing lr to 9.580358188015892e-05
Epoch 55: reducing lr to 7.74813259382226e-05
Epoch 61: reducing lr to 6.240364940985507e-05
Epoch 64: reducing lr to 5.476796884998881e-05
Epoch 72: reducing lr to 3.519624179730973e-05
Epoch 75: reducing lr to 2.8483328242736412e-05
Epoch 78: reducing lr to 2.2280807531461666e-05
Epoch 81: reducing lr to 1.6686478764479104e-05
Epoch 84: reducing lr to 1.1788584330722383e-05
Epoch 87: reducing lr to 7.664346921124554e-06
Epoch 90: reducing lr to 4.378832649404481e-06
Epoch 93: reducing lr to 1.9838395753663016e-06
Epoch 96: reducing lr to 5.171452267770755e-07
Epoch 99: reducing lr to 1.8759091182990116e-09
[I 2024-06-21 00:36:20,048] Trial 327 finished with value: 0.9780260920524597 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.3386416484216566, 'bidirectional': True, 'fc_dropout': 0.5755194766951085, 'learning_rate_model': 0.0012169139406952877}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.639704345206629e-05
Epoch 40: reducing lr to 7.991541672584845e-05
Epoch 43: reducing lr to 7.617617407777503e-05
Epoch 48: reducing lr to 6.88747861624813e-05
Epoch 51: reducing lr to 6.39649503187914e-05
Epoch 54: reducing lr to 5.875179468203633e-05
Epoch 64: reducing lr to 4.026292589875078e-05
Epoch 72: reducing lr to 2.587468013066338e-05
Epoch 75: reducing lr to 2.0939650647411692e-05
Epoch 78: reducing lr to 1.6379838826244014e-05
Epoch 81: reducing lr to 1.226714212910669e-05
Epoch 84: reducing lr to 8.666432356823598e-06
Epoch 87: reducing lr to 5.634480128207634e-06
Epoch 90: reducing lr to 3.219119098042645e-06
Epoch 93: reducing lr to 1.4584288498403872e-06
Epoch 96: reducing lr to 3.801817080646198e-07
Epoch 99: reducing lr to 1.3790832745740057e-09
[I 2024-06-21 00:36:36,300] Trial 328 finished with value: 0.9801841974258423 and parameters: {'hidden_size': 156, 'n_layers': 1, 'rnn_dropout': 0.3153726401250342, 'bidirectional': True, 'fc_dropout': 0.5510901701185682, 'learning_rate_model': 0.0008946199183244162}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 6.120851333407329e-05
Epoch 38: reducing lr to 5.8166563794075965e-05
Epoch 41: reducing lr to 5.577526984954744e-05
Epoch 50: reducing lr to 4.6502490849350324e-05
Epoch 55: reducing lr to 4.03541952982363e-05
Epoch 61: reducing lr to 3.250136758908605e-05
Epoch 64: reducing lr to 2.852451586621454e-05
Epoch 67: reducing lr to 2.4597575258762868e-05
Epoch 70: reducing lr to 2.078250218954264e-05
Epoch 73: reducing lr to 1.713945071664224e-05
Epoch 76: reducing lr to 1.3725868940199483e-05
Epoch 79: reducing lr to 1.05956011413284e-05
Epoch 82: reducing lr to 7.79800424938875e-06
Epoch 85: reducing lr to 5.377206281038266e-06
Epoch 88: reducing lr to 3.371374839035955e-06
Epoch 91: reducing lr to 1.81215496311756e-06
Epoch 94: reducing lr to 7.241285609903953e-07
Epoch 97: reducing lr to 1.2445766458816153e-07
[I 2024-06-21 00:36:50,874] Trial 329 finished with value: 0.9831456542015076 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.41564092171261313, 'bidirectional': True, 'fc_dropout': 0.4967512174144121, 'learning_rate_model': 0.0006337989474149995}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001493234722896338
Epoch 33: reducing lr to 0.00014473688708400366
Epoch 36: reducing lr to 0.00014076980174296334
Epoch 40: reducing lr to 0.0001338785239027018
Epoch 44: reducing lr to 0.00012533639410652097
Epoch 48: reducing lr to 0.00011538267687672293
Epoch 61: reducing lr to 7.6854452339697e-05
Epoch 64: reducing lr to 6.745057847624444e-05
Epoch 67: reducing lr to 5.816472707540798e-05
Epoch 72: reducing lr to 4.334662977772271e-05
Epoch 75: reducing lr to 3.507920792468306e-05
Epoch 78: reducing lr to 2.7440370502534355e-05
Epoch 81: reducing lr to 2.0550563934158287e-05
Epoch 84: reducing lr to 1.4518464884121364e-05
Epoch 87: reducing lr to 9.439178489318397e-06
Epoch 90: reducing lr to 5.392838212824459e-06
Epoch 93: reducing lr to 2.4432369827158926e-06
Epoch 96: reducing lr to 6.369004627117909e-07
Epoch 99: reducing lr to 2.310313087301903e-09
[I 2024-06-21 00:37:05,040] Trial 330 finished with value: 0.9781490564346313 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.3491130599622304, 'bidirectional': True, 'fc_dropout': 0.5353421425565738, 'learning_rate_model': 0.0014987145037371628}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 4.5031577668613176e-05
Epoch 41: reducing lr to 4.3180278022088624e-05
Epoch 50: reducing lr to 3.600144811510709e-05
Epoch 55: reducing lr to 3.124154086633511e-05
Epoch 61: reducing lr to 2.516201342244556e-05
Epoch 64: reducing lr to 2.2083201549200857e-05
Epoch 67: reducing lr to 1.9043030024017842e-05
Epoch 73: reducing lr to 1.3269075149019709e-05
Epoch 76: reducing lr to 1.0626337416767757e-05
Epoch 79: reducing lr to 8.202936612012322e-06
Epoch 82: reducing lr to 6.037084041266488e-06
Epoch 85: reducing lr to 4.162942874569303e-06
Epoch 88: reducing lr to 2.6100618295337275e-06
Epoch 91: reducing lr to 1.4029399649271126e-06
Epoch 94: reducing lr to 5.60608181218047e-07
Epoch 97: reducing lr to 9.635303555488497e-08
[I 2024-06-21 00:37:19,925] Trial 331 finished with value: 0.9826584458351135 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.36637715486076167, 'bidirectional': True, 'fc_dropout': 0.5460018231035171, 'learning_rate_model': 0.0004906765100968645}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.00011927614138228213
Epoch 27: reducing lr to 0.000119397775546078
Epoch 33: reducing lr to 0.00011573038111365689
Epoch 36: reducing lr to 0.00011255833349208177
Epoch 39: reducing lr to 0.00010855612451370953
Epoch 44: reducing lr to 0.00010021791230690555
Epoch 48: reducing lr to 9.225900485967321e-05
Epoch 55: reducing lr to 7.629994827533309e-05
Epoch 61: reducing lr to 6.145216495082075e-05
Epoch 64: reducing lr to 5.3932907572219856e-05
Epoch 67: reducing lr to 4.6508019948653165e-05
Epoch 72: reducing lr to 3.465959566518039e-05
Epoch 75: reducing lr to 2.804903562650591e-05
Epoch 78: reducing lr to 2.1941086340451125e-05
Epoch 81: reducing lr to 1.643205574001572e-05
Epoch 84: reducing lr to 1.1608840759781062e-05
Epoch 87: reducing lr to 7.5474866564916036e-06
Epoch 90: reducing lr to 4.312067464130528e-06
Epoch 93: reducing lr to 1.953591464189672e-06
Epoch 96: reducing lr to 5.092601807742472e-07
Epoch 99: reducing lr to 1.8473066505089382e-09
[I 2024-06-21 00:37:35,661] Trial 332 finished with value: 0.9783880710601807 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.39938509905507985, 'bidirectional': True, 'fc_dropout': 0.5639265723737766, 'learning_rate_model': 0.0011983593415091235}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 3.232602925510198e-05
Epoch 41: reducing lr to 3.099706923122912e-05
Epoch 55: reducing lr to 2.2426817275902805e-05
Epoch 61: reducing lr to 1.806261348418561e-05
Epoch 65: reducing lr to 1.512022124254678e-05
Epoch 68: reducing lr to 1.2954761076837157e-05
Epoch 73: reducing lr to 9.525238369659092e-06
Epoch 76: reducing lr to 7.6281425611353275e-06
Epoch 79: reducing lr to 5.888498307765904e-06
Epoch 82: reducing lr to 4.3337356903112e-06
Epoch 85: reducing lr to 2.9883788247650337e-06
Epoch 88: reducing lr to 1.8736393310496846e-06
Epoch 91: reducing lr to 1.0071039190127114e-06
Epoch 94: reducing lr to 4.0243396756083743e-07
Epoch 97: reducing lr to 6.916726456013115e-08
[I 2024-06-21 00:37:49,235] Trial 333 finished with value: 0.9834310412406921 and parameters: {'hidden_size': 137, 'n_layers': 1, 'rnn_dropout': 0.3711366599415816, 'bidirectional': True, 'fc_dropout': 0.5313056601935393, 'learning_rate_model': 0.00035223334471885646}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 8.995587515463961e-05
Epoch 33: reducing lr to 8.728172788296978e-05
Epoch 40: reducing lr to 8.073373089658448e-05
Epoch 43: reducing lr to 7.695619932539582e-05
Epoch 46: reducing lr to 7.26776844989923e-05
Epoch 49: reducing lr to 6.796566749102012e-05
Epoch 52: reducing lr to 6.289445658452931e-05
Epoch 55: reducing lr to 5.754401963225211e-05
Epoch 64: reducing lr to 4.0675208336459506e-05
Epoch 72: reducing lr to 2.613963047793892e-05
Epoch 75: reducing lr to 2.1154067509102113e-05
Epoch 78: reducing lr to 1.6547564338730165e-05
Epoch 81: reducing lr to 1.2392754641059388e-05
Epoch 84: reducing lr to 8.755174488165316e-06
Epoch 87: reducing lr to 5.692175816005398e-06
Epoch 90: reducing lr to 3.252082084199016e-06
Epoch 93: reducing lr to 1.4733628018077277e-06
Epoch 96: reducing lr to 3.840746750528347e-07
Epoch 99: reducing lr to 1.3932047474018993e-09
[I 2024-06-21 00:38:05,888] Trial 334 finished with value: 0.9801112413406372 and parameters: {'hidden_size': 157, 'n_layers': 1, 'rnn_dropout': 0.34143933275762756, 'bidirectional': True, 'fc_dropout': 0.5003813310150047, 'learning_rate_model': 0.0009037806057935061}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014536966370977256
Epoch 33: reducing lr to 0.00014090452277315267
Epoch 36: reducing lr to 0.00013704247849376152
Epoch 40: reducing lr to 0.00013033366890871298
Epoch 46: reducing lr to 0.00011732827361437229
Epoch 61: reducing lr to 7.481948899199345e-05
Epoch 64: reducing lr to 6.56646122660648e-05
Epoch 67: reducing lr to 5.662463298685121e-05
Epoch 70: reducing lr to 4.784217739559713e-05
Epoch 73: reducing lr to 3.945572261559991e-05
Epoch 76: reducing lr to 3.159751654332396e-05
Epoch 79: reducing lr to 2.4391510935170038e-05
Epoch 82: reducing lr to 1.7951327478680622e-05
Epoch 85: reducing lr to 1.2378550688646092e-05
Epoch 88: reducing lr to 7.761043961172841e-06
Epoch 91: reducing lr to 4.171655483207744e-06
Epoch 94: reducing lr to 1.666973820387864e-06
Epoch 97: reducing lr to 2.8650667822207114e-07
[I 2024-06-21 00:38:21,695] Trial 335 finished with value: 0.9790756106376648 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.41088568447279245, 'bidirectional': True, 'fc_dropout': 0.5856359158834807, 'learning_rate_model': 0.0014590313235058265}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011806839117180925
Epoch 36: reducing lr to 0.00011130510001219707
Epoch 40: reducing lr to 0.00010585624408056267
Epoch 44: reducing lr to 9.910207806264677e-05
Epoch 48: reducing lr to 9.123178572694574e-05
Epoch 56: reducing lr to 7.30495990789078e-05
Epoch 61: reducing lr to 6.076795163548073e-05
Epoch 64: reducing lr to 5.3332414269413557e-05
Epoch 67: reducing lr to 4.5990195937986754e-05
Epoch 72: reducing lr to 3.427369295731979e-05
Epoch 75: reducing lr to 2.77367354223803e-05
Epoch 78: reducing lr to 2.1696792531776097e-05
Epoch 81: reducing lr to 1.6249099918284685e-05
Epoch 84: reducing lr to 1.1479586999073678e-05
Epoch 87: reducing lr to 7.463452336921978e-06
Epoch 90: reducing lr to 4.264056560398123e-06
Epoch 93: reducing lr to 1.9318400207115866e-06
Epoch 96: reducing lr to 5.03590037225399e-07
Epoch 99: reducing lr to 1.8267385906451522e-09
[I 2024-06-21 00:38:40,054] Trial 336 finished with value: 0.9784876108169556 and parameters: {'hidden_size': 172, 'n_layers': 1, 'rnn_dropout': 0.39261575231379864, 'bidirectional': True, 'fc_dropout': 0.5649837208401428, 'learning_rate_model': 0.0011850167128372316}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00016596285864821104
Epoch 36: reducing lr to 0.00015645603702068455
Epoch 40: reducing lr to 0.00014879685154520547
Epoch 48: reducing lr to 0.00012823997861368465
Epoch 52: reducing lr to 0.00011591805575555903
Epoch 57: reducing lr to 9.927344570813054e-05
Epoch 60: reducing lr to 8.890311015207182e-05
Epoch 63: reducing lr to 7.844418635079411e-05
Epoch 66: reducing lr to 6.806164859528777e-05
Epoch 69: reducing lr to 5.791919012754711e-05
Epoch 72: reducing lr to 4.8176823645802245e-05
Epoch 75: reducing lr to 3.898814792494974e-05
Epoch 78: reducing lr to 3.0498100942451756e-05
Epoch 81: reducing lr to 2.284055068536271e-05
Epoch 84: reducing lr to 1.613628385682619e-05
Epoch 87: reducing lr to 1.0491003332278762e-05
Epoch 90: reducing lr to 5.9937719924678605e-06
Epoch 93: reducing lr to 2.7154913275795205e-06
Epoch 96: reducing lr to 7.078714407403666e-07
Epoch 99: reducing lr to 2.5677554805082077e-09
[I 2024-06-21 00:38:55,044] Trial 337 finished with value: 0.9812275171279907 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.3545432175526798, 'bidirectional': True, 'fc_dropout': 0.5263122496240675, 'learning_rate_model': 0.0016657189892778943}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 5.8969255223806835e-05
Epoch 38: reducing lr to 5.6038592656935e-05
Epoch 41: reducing lr to 5.3734782038951105e-05
Epoch 50: reducing lr to 4.480123927322305e-05
Epoch 55: reducing lr to 3.887787355502302e-05
Epoch 61: reducing lr to 3.131233444640231e-05
Epoch 64: reducing lr to 2.7480972247596895e-05
Epoch 67: reducing lr to 2.3697695211187643e-05
Epoch 72: reducing lr to 1.766044942621194e-05
Epoch 75: reducing lr to 1.429210484511134e-05
Epoch 78: reducing lr to 1.117986053313845e-05
Epoch 81: reducing lr to 8.372789231836896e-06
Epoch 84: reducing lr to 5.915168402873926e-06
Epoch 87: reducing lr to 3.845746144288888e-06
Epoch 90: reducing lr to 2.1971707376031416e-06
Epoch 93: reducing lr to 9.954329411713679e-07
Epoch 96: reducing lr to 2.5948841856750576e-07
Epoch 99: reducing lr to 9.412765801172784e-10
[I 2024-06-21 00:39:10,696] Trial 338 finished with value: 0.9830546975135803 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.3061726227607775, 'bidirectional': True, 'fc_dropout': 0.5439114420427312, 'learning_rate_model': 0.0006106119860599469}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.829418107996007e-05
Epoch 38: reducing lr to 8.390612411058569e-05
Epoch 41: reducing lr to 8.045664741113973e-05
Epoch 44: reducing lr to 7.645925306330201e-05
Epoch 49: reducing lr to 6.87540576730881e-05
Epoch 52: reducing lr to 6.362402158268513e-05
Epoch 55: reducing lr to 5.821152047186043e-05
Epoch 58: reducing lr to 5.260193302116077e-05
Epoch 64: reducing lr to 4.1147033834388624e-05
Epoch 72: reducing lr to 2.644284574518272e-05
Epoch 75: reducing lr to 2.1399451093941958e-05
Epoch 78: reducing lr to 1.6739513270350928e-05
Epoch 81: reducing lr to 1.2536508486912215e-05
Epoch 84: reducing lr to 8.856733022989888e-06
Epoch 87: reducing lr to 5.758204087243026e-06
Epoch 90: reducing lr to 3.2898056832028947e-06
Epoch 93: reducing lr to 1.490453559692558e-06
Epoch 96: reducing lr to 3.885298759530916e-07
Epoch 99: reducing lr to 1.4093656854952986e-09
[I 2024-06-21 00:39:25,515] Trial 339 finished with value: 0.9801337718963623 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.3769431697804597, 'bidirectional': True, 'fc_dropout': 0.5016587036126351, 'learning_rate_model': 0.0009142643070893877}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013714944672377146
Epoch 33: reducing lr to 0.00013293679606906736
Epoch 36: reducing lr to 0.00012929313877067333
Epoch 39: reducing lr to 0.00012469589443721562
Epoch 48: reducing lr to 0.00010597577227815916
Epoch 61: reducing lr to 7.058867206223973e-05
Epoch 64: reducing lr to 6.195147606313365e-05
Epoch 67: reducing lr to 5.342268040592009e-05
Epoch 72: reducing lr to 3.981267111056543e-05
Epoch 75: reducing lr to 3.221927460303512e-05
Epoch 78: reducing lr to 2.5203215372719038e-05
Epoch 81: reducing lr to 1.8875120101442756e-05
Epoch 84: reducing lr to 1.3334805276115865e-05
Epoch 87: reducing lr to 8.66962231380426e-06
Epoch 90: reducing lr to 4.953171566524257e-06
Epoch 93: reducing lr to 2.2440450604081263e-06
Epoch 96: reducing lr to 5.84975320622118e-07
Epoch 99: reducing lr to 2.1219581678800017e-09
[I 2024-06-21 00:39:41,608] Trial 340 finished with value: 0.9793154001235962 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.4236585213188588, 'bidirectional': True, 'fc_dropout': 0.5275537249678969, 'learning_rate_model': 0.0013765274931844247}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010911553434550701
Epoch 40: reducing lr to 9.782940651618611e-05
Epoch 43: reducing lr to 9.325196821869283e-05
Epoch 46: reducing lr to 8.806746154980228e-05
Epoch 49: reducing lr to 8.235765695803162e-05
Epoch 55: reducing lr to 6.97291853932784e-05
Epoch 64: reducing lr to 4.928833892955152e-05
Epoch 72: reducing lr to 3.167479698770321e-05
Epoch 75: reducing lr to 2.5633522033927798e-05
Epoch 78: reducing lr to 2.0051574237539225e-05
Epoch 81: reducing lr to 1.501696773048354e-05
Epoch 84: reducing lr to 1.0609116098202046e-05
Epoch 87: reducing lr to 6.897515767962043e-06
Epoch 90: reducing lr to 3.940722876372995e-06
Epoch 93: reducing lr to 1.7853529978505275e-06
Epoch 96: reducing lr to 4.6540395323047165e-07
Epoch 99: reducing lr to 1.68822116952249e-09
[I 2024-06-21 00:39:56,751] Trial 341 finished with value: 0.9782894849777222 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.32698041925992494, 'bidirectional': True, 'fc_dropout': 0.5574619172367368, 'learning_rate_model': 0.001095159598147079}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00019600105656973843
Epoch 31: reducing lr to 0.00019265360244659067
Epoch 34: reducing lr to 0.00018840195685610288
Epoch 40: reducing lr to 0.00017572811383618066
Epoch 46: reducing lr to 0.0001581930163904918
Epoch 52: reducing lr to 0.00013689846986643435
Epoch 57: reducing lr to 0.00011724129366412245
Epoch 60: reducing lr to 0.00010499399482553891
Epoch 65: reducing lr to 8.444557974654809e-05
Epoch 68: reducing lr to 7.235160729878751e-05
Epoch 71: reducing lr to 6.0667856532592134e-05
Epoch 74: reducing lr to 4.957853436717418e-05
Epoch 77: reducing lr to 3.92585587096303e-05
Epoch 80: reducing lr to 2.9870651199849473e-05
Epoch 83: reducing lr to 2.1562892406610622e-05
Epoch 86: reducing lr to 1.4466266448503301e-05
Epoch 89: reducing lr to 8.692733386443511e-06
Epoch 92: reducing lr to 4.3333166920655685e-06
Epoch 95: reducing lr to 1.4567798148362775e-06
Epoch 98: reducing lr to 1.0847980371304564e-07
[I 2024-06-21 00:40:12,847] Trial 342 finished with value: 0.9813898801803589 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.37000130762872824, 'bidirectional': True, 'fc_dropout': 0.4989091473885353, 'learning_rate_model': 0.0019672032917846063}. Best is trial 235 with value: 0.9706074595451355.
Epoch 50: reducing lr to 3.125286548735999e-05
Epoch 55: reducing lr to 2.7120788896925367e-05
Epoch 61: reducing lr to 2.18431497079933e-05
Epoch 64: reducing lr to 1.9170432404296106e-05
Epoch 67: reducing lr to 1.6531258795744046e-05
Epoch 73: reducing lr to 1.151888722498268e-05
Epoch 76: reducing lr to 9.224725984569053e-06
Epoch 79: reducing lr to 7.120971181961543e-06
Epoch 82: reducing lr to 5.240794061237106e-06
Epoch 85: reducing lr to 3.6138516782574426e-06
Epoch 88: reducing lr to 2.2657952816592535e-06
Epoch 91: reducing lr to 1.2178925100601596e-06
Epoch 94: reducing lr to 4.866640925860091e-07
Epoch 97: reducing lr to 8.364409258948524e-08
[I 2024-06-21 00:40:29,746] Trial 343 finished with value: 0.9827457070350647 and parameters: {'hidden_size': 158, 'n_layers': 1, 'rnn_dropout': 0.404254824336108, 'bidirectional': True, 'fc_dropout': 0.5443367554831149, 'learning_rate_model': 0.0004259563926104844}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012465991813431844
Epoch 36: reducing lr to 0.0001175190456797345
Epoch 40: reducing lr to 0.00011176599079669798
Epoch 44: reducing lr to 0.00010463475292259352
Epoch 48: reducing lr to 9.632507758506844e-05
Epoch 55: reducing lr to 7.966266760124905e-05
Epoch 61: reducing lr to 6.416050732025203e-05
Epoch 64: reducing lr to 5.630985847055453e-05
Epoch 67: reducing lr to 4.8557738474373065e-05
Epoch 72: reducing lr to 3.618712608697247e-05
Epoch 75: reducing lr to 2.9285223019899584e-05
Epoch 78: reducing lr to 2.2908081951016653e-05
Epoch 81: reducing lr to 1.7156255331895932e-05
Epoch 84: reducing lr to 1.212046985071475e-05
Epoch 87: reducing lr to 7.880122258684814e-06
Epoch 90: reducing lr to 4.502110484133105e-06
Epoch 93: reducing lr to 2.039690864255681e-06
Epoch 96: reducing lr to 5.317044823827934e-07
Epoch 99: reducing lr to 1.9287218272576874e-09
[I 2024-06-21 00:40:45,984] Trial 344 finished with value: 0.9776156544685364 and parameters: {'hidden_size': 156, 'n_layers': 1, 'rnn_dropout': 0.3455686622331682, 'bidirectional': True, 'fc_dropout': 0.5778792725248416, 'learning_rate_model': 0.0012511738742600903}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.178566046952016e-05
Epoch 33: reducing lr to 8.896639337143165e-05
Epoch 38: reducing lr to 8.454492869846444e-05
Epoch 41: reducing lr to 8.106919001200925e-05
Epoch 44: reducing lr to 7.704136220206101e-05
Epoch 49: reducing lr to 6.927750465556299e-05
Epoch 52: reducing lr to 6.410841193341505e-05
Epoch 55: reducing lr to 5.8654703692858844e-05
Epoch 64: reducing lr to 4.146029957356597e-05
Epoch 72: reducing lr to 2.6644163722358375e-05
Epoch 75: reducing lr to 2.1562372068802876e-05
Epoch 78: reducing lr to 1.6866956624329052e-05
Epoch 81: reducing lr to 1.2631952999721155e-05
Epoch 84: reducing lr to 8.924162209460791e-06
Epoch 87: reducing lr to 5.8020431660690906e-06
Epoch 90: reducing lr to 3.314852042880887e-06
Epoch 93: reducing lr to 1.501800866960583e-06
Epoch 96: reducing lr to 3.9148788015026413e-07
Epoch 99: reducing lr to 1.4200956444305503e-09
[I 2024-06-21 00:41:00,554] Trial 345 finished with value: 0.9807732105255127 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.3424920935592325, 'bidirectional': True, 'fc_dropout': 0.6032197138580495, 'learning_rate_model': 0.0009212248983465019}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015210238251934583
Epoch 33: reducing lr to 0.00014743044095043308
Epoch 36: reducing lr to 0.00014338952814029282
Epoch 40: reducing lr to 0.000136370003600484
Epoch 61: reducing lr to 7.828471390896619e-05
Epoch 64: reducing lr to 6.87058339270691e-05
Epoch 67: reducing lr to 5.924717280614176e-05
Epoch 72: reducing lr to 4.415331067702036e-05
Epoch 75: reducing lr to 3.573203208057339e-05
Epoch 78: reducing lr to 2.7951035873003868e-05
Epoch 81: reducing lr to 2.09330099854543e-05
Epoch 84: reducing lr to 1.4788653555517537e-05
Epoch 87: reducing lr to 9.614841626947321e-06
Epoch 90: reducing lr to 5.493198946786835e-06
Epoch 93: reducing lr to 2.4887056296792446e-06
Epoch 96: reducing lr to 6.487531820733184e-07
Epoch 99: reducing lr to 2.3533080201979665e-09
[I 2024-06-21 00:41:16,669] Trial 346 finished with value: 0.9780125617980957 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.32008891934650296, 'bidirectional': True, 'fc_dropout': 0.5846310028507093, 'learning_rate_model': 0.0015266055847707914}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011175993929374119
Epoch 33: reducing lr to 0.0001083271468714451
Epoch 36: reducing lr to 0.00010535801408816908
Epoch 40: reducing lr to 0.00010020029319355928
Epoch 43: reducing lr to 9.551192109954718e-05
Epoch 46: reducing lr to 9.02017683879405e-05
Epoch 55: reducing lr to 7.141906579386793e-05
Epoch 61: reducing lr to 5.7521090262877156e-05
Epoch 64: reducing lr to 5.048283729440251e-05
Epoch 72: reducing lr to 3.244243278210233e-05
Epoch 75: reducing lr to 2.6254748084955063e-05
Epoch 78: reducing lr to 2.053752229664634e-05
Epoch 81: reducing lr to 1.5380902563522288e-05
Epoch 84: reducing lr to 1.0866227051970044e-05
Epoch 87: reducing lr to 7.064676428785713e-06
Epoch 90: reducing lr to 4.03622593316878e-06
Epoch 93: reducing lr to 1.8286208636973319e-06
Epoch 96: reducing lr to 4.766829752710566e-07
Epoch 99: reducing lr to 1.7291350544543922e-09
[I 2024-06-21 00:41:33,356] Trial 347 finished with value: 0.9774096608161926 and parameters: {'hidden_size': 157, 'n_layers': 1, 'rnn_dropout': 0.39050266770498127, 'bidirectional': True, 'fc_dropout': 0.5667468037302629, 'learning_rate_model': 0.0011217006903739308}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010727757249733508
Epoch 33: reducing lr to 0.00010398245941586444
Epoch 38: reducing lr to 9.881472412287542e-05
Epoch 41: reducing lr to 9.475233783060909e-05
Epoch 44: reducing lr to 9.004467883814595e-05
Epoch 48: reducing lr to 8.289368907501973e-05
Epoch 55: reducing lr to 6.855465434941124e-05
Epoch 64: reducing lr to 4.845811721038332e-05
Epoch 72: reducing lr to 3.1141260963147365e-05
Epoch 75: reducing lr to 2.5201746340253855e-05
Epoch 78: reducing lr to 1.9713821884811076e-05
Epoch 81: reducing lr to 1.4764019202765549e-05
Epoch 84: reducing lr to 1.0430414222724089e-05
Epoch 87: reducing lr to 6.781332761530176e-06
Epoch 90: reducing lr to 3.874344625609387e-06
Epoch 93: reducing lr to 1.7552801881882616e-06
Epoch 96: reducing lr to 4.5756460464315646e-07
Epoch 99: reducing lr to 1.6597844659880372e-09
[I 2024-06-21 00:41:50,253] Trial 348 finished with value: 0.9779587984085083 and parameters: {'hidden_size': 158, 'n_layers': 1, 'rnn_dropout': 0.3939099926833096, 'bidirectional': True, 'fc_dropout': 0.5632632907950477, 'learning_rate_model': 0.0010767125312731724}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 6.046552892680045e-05
Epoch 38: reducing lr to 5.746050433323326e-05
Epoch 41: reducing lr to 5.5098237300439925e-05
Epoch 49: reducing lr to 4.708408201104632e-05
Epoch 55: reducing lr to 3.986435268907196e-05
Epoch 61: reducing lr to 3.210684715363722e-05
Epoch 64: reducing lr to 2.8178268761699314e-05
Epoch 67: reducing lr to 2.429899563513708e-05
Epoch 72: reducing lr to 1.8108562022499598e-05
Epoch 75: reducing lr to 1.465474976167007e-05
Epoch 78: reducing lr to 1.1463536005304118e-05
Epoch 81: reducing lr to 8.585238656554204e-06
Epoch 84: reducing lr to 6.065258664255171e-06
Epoch 87: reducing lr to 3.9433273126833175e-06
Epoch 90: reducing lr to 2.25292129411238e-06
Epoch 93: reducing lr to 1.020690851031604e-06
Epoch 96: reducing lr to 2.660726241075014e-07
Epoch 99: reducing lr to 9.651603376540848e-10
[I 2024-06-21 00:42:09,252] Trial 349 finished with value: 0.9823129773139954 and parameters: {'hidden_size': 174, 'n_layers': 1, 'rnn_dropout': 0.4170484170684142, 'bidirectional': True, 'fc_dropout': 0.5183138206208935, 'learning_rate_model': 0.0006261055284831408}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.881570435366325e-05
Epoch 40: reducing lr to 8.215262631250374e-05
Epoch 43: reducing lr to 7.830870461948713e-05
Epoch 46: reducing lr to 7.395499488995321e-05
Epoch 49: reducing lr to 6.91601641774972e-05
Epoch 52: reducing lr to 6.399982673333162e-05
Epoch 55: reducing lr to 5.855535584529496e-05
Epoch 64: reducing lr to 4.139007517104309e-05
Epoch 72: reducing lr to 2.6599034514480738e-05
Epoch 75: reducing lr to 2.1525850270574646e-05
Epoch 78: reducing lr to 1.6838387801539403e-05
Epoch 81: reducing lr to 1.2610557318521874e-05
Epoch 84: reducing lr to 8.90904669014177e-06
Epoch 87: reducing lr to 5.792215812698759e-06
Epoch 90: reducing lr to 3.309237430672166e-06
Epoch 93: reducing lr to 1.499257154790744e-06
Epoch 96: reducing lr to 3.9082478792079946e-07
Epoch 99: reducing lr to 1.4176903224928302e-09
[I 2024-06-21 00:42:23,770] Trial 350 finished with value: 0.980969250202179 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.40096491180763405, 'bidirectional': True, 'fc_dropout': 0.5005750179388789, 'learning_rate_model': 0.0009196645510084225}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00018154952474299627
Epoch 31: reducing lr to 0.00017844888479853646
Epoch 34: reducing lr to 0.0001745107211486172
Epoch 40: reducing lr to 0.00016277134475339077
Epoch 46: reducing lr to 0.0001465291434953877
Epoch 52: reducing lr to 0.00012680468451174595
Epoch 57: reducing lr to 0.00010859686941229376
Epoch 60: reducing lr to 9.725258728216577e-05
Epoch 63: reducing lr to 8.581139700073019e-05
Epoch 66: reducing lr to 7.445376668216669e-05
Epoch 69: reducing lr to 6.335876307990291e-05
Epoch 72: reducing lr to 5.270142673947501e-05
Epoch 75: reducing lr to 4.2649781909264316e-05
Epoch 78: reducing lr to 3.3362378647638e-05
Epoch 81: reducing lr to 2.4985657366782257e-05
Epoch 84: reducing lr to 1.765174864536748e-05
Epoch 87: reducing lr to 1.1476282612663523e-05
Epoch 90: reducing lr to 6.556686631657683e-06
Epoch 93: reducing lr to 2.970521018867147e-06
Epoch 96: reducing lr to 7.743523140798635e-07
Epoch 99: reducing lr to 2.808910324518868e-09
[I 2024-06-21 00:42:38,775] Trial 351 finished with value: 0.9834634065628052 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.37542262836689894, 'bidirectional': True, 'fc_dropout': 0.521134595402065, 'learning_rate_model': 0.001822157640100673}. Best is trial 235 with value: 0.9706074595451355.
Epoch 55: reducing lr to 2.050791499960594e-05
Epoch 65: reducing lr to 1.3826492105527839e-05
Epoch 68: reducing lr to 1.1846314870966683e-05
Epoch 73: reducing lr to 8.710231881446944e-06
Epoch 76: reducing lr to 6.975456986343242e-06
Epoch 79: reducing lr to 5.3846616434843734e-06
Epoch 82: reducing lr to 3.962928937899544e-06
Epoch 85: reducing lr to 2.7326846324624735e-06
Epoch 88: reducing lr to 1.7133254205611977e-06
Epoch 91: reducing lr to 9.209332431256066e-07
Epoch 94: reducing lr to 3.6800057262514417e-07
Epoch 97: reducing lr to 6.324911666705712e-08
[I 2024-06-21 00:42:52,028] Trial 352 finished with value: 0.9839057326316833 and parameters: {'hidden_size': 136, 'n_layers': 1, 'rnn_dropout': 0.3925804057312449, 'bidirectional': True, 'fc_dropout': 0.552018599833779, 'learning_rate_model': 0.0003220952578626839}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 4.9761612729131085e-05
Epoch 38: reducing lr to 4.728855290941741e-05
Epoch 41: reducing lr to 4.534446643014487e-05
Epoch 50: reducing lr to 3.780583475300916e-05
Epoch 55: reducing lr to 3.280736173849696e-05
Epoch 61: reducing lr to 2.6423129382476105e-05
Epoch 64: reducing lr to 2.3190007966267057e-05
Epoch 67: reducing lr to 1.999746354598739e-05
Epoch 72: reducing lr to 1.4902892051700408e-05
Epoch 75: reducing lr to 1.2060491245604992e-05
Epoch 78: reducing lr to 9.434202417925976e-06
Epoch 81: reducing lr to 7.0654359400676505e-06
Epoch 84: reducing lr to 4.991555653437727e-06
Epoch 87: reducing lr to 3.2452594078107595e-06
Epoch 90: reducing lr to 1.8540976807223914e-06
Epoch 93: reducing lr to 8.40002952867409e-07
Epoch 96: reducing lr to 2.1897109168910995e-07
Epoch 99: reducing lr to 7.943027340788327e-10
[I 2024-06-21 00:43:07,688] Trial 353 finished with value: 0.982915997505188 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.4198160905079016, 'bidirectional': True, 'fc_dropout': 0.5688755532792803, 'learning_rate_model': 0.0005152691358023755}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013849322719853302
Epoch 36: reducing lr to 0.00013055994370172493
Epoch 40: reducing lr to 0.0001241684816429779
Epoch 46: reducing lr to 0.00011177828193183438
Epoch 61: reducing lr to 7.128029482501786e-05
Epoch 64: reducing lr to 6.255847219694941e-05
Epoch 67: reducing lr to 5.3946112009574245e-05
Epoch 72: reducing lr to 4.020275281606612e-05
Epoch 75: reducing lr to 3.2534956752375036e-05
Epoch 78: reducing lr to 2.545015467526892e-05
Epoch 81: reducing lr to 1.906005717889365e-05
Epoch 84: reducing lr to 1.3465458744962032e-05
Epoch 87: reducing lr to 8.75456665348003e-06
Epoch 90: reducing lr to 5.001702387451637e-06
Epoch 93: reducing lr to 2.266032053492638e-06
Epoch 96: reducing lr to 5.907068669961519e-07
Epoch 99: reducing lr to 2.1427489623349756e-09
[I 2024-06-21 00:43:24,568] Trial 354 finished with value: 0.9788417816162109 and parameters: {'hidden_size': 158, 'n_layers': 1, 'rnn_dropout': 0.35395449304159676, 'bidirectional': True, 'fc_dropout': 0.5370839801802747, 'learning_rate_model': 0.0013900146111604766}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011093438351118457
Epoch 33: reducing lr to 0.00010752694866918467
Epoch 40: reducing lr to 9.946012697673273e-05
Epoch 43: reducing lr to 9.48063872627796e-05
Epoch 46: reducing lr to 8.953545994181852e-05
Epoch 55: reducing lr to 7.089150266951839e-05
Epoch 64: reducing lr to 5.0109927300789124e-05
Epoch 72: reducing lr to 3.220278485322258e-05
Epoch 75: reducing lr to 2.6060807758590577e-05
Epoch 78: reducing lr to 2.0385814355513513e-05
Epoch 81: reducing lr to 1.5267285885377137e-05
Epoch 84: reducing lr to 1.078595968036964e-05
Epoch 87: reducing lr to 7.012490605184398e-06
Epoch 90: reducing lr to 4.0064108699190085e-06
Epoch 93: reducing lr to 1.8151130849917377e-06
Epoch 96: reducing lr to 4.731617816379008e-07
Epoch 99: reducing lr to 1.7163621641351836e-09
[I 2024-06-21 00:43:40,662] Trial 355 finished with value: 0.9780194759368896 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3816940913403489, 'bidirectional': True, 'fc_dropout': 0.5992157321819613, 'learning_rate_model': 0.0011134148368105883}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.460723585335357e-05
Epoch 40: reducing lr to 7.825988299002596e-05
Epoch 49: reducing lr to 6.588305936213423e-05
Epoch 52: reducing lr to 6.096724080956312e-05
Epoch 55: reducing lr to 5.578075227273224e-05
Epoch 64: reducing lr to 3.9428836121593385e-05
Epoch 72: reducing lr to 2.5338658326424028e-05
Epoch 75: reducing lr to 2.0505863282177136e-05
Epoch 78: reducing lr to 1.6040512862929458e-05
Epoch 81: reducing lr to 1.2013015097441015e-05
Epoch 84: reducing lr to 8.48689789746934e-06
Epoch 87: reducing lr to 5.517755817452061e-06
Epoch 90: reducing lr to 3.152431586611365e-06
Epoch 93: reducing lr to 1.4282159289656778e-06
Epoch 96: reducing lr to 3.7230583543289287e-07
Epoch 99: reducing lr to 1.3505140825532252e-09
[I 2024-06-21 00:43:58,505] Trial 356 finished with value: 0.9804162383079529 and parameters: {'hidden_size': 170, 'n_layers': 1, 'rnn_dropout': 0.4241460904426822, 'bidirectional': True, 'fc_dropout': 0.516569978207381, 'learning_rate_model': 0.0008760869053438835}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015516797931531325
Epoch 33: reducing lr to 0.00015040187558491833
Epoch 36: reducing lr to 0.00014627951888706006
Epoch 40: reducing lr to 0.00013911851706345052
Epoch 46: reducing lr to 0.0001252365223162613
Epoch 61: reducing lr to 7.986252856352643e-05
Epoch 64: reducing lr to 7.009058793856069e-05
Epoch 67: reducing lr to 6.0441289164585613e-05
Epoch 72: reducing lr to 4.5043212896176026e-05
Epoch 75: reducing lr to 3.6452204909198736e-05
Epoch 78: reducing lr to 2.8514384090151948e-05
Epoch 81: reducing lr to 2.135491112387469e-05
Epoch 84: reducing lr to 1.5086716269628554e-05
Epoch 87: reducing lr to 9.808627070653666e-06
Epoch 90: reducing lr to 5.6039134064287795e-06
Epoch 93: reducing lr to 2.538865054390964e-06
Epoch 96: reducing lr to 6.618286884749799e-07
Epoch 99: reducing lr to 2.4007385298913804e-09
[I 2024-06-21 00:44:13,681] Trial 357 finished with value: 0.9785348176956177 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.40308879797482305, 'bidirectional': True, 'fc_dropout': 0.4958838445766089, 'learning_rate_model': 0.0015573740521140562}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010999449915607958
Epoch 33: reducing lr to 0.00010661593358434261
Epoch 38: reducing lr to 0.00010131731951160488
Epoch 41: reducing lr to 9.715204866147022e-05
Epoch 44: reducing lr to 9.232516284536491e-05
Epoch 47: reducing lr to 8.691278707595993e-05
Epoch 55: reducing lr to 7.029087902011354e-05
Epoch 64: reducing lr to 4.96853741981822e-05
Epoch 72: reducing lr to 3.192994885128722e-05
Epoch 75: reducing lr to 2.584000925844632e-05
Epoch 78: reducing lr to 2.02130968681811e-05
Epoch 81: reducing lr to 1.5137934797874737e-05
Epoch 84: reducing lr to 1.0694576337915221e-05
Epoch 87: reducing lr to 6.953077734246421e-06
Epoch 90: reducing lr to 3.972466814184663e-06
Epoch 93: reducing lr to 1.799734657336274e-06
Epoch 96: reducing lr to 4.691529491919087e-07
Epoch 99: reducing lr to 1.7018203972559916e-09
[I 2024-06-21 00:44:27,698] Trial 358 finished with value: 0.9791218638420105 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.36474669956260636, 'bidirectional': True, 'fc_dropout': 0.5517926667766078, 'learning_rate_model': 0.0011039815019622044}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013173475200497768
Epoch 36: reducing lr to 0.0001241886130696849
Epoch 40: reducing lr to 0.00011810905462274895
Epoch 44: reducing lr to 0.00011057309706001865
Epoch 48: reducing lr to 0.00010179182208235294
Epoch 55: reducing lr to 8.418376907000758e-05
Epoch 61: reducing lr to 6.780181350063077e-05
Epoch 64: reducing lr to 5.950561617617294e-05
Epoch 67: reducing lr to 5.131353952079244e-05
Epoch 72: reducing lr to 3.824085682218824e-05
Epoch 75: reducing lr to 3.094724952233765e-05
Epoch 78: reducing lr to 2.4208186078505837e-05
Epoch 81: reducing lr to 1.812992560324166e-05
Epoch 84: reducing lr to 1.2808343803397322e-05
Epoch 87: reducing lr to 8.327343440080181e-06
Epoch 90: reducing lr to 4.757619104861348e-06
Epoch 93: reducing lr to 2.155449595916026e-06
Epoch 96: reducing lr to 5.618803475481229e-07
Epoch 99: reducing lr to 2.0381827246784833e-09
[I 2024-06-21 00:44:44,925] Trial 359 finished with value: 0.977989912033081 and parameters: {'hidden_size': 159, 'n_layers': 1, 'rnn_dropout': 0.3059328809242027, 'bidirectional': True, 'fc_dropout': 0.5329107335249503, 'learning_rate_model': 0.001322181840863771}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.816340681348603e-05
Epoch 40: reducing lr to 8.154926504376508e-05
Epoch 43: reducing lr to 7.773357462677376e-05
Epoch 46: reducing lr to 7.341184025754223e-05
Epoch 49: reducing lr to 6.86522246717585e-05
Epoch 55: reducing lr to 5.812530223191603e-05
Epoch 64: reducing lr to 4.108609014476539e-05
Epoch 72: reducing lr to 2.6403680720789085e-05
Epoch 75: reducing lr to 2.136775594160544e-05
Epoch 78: reducing lr to 1.6714720044542758e-05
Epoch 81: reducing lr to 1.251794041502494e-05
Epoch 84: reducing lr to 8.843615139678996e-06
Epoch 87: reducing lr to 5.749675496723172e-06
Epoch 90: reducing lr to 3.284933086619517e-06
Epoch 93: reducing lr to 1.4882460193020326e-06
Epoch 96: reducing lr to 3.879544166316643e-07
Epoch 99: reducing lr to 1.407278245964174e-09
[I 2024-06-21 00:45:00,613] Trial 360 finished with value: 0.9797840118408203 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.3321463419581508, 'bidirectional': True, 'fc_dropout': 0.5733246567776328, 'learning_rate_model': 0.0009129101720528473}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00017132207536209578
Epoch 36: reducing lr to 0.00016150826265369042
Epoch 40: reducing lr to 0.00015360174934143348
Epoch 46: reducing lr to 0.0001382745396893615
Epoch 52: reducing lr to 0.00011966124255594445
Epoch 57: reducing lr to 0.00010247915036890452
Epoch 61: reducing lr to 8.817678877782312e-05
Epoch 64: reducing lr to 7.738751926763437e-05
Epoch 67: reducing lr to 6.673365950197335e-05
Epoch 70: reducing lr to 5.6383298358722384e-05
Epoch 73: reducing lr to 4.649963486818834e-05
Epoch 76: reducing lr to 3.723852674859353e-05
Epoch 79: reducing lr to 2.874605449300258e-05
Epoch 82: reducing lr to 2.1156124329297937e-05
Epoch 85: reducing lr to 1.4588456352129385e-05
Epoch 88: reducing lr to 9.146599947147021e-06
Epoch 91: reducing lr to 4.916408670420326e-06
Epoch 94: reducing lr to 1.964573675105295e-06
Epoch 97: reducing lr to 3.3765585931394385e-07
[I 2024-06-21 00:45:17,782] Trial 361 finished with value: 0.9813954830169678 and parameters: {'hidden_size': 163, 'n_layers': 1, 'rnn_dropout': 0.3824690087773764, 'bidirectional': True, 'fc_dropout': 0.5103600718403564, 'learning_rate_model': 0.001719507825651794}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011331654174690211
Epoch 36: reducing lr to 0.00010682544995316862
Epoch 40: reducing lr to 0.00010159589185957676
Epoch 44: reducing lr to 9.511355795175761e-05
Epoch 48: reducing lr to 8.75600179977743e-05
Epoch 55: reducing lr to 7.241379694458019e-05
Epoch 61: reducing lr to 5.8322249164514984e-05
Epoch 64: reducing lr to 5.118596677775389e-05
Epoch 67: reducing lr to 4.413924765326523e-05
Epoch 72: reducing lr to 3.2894293894181114e-05
Epoch 75: reducing lr to 2.662042656987932e-05
Epoch 78: reducing lr to 2.0823570748272483e-05
Epoch 81: reducing lr to 1.559512915324224e-05
Epoch 84: reducing lr to 1.1017572836448711e-05
Epoch 87: reducing lr to 7.163073875396083e-06
Epoch 90: reducing lr to 4.092442849791913e-06
Epoch 93: reducing lr to 1.8540900590129213e-06
Epoch 96: reducing lr to 4.833222584826749e-07
Epoch 99: reducing lr to 1.7532186024791716e-09
[I 2024-06-21 00:45:37,763] Trial 362 finished with value: 0.9773368835449219 and parameters: {'hidden_size': 185, 'n_layers': 1, 'rnn_dropout': 0.43728494007406615, 'bidirectional': True, 'fc_dropout': 0.5244952956015991, 'learning_rate_model': 0.001137323838143895}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010822497332189968
Epoch 33: reducing lr to 0.00010490076009603028
Epoch 36: reducing lr to 0.00010202554095856589
Epoch 40: reducing lr to 9.703095873394751e-05
Epoch 43: reducing lr to 9.249087981117723e-05
Epoch 48: reducing lr to 8.362574841932327e-05
Epoch 55: reducing lr to 6.916008132306862e-05
Epoch 61: reducing lr to 5.570169864520576e-05
Epoch 64: reducing lr to 4.888606556094006e-05
Epoch 72: reducing lr to 3.141627889679048e-05
Epoch 75: reducing lr to 2.542431061634071e-05
Epoch 78: reducing lr to 1.9887920633265268e-05
Epoch 81: reducing lr to 1.4894404740403776e-05
Epoch 84: reducing lr to 1.0522528378601403e-05
Epoch 87: reducing lr to 6.8412207707418226e-06
Epoch 90: reducing lr to 3.908560139695904e-06
Epoch 93: reducing lr to 1.7707816006356288e-06
Epoch 96: reducing lr to 4.6160549663613623e-07
Epoch 99: reducing lr to 1.6744425267124428e-09
[I 2024-06-21 00:45:58,098] Trial 363 finished with value: 0.9787854552268982 and parameters: {'hidden_size': 192, 'n_layers': 1, 'rnn_dropout': 0.4331235667855159, 'bidirectional': True, 'fc_dropout': 0.5222182042005331, 'learning_rate_model': 0.0010862213066509204}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 5.9479018928493116e-05
Epoch 49: reducing lr to 5.007239450341169e-05
Epoch 52: reducing lr to 4.633627768894266e-05
Epoch 55: reducing lr to 4.239444647135839e-05
Epoch 61: reducing lr to 3.414459087384333e-05
Epoch 64: reducing lr to 2.996667513933793e-05
Epoch 67: reducing lr to 2.5841193955821688e-05
Epoch 72: reducing lr to 1.9257868535429235e-05
Epoch 75: reducing lr to 1.5584851186924847e-05
Epoch 78: reducing lr to 1.2191098833083018e-05
Epoch 81: reducing lr to 9.130122932333442e-06
Epoch 84: reducing lr to 6.450205921622733e-06
Epoch 87: reducing lr to 4.193600733480031e-06
Epoch 90: reducing lr to 2.3959087446467865e-06
Epoch 93: reducing lr to 1.0854716238682758e-06
Epoch 96: reducing lr to 2.8295960825451767e-07
Epoch 99: reducing lr to 1.0264167234902422e-09
[I 2024-06-21 00:46:17,589] Trial 364 finished with value: 0.9825631380081177 and parameters: {'hidden_size': 175, 'n_layers': 1, 'rnn_dropout': 0.48977546862385346, 'bidirectional': True, 'fc_dropout': 0.5025611603490159, 'learning_rate_model': 0.0006658429278842003}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 3.6683328822345235e-05
Epoch 41: reducing lr to 3.5175235231178734e-05
Epoch 50: reducing lr to 2.9327263837073248e-05
Epoch 55: reducing lr to 2.544977937371479e-05
Epoch 61: reducing lr to 2.0497314551143972e-05
Epoch 64: reducing lr to 1.7989272990630413e-05
Epoch 67: reducing lr to 1.551270837734237e-05
Epoch 73: reducing lr to 1.0809167079197512e-05
Epoch 76: reducing lr to 8.656357378928291e-06
Epoch 79: reducing lr to 6.682222489776016e-06
Epoch 82: reducing lr to 4.91788985595031e-06
Epoch 85: reducing lr to 3.391189255243514e-06
Epoch 88: reducing lr to 2.1261914704394596e-06
Epoch 91: reducing lr to 1.1428537643108432e-06
Epoch 94: reducing lr to 4.5667896433599874e-07
Epoch 97: reducing lr to 7.849047866591536e-08
[I 2024-06-21 00:46:37,416] Trial 365 finished with value: 0.982935905456543 and parameters: {'hidden_size': 185, 'n_layers': 1, 'rnn_dropout': 0.41089528375047396, 'bidirectional': True, 'fc_dropout': 0.55264539814079, 'learning_rate_model': 0.0003997116844926746}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 7.334309651327576e-05
Epoch 43: reducing lr to 6.99113727525774e-05
Epoch 49: reducing lr to 6.174386412004632e-05
Epoch 52: reducing lr to 5.7136888735366465e-05
Epoch 55: reducing lr to 5.227624858630995e-05
Epoch 66: reducing lr to 3.354810273159773e-05
Epoch 72: reducing lr to 2.374672171933462e-05
Epoch 75: reducing lr to 1.921755377508591e-05
Epoch 78: reducing lr to 1.5032745233956122e-05
Epoch 81: reducing lr to 1.1258280641939432e-05
Epoch 84: reducing lr to 7.953696680989686e-06
Epoch 87: reducing lr to 5.171095099997165e-06
Epoch 90: reducing lr to 2.954375668282093e-06
Epoch 93: reducing lr to 1.3384862680318206e-06
Epoch 96: reducing lr to 3.4891520121606374e-07
Epoch 99: reducing lr to 1.2656661486660365e-09
[I 2024-06-21 00:46:51,226] Trial 366 finished with value: 0.9810000658035278 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.44049892055055545, 'bidirectional': True, 'fc_dropout': 0.5871172470359587, 'learning_rate_model': 0.0008210455216351738}. Best is trial 235 with value: 0.9706074595451355.
Epoch 24: reducing lr to 0.00014618412683857336
Epoch 27: reducing lr to 0.00014565644576092484
Epoch 36: reducing lr to 0.00013731283285845134
Epoch 40: reducing lr to 0.0001305907882825217
Epoch 46: reducing lr to 0.00011755973623254592
Epoch 61: reducing lr to 7.496709122186532e-05
Epoch 64: reducing lr to 6.579415395800527e-05
Epoch 67: reducing lr to 5.6736340807996984e-05
Epoch 70: reducing lr to 4.7936559382973044e-05
Epoch 73: reducing lr to 3.9533559990831e-05
Epoch 76: reducing lr to 3.1659851423754745e-05
Epoch 79: reducing lr to 2.443962996742325e-05
Epoch 82: reducing lr to 1.7986741459726343e-05
Epoch 85: reducing lr to 1.2402970819134031e-05
Epoch 88: reducing lr to 7.776354776713483e-06
Epoch 91: reducing lr to 4.179885232700456e-06
Epoch 94: reducing lr to 1.6702623894003249e-06
Epoch 97: reducing lr to 2.8707189224782757e-07
[I 2024-06-21 00:47:10,670] Trial 367 finished with value: 0.9798152446746826 and parameters: {'hidden_size': 184, 'n_layers': 1, 'rnn_dropout': 0.42315346123970965, 'bidirectional': True, 'fc_dropout': 0.4922470727866718, 'learning_rate_model': 0.0014619096681684775}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010446890087789036
Epoch 33: reducing lr to 0.00010126005830365939
Epoch 38: reducing lr to 9.622762129451792e-05
Epoch 41: reducing lr to 9.227159375758783e-05
Epoch 44: reducing lr to 8.768718763054988e-05
Epoch 47: reducing lr to 8.254670376902932e-05
Epoch 55: reducing lr to 6.67598009837945e-05
Epoch 64: reducing lr to 4.7189418307414476e-05
Epoch 72: reducing lr to 3.032594072589006e-05
Epoch 75: reducing lr to 2.454193061121996e-05
Epoch 78: reducing lr to 1.919768742399418e-05
Epoch 81: reducing lr to 1.4377477255940874e-05
Epoch 86: reducing lr to 7.710544943639722e-06
Epoch 89: reducing lr to 4.633241873281324e-06
Epoch 92: reducing lr to 2.309665263538173e-06
Epoch 95: reducing lr to 7.76466151461235e-07
Epoch 98: reducing lr to 5.781992229881433e-08
[I 2024-06-21 00:47:25,528] Trial 368 finished with value: 0.9786309003829956 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.39759146103482507, 'bidirectional': True, 'fc_dropout': 0.5138979650686849, 'learning_rate_model': 0.0010485227441770616}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 8.015231483579234e-05
Epoch 49: reducing lr to 6.747620255210438e-05
Epoch 52: reducing lr to 6.244151273086449e-05
Epoch 55: reducing lr to 5.712960775204822e-05
Epoch 64: reducing lr to 4.038227972854993e-05
Epoch 72: reducing lr to 2.5951382012096153e-05
Epoch 75: reducing lr to 2.1001723322053116e-05
Epoch 78: reducing lr to 1.642839457453516e-05
Epoch 81: reducing lr to 1.230350636148963e-05
Epoch 84: reducing lr to 8.69212278714858e-06
Epoch 87: reducing lr to 5.651182758908632e-06
Epoch 90: reducing lr to 3.228661727753591e-06
Epoch 93: reducing lr to 1.4627521588109096e-06
Epoch 96: reducing lr to 3.813087037278506e-07
Epoch 99: reducing lr to 1.3831713746473122e-09
[I 2024-06-21 00:47:41,129] Trial 369 finished with value: 0.9793269634246826 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.4400652105954096, 'bidirectional': True, 'fc_dropout': 0.5680899939500962, 'learning_rate_model': 0.0008972718943317024}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 5.450054403842839e-05
Epoch 38: reducing lr to 5.1791968125753656e-05
Epoch 41: reducing lr to 4.966274109778638e-05
Epoch 50: reducing lr to 4.140618538795298e-05
Epoch 55: reducing lr to 3.59316944357568e-05
Epoch 61: reducing lr to 2.8939474578155943e-05
Epoch 64: reducing lr to 2.5398454372785192e-05
Epoch 67: reducing lr to 2.1901875419060263e-05
Epoch 72: reducing lr to 1.6322134272150734e-05
Epoch 75: reducing lr to 1.3209044044333818e-05
Epoch 78: reducing lr to 1.0332646715941764e-05
Epoch 81: reducing lr to 7.738296278667967e-06
Epoch 84: reducing lr to 5.466914832348102e-06
Epoch 87: reducing lr to 3.5543141303371206e-06
Epoch 90: reducing lr to 2.030668355742474e-06
Epoch 93: reducing lr to 9.199986779841532e-07
Epoch 96: reducing lr to 2.398242936921366e-07
Epoch 99: reducing lr to 8.69946305281661e-10
[I 2024-06-21 00:47:58,449] Trial 370 finished with value: 0.9826048016548157 and parameters: {'hidden_size': 164, 'n_layers': 1, 'rnn_dropout': 0.391410096018899, 'bidirectional': True, 'fc_dropout': 0.5436814170939982, 'learning_rate_model': 0.0005643395920526603}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.0001987782082916562
Epoch 31: reducing lr to 0.0001979936672828904
Epoch 34: reducing lr to 0.00019362417254332966
Epoch 40: reducing lr to 0.0001805990298716387
Epoch 46: reducing lr to 0.0001625778861954011
Epoch 49: reducing lr to 0.00015203723991375574
Epoch 52: reducing lr to 0.0001406930872304136
Epoch 55: reducing lr to 0.00012872431392785944
Epoch 58: reducing lr to 0.00011631971961119486
Epoch 61: reducing lr to 0.00010367487726375553
Epoch 64: reducing lr to 9.098926909250711e-05
Epoch 67: reducing lr to 7.846287049147341e-05
Epoch 70: reducing lr to 6.629331389913841e-05
Epoch 73: reducing lr to 5.467248245925394e-05
Epoch 76: reducing lr to 4.3783627683144865e-05
Epoch 79: reducing lr to 3.379850539679461e-05
Epoch 82: reducing lr to 2.4874557393366522e-05
Epoch 85: reducing lr to 1.715254595611971e-05
Epoch 88: reducing lr to 1.075422047054223e-05
Epoch 91: reducing lr to 5.780524246223016e-06
Epoch 94: reducing lr to 2.3098701763266463e-06
Epoch 97: reducing lr to 3.9700277427849075e-07
[I 2024-06-21 00:48:19,760] Trial 371 finished with value: 0.989174485206604 and parameters: {'hidden_size': 195, 'n_layers': 1, 'rnn_dropout': 0.4089369065662623, 'bidirectional': True, 'fc_dropout': 0.5248130789937897, 'learning_rate_model': 0.002021731174943315}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012698860309035684
Epoch 36: reducing lr to 0.00011971433697960138
Epoch 40: reducing lr to 0.00011385381329217376
Epoch 44: reducing lr to 0.00010658936173877369
Epoch 47: reducing lr to 0.00010034077618558566
Epoch 72: reducing lr to 3.6863112541818376e-05
Epoch 75: reducing lr to 2.9832279838974274e-05
Epoch 78: reducing lr to 2.3336011847084377e-05
Epoch 81: reducing lr to 1.7476739367913783e-05
Epoch 84: reducing lr to 1.2346883891602092e-05
Epoch 87: reducing lr to 8.027325324675696e-06
Epoch 90: reducing lr to 4.586211268986246e-06
Epoch 93: reducing lr to 2.0777929062082944e-06
Epoch 96: reducing lr to 5.416368828505121e-07
Epoch 99: reducing lr to 1.9647509340607257e-09
[I 2024-06-21 00:48:34,604] Trial 372 finished with value: 0.9760438203811646 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.3748168842241444, 'bidirectional': True, 'fc_dropout': 0.6548037022312542, 'learning_rate_model': 0.0012745461804671136}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013210610089957903
Epoch 36: reducing lr to 0.0001245386900500079
Epoch 40: reducing lr to 0.00011844199385259554
Epoch 44: reducing lr to 0.00011088479307599728
Epoch 47: reducing lr to 0.00010438439655630523
Epoch 61: reducing lr to 6.79929409602587e-05
Epoch 72: reducing lr to 3.834865449663174e-05
Epoch 75: reducing lr to 3.103448714738468e-05
Epoch 78: reducing lr to 2.4276426865418453e-05
Epoch 81: reducing lr to 1.8181032298547965e-05
Epoch 84: reducing lr to 1.2844449418967066e-05
Epoch 87: reducing lr to 8.350817502424221e-06
Epoch 90: reducing lr to 4.7710304224418295e-06
Epoch 93: reducing lr to 2.1615256222692154e-06
Epoch 96: reducing lr to 5.634642397465538e-07
Epoch 99: reducing lr to 2.0439281858382686e-09
[I 2024-06-21 00:48:49,448] Trial 373 finished with value: 0.9763188362121582 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.3687541095351991, 'bidirectional': True, 'fc_dropout': 0.6087351795247093, 'learning_rate_model': 0.001325908957342862}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014441878686374197
Epoch 36: reducing lr to 0.00013614607056106843
Epoch 40: reducing lr to 0.00012948114393987944
Epoch 46: reducing lr to 0.00011656081817753928
Epoch 61: reducing lr to 7.433008757288879e-05
Epoch 64: reducing lr to 6.523509376946818e-05
Epoch 67: reducing lr to 5.6254245857596536e-05
Epoch 72: reducing lr to 4.192286444416578e-05
Epoch 75: reducing lr to 3.3926994697774526e-05
Epoch 78: reducing lr to 2.6539062870042344e-05
Epoch 81: reducing lr to 1.987555919527688e-05
Epoch 84: reducing lr to 1.4041590739476813e-05
Epoch 87: reducing lr to 9.129138811971786e-06
Epoch 90: reducing lr to 5.215704808537351e-06
Epoch 93: reducing lr to 2.362986311891125e-06
Epoch 96: reducing lr to 6.159808017281062e-07
Epoch 99: reducing lr to 2.2344284406668885e-09
[I 2024-06-21 00:49:04,280] Trial 374 finished with value: 0.9776208996772766 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.37831523366184844, 'bidirectional': True, 'fc_dropout': 0.5996274591876131, 'learning_rate_model': 0.0014494876603525228}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001777511512107765
Epoch 31: reducing lr to 0.000174715382759176
Epoch 34: reducing lr to 0.000170859613247135
Epoch 40: reducing lr to 0.00015936584772116046
Epoch 46: reducing lr to 0.00014346346529468956
Epoch 52: reducing lr to 0.0001241516808308345
Epoch 57: reducing lr to 0.00010632480907481006
Epoch 60: reducing lr to 9.521787166395966e-05
Epoch 63: reducing lr to 8.401605361114112e-05
Epoch 66: reducing lr to 7.289604728224015e-05
Epoch 69: reducing lr to 6.203317300161673e-05
Epoch 72: reducing lr to 5.159880912193615e-05
Epoch 75: reducing lr to 4.175746449346044e-05
Epoch 78: reducing lr to 3.266437199514762e-05
Epoch 81: reducing lr to 2.446290821741684e-05
Epoch 84: reducing lr to 1.7282439307065048e-05
Epoch 87: reducing lr to 1.1236176183379706e-05
Epoch 90: reducing lr to 6.419507837078075e-06
Epoch 93: reducing lr to 2.90837187013795e-06
Epoch 96: reducing lr to 7.581513389543544e-07
Epoch 99: reducing lr to 2.7501423897294957e-09
[I 2024-06-21 00:49:19,414] Trial 375 finished with value: 0.9823522567749023 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.3678833041474778, 'bidirectional': True, 'fc_dropout': 0.6573445905346748, 'learning_rate_model': 0.0017840345144054216}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012933503625732056
Epoch 36: reducing lr to 0.00012192635982270753
Epoch 40: reducing lr to 0.00011595754825100281
Epoch 44: reducing lr to 0.00010855886772232559
Epoch 61: reducing lr to 6.656671739196671e-05
Epoch 64: reducing lr to 5.842164583396129e-05
Epoch 67: reducing lr to 5.037879825486272e-05
Epoch 72: reducing lr to 3.754425106764416e-05
Epoch 75: reducing lr to 3.0383506084139802e-05
Epoch 78: reducing lr to 2.3767203236312407e-05
Epoch 81: reducing lr to 1.779966599207766e-05
Epoch 84: reducing lr to 1.2575023560571375e-05
Epoch 87: reducing lr to 8.175650307591089e-06
Epoch 90: reducing lr to 4.670953032974282e-06
Epoch 93: reducing lr to 2.116185345140309e-06
Epoch 96: reducing lr to 5.516449836992857e-07
Epoch 99: reducing lr to 2.001054638829658e-09
[I 2024-06-21 00:49:35,033] Trial 376 finished with value: 0.9766807556152344 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.3718127871820902, 'bidirectional': True, 'fc_dropout': 0.6245863251442325, 'learning_rate_model': 0.0012980966200963064}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001541932607899688
Epoch 33: reducing lr to 0.00014945709628814562
Epoch 36: reducing lr to 0.00014536063499383134
Epoch 40: reducing lr to 0.00013824461642751687
Epoch 46: reducing lr to 0.00012444982419149346
Epoch 61: reducing lr to 7.936085620551027e-05
Epoch 64: reducing lr to 6.965029996924217e-05
Epoch 67: reducing lr to 6.006161518478399e-05
Epoch 72: reducing lr to 4.476026499516791e-05
Epoch 75: reducing lr to 3.62232231336325e-05
Epoch 78: reducing lr to 2.8335265314911743e-05
Epoch 81: reducing lr to 2.1220766002108125e-05
Epoch 84: reducing lr to 1.4991946060597598e-05
Epoch 87: reducing lr to 9.747012228750424e-06
Epoch 90: reducing lr to 5.568711309734769e-06
Epoch 93: reducing lr to 2.522916668565604e-06
Epoch 96: reducing lr to 6.576712799290403e-07
Epoch 99: reducing lr to 2.385657813311348e-09
[I 2024-06-21 00:49:50,190] Trial 377 finished with value: 0.9784600138664246 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.35328676281257676, 'bidirectional': True, 'fc_dropout': 0.6480146454695054, 'learning_rate_model': 0.0015475910972403474}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001338146261845209
Epoch 33: reducing lr to 0.00012970440710547088
Epoch 36: reducing lr to 0.00012614934617758314
Epoch 40: reducing lr to 0.00011997380154291498
Epoch 44: reducing lr to 0.00011231886365559831
Epoch 48: reducing lr to 0.00010339894684795576
Epoch 61: reducing lr to 6.887229216385275e-05
Epoch 64: reducing lr to 6.0445111584476097e-05
Epoch 67: reducing lr to 5.212369556759043e-05
Epoch 72: reducing lr to 3.884461679818207e-05
Epoch 75: reducing lr to 3.1435855484164336e-05
Epoch 78: reducing lr to 2.4590393357845154e-05
Epoch 81: reducing lr to 1.84161671876779e-05
Epoch 84: reducing lr to 1.3010566399591178e-05
Epoch 87: reducing lr to 8.458818440728136e-06
Epoch 90: reducing lr to 4.832734053511565e-06
Epoch 93: reducing lr to 2.1894805853976813e-06
Epoch 96: reducing lr to 5.707515103132434e-07
Epoch 99: reducing lr to 2.0703622639201003e-09
[I 2024-06-21 00:50:06,262] Trial 378 finished with value: 0.9771783351898193 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3221435799101135, 'bidirectional': True, 'fc_dropout': 0.6317607596071536, 'learning_rate_model': 0.0013430569086011709}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 4.50105420401734e-05
Epoch 38: reducing lr to 4.2773601618065226e-05
Epoch 41: reducing lr to 4.101512994872154e-05
Epoch 50: reducing lr to 3.419626135866761e-05
Epoch 55: reducing lr to 2.9675025662770133e-05
Epoch 61: reducing lr to 2.3900338246204772e-05
Epoch 64: reducing lr to 2.0975904341351233e-05
Epoch 67: reducing lr to 1.8088173277924585e-05
Epoch 73: reducing lr to 1.260373639229455e-05
Epoch 76: reducing lr to 1.0093510972873762e-05
Epoch 79: reducing lr to 7.791624475662354e-06
Epoch 82: reducing lr to 5.734372213565499e-06
Epoch 85: reducing lr to 3.954204344914624e-06
Epoch 88: reducing lr to 2.4791879537635622e-06
Epoch 91: reducing lr to 1.3325936656152392e-06
Epoch 94: reducing lr to 5.324981323930392e-07
Epoch 97: reducing lr to 9.152169590515143e-08
[I 2024-06-21 00:50:21,943] Trial 379 finished with value: 0.9827762246131897 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.5133892087179434, 'bidirectional': True, 'fc_dropout': 0.649575680998764, 'learning_rate_model': 0.00046607297929191567}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012121222724492309
Epoch 36: reducing lr to 0.00011426884826917711
Epoch 40: reducing lr to 0.00010867490431131436
Epoch 44: reducing lr to 0.00010174089345465562
Epoch 48: reducing lr to 9.366103691040257e-05
Epoch 61: reducing lr to 6.238603482052946e-05
Epoch 64: reducing lr to 5.4752509573350116e-05
Epoch 72: reducing lr to 3.5186307004217856e-05
Epoch 75: reducing lr to 2.8475288294201873e-05
Epoch 78: reducing lr to 2.2274518359622818e-05
Epoch 81: reducing lr to 1.6681768695861204e-05
Epoch 84: reducing lr to 1.1785256783797162e-05
Epoch 87: reducing lr to 7.662183516824621e-06
Epoch 90: reducing lr to 4.377596642543124e-06
Epoch 93: reducing lr to 1.9832795997921447e-06
Epoch 96: reducing lr to 5.169992529297646e-07
Epoch 99: reducing lr to 1.87537960811765e-09
[I 2024-06-21 00:50:37,715] Trial 380 finished with value: 0.9769428968429565 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.47658296446096027, 'bidirectional': True, 'fc_dropout': 0.6226689478678064, 'learning_rate_model': 0.001216570444128778}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001318311874967907
Epoch 33: reducing lr to 0.00012778189126129582
Epoch 36: reducing lr to 0.00012427952446395768
Epoch 40: reducing lr to 0.00011819551551933668
Epoch 48: reducing lr to 0.00010186633806447346
Epoch 61: reducing lr to 6.785144733780155e-05
Epoch 64: reducing lr to 5.954917684087415e-05
Epoch 67: reducing lr to 5.135110323382214e-05
Epoch 72: reducing lr to 3.826885077047351e-05
Epoch 75: reducing lr to 3.096990423707712e-05
Epoch 78: reducing lr to 2.4225907509600155e-05
Epoch 81: reducing lr to 1.8143197486821905e-05
Epoch 84: reducing lr to 1.281772006072649e-05
Epoch 87: reducing lr to 8.33343941284302e-06
Epoch 90: reducing lr to 4.761101886217476e-06
Epoch 93: reducing lr to 2.1570274775205245e-06
Epoch 96: reducing lr to 5.622916680754139e-07
Epoch 99: reducing lr to 2.0396747619014222e-09
[I 2024-06-21 00:50:53,496] Trial 381 finished with value: 0.9775391817092896 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.49713131993590964, 'bidirectional': True, 'fc_dropout': 0.6331025511641362, 'learning_rate_model': 0.0013231497347121997}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.00020099105786044154
Epoch 27: reducing lr to 0.00020367632123338431
Epoch 31: reducing lr to 0.00020019778314163847
Epoch 34: reducing lr to 0.0001957796460753693
Epoch 40: reducing lr to 0.0001826095042028498
Epoch 46: reducing lr to 0.00016438774457199772
Epoch 49: reducing lr to 0.00015372975713520503
Epoch 52: reducing lr to 0.00014225931845910085
Epoch 55: reducing lr to 0.0001301573057281956
Epoch 58: reducing lr to 0.00011761462031281101
Epoch 61: reducing lr to 0.00010482901236447222
Epoch 64: reducing lr to 9.200218477680598e-05
Epoch 67: reducing lr to 7.933633912078091e-05
Epoch 70: reducing lr to 6.703130792944888e-05
Epoch 73: reducing lr to 5.52811104385185e-05
Epoch 76: reducing lr to 4.427103816174393e-05
Epoch 79: reducing lr to 3.4174758954645083e-05
Epoch 82: reducing lr to 2.515146729246808e-05
Epoch 85: reducing lr to 1.7343492460008475e-05
Epoch 88: reducing lr to 1.0873939187877384e-05
Epoch 91: reducing lr to 5.844874512258387e-06
Epoch 94: reducing lr to 2.335584238585082e-06
Epoch 97: reducing lr to 4.0142231012910145e-07
[I 2024-06-21 00:51:09,297] Trial 382 finished with value: 0.9877716302871704 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.4730091612535399, 'bidirectional': True, 'fc_dropout': 0.6152895753416424, 'learning_rate_model': 0.0020442376005576817}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 6.471552133483117e-05
Epoch 38: reducing lr to 6.149928000446726e-05
Epoch 41: reducing lr to 5.897097428594166e-05
Epoch 49: reducing lr to 5.039352120123834e-05
Epoch 55: reducing lr to 4.2666332582189546e-05
Epoch 61: reducing lr to 3.4363568612469605e-05
Epoch 64: reducing lr to 3.0158858867074057e-05
Epoch 67: reducing lr to 2.6006919948461516e-05
Epoch 72: reducing lr to 1.9381374027652915e-05
Epoch 75: reducing lr to 1.5684800706962994e-05
Epoch 78: reducing lr to 1.2269283376681772e-05
Epoch 81: reducing lr to 9.188676677507573e-06
Epoch 84: reducing lr to 6.4915726936426115e-06
Epoch 87: reducing lr to 4.220495336162826e-06
Epoch 90: reducing lr to 2.411274302277738e-06
Epoch 93: reducing lr to 1.0924330228909152e-06
Epoch 96: reducing lr to 2.8477429847490776e-07
Epoch 99: reducing lr to 1.0329993887733292e-09
[I 2024-06-21 00:51:23,984] Trial 383 finished with value: 0.9830515384674072 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.317574818247516, 'bidirectional': True, 'fc_dropout': 0.6191867556957373, 'learning_rate_model': 0.0006701131438949029}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012571031231924118
Epoch 33: reducing lr to 0.00012184902346868312
Epoch 36: reducing lr to 0.00011850927031686813
Epoch 40: reducing lr to 0.0001127077397450527
Epoch 48: reducing lr to 9.713672019539463e-05
Epoch 61: reducing lr to 6.470112875494825e-05
Epoch 64: reducing lr to 5.678432972624366e-05
Epoch 67: reducing lr to 4.896688976285318e-05
Epoch 72: reducing lr to 3.649204163143696e-05
Epoch 75: reducing lr to 2.953198259125696e-05
Epoch 78: reducing lr to 2.3101107234758272e-05
Epoch 81: reducing lr to 1.730081527630612e-05
Epoch 84: reducing lr to 1.2222597874222714e-05
Epoch 87: reducing lr to 7.946520782932896e-06
Epoch 90: reducing lr to 4.54004561792102e-06
Epoch 93: reducing lr to 2.056877458430609e-06
Epoch 96: reducing lr to 5.361846657869849e-07
Epoch 99: reducing lr to 1.9449733876849425e-09
[I 2024-06-21 00:51:39,694] Trial 384 finished with value: 0.977777361869812 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.2891916893987088, 'bidirectional': True, 'fc_dropout': 0.632731084037357, 'learning_rate_model': 0.0012617163628283405}. Best is trial 235 with value: 0.9706074595451355.
Epoch 42: reducing lr to 2.629163482136539e-05
Epoch 55: reducing lr to 1.9328556102788434e-05
Epoch 65: reducing lr to 1.303136512763932e-05
Epoch 68: reducing lr to 1.1165062932978618e-05
Epoch 73: reducing lr to 8.209328232152254e-06
Epoch 76: reducing lr to 6.57431590221208e-06
Epoch 79: reducing lr to 5.075003220591687e-06
Epoch 82: reducing lr to 3.7350308068385812e-06
Epoch 85: reducing lr to 2.5755347742953764e-06
Epoch 88: reducing lr to 1.6147963610287585e-06
Epoch 91: reducing lr to 8.679726757702239e-07
Epoch 94: reducing lr to 3.4683778014391825e-07
Epoch 97: reducing lr to 5.96118181675064e-08
[I 2024-06-21 00:51:54,230] Trial 385 finished with value: 0.984012246131897 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.48249238458997584, 'bidirectional': True, 'fc_dropout': 0.6501357173294754, 'learning_rate_model': 0.00030357236521414376}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00017360558113787566
Epoch 30: reducing lr to 0.00017160554297820826
Epoch 33: reducing lr to 0.00016827315229830678
Epoch 36: reducing lr to 0.0001636609627644403
Epoch 39: reducing lr to 0.00015784171016657763
Epoch 42: reducing lr to 0.00015090716836633366
Epoch 45: reducing lr to 0.00014296669926619718
Epoch 48: reducing lr to 0.00013414553228158346
Epoch 51: reducing lr to 0.00012458277964939074
Epoch 54: reducing lr to 0.0001144292593740684
Epoch 57: reducing lr to 0.00010384506734877892
Epoch 60: reducing lr to 9.299716953918212e-05
Epoch 63: reducing lr to 8.205660392476145e-05
Epoch 66: reducing lr to 7.119594199466494e-05
Epoch 69: reducing lr to 6.05864151957129e-05
Epoch 72: reducing lr to 5.0395404938974904e-05
Epoch 75: reducing lr to 4.078358334588328e-05
Epoch 78: reducing lr to 3.1902562903781374e-05
Epoch 81: reducing lr to 2.389237632768528e-05
Epoch 84: reducing lr to 1.6879372645104948e-05
Epoch 87: reducing lr to 1.0974122433503145e-05
Epoch 90: reducing lr to 6.26978999057825e-06
Epoch 93: reducing lr to 2.840541876894119e-06
Epoch 96: reducing lr to 7.40469487218987e-07
Epoch 99: reducing lr to 2.686002675811837e-09
[I 2024-06-21 00:53:30,735] Trial 386 finished with value: 1.0580103397369385 and parameters: {'hidden_size': 149, 'n_layers': 4, 'rnn_dropout': 0.4479912561511594, 'bidirectional': True, 'fc_dropout': 0.610985707563691, 'learning_rate_model': 0.0017424266820984938}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.524168618078822e-05
Epoch 40: reducing lr to 7.884673596881939e-05
Epoch 43: reducing lr to 7.515749689737208e-05
Epoch 49: reducing lr to 6.637710136886226e-05
Epoch 52: reducing lr to 6.142442021631572e-05
Epoch 55: reducing lr to 5.6199039387150275e-05
Epoch 64: reducing lr to 3.972450395349219e-05
Epoch 72: reducing lr to 2.5528667134888335e-05
Epoch 75: reducing lr to 2.0659632064982632e-05
Epoch 78: reducing lr to 1.61607970033515e-05
Epoch 81: reducing lr to 1.2103097952473167e-05
Epoch 84: reducing lr to 8.55053920539821e-06
Epoch 87: reducing lr to 5.559132207423655e-06
Epoch 90: reducing lr to 3.1760709506937503e-06
Epoch 93: reducing lr to 1.4389257938447254e-06
Epoch 96: reducing lr to 3.750976718144253e-07
Epoch 99: reducing lr to 1.360641279052159e-09
[I 2024-06-21 00:53:46,479] Trial 387 finished with value: 0.980132520198822 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.4278435763754646, 'bidirectional': True, 'fc_dropout': 0.6295679437019118, 'learning_rate_model': 0.0008826564808459131}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001235085149020672
Epoch 36: reducing lr to 0.00011643359807899944
Epoch 40: reducing lr to 0.00011073368045200148
Epoch 48: reducing lr to 9.54353849931096e-05
Epoch 55: reducing lr to 7.892687493959335e-05
Epoch 61: reducing lr to 6.356789811099669e-05
Epoch 64: reducing lr to 5.578976063942371e-05
Epoch 67: reducing lr to 4.810924197391844e-05
Epoch 72: reducing lr to 3.5852888951524284e-05
Epoch 75: reducing lr to 2.9014734309920036e-05
Epoch 78: reducing lr to 2.2696494778509028e-05
Epoch 81: reducing lr to 1.699779407074555e-05
Epoch 84: reducing lr to 1.2008520890925776e-05
Epoch 87: reducing lr to 7.807338653697963e-06
Epoch 90: reducing lr to 4.460527394388133e-06
Epoch 93: reducing lr to 2.020851555767085e-06
Epoch 96: reducing lr to 5.267934711389224e-07
Epoch 99: reducing lr to 1.910907468176924e-09
[I 2024-06-21 00:54:06,454] Trial 388 finished with value: 0.9775076508522034 and parameters: {'hidden_size': 185, 'n_layers': 1, 'rnn_dropout': 0.41412118586324786, 'bidirectional': True, 'fc_dropout': 0.6099274263217719, 'learning_rate_model': 0.0012396175884507323}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.000160087160786725
Epoch 31: reducing lr to 0.0001573530713087539
Epoch 36: reducing lr to 0.00015091691574001527
Epoch 40: reducing lr to 0.00014352889370487185
Epoch 46: reducing lr to 0.00012920680782774763
Epoch 52: reducing lr to 0.0001118141286609175
Epoch 57: reducing lr to 9.575879925409456e-05
Epoch 61: reducing lr to 8.239435421791274e-05
Epoch 64: reducing lr to 7.231262062229557e-05
Epoch 67: reducing lr to 6.235741690613781e-05
Epoch 70: reducing lr to 5.268580905853123e-05
Epoch 73: reducing lr to 4.34502938861469e-05
Epoch 76: reducing lr to 3.479650831022907e-05
Epoch 79: reducing lr to 2.686095319519696e-05
Epoch 82: reducing lr to 1.9768753501088995e-05
Epoch 85: reducing lr to 1.363177834927255e-05
Epoch 88: reducing lr to 8.546786590671535e-06
Epoch 91: reducing lr to 4.5940016991468305e-06
Epoch 94: reducing lr to 1.8357413727287334e-06
Epoch 97: reducing lr to 3.1551315104211707e-07
[I 2024-06-21 00:54:21,317] Trial 389 finished with value: 0.9795482158660889 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.31494518043423786, 'bidirectional': True, 'fc_dropout': 0.6422366105101192, 'learning_rate_model': 0.0016067463879208478}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.819331697678371e-05
Epoch 33: reducing lr to 9.517723378482639e-05
Epoch 38: reducing lr to 9.044710186755927e-05
Epoch 41: reducing lr to 8.672871809364829e-05
Epoch 44: reducing lr to 8.241970325574243e-05
Epoch 47: reducing lr to 7.758801500223696e-05
Epoch 50: reducing lr to 7.230985041228785e-05
Epoch 55: reducing lr to 6.274945217401152e-05
Epoch 64: reducing lr to 4.4354688054257656e-05
Epoch 72: reducing lr to 2.850422168983206e-05
Epoch 75: reducing lr to 2.3067664649277182e-05
Epoch 78: reducing lr to 1.8044457160020074e-05
Epoch 81: reducing lr to 1.3513803339132176e-05
Epoch 84: reducing lr to 9.547167652367819e-06
Epoch 87: reducing lr to 6.207090092335231e-06
Epoch 90: reducing lr to 3.5462654592525577e-06
Epoch 93: reducing lr to 1.6066432143225737e-06
Epoch 96: reducing lr to 4.1881807366771653e-07
Epoch 99: reducing lr to 1.519234061600136e-09
[I 2024-06-21 00:54:37,482] Trial 390 finished with value: 0.980226993560791 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.4772582652531896, 'bidirectional': True, 'fc_dropout': 0.660670210059402, 'learning_rate_model': 0.0009855366076521551}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014050469837044896
Epoch 33: reducing lr to 0.00013618898858292505
Epoch 36: reducing lr to 0.00013245619211960978
Epoch 39: reducing lr to 0.00012774647987622982
Epoch 46: reducing lr to 0.00011340174609900168
Epoch 61: reducing lr to 7.231556753160774e-05
Epoch 64: reducing lr to 6.346706943255922e-05
Epoch 67: reducing lr to 5.4729623603167596e-05
Epoch 72: reducing lr to 4.0786656303312144e-05
Epoch 75: reducing lr to 3.300749341651908e-05
Epoch 78: reducing lr to 2.5819791902197867e-05
Epoch 81: reducing lr to 1.9336884835566368e-05
Epoch 84: reducing lr to 1.366103063414393e-05
Epoch 87: reducing lr to 8.881717697630725e-06
Epoch 90: reducing lr to 5.07434695185651e-06
Epoch 93: reducing lr to 2.2989438300642475e-06
Epoch 96: reducing lr to 5.992862745097822e-07
Epoch 99: reducing lr to 2.173870179266596e-09
[I 2024-06-21 00:54:51,498] Trial 391 finished with value: 0.977875292301178 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.4366338034555803, 'bidirectional': True, 'fc_dropout': 0.5937571415000535, 'learning_rate_model': 0.0014102031386101486}. Best is trial 235 with value: 0.9706074595451355.
Epoch 55: reducing lr to 2.4180009122042415e-05
Epoch 61: reducing lr to 1.947463848491861e-05
Epoch 64: reducing lr to 1.7091731076522192e-05
Epoch 67: reducing lr to 1.4738730130569735e-05
Epoch 73: reducing lr to 1.0269863433339698e-05
Epoch 76: reducing lr to 8.224464240437658e-06
Epoch 79: reducing lr to 6.3488252053446585e-06
Epoch 82: reducing lr to 4.672520725303284e-06
Epoch 85: reducing lr to 3.2219920621807463e-06
Epoch 88: reducing lr to 2.0201090310249966e-06
Epoch 91: reducing lr to 1.0858331634394409e-06
Epoch 94: reducing lr to 4.3389380164506445e-07
Epoch 97: reducing lr to 7.45743396147287e-08
[I 2024-06-21 00:55:07,336] Trial 392 finished with value: 0.9829331040382385 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.3607324969492452, 'bidirectional': True, 'fc_dropout': 0.6145086725679166, 'learning_rate_model': 0.00037976880016500706}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.890765376708983e-05
Epoch 40: reducing lr to 7.298789150296017e-05
Epoch 43: reducing lr to 6.957278778602556e-05
Epoch 49: reducing lr to 6.1444834887108e-05
Epoch 52: reducing lr to 5.686017136021484e-05
Epoch 55: reducing lr to 5.2023071582595645e-05
Epoch 58: reducing lr to 4.700983765345186e-05
Epoch 61: reducing lr to 4.189950908755143e-05
Epoch 64: reducing lr to 3.6772705286278374e-05
Epoch 72: reducing lr to 2.36317149234087e-05
Epoch 75: reducing lr to 1.9124482010851302e-05
Epoch 78: reducing lr to 1.4959940748193343e-05
Epoch 81: reducing lr to 1.1203756114319686e-05
Epoch 84: reducing lr to 7.915176451466704e-06
Epoch 87: reducing lr to 5.14605117663343e-06
Epoch 90: reducing lr to 2.940067449927312e-06
Epoch 93: reducing lr to 1.3320038988485441e-06
Epoch 96: reducing lr to 3.4722538399345556e-07
Epoch 99: reducing lr to 1.259536451688175e-09
[I 2024-06-21 00:55:24,074] Trial 393 finished with value: 0.981341540813446 and parameters: {'hidden_size': 157, 'n_layers': 1, 'rnn_dropout': 0.3285550241086445, 'bidirectional': True, 'fc_dropout': 0.6302376367406831, 'learning_rate_model': 0.000817069148985988}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011420523190996041
Epoch 33: reducing lr to 0.00011069733044576343
Epoch 40: reducing lr to 0.00010239266228965571
Epoch 43: reducing lr to 9.760170923742168e-05
Epoch 46: reducing lr to 9.21753710903294e-05
Epoch 55: reducing lr to 7.2981705460162e-05
Epoch 61: reducing lr to 5.877964407192141e-05
Epoch 64: reducing lr to 5.15873950640461e-05
Epoch 72: reducing lr to 3.31522688990114e-05
Epoch 75: reducing lr to 2.682919848317957e-05
Epoch 78: reducing lr to 2.0986880554576593e-05
Epoch 81: reducing lr to 1.5717434667127973e-05
Epoch 84: reducing lr to 1.1103978655489679e-05
Epoch 87: reducing lr to 7.219250609985669e-06
Epoch 90: reducing lr to 4.124538020077025e-06
Epoch 93: reducing lr to 1.868630845128249e-06
Epoch 96: reducing lr to 4.871127354075895e-07
Epoch 99: reducing lr to 1.7669682995848784e-09
[I 2024-06-21 00:55:38,630] Trial 394 finished with value: 0.9786986112594604 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.40771850262998705, 'bidirectional': True, 'fc_dropout': 0.06338252141555979, 'learning_rate_model': 0.001146243352378875}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00019050494341716563
Epoch 31: reducing lr to 0.00018725135606675807
Epoch 34: reducing lr to 0.0001831189319011897
Epoch 40: reducing lr to 0.0001708004791864756
Epoch 46: reducing lr to 0.00015375708766007912
Epoch 49: reducing lr to 0.0001437883329158273
Epoch 52: reducing lr to 0.00013305966667849189
Epoch 57: reducing lr to 0.00011395370212043726
Epoch 60: reducing lr to 0.00010204983275824676
Epoch 63: reducing lr to 9.004427498950359e-05
Epoch 66: reducing lr to 7.812639900357653e-05
Epoch 69: reducing lr to 6.648410450319365e-05
Epoch 72: reducing lr to 5.5301066379656855e-05
Epoch 75: reducing lr to 4.4753597129382165e-05
Epoch 78: reducing lr to 3.50080677188637e-05
Epoch 81: reducing lr to 2.6218142127541776e-05
Epoch 84: reducing lr to 1.8522468630309583e-05
Epoch 87: reducing lr to 1.2042381123606913e-05
Epoch 90: reducing lr to 6.880112837178983e-06
Epoch 93: reducing lr to 3.1170499587915812e-06
Epoch 96: reducing lr to 8.125493249710493e-07
Epoch 99: reducing lr to 2.9474673822081283e-09
[I 2024-06-21 00:55:53,808] Trial 395 finished with value: 0.9817882180213928 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.37102098907591846, 'bidirectional': True, 'fc_dropout': 0.6658814441573178, 'learning_rate_model': 0.0019120404672826124}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 6.560728425311491e-05
Epoch 38: reducing lr to 6.234672396038291e-05
Epoch 41: reducing lr to 5.978357885187232e-05
Epoch 44: reducing lr to 5.681330171647558e-05
Epoch 49: reducing lr to 5.1087930712312206e-05
Epoch 52: reducing lr to 4.7276040371463957e-05
Epoch 55: reducing lr to 4.325426345983945e-05
Epoch 61: reducing lr to 3.483708957925515e-05
Epoch 64: reducing lr to 3.0574440035867555e-05
Epoch 67: reducing lr to 2.6365288487421725e-05
Epoch 72: reducing lr to 1.964844428076615e-05
Epoch 75: reducing lr to 1.5900933148804457e-05
Epoch 78: reducing lr to 1.243835088511813e-05
Epoch 81: reducing lr to 9.315294233235884e-06
Epoch 84: reducing lr to 6.5810248635414364e-06
Epoch 87: reducing lr to 4.2786526554573225e-06
Epoch 90: reducing lr to 2.444501030027593e-06
Epoch 93: reducing lr to 1.1074864635559788e-06
Epoch 96: reducing lr to 2.886984136519469e-07
Epoch 99: reducing lr to 1.0472338495405401e-09
[I 2024-06-21 00:56:09,615] Trial 396 finished with value: 0.9824060201644897 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.30007695835061476, 'bidirectional': True, 'fc_dropout': 0.5921051285910505, 'learning_rate_model': 0.0006793471273420607}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012880430579624227
Epoch 30: reducing lr to 0.0001273204046161451
Epoch 33: reducing lr to 0.00012484798255832144
Epoch 36: reducing lr to 0.00012142603110251791
Epoch 39: reducing lr to 0.00011710851558137001
Epoch 42: reducing lr to 0.00011196352636650095
Epoch 45: reducing lr to 0.0001060722030378614
Epoch 48: reducing lr to 9.952745786135962e-05
Epoch 51: reducing lr to 9.243250327396906e-05
Epoch 54: reducing lr to 8.489923664809769e-05
Epoch 57: reducing lr to 7.704643895982046e-05
Epoch 60: reducing lr to 6.899798834230253e-05
Epoch 63: reducing lr to 6.088078410412469e-05
Epoch 66: reducing lr to 5.282286332055975e-05
Epoch 69: reducing lr to 4.495126884065449e-05
Epoch 72: reducing lr to 3.739018703165992e-05
Epoch 75: reducing lr to 3.0258826394398744e-05
Epoch 78: reducing lr to 2.36696737570841e-05
Epoch 81: reducing lr to 1.772662449294189e-05
Epoch 84: reducing lr to 1.2523421548885278e-05
Epoch 87: reducing lr to 8.142101264865114e-06
Epoch 90: reducing lr to 4.651785627694132e-06
Epoch 93: reducing lr to 2.107501510841014e-06
Epoch 96: reducing lr to 5.493812908514452e-07
Epoch 99: reducing lr to 1.992843247085145e-09
[I 2024-06-21 00:57:47,026] Trial 397 finished with value: 1.0062320232391357 and parameters: {'hidden_size': 150, 'n_layers': 4, 'rnn_dropout': 0.42268678156423706, 'bidirectional': True, 'fc_dropout': 0.6432782149600954, 'learning_rate_model': 0.0012927698390658578}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.488716923133285e-05
Epoch 33: reducing lr to 9.197263690813313e-05
Epoch 38: reducing lr to 8.740176750949194e-05
Epoch 41: reducing lr to 8.380858091304004e-05
Epoch 44: reducing lr to 7.964464967277684e-05
Epoch 47: reducing lr to 7.497564331777394e-05
Epoch 50: reducing lr to 6.987519338801198e-05
Epoch 55: reducing lr to 6.0636691690704544e-05
Epoch 64: reducing lr to 4.286127530046066e-05
Epoch 72: reducing lr to 2.7544490710401443e-05
Epoch 75: reducing lr to 2.2290981369588636e-05
Epoch 78: reducing lr to 1.7436904190080265e-05
Epoch 81: reducing lr to 1.3058796503456153e-05
Epoch 84: reducing lr to 9.22571658236494e-06
Epoch 87: reducing lr to 5.998098711389849e-06
Epoch 90: reducing lr to 3.426863468222437e-06
Epoch 93: reducing lr to 1.5525478847795899e-06
Epoch 96: reducing lr to 4.0471655970889627e-07
Epoch 99: reducing lr to 1.4680817793095817e-09
[I 2024-06-21 00:58:03,924] Trial 398 finished with value: 0.9791598916053772 and parameters: {'hidden_size': 158, 'n_layers': 1, 'rnn_dropout': 0.3902961575651658, 'bidirectional': True, 'fc_dropout': 0.6038913688306404, 'learning_rate_model': 0.0009523538032234298}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001633533800936291
Epoch 31: reducing lr to 0.00016056350765470182
Epoch 36: reducing lr to 0.00015399603677324514
Epoch 40: reducing lr to 0.00014645727872596663
Epoch 46: reducing lr to 0.00013184298282289708
Epoch 52: reducing lr to 0.00011409544506394556
Epoch 57: reducing lr to 9.771254268606385e-05
Epoch 61: reducing lr to 8.407542613650946e-05
Epoch 64: reducing lr to 7.378799738860624e-05
Epoch 67: reducing lr to 6.36296800784408e-05
Epoch 70: reducing lr to 5.376074477418239e-05
Epoch 73: reducing lr to 4.433679963766473e-05
Epoch 76: reducing lr to 3.550645298472505e-05
Epoch 79: reducing lr to 2.7408990673635868e-05
Epoch 82: reducing lr to 2.0172090558485545e-05
Epoch 85: reducing lr to 1.3909904199047289e-05
Epoch 88: reducing lr to 8.721164593487331e-06
Epoch 91: reducing lr to 4.68773199564258e-06
Epoch 94: reducing lr to 1.8731955563411079e-06
Epoch 97: reducing lr to 3.2195048893014257e-07
[I 2024-06-21 00:58:18,781] Trial 399 finished with value: 0.979913055896759 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.49335045451501325, 'bidirectional': True, 'fc_dropout': 0.5810373681777206, 'learning_rate_model': 0.0016395284426948536}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 4.7032537425940664e-05
Epoch 55: reducing lr to 3.100810814173899e-05
Epoch 61: reducing lr to 2.497400613513999e-05
Epoch 64: reducing lr to 2.1918198743241627e-05
Epoch 67: reducing lr to 1.8900742983756576e-05
Epoch 73: reducing lr to 1.3169930347610627e-05
Epoch 76: reducing lr to 1.054693880751584e-05
Epoch 79: reducing lr to 8.141645337960769e-06
Epoch 82: reducing lr to 5.991975735552694e-06
Epoch 85: reducing lr to 4.13183790757339e-06
Epoch 88: reducing lr to 2.590559787466108e-06
Epoch 91: reducing lr to 1.392457380221734e-06
Epoch 94: reducing lr to 5.564193898990562e-07
Epoch 97: reducing lr to 9.563309822180148e-08
[I 2024-06-21 00:58:34,004] Trial 400 finished with value: 0.9831601977348328 and parameters: {'hidden_size': 152, 'n_layers': 1, 'rnn_dropout': 0.3381113957769308, 'bidirectional': True, 'fc_dropout': 0.6202058533649876, 'learning_rate_model': 0.00048701023911691254}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011485750109565087
Epoch 36: reducing lr to 0.00010827813879499101
Epoch 40: reducing lr to 0.00010297746543164318
Epoch 43: reducing lr to 9.815914943820409e-05
Epoch 46: reducing lr to 9.270181942580677e-05
Epoch 55: reducing lr to 7.339853152666246e-05
Epoch 61: reducing lr to 5.911535680532923e-05
Epoch 64: reducing lr to 5.1882030148687816e-05
Epoch 72: reducing lr to 3.3341614019868883e-05
Epoch 75: reducing lr to 2.6982430162277665e-05
Epoch 78: reducing lr to 2.110674455082774e-05
Epoch 81: reducing lr to 1.5807202868987186e-05
Epoch 86: reducing lr to 8.477297232669518e-06
Epoch 89: reducing lr to 5.0939808791407824e-06
Epoch 92: reducing lr to 2.5393430801717723e-06
Epoch 95: reducing lr to 8.536795265649301e-07
Epoch 98: reducing lr to 6.356965310230951e-08
[I 2024-06-21 00:58:48,996] Trial 401 finished with value: 0.9768834114074707 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.4486438507178295, 'bidirectional': True, 'fc_dropout': 0.6053843361595034, 'learning_rate_model': 0.0011527899808087242}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013684550808087877
Epoch 33: reducing lr to 0.00013264219313516632
Epoch 36: reducing lr to 0.00012900661059231003
Epoch 40: reducing lr to 0.0001226911907663638
Epoch 44: reducing lr to 0.00011486286964492729
Epoch 48: reducing lr to 0.00010574091801388641
Epoch 55: reducing lr to 8.744974636695151e-05
Epoch 61: reducing lr to 7.043223960332209e-05
Epoch 64: reducing lr to 6.181418460473099e-05
Epoch 72: reducing lr to 3.972444174095195e-05
Epoch 75: reducing lr to 3.2147873056534134e-05
Epoch 78: reducing lr to 2.514736220480692e-05
Epoch 81: reducing lr to 1.8833290706391513e-05
Epoch 84: reducing lr to 1.3305253843604254e-05
Epoch 87: reducing lr to 8.650409452918596e-06
Epoch 90: reducing lr to 4.9421947796694225e-06
Epoch 93: reducing lr to 2.2390720034505932e-06
Epoch 96: reducing lr to 5.8367894933283e-07
Epoch 99: reducing lr to 2.117255669246455e-09
[I 2024-06-21 00:59:03,171] Trial 402 finished with value: 0.9773521423339844 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.44978185210208016, 'bidirectional': True, 'fc_dropout': 0.6285994587453031, 'learning_rate_model': 0.00137347695300233}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014596496321511281
Epoch 33: reducing lr to 0.00014148153719670205
Epoch 36: reducing lr to 0.00013760367756085722
Epoch 40: reducing lr to 0.00013086739490525542
Epoch 46: reducing lr to 0.00011780874155701338
Epoch 61: reducing lr to 7.512588032323892e-05
Epoch 64: reducing lr to 6.593351370122526e-05
Epoch 67: reducing lr to 5.685651503945363e-05
Epoch 72: reducing lr to 4.237169899673999e-05
Epoch 75: reducing lr to 3.429022387324377e-05
Epoch 78: reducing lr to 2.6823195373079643e-05
Epoch 81: reducing lr to 2.0088350898249742e-05
Epoch 84: reducing lr to 1.4191922811975737e-05
Epoch 87: reducing lr to 9.22687719384014e-06
Epoch 90: reducing lr to 5.271545185027297e-06
Epoch 93: reducing lr to 2.388284914887342e-06
Epoch 96: reducing lr to 6.225756151122513e-07
Epoch 99: reducing lr to 2.258350677449526e-09
[I 2024-06-21 00:59:16,718] Trial 403 finished with value: 0.9779147505760193 and parameters: {'hidden_size': 133, 'n_layers': 1, 'rnn_dropout': 0.44394663861450606, 'bidirectional': True, 'fc_dropout': 0.6189881978245951, 'learning_rate_model': 0.001465006164493923}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00018704864561418892
Epoch 31: reducing lr to 0.000183854087528898
Epoch 34: reducing lr to 0.00017979663721077018
Epoch 40: reducing lr to 0.00016770167602488586
Epoch 46: reducing lr to 0.00015096749976414766
Epoch 52: reducing lr to 0.00013064558846426617
Epoch 55: reducing lr to 0.00011953155676527615
Epoch 61: reducing lr to 9.627100816191319e-05
Epoch 64: reducing lr to 8.44913338567666e-05
Epoch 67: reducing lr to 7.285949928134159e-05
Epoch 70: reducing lr to 6.155902309127111e-05
Epoch 73: reducing lr to 5.0768085229332136e-05
Epoch 76: reducing lr to 4.065685042788851e-05
Epoch 79: reducing lr to 3.1384808690319086e-05
Epoch 82: reducing lr to 2.309815821386022e-05
Epoch 85: reducing lr to 1.5927608841418686e-05
Epoch 88: reducing lr to 9.986215310973197e-06
Epoch 91: reducing lr to 5.367712136012553e-06
Epoch 94: reducing lr to 2.1449124075871313e-06
Epoch 97: reducing lr to 3.68651097851138e-07
[I 2024-06-21 00:59:30,543] Trial 404 finished with value: 0.9818533658981323 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.4734422269724626, 'bidirectional': True, 'fc_dropout': 0.639658484603022, 'learning_rate_model': 0.0018773506521642714}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.00021317863470509882
Epoch 27: reducing lr to 0.00021602672548964023
Epoch 31: reducing lr to 0.00021233725786325982
Epoch 34: reducing lr to 0.0002076512164156784
Epoch 40: reducing lr to 0.00019368247127276957
Epoch 46: reducing lr to 0.0001743557913628268
Epoch 49: reducing lr to 0.00016305153118993313
Epoch 52: reducing lr to 0.00015088555484018767
Epoch 55: reducing lr to 0.00013804970742179427
Epoch 58: reducing lr to 0.0001247464660694162
Epoch 61: reducing lr to 0.00011118557199126242
Epoch 64: reducing lr to 9.75809588216804e-05
Epoch 67: reducing lr to 8.414708911086103e-05
Epoch 70: reducing lr to 7.109591271875894e-05
Epoch 73: reducing lr to 5.8633213704700846e-05
Epoch 76: reducing lr to 4.695551917961913e-05
Epoch 79: reducing lr to 3.624702776770135e-05
Epoch 82: reducing lr to 2.667658708459179e-05
Epoch 85: reducing lr to 1.8395156894044414e-05
Epoch 88: reducing lr to 1.1533306678486889e-05
Epoch 91: reducing lr to 6.19929255465208e-06
Epoch 94: reducing lr to 2.477208013731795e-06
Epoch 97: reducing lr to 4.257635186581416e-07
[I 2024-06-21 00:59:45,051] Trial 405 finished with value: 0.9844325184822083 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.45456809107779705, 'bidirectional': True, 'fc_dropout': 0.6084091698792288, 'learning_rate_model': 0.0021681948706509307}. Best is trial 235 with value: 0.9706074595451355.
Epoch 35: reducing lr to 7.626880789350074e-05
Epoch 39: reducing lr to 7.281151547231501e-05
Epoch 45: reducing lr to 6.594975450190427e-05
Epoch 49: reducing lr to 6.044485978128144e-05
Epoch 52: reducing lr to 5.5934808699908014e-05
Epoch 55: reducing lr to 5.1176429605172995e-05
Epoch 65: reducing lr to 3.450328811771562e-05
Epoch 68: reducing lr to 2.9561859364366862e-05
Epoch 71: reducing lr to 2.4788041478439e-05
Epoch 74: reducing lr to 2.0257098842341744e-05
Epoch 77: reducing lr to 1.6040500477468722e-05
Epoch 80: reducing lr to 1.2204732180245622e-05
Epoch 83: reducing lr to 8.810297609295544e-06
Epoch 86: reducing lr to 5.910715051734314e-06
Epoch 89: reducing lr to 3.5517298295913026e-06
Epoch 92: reducing lr to 1.7705328660231755e-06
Epoch 95: reducing lr to 5.952199490633796e-07
Epoch 98: reducing lr to 4.432333739312188e-08
[I 2024-06-21 01:01:07,438] Trial 406 finished with value: 0.977483868598938 and parameters: {'hidden_size': 137, 'n_layers': 4, 'rnn_dropout': 0.46865997718204866, 'bidirectional': True, 'fc_dropout': 0.6322203887088355, 'learning_rate_model': 0.0008037718749315285}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013603870472569214
Epoch 36: reducing lr to 0.00012824602321369027
Epoch 40: reducing lr to 0.00012196783736039276
Epoch 46: reducing lr to 0.00010979722978562418
Epoch 61: reducing lr to 7.001699055333733e-05
Epoch 64: reducing lr to 6.144974522899534e-05
Epoch 67: reducing lr to 5.299002233697187e-05
Epoch 72: reducing lr to 3.949023739380908e-05
Epoch 75: reducing lr to 3.1958338067714524e-05
Epoch 78: reducing lr to 2.499910029628348e-05
Epoch 81: reducing lr to 1.8722254821149737e-05
Epoch 84: reducing lr to 1.3226809738326904e-05
Epoch 87: reducing lr to 8.599409025734483e-06
Epoch 90: reducing lr to 4.91305696297276e-06
Epoch 93: reducing lr to 2.225871052756456e-06
Epoch 96: reducing lr to 5.802377393049601e-07
Epoch 99: reducing lr to 2.1047729140523734e-09
[I 2024-06-21 01:01:22,491] Trial 407 finished with value: 0.9771871566772461 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.4395712046486434, 'bidirectional': True, 'fc_dropout': 0.6548401482945787, 'learning_rate_model': 0.0013653793118777208}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.00010328837070603024
Epoch 27: reducing lr to 0.00010339370102989106
Epoch 33: reducing lr to 0.00010021788404527646
Epoch 40: reducing lr to 9.269939857727252e-05
Epoch 43: reducing lr to 8.836199337046494e-05
Epoch 46: reducing lr to 8.344935342670224e-05
Epoch 55: reducing lr to 6.607270532884795e-05
Epoch 64: reducing lr to 4.670374214001023e-05
Epoch 72: reducing lr to 3.001382442539385e-05
Epoch 75: reducing lr to 2.4289343670598887e-05
Epoch 78: reducing lr to 1.900010373710977e-05
Epoch 81: reducing lr to 1.4229503445263299e-05
Epoch 84: reducing lr to 1.0052792066944368e-05
Epoch 87: reducing lr to 6.5358217547966685e-06
Epoch 90: reducing lr to 3.7340780610688573e-06
Epoch 93: reducing lr to 1.69173211861909e-06
Epoch 96: reducing lr to 4.4099896029537593e-07
Epoch 99: reducing lr to 1.5996937184192123e-09
[I 2024-06-21 01:01:37,304] Trial 408 finished with value: 0.9778271913528442 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.4348745371406271, 'bidirectional': True, 'fc_dropout': 0.12874679793997176, 'learning_rate_model': 0.0010377312886750963}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001679575417526418
Epoch 31: reducing lr to 0.00016508903596245184
Epoch 34: reducing lr to 0.0001614457089606522
Epoch 40: reducing lr to 0.0001505852189437138
Epoch 46: reducing lr to 0.00013555901493790518
Epoch 52: reducing lr to 0.00011731125775989537
Epoch 57: reducing lr to 0.000100466598600805
Epoch 60: reducing lr to 8.997162351220604e-05
Epoch 63: reducing lr to 7.938699544934318e-05
Epoch 66: reducing lr to 6.887967150486234e-05
Epoch 69: reducing lr to 5.861531232567469e-05
Epoch 72: reducing lr to 4.875585377901487e-05
Epoch 75: reducing lr to 3.945674072078601e-05
Epoch 78: reducing lr to 3.086465311661066e-05
Epoch 81: reducing lr to 2.311506789312277e-05
Epoch 84: reducing lr to 1.633022347102464e-05
Epoch 87: reducing lr to 1.0617093153012566e-05
Epoch 90: reducing lr to 6.065810253453262e-06
Epoch 93: reducing lr to 2.748128417079344e-06
Epoch 96: reducing lr to 7.163792431152732e-07
Epoch 99: reducing lr to 2.598616954670001e-09
[I 2024-06-21 01:01:52,272] Trial 409 finished with value: 0.9811692833900452 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.42946727448910693, 'bidirectional': True, 'fc_dropout': 0.6513302038571396, 'learning_rate_model': 0.001685739019974551}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 4.974983663404892e-05
Epoch 50: reducing lr to 4.086244567382759e-05
Epoch 55: reducing lr to 3.545984490222736e-05
Epoch 61: reducing lr to 2.8559445809829857e-05
Epoch 64: reducing lr to 2.5064925741966113e-05
Epoch 67: reducing lr to 2.1614263330006665e-05
Epoch 72: reducing lr to 1.6107794493204627e-05
Epoch 75: reducing lr to 1.3035584891667574e-05
Epoch 78: reducing lr to 1.0196959974484064e-05
Epoch 81: reducing lr to 7.636678151642798e-06
Epoch 84: reducing lr to 5.39512414020302e-06
Epoch 87: reducing lr to 3.5076394190341164e-06
Epoch 90: reducing lr to 2.004001928471058e-06
Epoch 93: reducing lr to 9.079173955990182e-07
Epoch 96: reducing lr to 2.3667495762867394e-07
Epoch 99: reducing lr to 8.585223030209941e-10
[I 2024-06-21 01:02:07,257] Trial 410 finished with value: 0.983313798904419 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.4668514172499382, 'bidirectional': True, 'fc_dropout': 0.6578728413071202, 'learning_rate_model': 0.0005569287705636171}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 3.220581032978964e-05
Epoch 55: reducing lr to 2.2343413036866556e-05
Epoch 61: reducing lr to 1.7995439506080696e-05
Epoch 65: reducing lr to 1.5063989877601862e-05
Epoch 68: reducing lr to 1.2906582952575587e-05
Epoch 73: reducing lr to 9.48981447298723e-06
Epoch 76: reducing lr to 7.599773871198428e-06
Epoch 79: reducing lr to 5.866599269913877e-06
Epoch 82: reducing lr to 4.317618738762172e-06
Epoch 85: reducing lr to 2.977265189746506e-06
Epoch 88: reducing lr to 1.8666713578097843e-06
Epoch 91: reducing lr to 1.0033585486838594e-06
Epoch 94: reducing lr to 4.009373352739697e-07
Epoch 97: reducing lr to 6.891003487854942e-08
[I 2024-06-21 01:02:21,946] Trial 411 finished with value: 0.9831280708312988 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.42099091204929034, 'bidirectional': True, 'fc_dropout': 0.6725873755306445, 'learning_rate_model': 0.00035092340609858537}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012738306069956732
Epoch 33: reducing lr to 0.00012347039209700976
Epoch 36: reducing lr to 0.00012008619894204531
Epoch 39: reducing lr to 0.0001158163235034732
Epoch 44: reducing lr to 0.00010692045432326991
Epoch 47: reducing lr to 0.00010065245913757725
Epoch 55: reducing lr to 8.140286448452066e-05
Epoch 61: reducing lr to 6.556206614610887e-05
Epoch 64: reducing lr to 5.7539923231860474e-05
Epoch 72: reducing lr to 3.697761837058916e-05
Epoch 75: reducing lr to 2.9924946184584903e-05
Epoch 78: reducing lr to 2.3408499198056807e-05
Epoch 81: reducing lr to 1.753102638785177e-05
Epoch 84: reducing lr to 1.2385236327825191e-05
Epoch 87: reducing lr to 8.052260157242313e-06
Epoch 90: reducing lr to 4.600457161046435e-06
Epoch 93: reducing lr to 2.0842470383293815e-06
Epoch 96: reducing lr to 5.433193392652804e-07
Epoch 99: reducing lr to 1.970853930207371e-09
[I 2024-06-21 01:02:35,675] Trial 412 finished with value: 0.9786843061447144 and parameters: {'hidden_size': 134, 'n_layers': 1, 'rnn_dropout': 0.45407580871497666, 'bidirectional': True, 'fc_dropout': 0.5971366447865704, 'learning_rate_model': 0.001278505232121676}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.24697077816425e-05
Epoch 33: reducing lr to 8.962942964467564e-05
Epoch 40: reducing lr to 8.290530479701341e-05
Epoch 43: reducing lr to 7.902616527488603e-05
Epoch 48: reducing lr to 7.145160938368379e-05
Epoch 51: reducing lr to 6.635808107836554e-05
Epoch 55: reducing lr to 5.909183725162162e-05
Epoch 64: reducing lr to 4.176928908606726e-05
Epoch 72: reducing lr to 2.684273361317568e-05
Epoch 75: reducing lr to 2.172306909469118e-05
Epoch 78: reducing lr to 1.6992660315772085e-05
Epoch 81: reducing lr to 1.2726094649431069e-05
Epoch 84: reducing lr to 8.990670955392317e-06
Epoch 87: reducing lr to 5.845283820570717e-06
Epoch 90: reducing lr to 3.3395565078094535e-06
Epoch 93: reducing lr to 1.5129932780750437e-06
Epoch 96: reducing lr to 3.944055061800254e-07
Epoch 99: reducing lr to 1.4306791342984584e-09
[I 2024-06-21 01:02:56,657] Trial 413 finished with value: 0.9799507856369019 and parameters: {'hidden_size': 189, 'n_layers': 1, 'rnn_dropout': 0.44691996358048075, 'bidirectional': True, 'fc_dropout': 0.6466136119449369, 'learning_rate_model': 0.0009280904742147866}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 0.00010824138494127433
Epoch 36: reducing lr to 0.00010527460280200381
Epoch 39: reducing lr to 0.00010153137964422323
Epoch 42: reducing lr to 9.707074882974293e-05
Epoch 45: reducing lr to 9.19630571941901e-05
Epoch 48: reducing lr to 8.628885832068128e-05
Epoch 51: reducing lr to 8.013763589082862e-05
Epoch 54: reducing lr to 7.360640329894201e-05
Epoch 57: reducing lr to 6.679814192358742e-05
Epoch 60: reducing lr to 5.9820252304388854e-05
Epoch 63: reducing lr to 5.2782754296112154e-05
Epoch 66: reducing lr to 4.5796654180696125e-05
Epoch 69: reducing lr to 3.8972096260402805e-05
Epoch 72: reducing lr to 3.2416748309325335e-05
Epoch 75: reducing lr to 2.6233962363767514e-05
Epoch 78: reducing lr to 2.0521262867648583e-05
Epoch 81: reducing lr to 1.536872559837816e-05
Epoch 84: reducing lr to 1.0857624327421629e-05
Epoch 87: reducing lr to 7.059083368282756e-06
Epoch 90: reducing lr to 4.033030478136214e-06
Epoch 93: reducing lr to 1.827173155903388e-06
Epoch 96: reducing lr to 4.7630558831662265e-07
Epoch 99: reducing lr to 1.727766108937552e-09
[I 2024-06-21 01:04:30,426] Trial 414 finished with value: 0.990432858467102 and parameters: {'hidden_size': 148, 'n_layers': 4, 'rnn_dropout': 0.4926423300232585, 'bidirectional': True, 'fc_dropout': 0.6086665566242658, 'learning_rate_model': 0.0011208126468958308}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 6.800667708869795e-05
Epoch 38: reducing lr to 6.462687142412302e-05
Epoch 41: reducing lr to 6.19699868462847e-05
Epoch 44: reducing lr to 5.8891080588992425e-05
Epoch 49: reducing lr to 5.295632103407841e-05
Epoch 55: reducing lr to 4.483616051647661e-05
Epoch 61: reducing lr to 3.6111153337580284e-05
Epoch 64: reducing lr to 3.16926099648498e-05
Epoch 67: reducing lr to 2.7329521118370724e-05
Epoch 72: reducing lr to 2.0367028154101595e-05
Epoch 75: reducing lr to 1.6482462860187307e-05
Epoch 78: reducing lr to 1.2893246867172199e-05
Epoch 81: reducing lr to 9.655973633382113e-06
Epoch 84: reducing lr to 6.821706429439747e-06
Epoch 87: reducing lr to 4.435131751404141e-06
Epoch 90: reducing lr to 2.5339014422652835e-06
Epoch 93: reducing lr to 1.147989513124528e-06
Epoch 96: reducing lr to 2.9925670627519535e-07
Epoch 99: reducing lr to 1.0855333375464974e-09
[I 2024-06-21 01:04:45,196] Trial 415 finished with value: 0.9826799631118774 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.5167946392127584, 'bidirectional': True, 'fc_dropout': 0.6671217054931814, 'learning_rate_model': 0.0007041922439899284}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015302033326597178
Epoch 33: reducing lr to 0.0001483201961344364
Epoch 36: reducing lr to 0.0001442548960736188
Epoch 40: reducing lr to 0.00013719300810934845
Epoch 46: reducing lr to 0.00012350315101393076
Epoch 61: reducing lr to 7.875716878042749e-05
Epoch 64: reducing lr to 6.912047944745002e-05
Epoch 67: reducing lr to 5.9604734506438975e-05
Epoch 72: reducing lr to 4.441977964240178e-05
Epoch 75: reducing lr to 3.594767791717068e-05
Epoch 78: reducing lr to 2.8119722739202876e-05
Epoch 81: reducing lr to 2.105934247168496e-05
Epoch 84: reducing lr to 1.4877904330870464e-05
Epoch 87: reducing lr to 9.672868009597989e-06
Epoch 90: reducing lr to 5.526350867164718e-06
Epoch 93: reducing lr to 2.503725178703122e-06
Epoch 96: reducing lr to 6.526684624127542e-07
Epoch 99: reducing lr to 2.3675104331849496e-09
[I 2024-06-21 01:05:00,326] Trial 416 finished with value: 0.9783706068992615 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.4173105032854424, 'bidirectional': True, 'fc_dropout': 0.6233718394504141, 'learning_rate_model': 0.0015358187786283265}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.575462180990153e-05
Epoch 33: reducing lr to 9.281344501411861e-05
Epoch 38: reducing lr to 8.820078901273364e-05
Epoch 41: reducing lr to 8.457475372869183e-05
Epoch 44: reducing lr to 8.037275608892898e-05
Epoch 48: reducing lr to 7.398987190918453e-05
Epoch 51: reducing lr to 6.87153999955769e-05
Epoch 55: reducing lr to 6.119102854139865e-05
Epoch 64: reducing lr to 4.325311040399802e-05
Epoch 72: reducing lr to 2.779630072523966e-05
Epoch 75: reducing lr to 2.2494764129939877e-05
Epoch 78: reducing lr to 1.759631127983193e-05
Epoch 81: reducing lr to 1.3178179206003733e-05
Epoch 84: reducing lr to 9.310057507522121e-06
Epoch 87: reducing lr to 6.052932955427832e-06
Epoch 90: reducing lr to 3.458191640155414e-06
Epoch 93: reducing lr to 1.5667411806373206e-06
Epoch 96: reducing lr to 4.0841645323667996e-07
Epoch 99: reducing lr to 1.4815028913069476e-09
[I 2024-06-21 01:05:20,241] Trial 417 finished with value: 0.9791710376739502 and parameters: {'hidden_size': 181, 'n_layers': 1, 'rnn_dropout': 0.40444679245883225, 'bidirectional': True, 'fc_dropout': 6.767173423821715e-06, 'learning_rate_model': 0.0009610601622497148}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012489698258495897
Epoch 36: reducing lr to 0.00011774253040859378
Epoch 40: reducing lr to 0.00011197853500180464
Epoch 44: reducing lr to 0.00010483373572789888
Epoch 48: reducing lr to 9.650825796848573e-05
Epoch 55: reducing lr to 7.981416125545715e-05
Epoch 61: reducing lr to 6.428252067986541e-05
Epoch 64: reducing lr to 5.6416942334107105e-05
Epoch 67: reducing lr to 4.865008021314569e-05
Epoch 72: reducing lr to 3.6255942762728884e-05
Epoch 75: reducing lr to 2.934091441943685e-05
Epoch 78: reducing lr to 2.2951646008688323e-05
Epoch 81: reducing lr to 1.7188881201591482e-05
Epoch 84: reducing lr to 1.2143519220308985e-05
Epoch 87: reducing lr to 7.895107804016424e-06
Epoch 90: reducing lr to 4.5106721001249855e-06
Epoch 93: reducing lr to 2.0435697228451045e-06
Epoch 96: reducing lr to 5.327156191852704e-07
Epoch 99: reducing lr to 1.9323896572080375e-09
[I 2024-06-21 01:05:38,427] Trial 418 finished with value: 0.9776156544685364 and parameters: {'hidden_size': 171, 'n_layers': 1, 'rnn_dropout': 0.4288857549952394, 'bidirectional': True, 'fc_dropout': 0.5894743037666187, 'learning_rate_model': 0.0012535532184117341}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001561915815604359
Epoch 33: reducing lr to 0.00015139403710044948
Epoch 36: reducing lr to 0.0001472444863024312
Epoch 40: reducing lr to 0.0001400362452379917
Epoch 46: reducing lr to 0.00012606267463183529
Epoch 61: reducing lr to 8.038936060644867e-05
Epoch 64: reducing lr to 7.055295706582817e-05
Epoch 67: reducing lr to 6.084000441215134e-05
Epoch 72: reducing lr to 4.5340351094736776e-05
Epoch 75: reducing lr to 3.6692670493331314e-05
Epoch 78: reducing lr to 2.8702485963371934e-05
Epoch 81: reducing lr to 2.1495783841733403e-05
Epoch 84: reducing lr to 1.5186239358820413e-05
Epoch 87: reducing lr to 9.873331997117211e-06
Epoch 90: reducing lr to 5.640880945540937e-06
Epoch 93: reducing lr to 2.5556132777113018e-06
Epoch 96: reducing lr to 6.661945978230031e-07
Epoch 99: reducing lr to 2.416575568950016e-09
[I 2024-06-21 01:05:53,043] Trial 419 finished with value: 0.9795729517936707 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.44152783985270155, 'bidirectional': True, 'fc_dropout': 0.606368415434147, 'learning_rate_model': 0.001567647638090196}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 4.05518616019887e-05
Epoch 38: reducing lr to 3.85365093245542e-05
Epoch 41: reducing lr to 3.6952229364037455e-05
Epoch 50: reducing lr to 3.0808828222606776e-05
Epoch 55: reducing lr to 2.6735459720481996e-05
Epoch 61: reducing lr to 2.1532804646870665e-05
Epoch 64: reducing lr to 1.8898061015746706e-05
Epoch 67: reducing lr to 1.629638449464809e-05
Epoch 73: reducing lr to 1.1355228146155155e-05
Epoch 76: reducing lr to 9.093662095533263e-06
Epoch 79: reducing lr to 7.019797209056454e-06
Epoch 82: reducing lr to 5.166333437425579e-06
Epoch 85: reducing lr to 3.5625064723246846e-06
Epoch 88: reducing lr to 2.2336031122799185e-06
Epoch 91: reducing lr to 1.2005888276458455e-06
Epoch 94: reducing lr to 4.797496228516139e-07
Epoch 97: reducing lr to 8.245568655032716e-08
[I 2024-06-21 01:06:12,525] Trial 420 finished with value: 0.9829869270324707 and parameters: {'hidden_size': 184, 'n_layers': 1, 'rnn_dropout': 0.47404050112442403, 'bidirectional': True, 'fc_dropout': 0.639017333630696, 'learning_rate_model': 0.00041990445117953305}. Best is trial 235 with value: 0.9706074595451355.
Epoch 55: reducing lr to 1.7136434927832993e-05
Epoch 65: reducing lr to 1.1553431065572837e-05
Epoch 68: reducing lr to 9.898792925796778e-06
Epoch 73: reducing lr to 7.278278744846561e-06
Epoch 76: reducing lr to 5.828699053056635e-06
Epoch 79: reducing lr to 4.499428823639132e-06
Epoch 87: reducing lr to 1.6951127344272355e-06
Epoch 90: reducing lr to 9.684602044138973e-07
Epoch 93: reducing lr to 4.3876298422706885e-07
[I 2024-06-21 01:06:27,079] Trial 421 finished with value: 0.9844753742218018 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.4052951184922533, 'bidirectional': True, 'fc_dropout': 0.5812864397040678, 'learning_rate_model': 0.00026914312971521135}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.052177321354439e-05
Epoch 40: reducing lr to 7.448091745679709e-05
Epoch 43: reducing lr to 7.099595504988756e-05
Epoch 49: reducing lr to 6.270173834501864e-05
Epoch 52: reducing lr to 5.8023291842698875e-05
Epoch 55: reducing lr to 5.308724530335563e-05
Epoch 64: reducing lr to 3.75249205134162e-05
Epoch 72: reducing lr to 2.411512063616166e-05
Epoch 75: reducing lr to 1.9515688653596013e-05
Epoch 78: reducing lr to 1.5265958353921954e-05
Epoch 81: reducing lr to 1.1432937945917886e-05
Epoch 84: reducing lr to 8.0770877442564e-06
Epoch 87: reducing lr to 5.251317787413319e-06
Epoch 90: reducing lr to 3.000208891451128e-06
Epoch 93: reducing lr to 1.3592511086341964e-06
Epoch 96: reducing lr to 3.543281581585688e-07
Epoch 99: reducing lr to 1.285301281627859e-09
[I 2024-06-21 01:06:43,149] Trial 422 finished with value: 0.9809772372245789 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.32493941781540275, 'bidirectional': True, 'fc_dropout': 0.6536670437192575, 'learning_rate_model': 0.0008337829548022041}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010977605298705183
Epoch 33: reducing lr to 0.00010640419715727121
Epoch 38: reducing lr to 0.00010111610599208091
Epoch 41: reducing lr to 9.695910725979903e-05
Epoch 44: reducing lr to 9.214180751138756e-05
Epoch 48: reducing lr to 8.482427214148244e-05
Epoch 55: reducing lr to 7.015128319161899e-05
Epoch 61: reducing lr to 5.65000150543587e-05
Epoch 64: reducing lr to 4.9586700357821314e-05
Epoch 72: reducing lr to 3.1866536816527925e-05
Epoch 75: reducing lr to 2.5788691682808832e-05
Epoch 78: reducing lr to 2.0172954191875303e-05
Epoch 81: reducing lr to 1.5107871259343632e-05
Epoch 84: reducing lr to 1.0673337191882313e-05
Epoch 87: reducing lr to 6.939269105581797e-06
Epoch 90: reducing lr to 3.964577600052972e-06
Epoch 93: reducing lr to 1.7961604318597266e-06
Epoch 96: reducing lr to 4.6822122383087355e-07
Epoch 99: reducing lr to 1.6984406268991808e-09
[I 2024-06-21 01:06:58,908] Trial 423 finished with value: 0.9788810014724731 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.41169954329581415, 'bidirectional': True, 'fc_dropout': 0.6754605296691265, 'learning_rate_model': 0.0011017890238689233}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013617465524273212
Epoch 36: reducing lr to 0.00012837418610085811
Epoch 40: reducing lr to 0.00012208972613156878
Epoch 46: reducing lr to 0.00010990695583887504
Epoch 61: reducing lr to 7.008696215507011e-05
Epoch 64: reducing lr to 6.151115513915806e-05
Epoch 67: reducing lr to 5.3042978008294967e-05
Epoch 72: reducing lr to 3.952970203148368e-05
Epoch 75: reducing lr to 3.199027568864972e-05
Epoch 78: reducing lr to 2.5024083190803604e-05
Epoch 81: reducing lr to 1.8740964939187266e-05
Epoch 84: reducing lr to 1.3240027973727954e-05
Epoch 87: reducing lr to 8.608002860155698e-06
Epoch 90: reducing lr to 4.917966835025068e-06
Epoch 93: reducing lr to 2.228095480878581e-06
Epoch 96: reducing lr to 5.808176009026306e-07
Epoch 99: reducing lr to 2.106876322537088e-09
[I 2024-06-21 01:07:17,648] Trial 424 finished with value: 0.9794604182243347 and parameters: {'hidden_size': 177, 'n_layers': 1, 'rnn_dropout': 0.37289008029266935, 'bidirectional': True, 'fc_dropout': 0.6299658346723745, 'learning_rate_model': 0.001366743806076484}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.00019503251427092246
Epoch 27: reducing lr to 0.00019523140246860751
Epoch 30: reducing lr to 0.0001929822221580247
Epoch 33: reducing lr to 0.00018923471990754236
Epoch 36: reducing lr to 0.00018404799592524902
Epoch 39: reducing lr to 0.00017750384660381946
Epoch 42: reducing lr to 0.00016970547795538526
Epoch 45: reducing lr to 0.0001607758749523164
Epoch 48: reducing lr to 0.00015085586667534643
Epoch 51: reducing lr to 0.00014010189439169821
Epoch 54: reducing lr to 0.00012868356330837714
Epoch 57: reducing lr to 0.00011678091225562539
Epoch 60: reducing lr to 0.00010458170593217347
Epoch 63: reducing lr to 9.227828829603867e-05
Epoch 66: reducing lr to 8.006472784220623e-05
Epoch 69: reducing lr to 6.813358609600473e-05
Epoch 72: reducing lr to 5.6673094953068484e-05
Epoch 75: reducing lr to 4.586394125191528e-05
Epoch 78: reducing lr to 3.587662365014462e-05
Epoch 81: reducing lr to 2.6868618555858692e-05
Epoch 84: reducing lr to 1.8982014130507293e-05
Epoch 87: reducing lr to 1.2341154584503143e-05
Epoch 90: reducing lr to 7.050809570874881e-06
Epoch 93: reducing lr to 3.194384482123444e-06
Epoch 96: reducing lr to 8.327088076745797e-07
Epoch 99: reducing lr to 3.0205945338729336e-09
[I 2024-06-21 01:08:51,600] Trial 425 finished with value: 1.0554540157318115 and parameters: {'hidden_size': 148, 'n_layers': 4, 'rnn_dropout': 0.5072334430891039, 'bidirectional': True, 'fc_dropout': 0.5945533615370496, 'learning_rate_model': 0.001959478506481005}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 6.526955198869114e-05
Epoch 40: reducing lr to 5.8518537923412856e-05
Epoch 50: reducing lr to 4.806469203902324e-05
Epoch 53: reducing lr to 4.431482946794374e-05
Epoch 56: reducing lr to 4.038265074600452e-05
Epoch 61: reducing lr to 3.3593216094108145e-05
Epoch 64: reducing lr to 2.948276631274232e-05
Epoch 67: reducing lr to 2.5423904357064167e-05
Epoch 72: reducing lr to 1.894688800380933e-05
Epoch 75: reducing lr to 1.5333183392100595e-05
Epoch 78: reducing lr to 1.1994234139092187e-05
Epoch 81: reducing lr to 8.982687587760979e-06
Epoch 84: reducing lr to 6.3460464990535106e-06
Epoch 87: reducing lr to 4.1258814953358884e-06
Epoch 90: reducing lr to 2.357219054053433e-06
Epoch 93: reducing lr to 1.0679431761053327e-06
Epoch 96: reducing lr to 2.783903108143407e-07
Epoch 99: reducing lr to 1.0098419079684201e-09
[I 2024-06-21 01:09:06,626] Trial 426 finished with value: 0.9831622242927551 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.437058661509735, 'bidirectional': True, 'fc_dropout': 0.5455020543339734, 'learning_rate_model': 0.0006550907417163576}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001023932052794978
Epoch 40: reducing lr to 9.180238692746458e-05
Epoch 43: reducing lr to 8.750695289911293e-05
Epoch 48: reducing lr to 7.911952446578902e-05
Epoch 55: reducing lr to 6.543334857655038e-05
Epoch 61: reducing lr to 5.270017897653906e-05
Epoch 64: reducing lr to 4.625181039684695e-05
Epoch 72: reducing lr to 2.9723393736758627e-05
Epoch 75: reducing lr to 2.405430628553371e-05
Epoch 78: reducing lr to 1.8816248020014253e-05
Epoch 81: reducing lr to 1.4091810746526484e-05
Epoch 84: reducing lr to 9.955515582570736e-06
Epoch 87: reducing lr to 6.472577458230549e-06
Epoch 90: reducing lr to 3.6979450162651213e-06
Epoch 93: reducing lr to 1.6753619647448797e-06
Epoch 96: reducing lr to 4.367316056953308e-07
Epoch 99: reducing lr to 1.5842141799999113e-09
[I 2024-06-21 01:09:27,883] Trial 427 finished with value: 0.9791538715362549 and parameters: {'hidden_size': 194, 'n_layers': 1, 'rnn_dropout': 0.38950968383376744, 'bidirectional': True, 'fc_dropout': 0.615808424336385, 'learning_rate_model': 0.0010276896155941666}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.00023677709008612947
Epoch 27: reducing lr to 0.0002399404589162081
Epoch 30: reducing lr to 0.00023717620404182477
Epoch 33: reducing lr to 0.00023257050332769485
Epoch 36: reducing lr to 0.00022619599125204003
Epoch 39: reducing lr to 0.00021815319602778053
Epoch 42: reducing lr to 0.0002085689584069708
Epoch 45: reducing lr to 0.00019759442759172244
Epoch 48: reducing lr to 0.0001854026832906923
Epoch 51: reducing lr to 0.0001721859926749209
Epoch 54: reducing lr to 0.0001581528014692705
Epoch 57: reducing lr to 0.00014352437837849256
Epoch 60: reducing lr to 0.00012853148724186698
Epoch 63: reducing lr to 0.00011341051983332401
Epoch 66: reducing lr to 9.839998739213734e-05
Epoch 69: reducing lr to 8.373654908364983e-05
Epoch 72: reducing lr to 6.965154293468553e-05
Epoch 75: reducing lr to 5.636703405570249e-05
Epoch 78: reducing lr to 4.409256622721828e-05
Epoch 81: reducing lr to 3.302167881406226e-05
Epoch 84: reducing lr to 2.3328998941961808e-05
Epoch 87: reducing lr to 1.5167346323999732e-05
Epoch 90: reducing lr to 8.665483435425e-06
Epoch 93: reducing lr to 3.925915958723691e-06
Epoch 96: reducing lr to 1.0234036683167812e-06
Epoch 99: reducing lr to 3.7123271640331775e-09
[I 2024-06-21 01:10:25,899] Trial 428 finished with value: 1.1220788955688477 and parameters: {'hidden_size': 137, 'n_layers': 3, 'rnn_dropout': 0.2781884585593643, 'bidirectional': True, 'fc_dropout': 0.5749760173073585, 'learning_rate_model': 0.002408209776381123}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013446280047897658
Epoch 33: reducing lr to 0.00013033267222835402
Epoch 36: reducing lr to 0.00012676039121642624
Epoch 40: reducing lr to 0.00012055493333983089
Epoch 44: reducing lr to 0.00011286291629229318
Epoch 48: reducing lr to 0.0001038997929910987
Epoch 61: reducing lr to 6.920589732088014e-05
Epoch 64: reducing lr to 6.07378969747707e-05
Epoch 67: reducing lr to 5.2376173495917626e-05
Epoch 72: reducing lr to 3.903277322625327e-05
Epoch 75: reducing lr to 3.1588125187634276e-05
Epoch 78: reducing lr to 2.470950485798215e-05
Epoch 81: reducing lr to 1.850537183229568e-05
Epoch 84: reducing lr to 1.3073587273594113e-05
Epoch 87: reducing lr to 8.499791455644952e-06
Epoch 90: reducing lr to 4.856142959359477e-06
Epoch 93: reducing lr to 2.2000860406766117e-06
Epoch 96: reducing lr to 5.73516129309345e-07
Epoch 99: reducing lr to 2.080390731191116e-09
[I 2024-06-21 01:10:42,017] Trial 429 finished with value: 0.9771947860717773 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.30735910886376205, 'bidirectional': True, 'fc_dropout': 0.7043523759260478, 'learning_rate_model': 0.001349562437846875}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00016017374416585136
Epoch 33: reducing lr to 0.00015525388452116988
Epoch 36: reducing lr to 0.00015099853937846247
Epoch 40: reducing lr to 0.00014360652152060777
Epoch 46: reducing lr to 0.00012927668952202514
Epoch 61: reducing lr to 8.243891732699694e-05
Epoch 64: reducing lr to 7.235173101076265e-05
Epoch 67: reducing lr to 6.239114300786134e-05
Epoch 72: reducing lr to 4.6496320250321545e-05
Epoch 75: reducing lr to 3.762816380783391e-05
Epoch 78: reducing lr to 2.9434266544270762e-05
Epoch 81: reducing lr to 2.204382686513747e-05
Epoch 84: reducing lr to 1.5573418193219916e-05
Epoch 87: reducing lr to 1.012505627749757e-05
Epoch 90: reducing lr to 5.784697308359706e-06
Epoch 93: reducing lr to 2.620769591046065e-06
Epoch 96: reducing lr to 6.831794774745043e-07
Epoch 99: reducing lr to 2.4781870640441446e-09
[I 2024-06-21 01:10:58,081] Trial 430 finished with value: 0.9785199165344238 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.31648180605808746, 'bidirectional': True, 'fc_dropout': 0.6921750507262113, 'learning_rate_model': 0.0016076153990956452}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013641971025650022
Epoch 35: reducing lr to 0.00012992182291517027
Epoch 38: reducing lr to 0.00012565791451194083
Epoch 47: reducing lr to 0.00010779281983604556
Epoch 50: reducing lr to 0.00010045988001675804
Epoch 53: reducing lr to 9.262230261869357e-05
Epoch 56: reducing lr to 8.440366673749942e-05
Epoch 59: reducing lr to 7.593362161546364e-05
Epoch 62: reducing lr to 6.73457138603706e-05
Epoch 65: reducing lr to 5.8775405209957973e-05
Epoch 68: reducing lr to 5.035781682524146e-05
Epoch 71: reducing lr to 4.2225748957197e-05
Epoch 74: reducing lr to 3.4507412417470034e-05
Epoch 77: reducing lr to 2.7324552724285928e-05
Epoch 80: reducing lr to 2.0790426608779792e-05
Epoch 83: reducing lr to 1.5008100394373471e-05
Epoch 86: reducing lr to 1.0068741015668805e-05
Epoch 89: reducing lr to 6.050274374381722e-06
Epoch 92: reducing lr to 3.0160541883145555e-06
Epoch 95: reducing lr to 1.0139408619808718e-06
Epoch 98: reducing lr to 7.550358987961585e-08
[I 2024-06-21 01:11:42,147] Trial 431 finished with value: 0.9946141242980957 and parameters: {'hidden_size': 155, 'n_layers': 2, 'rnn_dropout': 0.3286605490921565, 'bidirectional': True, 'fc_dropout': 0.7044096664765137, 'learning_rate_model': 0.0013692033490921686}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.281911653191299e-05
Epoch 40: reducing lr to 7.660591087455946e-05
Epoch 43: reducing lr to 7.302152001766973e-05
Epoch 49: reducing lr to 6.449066342563019e-05
Epoch 52: reducing lr to 5.967873752533511e-05
Epoch 55: reducing lr to 5.460186207619819e-05
Epoch 58: reducing lr to 4.9340121482503546e-05
Epoch 72: reducing lr to 2.4803142137106707e-05
Epoch 75: reducing lr to 2.0072485096873536e-05
Epoch 78: reducing lr to 1.5701506976651206e-05
Epoch 81: reducing lr to 1.175912777695553e-05
Epoch 84: reducing lr to 8.3075327881321e-06
Epoch 87: reducing lr to 5.401141609593079e-06
Epoch 90: reducing lr to 3.0858069797123845e-06
Epoch 93: reducing lr to 1.3980315071249823e-06
Epoch 96: reducing lr to 3.6443739189957224e-07
Epoch 99: reducing lr to 1.3219718390930155e-09
[I 2024-06-21 01:11:57,294] Trial 432 finished with value: 0.9815088510513306 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.26559217616910746, 'bidirectional': True, 'fc_dropout': 0.5950690876883173, 'learning_rate_model': 0.0008575713740550264}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00016770238869891714
Epoch 33: reducing lr to 0.00016255128095168128
Epoch 36: reducing lr to 0.00015809592187340793
Epoch 40: reducing lr to 0.00015035645709081727
Epoch 46: reducing lr to 0.00013535308017450975
Epoch 61: reducing lr to 8.631379274729744e-05
Epoch 64: reducing lr to 7.575247853632468e-05
Epoch 67: reducing lr to 6.53237131376542e-05
Epoch 70: reducing lr to 5.519203405338555e-05
Epoch 73: reducing lr to 4.5517192250567006e-05
Epoch 76: reducing lr to 3.645175249114064e-05
Epoch 79: reducing lr to 2.8138708884753253e-05
Epoch 82: reducing lr to 2.070913849330769e-05
Epoch 85: reducing lr to 1.4280231969588134e-05
Epoch 88: reducing lr to 8.953350911538844e-06
Epoch 91: reducing lr to 4.812534964376055e-06
Epoch 94: reducing lr to 1.9230662329640423e-06
Epoch 97: reducing lr to 3.3052187842956895e-07
[I 2024-06-21 01:12:14,518] Trial 433 finished with value: 0.9798928499221802 and parameters: {'hidden_size': 159, 'n_layers': 1, 'rnn_dropout': 0.31362795625242945, 'bidirectional': True, 'fc_dropout': 0.6422727225991198, 'learning_rate_model': 0.0016831781259876483}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013153683721063464
Epoch 36: reducing lr to 0.00012400203539415623
Epoch 40: reducing lr to 0.00011793161071444122
Epoch 44: reducing lr to 0.00011040697497430142
Epoch 47: reducing lr to 0.00010393458957352972
Epoch 61: reducing lr to 6.769994985591418e-05
Epoch 72: reducing lr to 3.818340477405156e-05
Epoch 75: reducing lr to 3.0900755196190026e-05
Epoch 78: reducing lr to 2.417181634237909e-05
Epoch 81: reducing lr to 1.8102687684297664e-05
Epoch 84: reducing lr to 1.278910088768116e-05
Epoch 87: reducing lr to 8.31483266035592e-06
Epoch 90: reducing lr to 4.750471384215365e-06
Epoch 93: reducing lr to 2.1522113056623257e-06
Epoch 96: reducing lr to 5.610361934298147e-07
Epoch 99: reducing lr to 2.035120613068202e-09
[I 2024-06-21 01:12:29,334] Trial 434 finished with value: 0.9762791395187378 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.30652125928631074, 'bidirectional': True, 'fc_dropout': 0.4248483055428689, 'learning_rate_model': 0.0013201954299650828}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00018507367025140064
Epoch 31: reducing lr to 0.00018191284228745184
Epoch 34: reducing lr to 0.00017789823304089513
Epoch 40: reducing lr to 0.0001659309779406527
Epoch 46: reducing lr to 0.00014937349146947697
Epoch 49: reducing lr to 0.0001396889447314184
Epoch 52: reducing lr to 0.0001292661514861118
Epoch 55: reducing lr to 0.00011826946861215435
Epoch 61: reducing lr to 9.525451927664885e-05
Epoch 64: reducing lr to 8.359922206312964e-05
Epoch 67: reducing lr to 7.209020359596924e-05
Epoch 70: reducing lr to 6.090904482725661e-05
Epoch 73: reducing lr to 5.023204436565929e-05
Epoch 76: reducing lr to 4.02275702389837e-05
Epoch 79: reducing lr to 3.105342845644637e-05
Epoch 82: reducing lr to 2.2854273564236753e-05
Epoch 85: reducing lr to 1.5759435289845292e-05
Epoch 88: reducing lr to 9.880774669358783e-06
Epoch 91: reducing lr to 5.3110365092612705e-06
Epoch 94: reducing lr to 2.1222650949246324e-06
Epoch 97: reducing lr to 3.647586514058309e-07
[I 2024-06-21 01:12:43,574] Trial 435 finished with value: 0.9834800958633423 and parameters: {'hidden_size': 140, 'n_layers': 1, 'rnn_dropout': 0.27720528598360983, 'bidirectional': True, 'fc_dropout': 0.4287658706867108, 'learning_rate_model': 0.0018575284221066073}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013204033486427744
Epoch 36: reducing lr to 0.00012447669127909217
Epoch 40: reducing lr to 0.0001183830301840302
Epoch 44: reducing lr to 0.00011082959158896384
Epoch 47: reducing lr to 0.00010433243114469962
Epoch 61: reducing lr to 6.795909221197984e-05
Epoch 72: reducing lr to 3.8329563486086274e-05
Epoch 75: reducing lr to 3.1019037329674476e-05
Epoch 78: reducing lr to 2.426434139521415e-05
Epoch 81: reducing lr to 1.8171981282706726e-05
Epoch 84: reducing lr to 1.2838055100247757e-05
Epoch 87: reducing lr to 8.346660236749717e-06
Epoch 90: reducing lr to 4.768655272823071e-06
Epoch 93: reducing lr to 2.1604495556121033e-06
Epoch 96: reducing lr to 5.631837318151962e-07
Epoch 99: reducing lr to 2.042910662402968e-09
[I 2024-06-21 01:12:58,386] Trial 436 finished with value: 0.9763141870498657 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.2890476579871458, 'bidirectional': True, 'fc_dropout': 0.4097396133901438, 'learning_rate_model': 0.0013252488835483777}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.000138188377250534
Epoch 33: reducing lr to 0.0001339438150462881
Epoch 36: reducing lr to 0.00013027255642038754
Epoch 39: reducing lr to 0.00012564048717446307
Epoch 46: reducing lr to 0.00011153223666215853
Epoch 61: reducing lr to 7.112339333163571e-05
Epoch 64: reducing lr to 6.242076909491395e-05
Epoch 67: reducing lr to 5.3827366351223065e-05
Epoch 72: reducing lr to 4.0114258906628744e-05
Epoch 75: reducing lr to 3.24633411212374e-05
Epoch 78: reducing lr to 2.5394134041724143e-05
Epoch 81: reducing lr to 1.9018102365958883e-05
Epoch 84: reducing lr to 1.3435818707819257e-05
Epoch 87: reducing lr to 8.7352961863023e-06
Epoch 90: reducing lr to 4.9906926886846526e-06
Epoch 93: reducing lr to 2.2610440857223287e-06
Epoch 96: reducing lr to 5.894066087717541e-07
Epoch 99: reducing lr to 2.1380323640858566e-09
[I 2024-06-21 01:13:12,378] Trial 437 finished with value: 0.9777585864067078 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.2886202365978869, 'bidirectional': True, 'fc_dropout': 0.40566640450240493, 'learning_rate_model': 0.0013869549244848046}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00020483037245412763
Epoch 31: reducing lr to 0.00020133212460374678
Epoch 40: reducing lr to 0.0001836441886471115
Epoch 46: reducing lr to 0.00016531918263092246
Epoch 52: reducing lr to 0.00014306537455406398
Epoch 57: reducing lr to 0.00012252269589006738
Epoch 60: reducing lr to 0.0001097236894634286
Epoch 63: reducing lr to 9.681534795175301e-05
Epoch 66: reducing lr to 8.400128164317456e-05
Epoch 69: reducing lr to 7.148351976278028e-05
Epoch 72: reducing lr to 5.945954903044729e-05
Epoch 75: reducing lr to 4.811894014004538e-05
Epoch 78: reducing lr to 3.764057468079356e-05
Epoch 81: reducing lr to 2.8189671725630123e-05
Epoch 84: reducing lr to 1.991530550474079e-05
Epoch 87: reducing lr to 1.2947933877923214e-05
Epoch 90: reducing lr to 7.397477722558736e-06
Epoch 93: reducing lr to 3.3514432358812493e-06
Epoch 96: reducing lr to 8.736507194289245e-07
Epoch 99: reducing lr to 3.16910853267148e-09
[I 2024-06-21 01:13:26,116] Trial 438 finished with value: 0.9842969179153442 and parameters: {'hidden_size': 134, 'n_layers': 1, 'rnn_dropout': 0.298073808800101, 'bidirectional': True, 'fc_dropout': 0.4220366140396261, 'learning_rate_model': 0.0020558204634262118}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001271884610970926
Epoch 33: reducing lr to 0.0001232817697708738
Epoch 36: reducing lr to 0.00011990274655482457
Epoch 40: reducing lr to 0.00011403299942092937
Epoch 44: reducing lr to 0.00010675711488242545
Epoch 47: reducing lr to 0.00010049869514078911
Epoch 55: reducing lr to 8.127850756467596e-05
Epoch 61: reducing lr to 6.54619087787212e-05
Epoch 64: reducing lr to 5.7452021071825604e-05
Epoch 72: reducing lr to 3.6921128678821194e-05
Epoch 75: reducing lr to 2.9879230666370658e-05
Epoch 78: reducing lr to 2.337273867688975e-05
Epoch 81: reducing lr to 1.7504244720436082e-05
Epoch 84: reducing lr to 1.2366315742523537e-05
Epoch 87: reducing lr to 8.03995894060475e-06
Epoch 90: reducing lr to 4.593429169021243e-06
Epoch 93: reducing lr to 2.081062991385525e-06
Epoch 96: reducing lr to 5.424893252362669e-07
Epoch 99: reducing lr to 1.967843110814091e-09
[I 2024-06-21 01:13:40,734] Trial 439 finished with value: 0.9783878326416016 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.29061332331289935, 'bidirectional': True, 'fc_dropout': 0.4441034112295975, 'learning_rate_model': 0.0012765520948005411}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00016430685861194364
Epoch 33: reducing lr to 0.0001592600471807751
Epoch 36: reducing lr to 0.00015489489734708025
Epoch 40: reducing lr to 0.00014731213626877337
Epoch 46: reducing lr to 0.000132612538076912
Epoch 61: reducing lr to 8.456616659558875e-05
Epoch 64: reducing lr to 7.421869108088956e-05
Epoch 67: reducing lr to 6.400108061540701e-05
Epoch 70: reducing lr to 5.407454125174161e-05
Epoch 73: reducing lr to 4.4595589422125266e-05
Epoch 76: reducing lr to 3.571370085534197e-05
Epoch 79: reducing lr to 2.7568974408291482e-05
Epoch 82: reducing lr to 2.0289833178846298e-05
Epoch 85: reducing lr to 1.3991095018839328e-05
Epoch 88: reducing lr to 8.772069221783416e-06
Epoch 91: reducing lr to 4.715093852219413e-06
Epoch 94: reducing lr to 1.8841292249468677e-06
Epoch 97: reducing lr to 3.2382968405289623e-07
[I 2024-06-21 01:13:54,758] Trial 440 finished with value: 0.9794144034385681 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.31414542336434326, 'bidirectional': True, 'fc_dropout': 0.4323979791565591, 'learning_rate_model': 0.001649098218045564}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011944729959572072
Epoch 36: reducing lr to 0.00011260502066418416
Epoch 40: reducing lr to 0.00010709252811253451
Epoch 43: reducing lr to 0.00010208166832083596
Epoch 46: reducing lr to 9.640625899392849e-05
Epoch 55: reducing lr to 7.633159612144113e-05
Epoch 64: reducing lr to 5.395527797216882e-05
Epoch 72: reducing lr to 3.46739718420268e-05
Epoch 75: reducing lr to 2.8060669861954987e-05
Epoch 78: reducing lr to 2.195018710840237e-05
Epoch 81: reducing lr to 1.6438871461166966e-05
Epoch 84: reducing lr to 1.161365589811554e-05
Epoch 87: reducing lr to 7.550617218197173e-06
Epoch 90: reducing lr to 4.313856032151843e-06
Epoch 93: reducing lr to 1.9544017787890114e-06
Epoch 96: reducing lr to 5.094714127369844e-07
Epoch 99: reducing lr to 1.8480728801017337e-09
[I 2024-06-21 01:14:09,916] Trial 441 finished with value: 0.9773805737495422 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.25688982634864144, 'bidirectional': True, 'fc_dropout': 0.3960751191613484, 'learning_rate_model': 0.0011988563994086302}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001448319370211951
Epoch 36: reducing lr to 0.0001365355543097583
Epoch 40: reducing lr to 0.0001298515608099259
Epoch 46: reducing lr to 0.00011689427285770047
Epoch 57: reducing lr to 8.663363329474669e-05
Epoch 61: reducing lr to 7.454272948777496e-05
Epoch 64: reducing lr to 6.5421716921813e-05
Epoch 67: reducing lr to 5.641517679351003e-05
Epoch 72: reducing lr to 4.2042796472554415e-05
Epoch 75: reducing lr to 3.402405231407015e-05
Epoch 78: reducing lr to 2.661498524995933e-05
Epoch 81: reducing lr to 1.9932418767284977e-05
Epoch 84: reducing lr to 1.4081760620078152e-05
Epoch 87: reducing lr to 9.155255255818796e-06
Epoch 90: reducing lr to 5.230625784607523e-06
Epoch 93: reducing lr to 2.369746292278847e-06
Epoch 96: reducing lr to 6.177429863492897e-07
Epoch 99: reducing lr to 2.2408206454662197e-09
[I 2024-06-21 01:14:25,029] Trial 442 finished with value: 0.9781379699707031 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.30061835850794083, 'bidirectional': True, 'fc_dropout': 0.6626243625426087, 'learning_rate_model': 0.001453634323457137}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.0002349963716520364
Epoch 27: reducing lr to 0.0002381359498814809
Epoch 31: reducing lr to 0.00023406888421738918
Epoch 34: reducing lr to 0.00022890325052658318
Epoch 39: reducing lr to 0.00021651254144636946
Epoch 42: reducing lr to 0.00020700038355507197
Epoch 45: reducing lr to 0.00019610838838261374
Epoch 48: reducing lr to 0.00018400833396514752
Epoch 51: reducing lr to 0.00017089104149895488
Epoch 54: reducing lr to 0.00015696338906084304
Epoch 57: reducing lr to 0.00014244498127032062
Epoch 60: reducing lr to 0.0001275648464718088
Epoch 63: reducing lr to 0.00011255759861863293
Epoch 66: reducing lr to 9.765995518969761e-05
Epoch 69: reducing lr to 8.310679551878275e-05
Epoch 72: reducing lr to 6.912771781958802e-05
Epoch 75: reducing lr to 5.594311712783743e-05
Epoch 78: reducing lr to 4.376096131789738e-05
Epoch 81: reducing lr to 3.2773334212109816e-05
Epoch 84: reducing lr to 2.3153549626110443e-05
Epoch 87: reducing lr to 1.5053277968883139e-05
Epoch 90: reducing lr to 8.600313337726048e-06
Epoch 93: reducing lr to 3.89639050541288e-06
Epoch 96: reducing lr to 1.0157070040110219e-06
Epoch 99: reducing lr to 3.684408038037712e-09
[I 2024-06-21 01:14:40,804] Trial 443 finished with value: 0.9939113855361938 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.30460855699791456, 'bidirectional': True, 'fc_dropout': 0.3992811240398393, 'learning_rate_model': 0.002390098465272413}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.00010031894442046342
Epoch 33: reducing lr to 9.733673085122617e-05
Epoch 40: reducing lr to 9.003439351513423e-05
Epoch 43: reducing lr to 8.582168390517081e-05
Epoch 46: reducing lr to 8.105027691997663e-05
Epoch 55: reducing lr to 6.417318821360185e-05
Epoch 64: reducing lr to 4.536106126899923e-05
Epoch 72: reducing lr to 2.9150960207767583e-05
Epoch 75: reducing lr to 2.3591051935898973e-05
Epoch 78: reducing lr to 1.8453871793670123e-05
Epoch 81: reducing lr to 1.382042097768149e-05
Epoch 86: reducing lr to 7.411799385347788e-06
Epoch 89: reducing lr to 4.453726619787243e-06
Epoch 92: reducing lr to 2.220177134791535e-06
Epoch 95: reducing lr to 7.463819206307912e-07
Epoch 98: reducing lr to 5.5579685701565885e-08
[I 2024-06-21 01:14:55,785] Trial 444 finished with value: 0.9773036241531372 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.3320784805078199, 'bidirectional': True, 'fc_dropout': 0.708895049629352, 'learning_rate_model': 0.0010078976632157779}. Best is trial 235 with value: 0.9706074595451355.
Epoch 35: reducing lr to 0.00012015174008973069
Epoch 47: reducing lr to 9.968683152587936e-05
Epoch 50: reducing lr to 9.290532662168829e-05
Epoch 53: reducing lr to 8.565713273604484e-05
Epoch 56: reducing lr to 7.805653585299364e-05
Epoch 59: reducing lr to 7.022343562998019e-05
Epoch 62: reducing lr to 6.228133600920864e-05
Epoch 65: reducing lr to 5.435551204562791e-05
Epoch 68: reducing lr to 4.657092382873413e-05
Epoch 71: reducing lr to 3.9050385069735234e-05
Epoch 74: reducing lr to 3.191246516499488e-05
Epoch 77: reducing lr to 2.5269754405617997e-05
Epoch 80: reducing lr to 1.9226992649908806e-05
Epoch 83: reducing lr to 1.3879495664117495e-05
Epoch 86: reducing lr to 9.311574656209595e-06
Epoch 89: reducing lr to 5.5952955230386285e-06
Epoch 92: reducing lr to 2.7892478014838714e-06
Epoch 95: reducing lr to 9.376928077327405e-07
Epoch 98: reducing lr to 6.982574215403953e-08
[I 2024-06-21 01:15:33,567] Trial 445 finished with value: 0.9856570959091187 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.2814360066699812, 'bidirectional': True, 'fc_dropout': 0.4070378744618032, 'learning_rate_model': 0.0012662396604266074}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00016635208753917001
Epoch 33: reducing lr to 0.0001612424553297547
Epoch 36: reducing lr to 0.0001568229697806371
Epoch 40: reducing lr to 0.00014914582139289825
Epoch 46: reducing lr to 0.00013426324822546628
Epoch 61: reducing lr to 8.56188138900905e-05
Epoch 64: reducing lr to 7.514253695818199e-05
Epoch 67: reducing lr to 6.479774158594829e-05
Epoch 70: reducing lr to 5.4747640457269964e-05
Epoch 73: reducing lr to 4.5150698261058484e-05
Epoch 76: reducing lr to 3.615825134279402e-05
Epoch 79: reducing lr to 2.7912142456358006e-05
Epoch 82: reducing lr to 2.0542393261222307e-05
Epoch 85: reducing lr to 1.4165250817920653e-05
Epoch 88: reducing lr to 8.881260584064867e-06
Epoch 91: reducing lr to 4.773785536928208e-06
Epoch 94: reducing lr to 1.9075821448435006e-06
Epoch 97: reducing lr to 3.278605920923683e-07
[I 2024-06-21 01:15:48,250] Trial 446 finished with value: 0.9807913303375244 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.353262326945909, 'bidirectional': True, 'fc_dropout': 0.42969028925002234, 'learning_rate_model': 0.0016696255618696585}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001421718350322154
Epoch 33: reducing lr to 0.00013780491786094014
Epoch 36: reducing lr to 0.00013402783048132753
Epoch 40: reducing lr to 0.00012746660068105564
Epoch 46: reducing lr to 0.00011474729689283717
Epoch 61: reducing lr to 7.31736166591218e-05
Epoch 64: reducing lr to 6.422012808108161e-05
Epoch 67: reducing lr to 5.5379009446144556e-05
Epoch 72: reducing lr to 4.127060403475982e-05
Epoch 75: reducing lr to 3.3399138699743744e-05
Epoch 78: reducing lr to 2.6126152630191767e-05
Epoch 81: reducing lr to 1.9566323637311878e-05
Epoch 84: reducing lr to 1.3823123469983806e-05
Epoch 87: reducing lr to 8.987102338606506e-06
Epoch 90: reducing lr to 5.134555826976522e-06
Epoch 93: reducing lr to 2.326221590786084e-06
Epoch 96: reducing lr to 6.063970126610066e-07
Epoch 99: reducing lr to 2.1996638980116905e-09
[I 2024-06-21 01:16:03,850] Trial 447 finished with value: 0.9787084460258484 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.342318367919481, 'bidirectional': True, 'fc_dropout': 0.41575639423704785, 'learning_rate_model': 0.0014269356847824946}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011455401624751242
Epoch 33: reducing lr to 0.0001110354016043487
Epoch 36: reducing lr to 0.00010799203841673584
Epoch 40: reducing lr to 0.00010270537087830462
Epoch 44: reducing lr to 9.615224657402876e-05
Epoch 47: reducing lr to 9.051551576855202e-05
Epoch 55: reducing lr to 7.32045925850917e-05
Epoch 64: reducing lr to 5.174494367291923e-05
Epoch 72: reducing lr to 3.3253516380872864e-05
Epoch 75: reducing lr to 2.6911135221659154e-05
Epoch 78: reducing lr to 2.1050974774334252e-05
Epoch 81: reducing lr to 1.57654359271991e-05
Epoch 84: reducing lr to 1.1137890357911505e-05
Epoch 87: reducing lr to 7.2412982999164705e-06
Epoch 90: reducing lr to 4.1371344155046594e-06
Epoch 93: reducing lr to 1.8743376692425834e-06
Epoch 96: reducing lr to 4.886003843523108e-07
Epoch 99: reducing lr to 1.772364644912934e-09
[I 2024-06-21 01:16:19,955] Trial 448 finished with value: 0.9777215719223022 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.29964519196199046, 'bidirectional': True, 'fc_dropout': 0.6175159816033816, 'learning_rate_model': 0.0011497439952272521}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.432941018736729e-05
Epoch 40: reducing lr to 7.800290019307702e-05
Epoch 49: reducing lr to 6.56667184704849e-05
Epoch 52: reducing lr to 6.0767042042750537e-05
Epoch 55: reducing lr to 5.55975844326169e-05
Epoch 64: reducing lr to 3.9299363239704975e-05
Epoch 67: reducing lr to 3.3889060534592964e-05
Epoch 72: reducing lr to 2.525545350884861e-05
Epoch 75: reducing lr to 2.043852796427514e-05
Epoch 78: reducing lr to 1.598784046293959e-05
Epoch 81: reducing lr to 1.1973567834020957e-05
Epoch 84: reducing lr to 8.459029382007976e-06
Epoch 87: reducing lr to 5.499637104917931e-06
Epoch 90: reducing lr to 3.1420799140127654e-06
Epoch 93: reducing lr to 1.4235260813700743e-06
Epoch 96: reducing lr to 3.7108329086402547e-07
Epoch 99: reducing lr to 1.3460793853257297e-09
[I 2024-06-21 01:16:33,517] Trial 449 finished with value: 0.9807578325271606 and parameters: {'hidden_size': 137, 'n_layers': 1, 'rnn_dropout': 0.33047236140762143, 'bidirectional': True, 'fc_dropout': 0.37398866425211935, 'learning_rate_model': 0.0008732100896024864}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0002027977239072953
Epoch 31: reducing lr to 0.00019933419116446583
Epoch 34: reducing lr to 0.0001949351125895775
Epoch 40: reducing lr to 0.00018182178267911304
Epoch 46: reducing lr to 0.00016367862614356113
Epoch 49: reducing lr to 0.00015306661400329167
Epoch 52: reducing lr to 0.00014164565528975193
Epoch 55: reducing lr to 0.0001295958469386257
Epoch 58: reducing lr to 0.00011710726683013786
Epoch 61: reducing lr to 0.00010437681208217019
Epoch 64: reducing lr to 9.160531550378614e-05
Epoch 67: reducing lr to 7.899410643024966e-05
Epoch 70: reducing lr to 6.674215537821255e-05
Epoch 73: reducing lr to 5.504264464376929e-05
Epoch 76: reducing lr to 4.40800664497818e-05
Epoch 79: reducing lr to 3.402733950178252e-05
Epoch 82: reducing lr to 2.5042971558763943e-05
Epoch 85: reducing lr to 1.72686779405389e-05
Epoch 88: reducing lr to 1.082703234157995e-05
Epoch 91: reducing lr to 5.819661512108466e-06
Epoch 94: reducing lr to 2.3255092428543787e-06
Epoch 97: reducing lr to 3.996906971159668e-07
[I 2024-06-21 01:16:48,357] Trial 450 finished with value: 0.985270619392395 and parameters: {'hidden_size': 146, 'n_layers': 1, 'rnn_dropout': 0.2686104913302195, 'bidirectional': True, 'fc_dropout': 0.5829659554040613, 'learning_rate_model': 0.002035419385073111}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 9.91432794058585e-05
Epoch 40: reducing lr to 8.897925595185138e-05
Epoch 43: reducing lr to 8.481591623242792e-05
Epoch 46: reducing lr to 8.010042666438104e-05
Epoch 55: reducing lr to 6.342112515418433e-05
Epoch 64: reducing lr to 4.4829462645554835e-05
Epoch 72: reducing lr to 2.8809332170746267e-05
Epoch 75: reducing lr to 2.3314581977218787e-05
Epoch 78: reducing lr to 1.8237605847321135e-05
Epoch 81: reducing lr to 1.365845570258377e-05
Epoch 84: reducing lr to 9.64936096764158e-06
Epoch 87: reducing lr to 6.273531066018276e-06
Epoch 90: reducing lr to 3.58422484546192e-06
Epoch 93: reducing lr to 1.623840796109331e-06
Epoch 96: reducing lr to 4.233011213110706e-07
Epoch 99: reducing lr to 1.5354960118283898e-09
[I 2024-06-21 01:17:03,497] Trial 451 finished with value: 0.978605329990387 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.3189322566087068, 'bidirectional': True, 'fc_dropout': 0.6041928834735001, 'learning_rate_model': 0.000996085836169648}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014045958622770514
Epoch 36: reducing lr to 0.00013241366412790911
Epoch 40: reducing lr to 0.00012593145460531094
Epoch 61: reducing lr to 7.229234901832725e-05
Epoch 65: reducing lr to 6.05159553603658e-05
Epoch 72: reducing lr to 4.077356084470801e-05
Epoch 75: reducing lr to 3.2996895630310564e-05
Epoch 78: reducing lr to 2.5811501886622472e-05
Epoch 81: reducing lr to 1.9330676300769747e-05
Epoch 84: reducing lr to 1.3656644457943822e-05
Epoch 87: reducing lr to 8.87886602561382e-06
Epoch 90: reducing lr to 5.072717720473654e-06
Epoch 93: reducing lr to 2.2982057032725933e-06
Epoch 96: reducing lr to 5.990938603893164e-07
Epoch 99: reducing lr to 2.1731722101318505e-09
[I 2024-06-21 01:17:18,660] Trial 452 finished with value: 0.9774678349494934 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.34630405092188604, 'bidirectional': True, 'fc_dropout': 0.631869454229158, 'learning_rate_model': 0.0014097503616850736}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001132896909212336
Epoch 33: reducing lr to 0.0001098099110020935
Epoch 40: reducing lr to 0.0001015718182906264
Epoch 43: reducing lr to 9.681927253218293e-05
Epoch 46: reducing lr to 9.143643532554031e-05
Epoch 55: reducing lr to 7.239663819434144e-05
Epoch 64: reducing lr to 5.117383805012563e-05
Epoch 72: reducing lr to 3.288649945448833e-05
Epoch 75: reducing lr to 2.661411874913198e-05
Epoch 78: reducing lr to 2.0818636516611875e-05
Epoch 81: reducing lr to 1.559143382255427e-05
Epoch 84: reducing lr to 1.1014962176760716e-05
Epoch 87: reducing lr to 7.161376555261598e-06
Epoch 90: reducing lr to 4.091473128444746e-06
Epoch 93: reducing lr to 1.8536507246652653e-06
Epoch 96: reducing lr to 4.832077332641779e-07
Epoch 99: reducing lr to 1.7528031700554668e-09
[I 2024-06-21 01:17:34,761] Trial 453 finished with value: 0.9778214693069458 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3660213779109298, 'bidirectional': True, 'fc_dropout': 0.3179057181993933, 'learning_rate_model': 0.0011370543445321428}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001697604213821887
Epoch 33: reducing lr to 0.00016454609957949955
Epoch 36: reducing lr to 0.0001600360646276746
Epoch 40: reducing lr to 0.00015220162164234532
Epoch 46: reducing lr to 0.00013701412427138737
Epoch 49: reducing lr to 0.00012813089018998039
Epoch 52: reducing lr to 0.00011857049312810124
Epoch 61: reducing lr to 8.73730299344909e-05
Epoch 64: reducing lr to 7.668210797020519e-05
Epoch 67: reducing lr to 6.612536144852776e-05
Epoch 70: reducing lr to 5.5869347064961267e-05
Epoch 73: reducing lr to 4.607577623991375e-05
Epoch 76: reducing lr to 3.689908600005072e-05
Epoch 79: reducing lr to 2.848402526932697e-05
Epoch 82: reducing lr to 2.0963279678727198e-05
Epoch 85: reducing lr to 1.4455478037018197e-05
Epoch 88: reducing lr to 9.063225845007015e-06
Epoch 91: reducing lr to 4.871594076908174e-06
Epoch 94: reducing lr to 1.9466659752828208e-06
Epoch 97: reducing lr to 3.345780211811691e-07
[I 2024-06-21 01:17:50,586] Trial 454 finished with value: 0.9818753004074097 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.31415222028438994, 'bidirectional': True, 'fc_dropout': 0.5662505103534411, 'learning_rate_model': 0.00170383397723655}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 6.89436167059428e-05
Epoch 43: reducing lr to 6.571774462191593e-05
Epoch 49: reducing lr to 5.804016334469544e-05
Epoch 52: reducing lr to 5.370953701181913e-05
Epoch 55: reducing lr to 4.9140461975269346e-05
Epoch 58: reducing lr to 4.440501241848774e-05
Epoch 61: reducing lr to 3.957784825969601e-05
Epoch 64: reducing lr to 3.473512176187404e-05
Epoch 67: reducing lr to 2.9953173461989245e-05
Epoch 72: reducing lr to 2.2322276506884976e-05
Epoch 75: reducing lr to 1.806479034131785e-05
Epoch 78: reducing lr to 1.4131007207479433e-05
Epoch 81: reducing lr to 1.0582953573623814e-05
Epoch 84: reducing lr to 7.4765948185760774e-06
Epoch 87: reducing lr to 4.860907371965833e-06
Epoch 90: reducing lr to 2.777157678944401e-06
Epoch 93: reducing lr to 1.2581972757674657e-06
Epoch 96: reducing lr to 3.279855506395563e-07
Epoch 99: reducing lr to 1.1897452654682263e-09
[I 2024-06-21 01:18:03,113] Trial 455 finished with value: 0.983430027961731 and parameters: {'hidden_size': 124, 'n_layers': 1, 'rnn_dropout': 0.2905552078461847, 'bidirectional': True, 'fc_dropout': 0.6760181629551636, 'learning_rate_model': 0.000771795171362858}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 0.00011763320462374934
Epoch 47: reducing lr to 0.00010090900268394602
Epoch 50: reducing lr to 9.404435580828965e-05
Epoch 53: reducing lr to 8.670729829462718e-05
Epoch 56: reducing lr to 7.901351728532411e-05
Epoch 59: reducing lr to 7.108438242037979e-05
Epoch 62: reducing lr to 6.30449118134667e-05
Epoch 65: reducing lr to 5.502191640503283e-05
Epoch 68: reducing lr to 4.714188830856335e-05
Epoch 71: reducing lr to 3.9529146944429146e-05
Epoch 74: reducing lr to 3.230371538240537e-05
Epoch 77: reducing lr to 2.557956428254197e-05
Epoch 80: reducing lr to 1.9462717624947026e-05
Epoch 83: reducing lr to 1.4049659757303569e-05
Epoch 86: reducing lr to 9.425735551954884e-06
Epoch 89: reducing lr to 5.6638944413154365e-06
Epoch 92: reducing lr to 2.8234442762187404e-06
Epoch 95: reducing lr to 9.491890212967287e-07
Epoch 98: reducing lr to 7.068181318012953e-08
[I 2024-06-21 01:18:38,675] Trial 456 finished with value: 0.9899387359619141 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.35950970499882234, 'bidirectional': True, 'fc_dropout': 0.41833293223906287, 'learning_rate_model': 0.001281763893351713}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 8.079364623512403e-05
Epoch 43: reducing lr to 7.701331122502057e-05
Epoch 46: reducing lr to 7.273162116242493e-05
Epoch 49: reducing lr to 6.801610720105913e-05
Epoch 52: reducing lr to 6.29411327707612e-05
Epoch 55: reducing lr to 5.758672507121678e-05
Epoch 61: reducing lr to 4.638048921453912e-05
Epoch 72: reducing lr to 2.615902961621722e-05
Epoch 75: reducing lr to 2.116976668591732e-05
Epoch 78: reducing lr to 1.6559844867678224e-05
Epoch 81: reducing lr to 1.2401951739738108e-05
Epoch 84: reducing lr to 8.761672010794408e-06
Epoch 87: reducing lr to 5.6964001796915e-06
Epoch 90: reducing lr to 3.2544955685861522e-06
Epoch 93: reducing lr to 1.474456236114335e-06
Epoch 96: reducing lr to 3.843597103649087e-07
Epoch 99: reducing lr to 1.3942386936052937e-09
[I 2024-06-21 01:18:52,950] Trial 457 finished with value: 0.9813567399978638 and parameters: {'hidden_size': 140, 'n_layers': 1, 'rnn_dropout': 0.3362449289408925, 'bidirectional': True, 'fc_dropout': 0.620801562323371, 'learning_rate_model': 0.0009044513331383252}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001559527375446362
Epoch 33: reducing lr to 0.00015116252936214536
Epoch 36: reducing lr to 0.00014701932394693492
Epoch 40: reducing lr to 0.0001398221055331753
Epoch 46: reducing lr to 0.00012586990293984835
Epoch 61: reducing lr to 8.026643133252114e-05
Epoch 64: reducing lr to 7.044506935879687e-05
Epoch 67: reducing lr to 6.074696949420061e-05
Epoch 72: reducing lr to 4.527101783474256e-05
Epoch 75: reducing lr to 3.6636561036704466e-05
Epoch 78: reducing lr to 2.8658594884592676e-05
Epoch 81: reducing lr to 2.1462913060323596e-05
Epoch 84: reducing lr to 1.516301696516053e-05
Epoch 87: reducing lr to 9.858233960206658e-06
Epoch 90: reducing lr to 5.632255060302946e-06
Epoch 93: reducing lr to 2.551705301801323e-06
Epoch 96: reducing lr to 6.65175870747845e-07
Epoch 99: reducing lr to 2.412880205793916e-09
[I 2024-06-21 01:19:07,595] Trial 458 finished with value: 0.9795483946800232 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.3697394758726426, 'bidirectional': True, 'fc_dropout': 0.5868023071881182, 'learning_rate_model': 0.0015652504329815742}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011258802748498288
Epoch 40: reducing lr to 0.00010094272988488977
Epoch 43: reducing lr to 9.621961917530813e-05
Epoch 46: reducing lr to 9.087012074839452e-05
Epoch 49: reducing lr to 8.497860731570035e-05
Epoch 55: reducing lr to 7.194824722853185e-05
Epoch 64: reducing lr to 5.085689119679408e-05
Epoch 72: reducing lr to 3.268281583574196e-05
Epoch 75: reducing lr to 2.6449283327105043e-05
Epoch 78: reducing lr to 2.0689695604887985e-05
Epoch 81: reducing lr to 1.549486776307393e-05
Epoch 84: reducing lr to 1.0946740645319773e-05
Epoch 87: reducing lr to 7.117022333432668e-06
Epoch 90: reducing lr to 4.066132454714523e-06
Epoch 93: reducing lr to 1.842170077781145e-06
Epoch 96: reducing lr to 4.802149702352659e-07
Epoch 99: reducing lr to 1.741947125823498e-09
[I 2024-06-21 01:19:22,737] Trial 459 finished with value: 0.9779208898544312 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.30317399890200775, 'bidirectional': True, 'fc_dropout': 0.6084764258070499, 'learning_rate_model': 0.001130011960956898}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.00018231817756971886
Epoch 25: reducing lr to 0.0001853677074890166
Epoch 28: reducing lr to 0.00018420456178545836
Epoch 31: reducing lr to 0.0001815986012732215
Epoch 34: reducing lr to 0.00017759092696795597
Epoch 37: reducing lr to 0.00017224473743620474
Epoch 40: reducing lr to 0.00016564434441800172
Epoch 43: reducing lr to 0.00015789384492195623
Epoch 46: reducing lr to 0.00014911545978315806
Epoch 49: reducing lr to 0.00013944764238510134
Epoch 52: reducing lr to 0.00012904285374617288
Epoch 55: reducing lr to 0.00011806516683058777
Epoch 58: reducing lr to 0.00010668774750106204
Epoch 61: reducing lr to 9.5089974122106e-05
Epoch 64: reducing lr to 8.345481057464071e-05
Epoch 67: reducing lr to 7.196567308779171e-05
Epoch 70: reducing lr to 6.080382894594969e-05
Epoch 73: reducing lr to 5.014527221494226e-05
Epoch 76: reducing lr to 4.015808007922935e-05
Epoch 79: reducing lr to 3.0999785949788886e-05
Epoch 82: reducing lr to 2.281479449275385e-05
Epoch 85: reducing lr to 1.5732212028051852e-05
Epoch 88: reducing lr to 9.863706360082528e-06
Epoch 91: reducing lr to 5.301862085519099e-06
Epoch 94: reducing lr to 2.118599038545589e-06
Epoch 97: reducing lr to 3.6412855774598745e-07
[I 2024-06-21 01:20:25,962] Trial 460 finished with value: 1.0528029203414917 and parameters: {'hidden_size': 142, 'n_layers': 3, 'rnn_dropout': 0.3825298726227458, 'bidirectional': True, 'fc_dropout': 0.5536286846106392, 'learning_rate_model': 0.0018543196787986329}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014043711049527982
Epoch 33: reducing lr to 0.00013612347672128263
Epoch 36: reducing lr to 0.0001323924758689634
Epoch 40: reducing lr to 0.0001259113036013556
Epoch 48: reducing lr to 0.000108516159538184
Epoch 61: reducing lr to 7.228078111088445e-05
Epoch 64: reducing lr to 6.343653946156244e-05
Epoch 67: reducing lr to 5.470329666171262e-05
Epoch 72: reducing lr to 4.076703643673993e-05
Epoch 75: reducing lr to 3.2991615610505765e-05
Epoch 78: reducing lr to 2.58073716422904e-05
Epoch 81: reducing lr to 1.932758308997647e-05
Epoch 84: reducing lr to 1.365445918106163e-05
Epoch 87: reducing lr to 8.877445268067876e-06
Epoch 90: reducing lr to 5.0719060062345575e-06
Epoch 93: reducing lr to 2.2978379543859985e-06
Epoch 96: reducing lr to 5.989979959939665e-07
Epoch 99: reducing lr to 2.1728244685480584e-09
[I 2024-06-21 01:20:42,105] Trial 461 finished with value: 0.9774088263511658 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3927620092968838, 'bidirectional': True, 'fc_dropout': 0.6498828837445867, 'learning_rate_model': 0.001409524779560231}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 9.73524042189106e-05
Epoch 33: reducing lr to 9.436215030854811e-05
Epoch 38: reducing lr to 8.96725265276605e-05
Epoch 41: reducing lr to 8.598598643161297e-05
Epoch 44: reducing lr to 8.171387334692841e-05
Epoch 47: reducing lr to 7.69235629429501e-05
Epoch 55: reducing lr to 6.221207532895444e-05
Epoch 64: reducing lr to 4.397484119497368e-05
Epoch 72: reducing lr to 2.826011583405482e-05
Epoch 75: reducing lr to 2.2870116648098014e-05
Epoch 78: reducing lr to 1.7889927150219674e-05
Epoch 81: reducing lr to 1.3398073165377556e-05
Epoch 84: reducing lr to 9.465407148418978e-06
Epoch 87: reducing lr to 6.153933508887223e-06
Epoch 90: reducing lr to 3.5158957766783828e-06
Epoch 93: reducing lr to 1.5928841641359485e-06
Epoch 96: reducing lr to 4.1523137884755606e-07
Epoch 99: reducing lr to 1.506223570196287e-09
[I 2024-06-21 01:20:57,941] Trial 462 finished with value: 0.9796821475028992 and parameters: {'hidden_size': 154, 'n_layers': 1, 'rnn_dropout': 0.34592976737905595, 'bidirectional': True, 'fc_dropout': 0.4575599001109994, 'learning_rate_model': 0.000977096620774824}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012165960787268151
Epoch 36: reducing lr to 0.0001146906016700828
Epoch 40: reducing lr to 0.00010907601109746512
Epoch 44: reducing lr to 0.0001021164075906221
Epoch 61: reducing lr to 6.261629462233125e-05
Epoch 64: reducing lr to 5.49545948964321e-05
Epoch 72: reducing lr to 3.531617568556996e-05
Epoch 75: reducing lr to 2.858038736417378e-05
Epoch 78: reducing lr to 2.2356731088761193e-05
Epoch 81: reducing lr to 1.6743339218249158e-05
Epoch 84: reducing lr to 1.182875483426674e-05
Epoch 87: reducing lr to 7.690463769977909e-06
Epoch 90: reducing lr to 4.393753856864907e-06
Epoch 93: reducing lr to 1.9905996605858784e-06
Epoch 96: reducing lr to 5.189074387257483e-07
Epoch 99: reducing lr to 1.8823014222377892e-09
[I 2024-06-21 01:21:13,550] Trial 463 finished with value: 0.9763278961181641 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.3231834423826242, 'bidirectional': True, 'fc_dropout': 0.634748110147963, 'learning_rate_model': 0.0012210606681051673}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011872676625347186
Epoch 36: reducing lr to 0.000111925761508324
Epoch 40: reducing lr to 0.00010644652156846053
Epoch 43: reducing lr to 0.00010146588842537998
Epoch 46: reducing lr to 9.58247144613881e-05
Epoch 55: reducing lr to 7.587114653188387e-05
Epoch 64: reducing lr to 5.362980743494584e-05
Epoch 72: reducing lr to 3.4464810539051025e-05
Epoch 75: reducing lr to 2.7891401504195465e-05
Epoch 78: reducing lr to 2.181777857565416e-05
Epoch 81: reducing lr to 1.6339708440849194e-05
Epoch 84: reducing lr to 1.1543599678106208e-05
Epoch 87: reducing lr to 7.505070173779386e-06
Epoch 90: reducing lr to 4.287833869111345e-06
Epoch 93: reducing lr to 1.9426123817031404e-06
Epoch 96: reducing lr to 5.063981650282141e-07
Epoch 99: reducing lr to 1.8369248831664326e-09
[I 2024-06-21 01:21:28,692] Trial 464 finished with value: 0.9774233102798462 and parameters: {'hidden_size': 148, 'n_layers': 1, 'rnn_dropout': 0.3267930202208085, 'bidirectional': True, 'fc_dropout': 0.6358221172571242, 'learning_rate_model': 0.001191624624297213}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.0001341750188901566
Epoch 27: reducing lr to 0.00013431184647391198
Epoch 36: reducing lr to 0.00012661808428343426
Epoch 40: reducing lr to 0.00012041959293376231
Epoch 44: reducing lr to 0.00011273621129152789
Epoch 47: reducing lr to 0.00010612727912694825
Epoch 61: reducing lr to 6.912820365886015e-05
Epoch 64: reducing lr to 6.0669709871907436e-05
Epoch 67: reducing lr to 5.2317373641006055e-05
Epoch 72: reducing lr to 3.8988953274368493e-05
Epoch 75: reducing lr to 3.155266293344524e-05
Epoch 78: reducing lr to 2.4681764853250816e-05
Epoch 81: reducing lr to 1.8484596867150357e-05
Epoch 84: reducing lr to 1.3058910274807218e-05
Epoch 87: reducing lr to 8.490249206354657e-06
Epoch 90: reducing lr to 4.850691234226058e-06
Epoch 93: reducing lr to 2.1976161248475234e-06
Epoch 96: reducing lr to 5.728722742328641e-07
Epoch 99: reducing lr to 2.0780551907278317e-09
[I 2024-06-21 01:21:42,242] Trial 465 finished with value: 0.9773565530776978 and parameters: {'hidden_size': 137, 'n_layers': 1, 'rnn_dropout': 0.30804789286751566, 'bidirectional': True, 'fc_dropout': 0.6509526050082511, 'learning_rate_model': 0.0013480473581791003}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 9.755763303473722e-05
Epoch 38: reducing lr to 9.270919969159237e-05
Epoch 41: reducing lr to 8.889781848967977e-05
Epoch 44: reducing lr to 8.448103443764446e-05
Epoch 47: reducing lr to 7.952850481654421e-05
Epoch 50: reducing lr to 7.411833240780066e-05
Epoch 53: reducing lr to 6.83358432512866e-05
Epoch 56: reducing lr to 6.227221281414681e-05
Epoch 64: reducing lr to 4.5464006553816894e-05
Epoch 72: reducing lr to 2.9217117255626326e-05
Epoch 75: reducing lr to 2.364459097340709e-05
Epoch 78: reducing lr to 1.849575218699958e-05
Epoch 81: reducing lr to 1.3851785922284739e-05
Epoch 84: reducing lr to 9.785943983794357e-06
Epoch 87: reducing lr to 6.36233050028117e-06
Epoch 90: reducing lr to 3.634958178125563e-06
Epoch 93: reducing lr to 1.6468256418860056e-06
Epoch 96: reducing lr to 4.292927868818162e-07
Epoch 99: reducing lr to 1.5572303709542045e-09
[I 2024-06-21 01:21:56,292] Trial 466 finished with value: 0.9801531434059143 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.3260228919322013, 'bidirectional': True, 'fc_dropout': 0.6210668249627631, 'learning_rate_model': 0.001010185050439624}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.0002197628529389182
Epoch 27: reducing lr to 0.00022269890962727112
Epoch 31: reducing lr to 0.00021889548939934455
Epoch 34: reducing lr to 0.00021406471525101068
Epoch 40: reducing lr to 0.0001996645325646503
Epoch 48: reducing lr to 0.0001720800885240716
Epoch 51: reducing lr to 0.0001598131177834632
Epoch 54: reducing lr to 0.0001467883182385864
Epoch 57: reducing lr to 0.00013321105875263906
Epoch 60: reducing lr to 0.00011929552102561945
Epoch 63: reducing lr to 0.00010526111028221085
Epoch 66: reducing lr to 9.132919891271383e-05
Epoch 69: reducing lr to 7.771944031911494e-05
Epoch 72: reducing lr to 6.46465491292087e-05
Epoch 75: reducing lr to 5.23166333840843e-05
Epoch 78: reducing lr to 4.09241795478055e-05
Epoch 81: reducing lr to 3.064881970789989e-05
Epoch 84: reducing lr to 2.165263270120267e-05
Epoch 87: reducing lr to 1.4077456980581688e-05
Epoch 90: reducing lr to 8.042802456822203e-06
Epoch 93: reducing lr to 3.6438089984706932e-06
Epoch 96: reducing lr to 9.498643208075489e-07
Epoch 99: reducing lr to 3.4455681852813003e-09
[I 2024-06-21 01:22:11,914] Trial 467 finished with value: 0.9908254146575928 and parameters: {'hidden_size': 149, 'n_layers': 1, 'rnn_dropout': 0.3461160555530797, 'bidirectional': True, 'fc_dropout': 0.5991591822070433, 'learning_rate_model': 0.002235161563732355}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015926516503136936
Epoch 33: reducing lr to 0.0001543732130930415
Epoch 36: reducing lr to 0.00015014200622484854
Epoch 40: reducing lr to 0.00014279191929158038
Epoch 46: reducing lr to 0.000128543372689817
Epoch 61: reducing lr to 8.197128587751894e-05
Epoch 64: reducing lr to 7.194131872076856e-05
Epoch 67: reducing lr to 6.20372317535004e-05
Epoch 72: reducing lr to 4.623257174004205e-05
Epoch 75: reducing lr to 3.741471955901077e-05
Epoch 78: reducing lr to 2.926730184877602e-05
Epoch 81: reducing lr to 2.1918784142073823e-05
Epoch 84: reducing lr to 1.5485078603628555e-05
Epoch 87: reducing lr to 1.006762230217847e-05
Epoch 90: reducing lr to 5.7518838450730825e-06
Epoch 93: reducing lr to 2.6059033807373285e-06
Epoch 96: reducing lr to 6.7930416931105e-07
Epoch 99: reducing lr to 2.4641296474201686e-09
[I 2024-06-21 01:22:26,596] Trial 468 finished with value: 0.9786673784255981 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.2863050872202745, 'bidirectional': True, 'fc_dropout': 0.6690535186410003, 'learning_rate_model': 0.0015984962652732014}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.362331354884163e-05
Epoch 38: reducing lr to 6.996437147389491e-05
Epoch 41: reducing lr to 6.70880561661767e-05
Epoch 49: reducing lr to 5.73299563335527e-05
Epoch 52: reducing lr to 5.3052321601785126e-05
Epoch 55: reducing lr to 4.853915593795925e-05
Epoch 58: reducing lr to 4.386165159156869e-05
Epoch 61: reducing lr to 3.9093554906607975e-05
Epoch 64: reducing lr to 3.431008631079041e-05
Epoch 67: reducing lr to 2.9586652202006886e-05
Epoch 72: reducing lr to 2.2049130527165653e-05
Epoch 75: reducing lr to 1.7843740984874233e-05
Epoch 78: reducing lr to 1.3958093490237496e-05
Epoch 81: reducing lr to 1.0453455526177806e-05
Epoch 84: reducing lr to 7.385107652558101e-06
Epoch 87: reducing lr to 4.801427000148426e-06
Epoch 90: reducing lr to 2.743175058273243e-06
Epoch 93: reducing lr to 1.2428013761842018e-06
Epoch 96: reducing lr to 3.239721636296984e-07
Epoch 99: reducing lr to 1.1751869772019114e-09
[I 2024-06-21 01:22:42,691] Trial 469 finished with value: 0.9819105863571167 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3616022613772631, 'bidirectional': True, 'fc_dropout': 0.6272312037497997, 'learning_rate_model': 0.0007623511189984169}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.0001193118833467559
Epoch 27: reducing lr to 0.00011943355395911483
Epoch 33: reducing lr to 0.00011576506056524179
Epoch 36: reducing lr to 0.00011259206241649432
Epoch 40: reducing lr to 0.00010708020422593666
Epoch 43: reducing lr to 0.00010206992106893816
Epoch 46: reducing lr to 9.639516485109606e-05
Epoch 55: reducing lr to 7.632281211053939e-05
Epoch 61: reducing lr to 6.147057953961493e-05
Epoch 64: reducing lr to 5.3949068960776476e-05
Epoch 72: reducing lr to 3.4669981665453234e-05
Epoch 75: reducing lr to 2.8057440724317334e-05
Epoch 78: reducing lr to 2.1947661146773727e-05
Epoch 81: reducing lr to 1.643697972519569e-05
Epoch 84: reducing lr to 1.1612319433463897e-05
Epoch 87: reducing lr to 7.549748315837865e-06
Epoch 90: reducing lr to 4.313359606551692e-06
Epoch 93: reducing lr to 1.9541768721002525e-06
Epoch 96: reducing lr to 5.094127842964489e-07
Epoch 99: reducing lr to 1.8478602094003718e-09
[I 2024-06-21 01:22:57,354] Trial 470 finished with value: 0.9786709547042847 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.31949203332750237, 'bidirectional': True, 'fc_dropout': 0.6609147127507696, 'learning_rate_model': 0.001198718438613663}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001491241984082652
Epoch 33: reducing lr to 0.00014454373405304216
Epoch 36: reducing lr to 0.0001405819428327553
Epoch 39: reducing lr to 0.0001355833052699324
Epoch 46: reducing lr to 0.00012035856936630355
Epoch 61: reducing lr to 7.675188919417569e-05
Epoch 64: reducing lr to 6.73605649079325e-05
Epoch 67: reducing lr to 5.808710558197939e-05
Epoch 72: reducing lr to 4.3288783204591184e-05
Epoch 75: reducing lr to 3.5032394320557055e-05
Epoch 78: reducing lr to 2.740375101430261e-05
Epoch 81: reducing lr to 2.0523138971580862e-05
Epoch 84: reducing lr to 1.4499089826706685e-05
Epoch 87: reducing lr to 9.426581797682067e-06
Epoch 90: reducing lr to 5.385641408559298e-06
Epoch 93: reducing lr to 2.4399764550226765e-06
Epoch 96: reducing lr to 6.360505117609808e-07
Epoch 99: reducing lr to 2.307229948067535e-09
[I 2024-06-21 01:23:13,533] Trial 471 finished with value: 0.979996919631958 and parameters: {'hidden_size': 155, 'n_layers': 1, 'rnn_dropout': 0.33604294902579346, 'bidirectional': True, 'fc_dropout': 0.6862702783436074, 'learning_rate_model': 0.0014967144520932802}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 9.130342071376224e-05
Epoch 40: reducing lr to 8.445371072082983e-05
Epoch 43: reducing lr to 8.050212127972468e-05
Epoch 49: reducing lr to 7.109733140646366e-05
Epoch 52: reducing lr to 6.579245357975084e-05
Epoch 55: reducing lr to 6.019548376825445e-05
Epoch 64: reducing lr to 4.2549405808547446e-05
Epoch 72: reducing lr to 2.7344070021501206e-05
Epoch 75: reducing lr to 2.2128786544883893e-05
Epoch 78: reducing lr to 1.731002886002584e-05
Epoch 81: reducing lr to 1.296377739350244e-05
Epoch 86: reducing lr to 6.952387157534038e-06
Epoch 89: reducing lr to 4.177667276827326e-06
Epoch 92: reducing lr to 2.0825618985167734e-06
Epoch 95: reducing lr to 7.001182587142424e-07
Epoch 98: reducing lr to 5.213464005180421e-08
[I 2024-06-21 01:23:28,496] Trial 472 finished with value: 0.9784106016159058 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.37379955750744476, 'bidirectional': True, 'fc_dropout': 0.3562046415968786, 'learning_rate_model': 0.0009454242358073686}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011759403409557327
Epoch 36: reducing lr to 0.00011085791545002999
Epoch 40: reducing lr to 0.0001054309510961744
Epoch 43: reducing lr to 0.00010049783649929785
Epoch 46: reducing lr to 9.491048307939181e-05
Epoch 55: reducing lr to 7.514728543261144e-05
Epoch 64: reducing lr to 5.3118143473899525e-05
Epoch 72: reducing lr to 3.413599336963433e-05
Epoch 75: reducing lr to 2.7625299020234833e-05
Epoch 78: reducing lr to 2.16096224859499e-05
Epoch 81: reducing lr to 1.6183816776436108e-05
Epoch 84: reducing lr to 1.1433466074826037e-05
Epoch 87: reducing lr to 7.433466822644782e-06
Epoch 90: reducing lr to 4.246925087843787e-06
Epoch 93: reducing lr to 1.9240785701244748e-06
Epoch 96: reducing lr to 5.015667903994838e-07
Epoch 99: reducing lr to 1.8193994004737577e-09
[I 2024-06-21 01:23:42,763] Trial 473 finished with value: 0.9783964157104492 and parameters: {'hidden_size': 140, 'n_layers': 1, 'rnn_dropout': 0.35830104279307845, 'bidirectional': True, 'fc_dropout': 0.6404181530409188, 'learning_rate_model': 0.0011802557344109713}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.00018669067459253888
Epoch 27: reducing lr to 0.00018918488321997644
Epoch 31: reducing lr to 0.00018595384085492363
Epoch 34: reducing lr to 0.00018185005137232474
Epoch 40: reducing lr to 0.00016961695654296507
Epoch 46: reducing lr to 0.00015269166327887897
Epoch 61: reducing lr to 9.737049619780493e-05
Epoch 65: reducing lr to 8.15088827702804e-05
Epoch 68: reducing lr to 6.983549281393026e-05
Epoch 71: reducing lr to 5.855805858495961e-05
Epoch 74: reducing lr to 4.785438098459508e-05
Epoch 77: reducing lr to 3.7893294938557516e-05
Epoch 80: reducing lr to 2.88318632452753e-05
Epoch 83: reducing lr to 2.0813016792988978e-05
Epoch 86: reducing lr to 1.3963184569443318e-05
Epoch 89: reducing lr to 8.390433089280653e-06
Epoch 92: reducing lr to 4.182620373027984e-06
Epoch 95: reducing lr to 1.406118538187359e-06
Epoch 98: reducing lr to 1.0470728758480482e-07
[I 2024-06-21 01:23:55,547] Trial 474 finished with value: 0.9816775918006897 and parameters: {'hidden_size': 130, 'n_layers': 1, 'rnn_dropout': 0.29801093998510636, 'bidirectional': True, 'fc_dropout': 0.5697847368064325, 'learning_rate_model': 0.0018987914225544252}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 7.937105560243436e-05
Epoch 40: reducing lr to 7.34165282861578e-05
Epoch 43: reducing lr to 6.998136865253112e-05
Epoch 49: reducing lr to 6.18056826363412e-05
Epoch 52: reducing lr to 5.719409470615667e-05
Epoch 55: reducing lr to 5.2328588040134616e-05
Epoch 64: reducing lr to 3.6988660752024986e-05
Epoch 67: reducing lr to 3.1896470069327966e-05
Epoch 72: reducing lr to 2.377049715231857e-05
Epoch 75: reducing lr to 1.9236794564079662e-05
Epoch 78: reducing lr to 1.5047796154714745e-05
Epoch 81: reducing lr to 1.1269552534543414e-05
Epoch 84: reducing lr to 7.961659994185023e-06
Epoch 87: reducing lr to 5.1762724472729974e-06
Epoch 90: reducing lr to 2.957333615974454e-06
Epoch 93: reducing lr to 1.3398263726130638e-06
Epoch 96: reducing lr to 3.4926453827746877e-07
Epoch 99: reducing lr to 1.2669333450832793e-09
[I 2024-06-21 01:24:11,326] Trial 475 finished with value: 0.9808540344238281 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.38993973279923055, 'bidirectional': True, 'fc_dropout': 0.6057083153140718, 'learning_rate_model': 0.0008218675598519489}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013853822043319205
Epoch 33: reducing lr to 0.0001342829125267383
Epoch 36: reducing lr to 0.00013060235959673196
Epoch 39: reducing lr to 0.00012595856362038687
Epoch 46: reducing lr to 0.00011181459610091308
Epoch 61: reducing lr to 7.130345213816957e-05
Epoch 64: reducing lr to 6.257879599238984e-05
Epoch 67: reducing lr to 5.396363784910942e-05
Epoch 72: reducing lr to 4.021581375722534e-05
Epoch 75: reducing lr to 3.254552660458654e-05
Epoch 78: reducing lr to 2.5458422839745804e-05
Epoch 81: reducing lr to 1.9066249349027954e-05
Epoch 84: reducing lr to 1.3469833359933147e-05
Epoch 87: reducing lr to 8.757410808965072e-06
Epoch 90: reducing lr to 5.00332732445226e-06
Epoch 93: reducing lr to 2.2667682347051776e-06
Epoch 96: reducing lr to 5.908987739450776e-07
Epoch 99: reducing lr to 2.1434450917087388e-09
[I 2024-06-21 01:24:25,370] Trial 476 finished with value: 0.9777746200561523 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.31036910294826203, 'bidirectional': True, 'fc_dropout': 0.5569386130784975, 'learning_rate_model': 0.0013904661946410881}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001040195538449581
Epoch 33: reducing lr to 0.00010082451331016435
Epoch 40: reducing lr to 9.326051766844266e-05
Epoch 43: reducing lr to 8.889685769725652e-05
Epoch 46: reducing lr to 8.395448103348488e-05
Epoch 55: reducing lr to 6.64726502792391e-05
Epoch 64: reducing lr to 4.698644474376016e-05
Epoch 72: reducing lr to 3.019550122311416e-05
Epoch 75: reducing lr to 2.44363695915298e-05
Epoch 78: reducing lr to 1.911511333916472e-05
Epoch 81: reducing lr to 1.4315636108080444e-05
Epoch 84: reducing lr to 1.011364266182308e-05
Epoch 87: reducing lr to 6.575383762958396e-06
Epoch 90: reducing lr to 3.7566808235478274e-06
Epoch 93: reducing lr to 1.7019723489061474e-06
Epoch 96: reducing lr to 4.436683728223801e-07
Epoch 99: reducing lr to 1.6093768307173102e-09
[I 2024-06-21 01:24:38,574] Trial 477 finished with value: 0.9797606468200684 and parameters: {'hidden_size': 136, 'n_layers': 1, 'rnn_dropout': 0.3479345061633646, 'bidirectional': True, 'fc_dropout': 0.5920194576138372, 'learning_rate_model': 0.001044012783987008}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 6.383023756763756e-05
Epoch 38: reducing lr to 6.0657993197854674e-05
Epoch 41: reducing lr to 5.816427374186619e-05
Epoch 49: reducing lr to 4.970415695953869e-05
Epoch 55: reducing lr to 4.208267334771829e-05
Epoch 61: reducing lr to 3.389348803754747e-05
Epoch 64: reducing lr to 2.9746297125448505e-05
Epoch 67: reducing lr to 2.5651155155252858e-05
Epoch 72: reducing lr to 1.911624418772139e-05
Epoch 75: reducing lr to 1.5470238586916112e-05
Epoch 78: reducing lr to 1.2101444237253738e-05
Epoch 81: reducing lr to 9.062979068389959e-06
Epoch 84: reducing lr to 6.402770443259659e-06
Epoch 87: reducing lr to 4.162760561976432e-06
Epoch 90: reducing lr to 2.3782889850921978e-06
Epoch 93: reducing lr to 1.0774889538026316e-06
Epoch 96: reducing lr to 2.80878694166166e-07
Epoch 99: reducing lr to 1.0188683492382e-09
[I 2024-06-21 01:24:54,132] Trial 478 finished with value: 0.9827680587768555 and parameters: {'hidden_size': 153, 'n_layers': 1, 'rnn_dropout': 0.40185018869674805, 'bidirectional': True, 'fc_dropout': 0.2908660021569275, 'learning_rate_model': 0.0006609462504474428}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001659329511218047
Epoch 36: reducing lr to 0.00015642784268192264
Epoch 40: reducing lr to 0.00014877003743870586
Epoch 48: reducing lr to 0.00012821686898193945
Epoch 52: reducing lr to 0.00011589716660998924
Epoch 57: reducing lr to 9.925555602351588e-05
Epoch 60: reducing lr to 8.888708926561483e-05
Epoch 63: reducing lr to 7.843005022664067e-05
Epoch 66: reducing lr to 6.804938346820328e-05
Epoch 69: reducing lr to 5.790875273376946e-05
Epoch 72: reducing lr to 4.816814188629837e-05
Epoch 75: reducing lr to 3.898112202124395e-05
Epoch 78: reducing lr to 3.0492604997354686e-05
Epoch 81: reducing lr to 2.2836434677851604e-05
Epoch 84: reducing lr to 1.6133375999372533e-05
Epoch 87: reducing lr to 1.0489112789046707e-05
Epoch 90: reducing lr to 5.992691877943434e-06
Epoch 93: reducing lr to 2.715001979364815e-06
Epoch 96: reducing lr to 7.077438779592819e-07
Epoch 99: reducing lr to 2.567292755207423e-09
[I 2024-06-21 01:25:09,101] Trial 479 finished with value: 0.9812233448028564 and parameters: {'hidden_size': 147, 'n_layers': 1, 'rnn_dropout': 0.3706928231448998, 'bidirectional': True, 'fc_dropout': 0.6359631884962905, 'learning_rate_model': 0.0016654188164858422}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.0002643109397273467
Epoch 30: reducing lr to 0.00026475646502985696
Epoch 33: reducing lr to 0.00025961518601755145
Epoch 36: reducing lr to 0.00025249940772833113
Epoch 39: reducing lr to 0.00024352134839418954
Epoch 42: reducing lr to 0.00023282259856495174
Epoch 45: reducing lr to 0.00022057188397178788
Epoch 48: reducing lr to 0.00020696241106227362
Epoch 51: reducing lr to 0.00019220880497872298
Epoch 54: reducing lr to 0.00017654375075582592
Epoch 57: reducing lr to 0.00016021424754060232
Epoch 60: reducing lr to 0.00014347789376537086
Epoch 63: reducing lr to 0.00012659857024684644
Epoch 66: reducing lr to 0.0001098425237311355
Epoch 69: reducing lr to 9.347393352023083e-05
Epoch 72: reducing lr to 7.775103900394125e-05
Epoch 75: reducing lr to 6.292172834578996e-05
Epoch 78: reducing lr to 4.921991232457092e-05
Epoch 81: reducing lr to 3.686163621465458e-05
Epoch 84: reducing lr to 2.6041833823556293e-05
Epoch 87: reducing lr to 1.6931095650378264e-05
Epoch 90: reducing lr to 9.67315743755355e-06
Epoch 93: reducing lr to 4.382444838575304e-06
Epoch 96: reducing lr to 1.1424111395018106e-06
Epoch 99: reducing lr to 4.144018667237223e-09
[I 2024-06-21 01:25:48,065] Trial 480 finished with value: 1.120242953300476 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.2684646788889644, 'bidirectional': True, 'fc_dropout': 0.614496775513793, 'learning_rate_model': 0.0026882507459836612}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001337174376495364
Epoch 33: reducing lr to 0.00012961020379102724
Epoch 36: reducing lr to 0.00012605772487658015
Epoch 40: reducing lr to 0.00011988666549253734
Epoch 44: reducing lr to 0.00011223728732780017
Epoch 48: reducing lr to 0.00010332384898720892
Epoch 61: reducing lr to 6.882227074715649e-05
Epoch 64: reducing lr to 6.040121076429393e-05
Epoch 67: reducing lr to 5.2085838527933145e-05
Epoch 72: reducing lr to 3.881640425142023e-05
Epoch 75: reducing lr to 3.141302386382803e-05
Epoch 78: reducing lr to 2.4572533544061824e-05
Epoch 81: reducing lr to 1.8402791666929302e-05
Epoch 84: reducing lr to 1.3001116925167124e-05
Epoch 87: reducing lr to 8.452674865878397e-06
Epoch 90: reducing lr to 4.829224075895348e-06
Epoch 93: reducing lr to 2.1878903824689266e-06
Epoch 96: reducing lr to 5.703369778760261e-07
Epoch 99: reducing lr to 2.068858575708828e-09
[I 2024-06-21 01:26:04,127] Trial 481 finished with value: 0.9771759510040283 and parameters: {'hidden_size': 151, 'n_layers': 1, 'rnn_dropout': 0.3313561625025014, 'bidirectional': True, 'fc_dropout': 0.5763220790590571, 'learning_rate_model': 0.0013420814566863122}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00019500830046536503
Epoch 31: reducing lr to 0.00019167780138099625
Epoch 34: reducing lr to 0.00018744769060867456
Epoch 40: reducing lr to 0.0001748380412989501
Epoch 46: reducing lr to 0.00015739176008381967
Epoch 49: reducing lr to 0.0001471873533867417
Epoch 52: reducing lr to 0.00013620507160615035
Epoch 55: reducing lr to 0.00012461809418740647
Epoch 58: reducing lr to 0.00011260919815415977
Epoch 61: reducing lr to 0.0001003677178420471
Epoch 64: reducing lr to 8.808677210870942e-05
Epoch 67: reducing lr to 7.595995726650757e-05
Epoch 70: reducing lr to 6.417860141098137e-05
Epoch 73: reducing lr to 5.2928466741605027e-05
Epoch 76: reducing lr to 4.238695916874184e-05
Epoch 79: reducing lr to 3.272035562210708e-05
Epoch 82: reducing lr to 2.408107560669399e-05
Epoch 85: reducing lr to 1.6605391182830165e-05
Epoch 88: reducing lr to 1.0411168011827447e-05
Epoch 91: reducing lr to 5.5961293790395645e-06
Epoch 94: reducing lr to 2.23618685864953e-06
Epoch 97: reducing lr to 3.8433865062526864e-07
[I 2024-06-21 01:26:20,836] Trial 482 finished with value: 0.9834237098693848 and parameters: {'hidden_size': 157, 'n_layers': 1, 'rnn_dropout': 0.28234283641207997, 'bidirectional': True, 'fc_dropout': 0.5829110726973991, 'learning_rate_model': 0.001957239299188639}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00014438107470168387
Epoch 33: reducing lr to 0.0001399462990361725
Epoch 36: reducing lr to 0.0001361105186582442
Epoch 39: reducing lr to 0.00013127087042497278
Epoch 46: reducing lr to 0.0001165303805830947
Epoch 61: reducing lr to 7.431067771376161e-05
Epoch 64: reducing lr to 6.521805889137832e-05
Epoch 67: reducing lr to 5.623955615356073e-05
Epoch 72: reducing lr to 4.1911917102118456e-05
Epoch 75: reducing lr to 3.391813532185833e-05
Epoch 78: reducing lr to 2.653213270907397e-05
Epoch 81: reducing lr to 1.9870369078910142e-05
Epoch 84: reducing lr to 1.4037924050696086e-05
Epoch 87: reducing lr to 9.126754914628478e-06
Epoch 90: reducing lr to 5.214342828498207e-06
Epoch 93: reducing lr to 2.362369263897088e-06
Epoch 96: reducing lr to 6.158199503020255e-07
Epoch 99: reducing lr to 2.233844963064717e-09
[I 2024-06-21 01:26:35,500] Trial 483 finished with value: 0.9786632061004639 and parameters: {'hidden_size': 145, 'n_layers': 1, 'rnn_dropout': 0.3244343349724785, 'bidirectional': True, 'fc_dropout': 0.5764924261353093, 'learning_rate_model': 0.0014491091547942398}. Best is trial 235 with value: 0.9706074595451355.
Epoch 38: reducing lr to 4.966193464380889e-05
Epoch 50: reducing lr to 3.970328502661113e-05
Epoch 55: reducing lr to 3.445394190035716e-05
Epoch 61: reducing lr to 2.774928907194596e-05
Epoch 64: reducing lr to 2.435389939328872e-05
Epoch 67: reducing lr to 2.1001123243612738e-05
Epoch 70: reducing lr to 1.7743858295046766e-05
Epoch 73: reducing lr to 1.4633463381712765e-05
Epoch 76: reducing lr to 1.1718987022353491e-05
Epoch 79: reducing lr to 9.04640083700644e-06
Epoch 82: reducing lr to 6.6578451970499465e-06
Epoch 85: reducing lr to 4.590996089103646e-06
Epoch 88: reducing lr to 2.8784405678273324e-06
Epoch 91: reducing lr to 1.5471967995462623e-06
Epoch 94: reducing lr to 6.182525307311081e-07
Epoch 97: reducing lr to 1.0626050434369262e-07
[I 2024-06-21 01:26:51,305] Trial 484 finished with value: 0.9832989573478699 and parameters: {'hidden_size': 150, 'n_layers': 1, 'rnn_dropout': 0.4037008931205024, 'bidirectional': True, 'fc_dropout': 0.5996940829542389, 'learning_rate_model': 0.0005411301588189099}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010810322691737578
Epoch 33: reducing lr to 0.00010478275322588276
Epoch 40: reducing lr to 9.692180490382153e-05
Epoch 43: reducing lr to 9.23868332891719e-05
Epoch 46: reducing lr to 8.72504253135022e-05
Epoch 55: reducing lr to 6.908228050705346e-05
Epoch 64: reducing lr to 4.883107175931849e-05
Epoch 72: reducing lr to 3.1380937525184616e-05
Epoch 75: reducing lr to 2.539570983862712e-05
Epoch 78: reducing lr to 1.986554795202325e-05
Epoch 81: reducing lr to 1.4877649455842285e-05
Epoch 84: reducing lr to 1.0510691184677732e-05
Epoch 87: reducing lr to 6.833524820298713e-06
Epoch 90: reducing lr to 3.904163251165771e-06
Epoch 93: reducing lr to 1.7687895808045536e-06
Epoch 96: reducing lr to 4.6108621898884384e-07
Epoch 99: reducing lr to 1.6725588823868555e-09
[I 2024-06-21 01:27:07,607] Trial 485 finished with value: 0.9783470630645752 and parameters: {'hidden_size': 156, 'n_layers': 1, 'rnn_dropout': 0.37810615503782774, 'bidirectional': True, 'fc_dropout': 0.6227365249201243, 'learning_rate_model': 0.001084999374831093}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00013075029943940495
Epoch 36: reducing lr to 0.00012326055272956482
Epoch 40: reducing lr to 0.00011722642676585404
Epoch 44: reducing lr to 0.00010974678534327479
Epoch 47: reducing lr to 0.0001033131022231388
Epoch 61: reducing lr to 6.729513118457298e-05
Epoch 67: reducing lr to 5.093007392146483e-05
Epoch 72: reducing lr to 3.7955083257997066e-05
Epoch 75: reducing lr to 3.071598101705717e-05
Epoch 78: reducing lr to 2.4027278531103725e-05
Epoch 81: reducing lr to 1.7994440839334564e-05
Epoch 84: reducing lr to 1.2712627170344314e-05
Epoch 87: reducing lr to 8.26511328069384e-06
Epoch 90: reducing lr to 4.722065461934825e-06
Epoch 93: reducing lr to 2.1393419413118576e-06
Epoch 96: reducing lr to 5.576814209834295e-07
Epoch 99: reducing lr to 2.022951404306861e-09
[I 2024-06-21 01:27:22,720] Trial 486 finished with value: 0.9768335819244385 and parameters: {'hidden_size': 143, 'n_layers': 1, 'rnn_dropout': 0.3537734926590209, 'bidirectional': True, 'fc_dropout': 0.6485306498700403, 'learning_rate_model': 0.0013123011883739643}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 7.92467049390836e-05
Epoch 43: reducing lr to 7.55387513180188e-05
Epoch 49: reducing lr to 6.671381512825143e-05
Epoch 52: reducing lr to 6.173601031324392e-05
Epoch 55: reducing lr to 5.648412248713307e-05
Epoch 72: reducing lr to 2.5658167418960928e-05
Epoch 75: reducing lr to 2.0764433001401048e-05
Epoch 78: reducing lr to 1.624277652040641e-05
Epoch 81: reducing lr to 1.2164493818333383e-05
Epoch 84: reducing lr to 8.593913865352925e-06
Epoch 87: reducing lr to 5.587332238246047e-06
Epoch 90: reducing lr to 3.192182331996046e-06
Epoch 93: reducing lr to 1.4462250898901398e-06
Epoch 96: reducing lr to 3.7700044467751415e-07
Epoch 99: reducing lr to 1.3675434581191664e-09
[I 2024-06-21 01:27:36,405] Trial 487 finished with value: 0.9801491498947144 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.34831179277922386, 'bidirectional': True, 'fc_dropout': 0.646772216211019, 'learning_rate_model': 0.0008871339674457463}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00017605380653123748
Epoch 31: reducing lr to 0.00017304702661442026
Epoch 34: reducing lr to 0.0001692280758223831
Epoch 40: reducing lr to 0.00015784406419465613
Epoch 46: reducing lr to 0.00014209353352284172
Epoch 61: reducing lr to 9.061213669765786e-05
Epoch 64: reducing lr to 7.952487918606456e-05
Epoch 67: reducing lr to 6.857677129027614e-05
Epoch 70: reducing lr to 5.794054432191253e-05
Epoch 73: reducing lr to 4.77839046926957e-05
Epoch 76: reducing lr to 3.8267014743131484e-05
Epoch 79: reducing lr to 2.9539989552141376e-05
Epoch 82: reducing lr to 2.174043369337494e-05
Epoch 85: reducing lr to 1.4991373801530706e-05
Epoch 88: reducing lr to 9.399219184744483e-06
Epoch 91: reducing lr to 5.052194581820824e-06
Epoch 94: reducing lr to 2.0188330837245595e-06
Epoch 97: reducing lr to 3.4698155041699206e-07
[I 2024-06-21 01:27:51,053] Trial 488 finished with value: 0.9801560640335083 and parameters: {'hidden_size': 141, 'n_layers': 1, 'rnn_dropout': 0.3332667844808722, 'bidirectional': True, 'fc_dropout': 0.6647502463473208, 'learning_rate_model': 0.001766998779500114}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00012154585311019222
Epoch 36: reducing lr to 0.00011458336310190996
Epoch 40: reducing lr to 0.00010897402231127193
Epoch 43: reducing lr to 0.00010387512739896382
Epoch 48: reducing lr to 9.391883057686246e-05
Epoch 55: reducing lr to 7.767265565018754e-05
Epoch 64: reducing lr to 5.4903210981923006e-05
Epoch 72: reducing lr to 3.5283154181988026e-05
Epoch 75: reducing lr to 2.855366399038264e-05
Epoch 78: reducing lr to 2.2335826988546588e-05
Epoch 81: reducing lr to 1.6727683779197906e-05
Epoch 86: reducing lr to 8.970944991702452e-06
Epoch 89: reducing lr to 5.390612243657958e-06
Epoch 92: reducing lr to 2.6872134433946013e-06
Epoch 95: reducing lr to 9.033907698603651e-07
Epoch 98: reducing lr to 6.727142454374893e-08
[I 2024-06-21 01:28:04,269] Trial 489 finished with value: 0.9771526455879211 and parameters: {'hidden_size': 132, 'n_layers': 1, 'rnn_dropout': 0.3597897594576978, 'bidirectional': True, 'fc_dropout': 0.7030747314720315, 'learning_rate_model': 0.0012199189459780443}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 9.926410105328238e-05
Epoch 40: reducing lr to 8.908769114135772e-05
Epoch 43: reducing lr to 8.491927773901091e-05
Epoch 46: reducing lr to 8.019804160678504e-05
Epoch 55: reducing lr to 6.34984137497253e-05
Epoch 64: reducing lr to 4.488409438219318e-05
Epoch 72: reducing lr to 2.8844440863891285e-05
Epoch 75: reducing lr to 2.33429944548004e-05
Epoch 78: reducing lr to 1.8259831232609432e-05
Epoch 81: reducing lr to 1.367510067468012e-05
Epoch 84: reducing lr to 9.661120228538379e-06
Epoch 87: reducing lr to 6.281176348311773e-06
Epoch 90: reducing lr to 3.5885927860138262e-06
Epoch 93: reducing lr to 1.6258197009964445e-06
Epoch 96: reducing lr to 4.23816980168459e-07
Epoch 99: reducing lr to 1.5373672547459512e-09
[I 2024-06-21 01:28:17,389] Trial 490 finished with value: 0.977994441986084 and parameters: {'hidden_size': 131, 'n_layers': 1, 'rnn_dropout': 0.350470239319122, 'bidirectional': True, 'fc_dropout': 0.6853420822650332, 'learning_rate_model': 0.0009972997231060376}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 6.543411982118501e-05
Epoch 45: reducing lr to 6.0102513883684495e-05
Epoch 49: reducing lr to 5.508569442964237e-05
Epoch 52: reducing lr to 5.0975513735542204e-05
Epoch 55: reducing lr to 4.6639022299521955e-05
Epoch 61: reducing lr to 3.7563182626976936e-05
Epoch 64: reducing lr to 3.2966969648025456e-05
Epoch 67: reducing lr to 2.842844102153983e-05
Epoch 72: reducing lr to 2.1185986251098874e-05
Epoch 75: reducing lr to 1.714522260675784e-05
Epoch 78: reducing lr to 1.3411684257181353e-05
Epoch 81: reducing lr to 1.0044240283362567e-05
Epoch 84: reducing lr to 7.0960072097724395e-06
Epoch 87: reducing lr to 4.613468376246014e-06
Epoch 90: reducing lr to 2.6357896061856624e-06
Epoch 93: reducing lr to 1.1941501655244594e-06
Epoch 96: reducing lr to 3.112898168905671e-07
Epoch 99: reducing lr to 1.129182627436498e-09
[I 2024-06-21 01:28:31,068] Trial 491 finished with value: 0.9832252264022827 and parameters: {'hidden_size': 133, 'n_layers': 1, 'rnn_dropout': 0.35956746673226875, 'bidirectional': True, 'fc_dropout': 0.6469790262112871, 'learning_rate_model': 0.0007325078104876404}. Best is trial 235 with value: 0.9706074595451355.
Epoch 23: reducing lr to 0.00011789482637927667
Epoch 27: reducing lr to 0.00011801505192024668
Epoch 33: reducing lr to 0.00011439012890660836
Epoch 36: reducing lr to 0.00011125481618372423
Epoch 39: reducing lr to 0.00010729895604966336
Epoch 42: reducing lr to 0.00010258493530658044
Epoch 45: reducing lr to 9.718709690195281e-05
Epoch 48: reducing lr to 9.11905703337224e-05
Epoch 55: reducing lr to 7.541633264139446e-05
Epoch 64: reducing lr to 5.3308320512965085e-05
Epoch 72: reducing lr to 3.425820927779739e-05
Epoch 75: reducing lr to 2.7724204916175077e-05
Epoch 78: reducing lr to 2.1686990664710166e-05
Epoch 81: reducing lr to 1.6241759131986133e-05
Epoch 84: reducing lr to 1.1474400915205645e-05
Epoch 87: reducing lr to 7.460080605014956e-06
Epoch 90: reducing lr to 4.262130205822637e-06
Epoch 93: reducing lr to 1.9309672816167052e-06
Epoch 96: reducing lr to 5.033625325104188e-07
Epoch 99: reducing lr to 1.825913332770962e-09
[I 2024-06-21 01:28:43,951] Trial 492 finished with value: 0.9799041748046875 and parameters: {'hidden_size': 126, 'n_layers': 1, 'rnn_dropout': 0.3651881229023454, 'bidirectional': True, 'fc_dropout': 0.6317924247011151, 'learning_rate_model': 0.0011844813629105955}. Best is trial 235 with value: 0.9706074595451355.
Epoch 33: reducing lr to 8.515576515270443e-05
Epoch 40: reducing lr to 7.876726085612483e-05
Epoch 43: reducing lr to 7.508174042549833e-05
Epoch 46: reducing lr to 7.090743942806505e-05
Epoch 49: reducing lr to 6.631019526873257e-05
Epoch 52: reducing lr to 6.136250626821241e-05
Epoch 55: reducing lr to 5.614239246405652e-05
Epoch 72: reducing lr to 2.5502935014559475e-05
Epoch 75: reducing lr to 2.063880778397191e-05
Epoch 78: reducing lr to 1.614450741130569e-05
Epoch 81: reducing lr to 1.2090898397705205e-05
Epoch 84: reducing lr to 8.541920521839555e-06
Epoch 87: reducing lr to 5.553528771171784e-06
Epoch 90: reducing lr to 3.1728695677369085e-06
Epoch 93: reducing lr to 1.4374754003919138e-06
Epoch 96: reducing lr to 3.7471958476525656e-07
Epoch 99: reducing lr to 1.3592697939977284e-09
[I 2024-06-21 01:28:57,169] Trial 493 finished with value: 0.9816501140594482 and parameters: {'hidden_size': 136, 'n_layers': 1, 'rnn_dropout': 0.37194718098844626, 'bidirectional': True, 'fc_dropout': 0.6656537698236212, 'learning_rate_model': 0.0008817667899484542}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015680337131140484
Epoch 33: reducing lr to 0.0001519870352590604
Epoch 36: reducing lr to 0.00014782123100727726
Epoch 40: reducing lr to 0.00014058475584749358
Epoch 46: reducing lr to 0.00012655645189914367
Epoch 61: reducing lr to 8.070423920883314e-05
Epoch 64: reducing lr to 7.082930727370877e-05
Epoch 67: reducing lr to 6.107830977263493e-05
Epoch 72: reducing lr to 4.551794557087928e-05
Epoch 75: reducing lr to 3.683639270626515e-05
Epoch 78: reducing lr to 2.8814911271857136e-05
Epoch 81: reducing lr to 2.1579981082788466e-05
Epoch 84: reducing lr to 1.5245722626117336e-05
Epoch 87: reducing lr to 9.912005037388678e-06
Epoch 90: reducing lr to 5.662975818481234e-06
Epoch 93: reducing lr to 2.565623407547535e-06
Epoch 96: reducing lr to 6.688040280050266e-07
Epoch 99: reducing lr to 2.4260410993711194e-09
[I 2024-06-21 01:29:09,257] Trial 494 finished with value: 0.9774270057678223 and parameters: {'hidden_size': 128, 'n_layers': 1, 'rnn_dropout': 0.34027488934555966, 'bidirectional': True, 'fc_dropout': 0.6537724421139189, 'learning_rate_model': 0.0015737879866834596}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00011830757084846852
Epoch 33: reducing lr to 0.00011467366289115071
Epoch 36: reducing lr to 0.00011153057880095016
Epoch 40: reducing lr to 0.0001060706847278881
Epoch 44: reducing lr to 9.930283630752533e-05
Epoch 47: reducing lr to 9.348140855695383e-05
Epoch 55: reducing lr to 7.560326392207204e-05
Epoch 64: reducing lr to 5.344045359707624e-05
Epoch 72: reducing lr to 3.4343123655223694e-05
Epoch 75: reducing lr to 2.7792923732766057e-05
Epoch 78: reducing lr to 2.174074529314422e-05
Epoch 81: reducing lr to 1.6282016894842832e-05
Epoch 84: reducing lr to 1.1502842028462729e-05
Epoch 87: reducing lr to 7.478571591948576e-06
Epoch 90: reducing lr to 4.272694568075276e-06
Epoch 93: reducing lr to 1.9357534887187723e-06
Epoch 96: reducing lr to 5.046101959747156e-07
Epoch 99: reducing lr to 1.8304391470862907e-09
[I 2024-06-21 01:29:23,390] Trial 495 finished with value: 0.9786076545715332 and parameters: {'hidden_size': 135, 'n_layers': 1, 'rnn_dropout': 0.38389570009539353, 'bidirectional': True, 'fc_dropout': 0.6185100888420191, 'learning_rate_model': 0.0011874172868723246}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00010556016177607889
Epoch 33: reducing lr to 0.00010231780028473218
Epoch 38: reducing lr to 9.723279546177698e-05
Epoch 41: reducing lr to 9.323544406552675e-05
Epoch 44: reducing lr to 8.86031501641769e-05
Epoch 47: reducing lr to 8.340896985339052e-05
Epoch 55: reducing lr to 6.745716029141868e-05
Epoch 64: reducing lr to 4.7682349376607365e-05
Epoch 72: reducing lr to 3.064271933691067e-05
Epoch 75: reducing lr to 2.479829062857464e-05
Epoch 78: reducing lr to 1.9398222563594612e-05
Epoch 81: reducing lr to 1.4527661460160111e-05
Epoch 86: reducing lr to 7.79108772843034e-06
Epoch 89: reducing lr to 4.681639775869304e-06
Epoch 92: reducing lr to 2.3337915572851487e-06
Epoch 95: reducing lr to 7.845769590100445e-07
Epoch 98: reducing lr to 5.8423897451344296e-08
[I 2024-06-21 01:29:38,153] Trial 496 finished with value: 0.978512167930603 and parameters: {'hidden_size': 142, 'n_layers': 1, 'rnn_dropout': 0.3502754198002758, 'bidirectional': True, 'fc_dropout': 0.6423140104096143, 'learning_rate_model': 0.0010594753995794499}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.0001347269348353836
Epoch 33: reducing lr to 0.00013058869349501822
Epoch 38: reducing lr to 0.00012409867773629718
Epoch 47: reducing lr to 0.00010645526358667203
Epoch 50: reducing lr to 9.921331516640799e-05
Epoch 53: reducing lr to 9.147299100510526e-05
Epoch 56: reducing lr to 8.335633675683376e-05
Epoch 59: reducing lr to 7.49913929003803e-05
Epoch 62: reducing lr to 6.651004891924121e-05
Epoch 65: reducing lr to 5.804608566281544e-05
Epoch 68: reducing lr to 4.973294762985479e-05
Epoch 71: reducing lr to 4.170178720827827e-05
Epoch 74: reducing lr to 3.407922429511736e-05
Epoch 77: reducing lr to 2.6985493719119394e-05
Epoch 80: reducing lr to 2.053244685576831e-05
Epoch 83: reducing lr to 1.4821871121363907e-05
Epoch 86: reducing lr to 9.943802197950577e-06
Epoch 89: reducing lr to 5.975199037154371e-06
Epoch 92: reducing lr to 2.97862922685457e-06
Epoch 95: reducing lr to 1.0013592917195065e-06
Epoch 98: reducing lr to 7.456669724942907e-08
[I 2024-06-21 01:30:12,663] Trial 497 finished with value: 0.99244225025177 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.35708384458100306, 'bidirectional': True, 'fc_dropout': 0.673998654645145, 'learning_rate_model': 0.0013522134744509173}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 7.338268379814729e-05
Epoch 43: reducing lr to 6.994910774824146e-05
Epoch 49: reducing lr to 6.177719066411373e-05
Epoch 52: reducing lr to 5.7167728642577525e-05
Epoch 55: reducing lr to 5.230446493990265e-05
Epoch 66: reducing lr to 3.356621047946866e-05
Epoch 72: reducing lr to 2.3759539125227738e-05
Epoch 75: reducing lr to 1.9227926541058386e-05
Epoch 78: reducing lr to 1.5040859229632189e-05
Epoch 81: reducing lr to 1.1264357352415595e-05
Epoch 84: reducing lr to 7.957989726569445e-06
Epoch 87: reducing lr to 5.1738862231505874e-06
Epoch 90: reducing lr to 2.955970306588336e-06
Epoch 93: reducing lr to 1.3392087223555185e-06
Epoch 96: reducing lr to 3.491035298539805e-07
Epoch 99: reducing lr to 1.2663492979930746e-09
[I 2024-06-21 01:30:26,486] Trial 498 finished with value: 0.9809944033622742 and parameters: {'hidden_size': 138, 'n_layers': 1, 'rnn_dropout': 0.378680135253618, 'bidirectional': True, 'fc_dropout': 0.6086668837809108, 'learning_rate_model': 0.0008214886848571084}. Best is trial 235 with value: 0.9706074595451355.
Epoch 22: reducing lr to 0.00020592349155455682
Epoch 27: reducing lr to 0.0002086746528021576
Epoch 30: reducing lr to 0.00020627059835975924
Epoch 33: reducing lr to 0.00020226505047603473
Epoch 40: reducing lr to 0.0001870908442235647
Epoch 46: reducing lr to 0.0001684219123546736
Epoch 49: reducing lr to 0.00015750237190699425
Epoch 52: reducing lr to 0.00014575044220927665
Epoch 55: reducing lr to 0.00013335143927395156
Epoch 58: reducing lr to 0.00012050094929842337
Epoch 61: reducing lr to 0.00010740157533424557
Epoch 64: reducing lr to 9.425997017758946e-05
Epoch 67: reducing lr to 8.128329753979082e-05
Epoch 70: reducing lr to 6.867629395674469e-05
Epoch 73: reducing lr to 5.663774000541387e-05
Epoch 76: reducing lr to 4.535747435759734e-05
Epoch 79: reducing lr to 3.501342677574388e-05
Epoch 82: reducing lr to 2.5768698457129886e-05
Epoch 85: reducing lr to 1.7769111527314326e-05
Epoch 88: reducing lr to 1.1140791776290994e-05
Epoch 91: reducing lr to 5.988311022763057e-06
Epoch 94: reducing lr to 2.392900790457956e-06
Epoch 97: reducing lr to 4.112734395729685e-07
[I 2024-06-21 01:30:40,435] Trial 499 finished with value: 0.9847826957702637 and parameters: {'hidden_size': 144, 'n_layers': 1, 'rnn_dropout': 0.5716326034547434, 'bidirectional': True, 'fc_dropout': 0.05018492124067646, 'learning_rate_model': 0.0020944043419396253}. Best is trial 235 with value: 0.9706074595451355.
Epoch 27: reducing lr to 0.00015848198110967083
Epoch 33: reducing lr to 0.0001536140852673704
Epoch 36: reducing lr to 0.00014940368529180772
Epoch 40: reducing lr to 0.000142089742294397
Epoch 61: reducing lr to 8.156819338000021e-05
Epoch 64: reducing lr to 7.158754842757987e-05
Epoch 67: reducing lr to 6.17321646508075e-05
Epoch 72: reducing lr to 4.60052238021647e-05
Epoch 75: reducing lr to 3.723073326930514e-05
Epoch 78: reducing lr to 2.9123380356370917e-05
Epoch 81: reducing lr to 2.1810998868879444e-05
Epoch 84: reducing lr to 1.540893097532446e-05
Epoch 87: reducing lr to 1.0018114929268344e-05
Epoch 90: reducing lr to 5.723599047540296e-06
Epoch 93: reducing lr to 2.593088892215048e-06
Epoch 96: reducing lr to 6.759637018381682e-07
Epoch 99: reducing lr to 2.4520123289651155e-09
[I 2024-06-21 01:30:54,532] Trial 500 finished with value: 0.9788996577262878 and parameters: {'hidden_size': 139, 'n_layers': 1, 'rnn_dropout': 0.32366592902698976, 'bidirectional': True, 'fc_dropout': 0.07664726240799158, 'learning_rate_model': 0.0015906356852548997}. Best is trial 235 with value: 0.9706074595451355.
Epoch 45: reducing lr to 8.422588021054989e-05
Epoch 52: reducing lr to 7.143557292578614e-05
Epoch 55: reducing lr to 6.535854245529418e-05
Epoch 58: reducing lr to 5.9060228022320874e-05
Epoch 61: reducing lr to 5.2639929943522966e-05
Epoch 64: reducing lr to 4.619893340656396e-05
Epoch 67: reducing lr to 3.983877401013148e-05
Epoch 70: reducing lr to 3.365979773958801e-05
Epoch 73: reducing lr to 2.7759431430740068e-05
Epoch 76: reducing lr to 2.2230719290369283e-05
Epoch 79: reducing lr to 1.7160868700686043e-05
Epoch 82: reducing lr to 1.2629819230282465e-05
Epoch 85: reducing lr to 8.709041585707789e-06
Epoch 88: reducing lr to 5.460352856038054e-06
Epoch 91: reducing lr to 2.9350060437871755e-06
Epoch 94: reducing lr to 1.1728145474542697e-06
Epoch 97: reducing lr to 2.015743715059982e-07
[I 2024-06-21 01:31:34,042] Trial 501 finished with value: 0.976027250289917 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.3904051542959512, 'bidirectional': True, 'fc_dropout': 0.631783888202613, 'learning_rate_model': 0.001026514718150122}. Best is trial 235 with value: 0.9706074595451355.
Epoch 40: reducing lr to 6.027914445316916e-05
Epoch 53: reducing lr to 4.5648098905201555e-05
Epoch 65: reducing lr to 2.896694893521212e-05
Epoch 72: reducing lr to 1.9516929838786928e-05
Epoch 79: reducing lr to 1.1281040604496313e-05
Epoch 85: reducing lr to 5.725062843158336e-06
Epoch 88: reducing lr to 3.589472267297297e-06
Epoch 99: reducing lr to 1.0402243187483673e-09
[I 2024-06-21 01:32:13,669] Trial 502 finished with value: 0.9691120386123657 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.3878638851398578, 'bidirectional': True, 'fc_dropout': 0.15813185092775106, 'learning_rate_model': 0.0006748000010105353}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.9309682368606035e-05
Epoch 53: reducing lr to 3.7341161328109414e-05
Epoch 65: reducing lr to 2.369560922173714e-05
Epoch 72: reducing lr to 1.5965283181957247e-05
Epoch 75: reducing lr to 1.2920254497020946e-05
Epoch 78: reducing lr to 1.01067438907543e-05
Epoch 81: reducing lr to 7.569113779783951e-06
Epoch 84: reducing lr to 5.347391583403443e-06
Epoch 87: reducing lr to 3.4766061020148123e-06
Epoch 90: reducing lr to 1.986271820063649e-06
Epoch 93: reducing lr to 8.998847317476353e-07
Epoch 96: reducing lr to 2.345810112125337e-07
Epoch 99: reducing lr to 8.509266548979363e-10
[I 2024-06-21 01:32:53,300] Trial 503 finished with value: 0.9706541299819946 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.3936168270556565, 'bidirectional': True, 'fc_dropout': 0.5926741525297837, 'learning_rate_model': 0.0005520014262646879}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.669587154103586e-05
Epoch 49: reducing lr to 3.931090565388079e-05
Epoch 61: reducing lr to 2.6806283257345717e-05
Epoch 65: reducing lr to 2.2439550837773987e-05
Epoch 72: reducing lr to 1.511899441995962e-05
Epoch 75: reducing lr to 1.2235376812212001e-05
Epoch 78: reducing lr to 9.571004957867742e-06
Epoch 86: reducing lr to 3.844091335251248e-06
Epoch 89: reducing lr to 2.3099022273251444e-06
Epoch 92: reducing lr to 1.151483363600865e-06
Epoch 95: reducing lr to 3.871071145769198e-07
Epoch 98: reducing lr to 2.882611591508181e-08
[I 2024-06-21 01:33:29,910] Trial 504 finished with value: 0.9695152044296265 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.39293200668127404, 'bidirectional': True, 'fc_dropout': 0.016570378775941717, 'learning_rate_model': 0.000522740899011253}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.5920875770867156e-05
Epoch 49: reducing lr to 3.8658475693847046e-05
Epoch 61: reducing lr to 2.636138833510124e-05
Epoch 65: reducing lr to 2.206712911375759e-05
Epoch 72: reducing lr to 1.4868069523646762e-05
Epoch 75: reducing lr to 1.2032310353380586e-05
Epoch 78: reducing lr to 9.41215818803943e-06
Epoch 81: reducing lr to 7.0489266383575705e-06
Epoch 84: reducing lr to 4.97989224031159e-06
Epoch 87: reducing lr to 3.237676440935751e-06
Epoch 90: reducing lr to 1.8497653425240696e-06
Epoch 93: reducing lr to 8.380401777033732e-07
Epoch 96: reducing lr to 2.1845943751106716e-07
Epoch 99: reducing lr to 7.924467433704889e-10
[I 2024-06-21 01:34:07,269] Trial 505 finished with value: 0.9727622270584106 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.3978909404488522, 'bidirectional': True, 'fc_dropout': 0.01878805305534364, 'learning_rate_model': 0.0005140651430555708}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.5558881580259334e-05
Epoch 49: reducing lr to 3.835373099148654e-05
Epoch 61: reducing lr to 2.615358155281601e-05
Epoch 65: reducing lr to 2.1893173970078884e-05
Epoch 72: reducing lr to 1.4750864555257893e-05
Epoch 75: reducing lr to 1.1937459670017147e-05
Epoch 78: reducing lr to 9.337962160024782e-06
Epoch 81: reducing lr to 6.9933599608874214e-06
Epoch 86: reducing lr to 3.7504921987055655e-06
Epoch 89: reducing lr to 2.2536588045947956e-06
Epoch 92: reducing lr to 1.1234460879015447e-06
Epoch 95: reducing lr to 3.7768150823327223e-07
Epoch 98: reducing lr to 2.8124233643215048e-08
[I 2024-06-21 01:34:43,248] Trial 506 finished with value: 0.9735270142555237 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.3937062033387904, 'bidirectional': True, 'fc_dropout': 0.02715366199399816, 'learning_rate_model': 0.0005100127683511199}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.695275603966991e-05
Epoch 61: reducing lr to 2.5198250436166056e-05
Epoch 65: reducing lr to 2.1093465895924304e-05
Epoch 72: reducing lr to 1.4212048872263628e-05
Epoch 75: reducing lr to 1.1501411297311825e-05
Epoch 79: reducing lr to 8.214750051643214e-06
Epoch 86: reducing lr to 3.613495210628016e-06
Epoch 89: reducing lr to 2.1713377512433235e-06
Epoch 92: reducing lr to 1.08240914604012e-06
Epoch 95: reducing lr to 3.6388565789170984e-07
Epoch 98: reducing lr to 2.7096919067695537e-08
[I 2024-06-21 01:35:19,742] Trial 507 finished with value: 0.9703141450881958 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.40018936141768835, 'bidirectional': True, 'fc_dropout': 0.018138132539507634, 'learning_rate_model': 0.0004913831567045976}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.568678019550201e-05
Epoch 49: reducing lr to 3.846140240292275e-05
Epoch 61: reducing lr to 2.6227003172228442e-05
Epoch 64: reducing lr to 2.3017879664876172e-05
Epoch 67: reducing lr to 1.9849031969883408e-05
Epoch 72: reducing lr to 1.4792275035164565e-05
Epoch 75: reducing lr to 1.197097200632463e-05
Epoch 78: reducing lr to 9.364176860386815e-06
Epoch 81: reducing lr to 7.012992599439259e-06
Epoch 86: reducing lr to 3.7610210515231643e-06
Epoch 89: reducing lr to 2.2599855586839976e-06
Epoch 92: reducing lr to 1.1265999668809857e-06
Epoch 95: reducing lr to 3.7874178320557344e-07
Epoch 98: reducing lr to 2.820318752472283e-08
[I 2024-06-21 01:35:55,687] Trial 508 finished with value: 0.973689079284668 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.3982991964013317, 'bidirectional': True, 'fc_dropout': 0.02000486737263006, 'learning_rate_model': 0.0005114445402595957}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.502908559696912e-05
Epoch 61: reducing lr to 2.388649091490633e-05
Epoch 65: reducing lr to 1.9995391456373817e-05
Epoch 72: reducing lr to 1.3472204236143852e-05
Epoch 75: reducing lr to 1.0902675848777711e-05
Epoch 79: reducing lr to 7.787110179489801e-06
Epoch 86: reducing lr to 3.425385454374288e-06
Epoch 89: reducing lr to 2.058303198456432e-06
Epoch 92: reducing lr to 1.0260615632262453e-06
Epoch 95: reducing lr to 3.449426571624148e-07
Epoch 98: reducing lr to 2.568631948365375e-08
[I 2024-06-21 01:36:32,253] Trial 509 finished with value: 0.9707748889923096 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.39900503044806873, 'bidirectional': True, 'fc_dropout': 0.021187965161579326, 'learning_rate_model': 0.0004658029468393611}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.707624718452182e-05
Epoch 49: reducing lr to 3.963112477691333e-05
Epoch 53: reducing lr to 3.5649828926060486e-05
Epoch 65: reducing lr to 2.2622339129495944e-05
Epoch 68: reducing lr to 1.9382454378190135e-05
Epoch 72: reducing lr to 1.5242150858453307e-05
Epoch 75: reducing lr to 1.233504385288707e-05
Epoch 78: reducing lr to 9.648968534721789e-06
Epoch 81: reducing lr to 7.226277967098426e-06
Epoch 84: reducing lr to 5.1051865654077804e-06
Epoch 87: reducing lr to 3.3191365338395837e-06
Epoch 90: reducing lr to 1.8963055263259886e-06
Epoch 93: reducing lr to 8.59125308345129e-07
Epoch 96: reducing lr to 2.2395588732623557e-07
Epoch 99: reducing lr to 8.123847410426189e-10
[I 2024-06-21 01:37:05,127] Trial 510 finished with value: 0.9722132086753845 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.40083766644594504, 'bidirectional': True, 'fc_dropout': 0.015742150494891513, 'learning_rate_model': 0.0005269990464507562}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.47916065332189e-05
Epoch 61: reducing lr to 2.372455287393043e-05
Epoch 65: reducing lr to 1.9859833055078012e-05
Epoch 72: reducing lr to 1.338086966676736e-05
Epoch 75: reducing lr to 1.0828761351473103e-05
Epoch 78: reducing lr to 8.470693642967461e-06
Epoch 81: reducing lr to 6.343847699155278e-06
Epoch 84: reducing lr to 4.481771417343464e-06
Epoch 87: reducing lr to 2.913823237806493e-06
Epoch 90: reducing lr to 1.6647399262595592e-06
Epoch 93: reducing lr to 7.542140138320303e-07
Epoch 96: reducing lr to 1.9660772073749888e-07
Epoch 99: reducing lr to 7.131811277823197e-10
[I 2024-06-21 01:37:39,240] Trial 511 finished with value: 0.9717972278594971 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.39731040964632974, 'bidirectional': True, 'fc_dropout': 0.01591321069677589, 'learning_rate_model': 0.0004626450440330977}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.400127725255965e-05
Epoch 49: reducing lr to 3.7042462248628415e-05
Epoch 53: reducing lr to 3.332122036901998e-05
Epoch 65: reducing lr to 2.1144672221571237e-05
Epoch 68: reducing lr to 1.8116413264357266e-05
Epoch 72: reducing lr to 1.4246549926109116e-05
Epoch 75: reducing lr to 1.152933202950422e-05
Epoch 78: reducing lr to 9.018708267746343e-06
Epoch 81: reducing lr to 6.754265247356094e-06
Epoch 84: reducing lr to 4.771721259132309e-06
Epoch 87: reducing lr to 3.1023340983856903e-06
Epoch 90: reducing lr to 1.7724408849409022e-06
Epoch 93: reducing lr to 8.03008165434514e-07
Epoch 96: reducing lr to 2.0932732917217134e-07
Epoch 99: reducing lr to 7.593206418124963e-10
[I 2024-06-21 01:38:12,036] Trial 512 finished with value: 0.9726839065551758 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.39784277834108545, 'bidirectional': True, 'fc_dropout': 0.013408487792258367, 'learning_rate_model': 0.0004925760344452109}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.767162824810484e-05
Epoch 52: reducing lr to 3.486078596420764e-05
Epoch 55: reducing lr to 3.189517583674526e-05
Epoch 58: reducing lr to 2.8821578434352127e-05
Epoch 61: reducing lr to 2.5688452626235366e-05
Epoch 65: reducing lr to 2.1503814352636747e-05
Epoch 68: reducing lr to 1.8424120435168218e-05
Epoch 72: reducing lr to 1.4488527491293327e-05
Epoch 75: reducing lr to 1.1725157664985755e-05
Epoch 78: reducing lr to 9.171890973668461e-06
Epoch 81: reducing lr to 6.86898640213675e-06
Epoch 86: reducing lr to 3.683791490544043e-06
Epoch 89: reducing lr to 2.2135785617208605e-06
Epoch 92: reducing lr to 1.1034661370913048e-06
Epoch 95: reducing lr to 3.7096462342882644e-07
Epoch 98: reducing lr to 2.762405761268455e-08
[I 2024-06-21 01:38:45,471] Trial 513 finished with value: 0.9754391312599182 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.3975619984292286, 'bidirectional': True, 'fc_dropout': 0.010605066297082482, 'learning_rate_model': 0.0005009424354406341}. Best is trial 502 with value: 0.9691120386123657.
Epoch 53: reducing lr to 3.048720056083402e-05
Epoch 61: reducing lr to 2.311106978952422e-05
Epoch 65: reducing lr to 1.934628611056174e-05
Epoch 68: reducing lr to 1.6575585123134497e-05
Epoch 72: reducing lr to 1.3034859470544604e-05
Epoch 75: reducing lr to 1.0548745034642939e-05
Epoch 78: reducing lr to 8.25165359231774e-06
Epoch 81: reducing lr to 6.1798048497847614e-06
Epoch 84: reducing lr to 4.3658792035965535e-06
Epoch 87: reducing lr to 2.838475926654081e-06
Epoch 90: reducing lr to 1.6216921272083898e-06
Epoch 93: reducing lr to 7.347111156334133e-07
Epoch 96: reducing lr to 1.9152372562168001e-07
Epoch 99: reducing lr to 6.947392814683762e-10
[I 2024-06-21 01:39:18,159] Trial 514 finished with value: 0.9735121726989746 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.40121273633895854, 'bidirectional': True, 'fc_dropout': 0.01621495471968368, 'learning_rate_model': 0.0004506817033494235}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 2.428934256599996e-05
Epoch 65: reducing lr to 2.033261874066432e-05
Epoch 72: reducing lr to 1.3699416334385297e-05
Epoch 75: reducing lr to 1.1086552207287861e-05
Epoch 78: reducing lr to 8.672348042089309e-06
Epoch 86: reducing lr to 3.483155437870091e-06
Epoch 89: reducing lr to 2.0930169973523596e-06
Epoch 92: reducing lr to 1.0433663484432068e-06
Epoch 95: reducing lr to 3.507602014582857e-07
Epoch 98: reducing lr to 2.6119525694287376e-08
[I 2024-06-21 01:39:49,539] Trial 515 finished with value: 0.9717136025428772 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.41136408616452774, 'bidirectional': True, 'fc_dropout': 0.005597262408138104, 'learning_rate_model': 0.00047365883018727527}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 2.4044102003002438e-05
Epoch 64: reducing lr to 2.1102077233938638e-05
Epoch 67: reducing lr to 1.8196976078841196e-05
Epoch 72: reducing lr to 1.3561098363635241e-05
Epoch 75: reducing lr to 1.0974615365126367e-05
Epoch 78: reducing lr to 8.584786531909552e-06
Epoch 86: reducing lr to 3.4479872978403615e-06
Epoch 89: reducing lr to 2.0718845741342655e-06
Epoch 92: reducing lr to 1.0328318619699892e-06
Epoch 95: reducing lr to 3.4721870464547915e-07
Epoch 98: reducing lr to 2.585580644502792e-08
[I 2024-06-21 01:40:20,844] Trial 516 finished with value: 0.9716489315032959 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.4012818593096595, 'bidirectional': True, 'fc_dropout': 0.01694133601070163, 'learning_rate_model': 0.0004688764710983769}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.300369133784111e-05
Epoch 61: reducing lr to 2.250536546600364e-05
Epoch 64: reducing lr to 1.9751619760318818e-05
Epoch 67: reducing lr to 1.7032434689359922e-05
Epoch 72: reducing lr to 1.2693236568199734e-05
Epoch 75: reducing lr to 1.0272279231311949e-05
Epoch 78: reducing lr to 8.03539089645039e-06
Epoch 81: reducing lr to 6.01784200902848e-06
Epoch 84: reducing lr to 4.251456464464604e-06
Epoch 87: reducing lr to 2.7640839942753044e-06
Epoch 90: reducing lr to 1.5791901598907755e-06
Epoch 93: reducing lr to 7.1545550767884e-07
Epoch 96: reducing lr to 1.8650419386817854e-07
Epoch 99: reducing lr to 6.765312716126587e-10
[I 2024-06-21 01:40:50,890] Trial 517 finished with value: 0.9754775762557983 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.4114003844470976, 'bidirectional': True, 'fc_dropout': 0.014278061758584216, 'learning_rate_model': 0.00043887005383529755}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.349649719225703e-05
Epoch 65: reducing lr to 1.9120555457332802e-05
Epoch 72: reducing lr to 1.2882769951852582e-05
Epoch 75: reducing lr to 1.0425663266193564e-05
Epoch 78: reducing lr to 8.155374071536957e-06
Epoch 81: reducing lr to 6.10769946596085e-06
Epoch 86: reducing lr to 3.2755183956264644e-06
Epoch 89: reducing lr to 1.9682485606725383e-06
Epoch 92: reducing lr to 9.811694392235204e-07
Epoch 95: reducing lr to 3.2985076687614614e-07
Epoch 98: reducing lr to 2.456249467551615e-08
[I 2024-06-21 01:41:21,442] Trial 518 finished with value: 0.9737691879272461 and parameters: {'hidden_size': 119, 'n_layers': 2, 'rnn_dropout': 0.4101312293072692, 'bidirectional': True, 'fc_dropout': 0.016479493773957597, 'learning_rate_model': 0.00044542319147205297}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 2.3208227870143116e-05
Epoch 65: reducing lr to 1.9427617180162773e-05
Epoch 68: reducing lr to 1.6645268268500134e-05
Epoch 72: reducing lr to 1.3089657536528946e-05
Epoch 75: reducing lr to 1.0593091567704277e-05
Epoch 78: reducing lr to 8.28634323811358e-06
Epoch 81: reducing lr to 6.2057845202748145e-06
Epoch 84: reducing lr to 4.384233197916089e-06
Epoch 87: reducing lr to 2.850408774221426e-06
Epoch 90: reducing lr to 1.6285096607916163e-06
Epoch 93: reducing lr to 7.377998139262412e-07
Epoch 96: reducing lr to 1.9232888426400477e-07
Epoch 99: reducing lr to 6.976599396552474e-10
[I 2024-06-21 01:41:52,605] Trial 519 finished with value: 0.9723542928695679 and parameters: {'hidden_size': 123, 'n_layers': 2, 'rnn_dropout': 0.40618999419650054, 'bidirectional': True, 'fc_dropout': 0.011351118880507171, 'learning_rate_model': 0.00045257635252258}. Best is trial 502 with value: 0.9691120386123657.
Epoch 56: reducing lr to 2.840590467403475e-05
Epoch 61: reducing lr to 2.363009055709151e-05
Epoch 65: reducing lr to 1.9780758610455778e-05
Epoch 68: reducing lr to 1.6947834135916493e-05
Epoch 72: reducing lr to 1.3327592036762735e-05
Epoch 75: reducing lr to 1.0785645264472794e-05
Epoch 78: reducing lr to 8.436966501680572e-06
Epoch 81: reducing lr to 6.31858886479411e-06
Epoch 84: reducing lr to 4.463926675911481e-06
Epoch 87: reducing lr to 2.902221480952956e-06
Epoch 90: reducing lr to 1.6581115530630522e-06
Epoch 93: reducing lr to 7.512110150603582e-07
Epoch 96: reducing lr to 1.958249021567673e-07
Epoch 99: reducing lr to 7.103415066551065e-10
[I 2024-06-21 01:42:23,118] Trial 520 finished with value: 0.9745354652404785 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.41580548291790526, 'bidirectional': True, 'fc_dropout': 0.015038932484396243, 'learning_rate_model': 0.0004608029641015752}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.443510922638383e-05
Epoch 65: reducing lr to 1.9656336358494906e-05
Epoch 72: reducing lr to 1.3243760620228824e-05
Epoch 75: reducing lr to 1.0717802857662997e-05
Epoch 78: reducing lr to 8.383897436305968e-06
Epoch 81: reducing lr to 6.278844531866604e-06
Epoch 86: reducing lr to 3.3673023504230694e-06
Epoch 89: reducing lr to 2.0234012464771617e-06
Epoch 92: reducing lr to 1.0086629839331864e-06
Epoch 95: reducing lr to 3.3909358105695314e-07
Epoch 98: reducing lr to 2.525076524178887e-08
[I 2024-06-21 01:42:53,630] Trial 521 finished with value: 0.9736230969429016 and parameters: {'hidden_size': 119, 'n_layers': 2, 'rnn_dropout': 0.4140926369753857, 'bidirectional': True, 'fc_dropout': 0.017302188126804265, 'learning_rate_model': 0.00045790448363210233}. Best is trial 502 with value: 0.9691120386123657.
Epoch 56: reducing lr to 2.748910115867102e-05
Epoch 65: reducing lr to 1.9142332577603383e-05
Epoch 68: reducing lr to 1.6400841033886806e-05
Epoch 72: reducing lr to 1.2897442623432954e-05
Epoch 75: reducing lr to 1.0437537446488963e-05
Epoch 78: reducing lr to 8.164662534019326e-06
Epoch 81: reducing lr to 6.11465575476447e-06
Epoch 84: reducing lr to 4.319852980116029e-06
Epoch 87: reducing lr to 2.8085519820711336e-06
Epoch 90: reducing lr to 1.6045958309567673e-06
Epoch 93: reducing lr to 7.269656017461071e-07
Epoch 96: reducing lr to 1.8950463315800872e-07
Epoch 99: reducing lr to 6.874151609590797e-10
[I 2024-06-21 01:43:23,987] Trial 522 finished with value: 0.9747313261032104 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.41842182637944364, 'bidirectional': True, 'fc_dropout': 0.016398955375861005, 'learning_rate_model': 0.0004459305007096763}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.4730567266998363e-05
Epoch 61: reducing lr to 2.3682929923939543e-05
Epoch 64: reducing lr to 2.0785097996944234e-05
Epoch 67: reducing lr to 1.7923635045675054e-05
Epoch 72: reducing lr to 1.3357393933760548e-05
Epoch 75: reducing lr to 1.0809763101238816e-05
Epoch 78: reducing lr to 8.455832445803373e-06
Epoch 81: reducing lr to 6.332717893804056e-06
Epoch 84: reducing lr to 4.4739084852761715e-06
Epoch 87: reducing lr to 2.9087111533109965e-06
Epoch 90: reducing lr to 1.6618192648221631e-06
Epoch 93: reducing lr to 7.528908018690158e-07
Epoch 96: reducing lr to 1.9626278722614995e-07
Epoch 99: reducing lr to 7.119299049482102e-10
[I 2024-06-21 01:43:53,997] Trial 523 finished with value: 0.9754380583763123 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.4163051341451042, 'bidirectional': True, 'fc_dropout': 0.01371312934008631, 'learning_rate_model': 0.0004618333679760757}. Best is trial 502 with value: 0.9691120386123657.
Epoch 56: reducing lr to 2.752016406272745e-05
Epoch 61: reducing lr to 2.289326731222521e-05
Epoch 65: reducing lr to 1.916396356643937e-05
Epoch 68: reducing lr to 1.6419374115362987e-05
Epoch 72: reducing lr to 1.2912016836699242e-05
Epoch 75: reducing lr to 1.0449331947240892e-05
Epoch 78: reducing lr to 8.17388866795086e-06
Epoch 81: reducing lr to 6.121565364647748e-06
Epoch 84: reducing lr to 4.324734448516349e-06
Epoch 87: reducing lr to 2.811725668262359e-06
Epoch 90: reducing lr to 1.606409037072841e-06
Epoch 93: reducing lr to 7.277870786874226e-07
Epoch 96: reducing lr to 1.8971877490836942e-07
Epoch 99: reducing lr to 6.881919455852848e-10
[I 2024-06-21 01:44:24,364] Trial 524 finished with value: 0.9746829271316528 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.4162949089366182, 'bidirectional': True, 'fc_dropout': 0.00899325086465922, 'learning_rate_model': 0.0004464344057402347}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 2.267477375418778e-05
Epoch 65: reducing lr to 1.8981062518344105e-05
Epoch 68: reducing lr to 1.626266745474152e-05
Epoch 72: reducing lr to 1.2788784426855186e-05
Epoch 75: reducing lr to 1.0349603425089419e-05
Epoch 78: reducing lr to 8.095876997807615e-06
Epoch 81: reducing lr to 6.063141087368361e-06
Epoch 86: reducing lr to 3.2516220350454636e-06
Epoch 89: reducing lr to 1.9538893137876307e-06
Epoch 92: reducing lr to 9.740113726585275e-07
Epoch 95: reducing lr to 3.2744435912288495e-07
Epoch 98: reducing lr to 2.4383300374451875e-08
[I 2024-06-21 01:44:54,297] Trial 525 finished with value: 0.9727082848548889 and parameters: {'hidden_size': 118, 'n_layers': 2, 'rnn_dropout': 0.41520364164291307, 'bidirectional': True, 'fc_dropout': 0.014421264275940955, 'learning_rate_model': 0.0004421736315829163}. Best is trial 502 with value: 0.9691120386123657.
Epoch 65: reducing lr to 1.6016493840608025e-05
Epoch 68: reducing lr to 1.3722672946732772e-05
Epoch 72: reducing lr to 1.0791360431147207e-05
Epoch 75: reducing lr to 8.733144382748784e-06
Epoch 78: reducing lr to 6.831417574458207e-06
Epoch 81: reducing lr to 5.11616574608093e-06
Epoch 87: reducing lr to 2.34993072759065e-06
Epoch 90: reducing lr to 1.3425740640016562e-06
Epoch 93: reducing lr to 6.082560751411912e-07
Epoch 96: reducing lr to 1.5855955785101405e-07
Epoch 99: reducing lr to 5.751640060980365e-10
[I 2024-06-21 01:45:22,848] Trial 526 finished with value: 0.9765751361846924 and parameters: {'hidden_size': 120, 'n_layers': 2, 'rnn_dropout': 0.4189297185421546, 'bidirectional': True, 'fc_dropout': 0.014216582096036875, 'learning_rate_model': 0.0003731124767058033}. Best is trial 502 with value: 0.9691120386123657.
Epoch 56: reducing lr to 2.9636862933677753e-05
Epoch 61: reducing lr to 2.4654090865518663e-05
Epoch 65: reducing lr to 2.0637949693540774e-05
Epoch 68: reducing lr to 1.768226159570219e-05
Epoch 72: reducing lr to 1.3905137786037938e-05
Epoch 75: reducing lr to 1.1253036790151541e-05
Epoch 78: reducing lr to 8.802578993898368e-06
Epoch 81: reducing lr to 6.5924023286376385e-06
Epoch 84: reducing lr to 4.65736911244746e-06
Epoch 87: reducing lr to 3.0279880616793215e-06
Epoch 90: reducing lr to 1.729965138966207e-06
Epoch 93: reducing lr to 7.837644371159037e-07
Epoch 96: reducing lr to 2.0431089418975705e-07
Epoch 99: reducing lr to 7.411238652833308e-10
[I 2024-06-21 01:45:53,198] Trial 527 finished with value: 0.9743248224258423 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.419259323737449, 'bidirectional': True, 'fc_dropout': 0.03496602028700851, 'learning_rate_model': 0.00048077167205993516}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 2.3742653297045575e-05
Epoch 65: reducing lr to 1.9874984926778323e-05
Epoch 68: reducing lr to 1.702856572016568e-05
Epoch 72: reducing lr to 1.3391078474659513e-05
Epoch 75: reducing lr to 1.0837023052476121e-05
Epoch 78: reducing lr to 8.47715627852613e-06
Epoch 81: reducing lr to 6.348687677726974e-06
Epoch 84: reducing lr to 4.485190742436351e-06
Epoch 87: reducing lr to 2.91604631167294e-06
Epoch 90: reducing lr to 1.6660100238332547e-06
Epoch 93: reducing lr to 7.54789434276933e-07
Epoch 96: reducing lr to 1.96757720737588e-07
Epoch 99: reducing lr to 7.137252425789414e-10
[I 2024-06-21 01:46:24,311] Trial 528 finished with value: 0.9724476337432861 and parameters: {'hidden_size': 123, 'n_layers': 2, 'rnn_dropout': 0.41995561656832664, 'bidirectional': True, 'fc_dropout': 0.03516023686161912, 'learning_rate_model': 0.00046299801469153846}. Best is trial 502 with value: 0.9691120386123657.
Epoch 65: reducing lr to 1.5010530005895343e-05
Epoch 73: reducing lr to 9.456136525221286e-06
Epoch 76: reducing lr to 7.572803397940415e-06
Epoch 79: reducing lr to 5.845779576932705e-06
Epoch 87: reducing lr to 2.2023362946541015e-06
Epoch 90: reducing lr to 1.2582496814464266e-06
Epoch 93: reducing lr to 5.700527317674365e-07
Epoch 96: reducing lr to 1.4860075023471922e-07
Epoch 99: reducing lr to 5.390391091656972e-10
[I 2024-06-21 01:46:53,839] Trial 529 finished with value: 0.9764529466629028 and parameters: {'hidden_size': 117, 'n_layers': 2, 'rnn_dropout': 0.42156764272807457, 'bidirectional': True, 'fc_dropout': 0.03345806966099393, 'learning_rate_model': 0.00034967803084134767}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 2.2040042596583537e-05
Epoch 65: reducing lr to 1.8449728802937046e-05
Epoch 72: reducing lr to 1.2430790118660174e-05
Epoch 75: reducing lr to 1.005988870361227e-05
Epoch 78: reducing lr to 7.86925046409453e-06
Epoch 86: reducing lr to 3.1605999220678293e-06
Epoch 89: reducing lr to 1.8991944163030631e-06
Epoch 92: reducing lr to 9.467460348523173e-07
Epoch 95: reducing lr to 3.1827826382375406e-07
Epoch 98: reducing lr to 2.3700742716293394e-08
[I 2024-06-21 01:47:25,133] Trial 530 finished with value: 0.9722492098808289 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.4193567521624643, 'bidirectional': True, 'fc_dropout': 0.04076507231080353, 'learning_rate_model': 0.00042979593890649613}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.759646496926789e-05
Epoch 49: reducing lr to 4.006906996522944e-05
Epoch 61: reducing lr to 2.7323278908999343e-05
Epoch 65: reducing lr to 2.2872328112296383e-05
Epoch 72: reducing lr to 1.5410584801865794e-05
Epoch 75: reducing lr to 1.2471352704413456e-05
Epoch 78: reducing lr to 9.755594813077038e-06
Epoch 81: reducing lr to 7.30613221506489e-06
Epoch 84: reducing lr to 5.161601615557421e-06
Epoch 87: reducing lr to 3.355814773040261e-06
Epoch 90: reducing lr to 1.91726071963696e-06
Epoch 93: reducing lr to 8.686191038674183e-07
Epoch 96: reducing lr to 2.2643072001901762e-07
Epoch 99: reducing lr to 8.213620282179756e-10
[I 2024-06-21 01:47:56,287] Trial 531 finished with value: 0.971647322177887 and parameters: {'hidden_size': 123, 'n_layers': 2, 'rnn_dropout': 0.40793778419802207, 'bidirectional': True, 'fc_dropout': 0.04290452117009404, 'learning_rate_model': 0.0005328226686148875}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.923871636833317e-05
Epoch 53: reducing lr to 3.72874203032699e-05
Epoch 61: reducing lr to 2.8266031549228562e-05
Epoch 65: reducing lr to 2.3661506738619825e-05
Epoch 68: reducing lr to 2.027279638304829e-05
Epoch 72: reducing lr to 1.5942306106538714e-05
Epoch 75: reducing lr to 1.2901659796343152e-05
Epoch 78: reducing lr to 1.009219836632063e-05
Epoch 81: reducing lr to 7.558220387152739e-06
Epoch 84: reducing lr to 5.3396956710726745e-06
Epoch 87: reducing lr to 3.4716026053842632e-06
Epoch 90: reducing lr to 1.983413197583167e-06
Epoch 93: reducing lr to 8.98589626667837e-07
Epoch 96: reducing lr to 2.3424340457410932e-07
Epoch 99: reducing lr to 8.497020097838863e-10
[I 2024-06-21 01:48:28,487] Trial 532 finished with value: 0.9724404215812683 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.40819365593229373, 'bidirectional': True, 'fc_dropout': 0.04732713439138746, 'learning_rate_model': 0.0005512069913487603}. Best is trial 502 with value: 0.9691120386123657.
Epoch 65: reducing lr to 1.618931775382711e-05
Epoch 73: reducing lr to 1.019873374692652e-05
Epoch 76: reducing lr to 8.16750111077816e-06
Epoch 79: reducing lr to 6.304852863465001e-06
Epoch 82: reducing lr to 4.6401585681911505e-06
Epoch 85: reducing lr to 3.199676353067744e-06
Epoch 88: reducing lr to 2.0061176354402474e-06
Epoch 91: reducing lr to 1.0783126182137163e-06
Epoch 94: reducing lr to 4.3088862730677523e-07
Epoch 97: reducing lr to 7.40578332925491e-08
[I 2024-06-21 01:49:00,692] Trial 533 finished with value: 0.9741705656051636 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.4054780087979806, 'bidirectional': True, 'fc_dropout': 0.04137689061782683, 'learning_rate_model': 0.0003771384988138175}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 4.185387872987765e-05
Epoch 53: reducing lr to 3.764928764982761e-05
Epoch 65: reducing lr to 2.3891137176698452e-05
Epoch 72: reducing lr to 1.609702316558605e-05
Epoch 75: reducing lr to 1.3026867959276417e-05
Epoch 78: reducing lr to 1.0190141238582948e-05
Epoch 81: reducing lr to 7.631571483419312e-06
Epoch 84: reducing lr to 5.3915164054705235e-06
Epoch 87: reducing lr to 3.5052938506593665e-06
Epoch 90: reducing lr to 2.002661846728089e-06
Epoch 93: reducing lr to 9.073102686753048e-07
Epoch 96: reducing lr to 2.3651669241683306e-07
Epoch 99: reducing lr to 8.579482067383286e-10
[I 2024-06-21 01:49:28,020] Trial 534 finished with value: 0.9707563519477844 and parameters: {'hidden_size': 113, 'n_layers': 2, 'rnn_dropout': 0.40253832556432323, 'bidirectional': True, 'fc_dropout': 0.04563382586012093, 'learning_rate_model': 0.0005565563507236146}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 4.242252090212569e-05
Epoch 53: reducing lr to 3.816080470302503e-05
Epoch 65: reducing lr to 2.421573094324787e-05
Epoch 72: reducing lr to 1.631572323586345e-05
Epoch 75: reducing lr to 1.3203855773040577e-05
Epoch 78: reducing lr to 1.0328588240993874e-05
Epoch 81: reducing lr to 7.735256817196991e-06
Epoch 84: reducing lr to 5.46476752803206e-06
Epoch 87: reducing lr to 3.5529180606512567e-06
Epoch 90: reducing lr to 2.0298707462939223e-06
Epoch 93: reducing lr to 9.196373193033278e-07
Epoch 96: reducing lr to 2.3973009508895457e-07
Epoch 99: reducing lr to 8.696046062601527e-10
[I 2024-06-21 01:49:55,262] Trial 535 finished with value: 0.9706781506538391 and parameters: {'hidden_size': 113, 'n_layers': 2, 'rnn_dropout': 0.40121721569783514, 'bidirectional': True, 'fc_dropout': 0.05570314932431539, 'learning_rate_model': 0.00056411792976618}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.3805345967610535e-05
Epoch 49: reducing lr to 4.529601459838675e-05
Epoch 53: reducing lr to 4.074563062629771e-05
Epoch 65: reducing lr to 2.5855985900663428e-05
Epoch 72: reducing lr to 1.7420870381087542e-05
Epoch 75: reducing lr to 1.4098220264432015e-05
Epoch 78: reducing lr to 1.1028196198527697e-05
Epoch 81: reducing lr to 8.259205211363778e-06
Epoch 84: reducing lr to 5.834924103110682e-06
Epoch 87: reducing lr to 3.793575320840191e-06
Epoch 90: reducing lr to 2.167364244314875e-06
Epoch 93: reducing lr to 9.819290451053178e-07
Epoch 96: reducing lr to 2.559682370567888e-07
Epoch 99: reducing lr to 9.285073612427287e-10
[I 2024-06-21 01:50:22,508] Trial 536 finished with value: 0.9703018665313721 and parameters: {'hidden_size': 113, 'n_layers': 2, 'rnn_dropout': 0.3996815221539086, 'bidirectional': True, 'fc_dropout': 0.057022126024537254, 'learning_rate_model': 0.0006023285141600399}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 3.958217397932706e-05
Epoch 61: reducing lr to 2.6991262347745805e-05
Epoch 65: reducing lr to 2.259439691110347e-05
Epoch 68: reducing lr to 1.935851393727828e-05
Epoch 72: reducing lr to 1.5223324356665848e-05
Epoch 75: reducing lr to 1.2319808094672798e-05
Epoch 78: reducing lr to 9.637050510484063e-06
Epoch 81: reducing lr to 7.217352354412421e-06
Epoch 84: reducing lr to 5.0988808409144605e-06
Epoch 87: reducing lr to 3.3150368676922343e-06
Epoch 90: reducing lr to 1.8939632847544052e-06
Epoch 93: reducing lr to 8.580641507497516e-07
Epoch 96: reducing lr to 2.2367926587351684e-07
Epoch 99: reducing lr to 8.113813155423769e-10
[I 2024-06-21 01:50:50,050] Trial 537 finished with value: 0.9745479822158813 and parameters: {'hidden_size': 114, 'n_layers': 2, 'rnn_dropout': 0.4019936217615355, 'bidirectional': True, 'fc_dropout': 0.05706289507453896, 'learning_rate_model': 0.0005263481180757434}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.378595734410796e-05
Epoch 53: reducing lr to 4.07309480389566e-05
Epoch 56: reducing lr to 3.7116777136661834e-05
Epoch 61: reducing lr to 3.0876425693578316e-05
Epoch 65: reducing lr to 2.584666876001689e-05
Epoch 68: reducing lr to 2.2145007870385583e-05
Epoch 72: reducing lr to 1.7414592813480985e-05
Epoch 75: reducing lr to 1.4093140005586957e-05
Epoch 79: reducing lr to 1.0065862318632958e-05
Epoch 82: reducing lr to 7.408134384022402e-06
Epoch 86: reducing lr to 4.42776043708664e-06
Epoch 89: reducing lr to 2.6606271297193567e-06
Epoch 92: reducing lr to 1.326319287619658e-06
Epoch 95: reducing lr to 4.4588367376198825e-07
Epoch 98: reducing lr to 3.320294042787313e-08
[I 2024-06-21 01:51:17,407] Trial 538 finished with value: 0.9728271961212158 and parameters: {'hidden_size': 110, 'n_layers': 2, 'rnn_dropout': 0.42529709044741604, 'bidirectional': True, 'fc_dropout': 0.054117214813620924, 'learning_rate_model': 0.0006021114665679114}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 4.391852949745788e-05
Epoch 52: reducing lr to 4.064157903104816e-05
Epoch 58: reducing lr to 3.360091935224672e-05
Epoch 61: reducing lr to 2.9948242666312783e-05
Epoch 65: reducing lr to 2.5069686362750892e-05
Epoch 68: reducing lr to 2.1479301915689266e-05
Epoch 72: reducing lr to 1.6891088906758544e-05
Epoch 75: reducing lr to 1.3669483022622661e-05
Epoch 78: reducing lr to 1.0692820645329837e-05
Epoch 81: reducing lr to 8.008036709564225e-06
Epoch 84: reducing lr to 5.657479771896336e-06
Epoch 87: reducing lr to 3.6782099066852987e-06
Epoch 90: reducing lr to 2.101453104420988e-06
Epoch 93: reducing lr to 9.520678610299739e-07
Epoch 96: reducing lr to 2.481840548062501e-07
Epoch 99: reducing lr to 9.002707698356656e-10
[I 2024-06-21 01:51:44,957] Trial 539 finished with value: 0.9747471213340759 and parameters: {'hidden_size': 114, 'n_layers': 2, 'rnn_dropout': 0.4265517900196575, 'bidirectional': True, 'fc_dropout': 0.0508744491680628, 'learning_rate_model': 0.0005840112612741839}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.231572152126896e-05
Epoch 53: reducing lr to 3.961757008935638e-05
Epoch 56: reducing lr to 3.61021923255089e-05
Epoch 61: reducing lr to 3.0032420503794362e-05
Epoch 65: reducing lr to 2.514015166543564e-05
Epoch 68: reducing lr to 2.153967544765315e-05
Epoch 72: reducing lr to 1.6938565994236518e-05
Epoch 75: reducing lr to 1.3707904893754018e-05
Epoch 78: reducing lr to 1.0722875781737387e-05
Epoch 81: reducing lr to 8.030545516514779e-06
Epoch 84: reducing lr to 5.673381687013718e-06
Epoch 87: reducing lr to 3.688548535205115e-06
Epoch 90: reducing lr to 2.1073598208807934e-06
Epoch 93: reducing lr to 9.547439116607383e-07
Epoch 96: reducing lr to 2.488816448873712e-07
Epoch 99: reducing lr to 9.028012303989759e-10
[I 2024-06-21 01:52:12,288] Trial 540 finished with value: 0.9729089736938477 and parameters: {'hidden_size': 110, 'n_layers': 2, 'rnn_dropout': 0.4005565858677057, 'bidirectional': True, 'fc_dropout': 0.06926780338135888, 'learning_rate_model': 0.0005856527868083097}. Best is trial 502 with value: 0.9691120386123657.
Epoch 73: reducing lr to 8.57420327223841e-06
Epoch 78: reducing lr to 5.8052302560606785e-06
Epoch 87: reducing lr to 1.996933844369456e-06
Epoch 90: reducing lr to 1.1408981360596868e-06
Epoch 93: reducing lr to 5.16886361045263e-07
Epoch 96: reducing lr to 1.347413962902598e-07
Epoch 99: reducing lr to 4.887652458652504e-10
[I 2024-06-21 01:52:40,260] Trial 541 finished with value: 0.9759871959686279 and parameters: {'hidden_size': 111, 'n_layers': 2, 'rnn_dropout': 0.39887236004770843, 'bidirectional': True, 'fc_dropout': 0.07552300424539292, 'learning_rate_model': 0.00031706506227707044}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.366259179037161e-05
Epoch 49: reducing lr to 4.51758370364752e-05
Epoch 53: reducing lr to 4.0637525959901944e-05
Epoch 61: reducing lr to 3.080560632842845e-05
Epoch 65: reducing lr to 2.5787385840064023e-05
Epoch 68: reducing lr to 2.2094215223134788e-05
Epoch 72: reducing lr to 1.737465002931181e-05
Epoch 75: reducing lr to 1.4060815434146314e-05
Epoch 78: reducing lr to 1.0998936632467144e-05
Epoch 81: reducing lr to 8.237292220686138e-06
Epoch 84: reducing lr to 5.819443117446312e-06
Epoch 87: reducing lr to 3.7835103595620642e-06
Epoch 90: reducing lr to 2.1616138807792425e-06
Epoch 93: reducing lr to 9.793238305040542e-07
Epoch 96: reducing lr to 2.5528911243779143e-07
Epoch 99: reducing lr to 9.26043882906475e-10
[I 2024-06-21 01:53:12,873] Trial 542 finished with value: 0.972256064414978 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.43109696389369007, 'bidirectional': True, 'fc_dropout': 0.06009866584572237, 'learning_rate_model': 0.00060073044040138}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.386133175648445e-05
Epoch 47: reducing lr to 4.7468658963278516e-05
Epoch 58: reducing lr to 3.4690856380860846e-05
Epoch 61: reducing lr to 3.091969521145731e-05
Epoch 65: reducing lr to 2.5882889691387732e-05
Epoch 68: reducing lr to 2.21760413787161e-05
Epoch 72: reducing lr to 1.743899722617358e-05
Epoch 75: reducing lr to 1.4112889810162613e-05
Epoch 78: reducing lr to 1.1039671308536335e-05
Epoch 81: reducing lr to 8.267799117989865e-06
Epoch 84: reducing lr to 5.84099548548092e-06
Epoch 87: reducing lr to 3.7975226294797017e-06
Epoch 90: reducing lr to 2.1696194402403547e-06
Epoch 93: reducing lr to 9.829507664830033e-07
Epoch 96: reducing lr to 2.562345783174995e-07
Epoch 99: reducing lr to 9.29473496042929e-10
[I 2024-06-21 01:53:38,677] Trial 543 finished with value: 0.9734294414520264 and parameters: {'hidden_size': 112, 'n_layers': 2, 'rnn_dropout': 0.4322124957842355, 'bidirectional': True, 'fc_dropout': 0.06180273181761194, 'learning_rate_model': 0.0006029552518274605}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.348037543003939e-05
Epoch 49: reducing lr to 4.5022438247391275e-05
Epoch 52: reducing lr to 4.166312039905048e-05
Epoch 55: reducing lr to 3.811883508305197e-05
Epoch 61: reducing lr to 3.0701003004665753e-05
Epoch 65: reducing lr to 2.5699822354337966e-05
Epoch 68: reducing lr to 2.2019192244407192e-05
Epoch 72: reducing lr to 1.7315652776574656e-05
Epoch 75: reducing lr to 1.4013070617389755e-05
Epoch 78: reducing lr to 1.0961588712178052e-05
Epoch 81: reducing lr to 8.20932172285201e-06
Epoch 84: reducing lr to 5.799682652872333e-06
Epoch 87: reducing lr to 3.770663095499754e-06
Epoch 90: reducing lr to 2.1542739182344285e-06
Epoch 93: reducing lr to 9.759984446434879e-07
Epoch 96: reducing lr to 2.544222543277149e-07
Epoch 99: reducing lr to 9.22899414095707e-10
[I 2024-06-21 01:54:08,225] Trial 544 finished with value: 0.9736599922180176 and parameters: {'hidden_size': 117, 'n_layers': 2, 'rnn_dropout': 0.4278344088588055, 'bidirectional': True, 'fc_dropout': 0.042973507545863876, 'learning_rate_model': 0.0005986906038832647}. Best is trial 502 with value: 0.9691120386123657.
Epoch 61: reducing lr to 1.9292080220792744e-05
Epoch 65: reducing lr to 1.6149408357917865e-05
Epoch 72: reducing lr to 1.0880913642798677e-05
Epoch 75: reducing lr to 8.80561727736491e-06
Epoch 78: reducing lr to 6.888108794052657e-06
Epoch 81: reducing lr to 5.158622772405468e-06
Epoch 84: reducing lr to 3.6444393356578327e-06
Epoch 87: reducing lr to 2.369431868819036e-06
Epoch 90: reducing lr to 1.353715552609926e-06
Epoch 93: reducing lr to 6.133037505833146e-07
Epoch 96: reducing lr to 1.5987538060887912e-07
Epoch 99: reducing lr to 5.799370636115023e-10
[I 2024-06-21 01:54:40,877] Trial 545 finished with value: 0.9743627309799194 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.4292818900990729, 'bidirectional': True, 'fc_dropout': 6.774696029997196e-05, 'learning_rate_model': 0.0003762087888723211}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.094716859398855e-05
Epoch 53: reducing lr to 3.858119440837714e-05
Epoch 65: reducing lr to 2.4482497959177992e-05
Epoch 68: reducing lr to 2.0976208385948353e-05
Epoch 72: reducing lr to 1.649546163858082e-05
Epoch 75: reducing lr to 1.3349313005432229e-05
Epoch 78: reducing lr to 1.0442370751638643e-05
Epoch 81: reducing lr to 7.820470490218579e-06
Epoch 84: reducing lr to 5.524968879361139e-06
Epoch 87: reducing lr to 3.592057963184257e-06
Epoch 90: reducing lr to 2.0522323492941232e-06
Epoch 93: reducing lr to 9.297682917684441e-07
Epoch 96: reducing lr to 2.423710264011438e-07
Epoch 99: reducing lr to 8.791844048790243e-10
[I 2024-06-21 01:55:09,294] Trial 546 finished with value: 0.9725194573402405 and parameters: {'hidden_size': 115, 'n_layers': 2, 'rnn_dropout': 0.40596116043155717, 'bidirectional': True, 'fc_dropout': 0.04036713636552337, 'learning_rate_model': 0.0005703324048571481}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.176676151648301e-05
Epoch 49: reducing lr to 4.3579832880794915e-05
Epoch 53: reducing lr to 3.920185449118731e-05
Epoch 56: reducing lr to 3.572336433469666e-05
Epoch 61: reducing lr to 2.9717283921059972e-05
Epoch 65: reducing lr to 2.487635070126531e-05
Epoch 68: reducing lr to 2.131365504703563e-05
Epoch 72: reducing lr to 1.6760826014764325e-05
Epoch 75: reducing lr to 1.3564064929069187e-05
Epoch 78: reducing lr to 1.0610358363085927e-05
Epoch 81: reducing lr to 7.946279292576962e-06
Epoch 84: reducing lr to 5.613849685017153e-06
Epoch 87: reducing lr to 3.6498438100735636e-06
Epoch 90: reducing lr to 2.0852468455893024e-06
Epoch 93: reducing lr to 9.447255805152552e-07
Epoch 96: reducing lr to 2.462700767966392e-07
Epoch 99: reducing lr to 8.933279448640474e-10
[I 2024-06-21 01:55:36,546] Trial 547 finished with value: 0.9729989767074585 and parameters: {'hidden_size': 110, 'n_layers': 2, 'rnn_dropout': 0.406484883664629, 'bidirectional': True, 'fc_dropout': 0.04093275078894239, 'learning_rate_model': 0.0005795074074213792}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.22612828627166e-05
Epoch 49: reducing lr to 4.3996145529944964e-05
Epoch 65: reducing lr to 2.5113991343209882e-05
Epoch 72: reducing lr to 1.6920940072549628e-05
Epoch 75: reducing lr to 1.3693640731236904e-05
Epoch 78: reducing lr to 1.0711717778819572e-05
Epoch 81: reducing lr to 8.022189096825806e-06
Epoch 84: reducing lr to 5.6674780832878e-06
Epoch 87: reducing lr to 3.684710307834394e-06
Epoch 90: reducing lr to 2.105166945806148e-06
Epoch 93: reducing lr to 9.537504248789316e-07
Epoch 96: reducing lr to 2.486226637915903e-07
Epoch 99: reducing lr to 9.01861794088115e-10
[I 2024-06-21 01:56:05,166] Trial 548 finished with value: 0.9703432321548462 and parameters: {'hidden_size': 116, 'n_layers': 2, 'rnn_dropout': 0.4330670852024925, 'bidirectional': True, 'fc_dropout': 0.06488631069050073, 'learning_rate_model': 0.0005850433686226437}. Best is trial 502 with value: 0.9691120386123657.
Epoch 65: reducing lr to 1.6589141153466624e-05
Epoch 73: reducing lr to 1.0450609240429048e-05
Epoch 76: reducing lr to 8.369211776436002e-06
Epoch 79: reducing lr to 6.460562186391943e-06
Epoch 86: reducing lr to 2.841864983320092e-06
Epoch 89: reducing lr to 1.7076676078247677e-06
Epoch 92: reducing lr to 8.512701610090734e-07
Epoch 95: reducing lr to 2.861810653721937e-07
Epoch 98: reducing lr to 2.1310609525057892e-08
[I 2024-06-21 01:56:33,773] Trial 549 finished with value: 0.9733811616897583 and parameters: {'hidden_size': 116, 'n_layers': 2, 'rnn_dropout': 0.4348679962231731, 'bidirectional': True, 'fc_dropout': 0.04620057967169963, 'learning_rate_model': 0.0003864525909221794}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.9345681703635174e-05
Epoch 49: reducing lr to 4.1541647556001524e-05
Epoch 65: reducing lr to 2.371290858636732e-05
Epoch 72: reducing lr to 1.5976938896423337e-05
Epoch 75: reducing lr to 1.2929687138805632e-05
Epoch 78: reducing lr to 1.0114122483393713e-05
Epoch 81: reducing lr to 7.574639734317487e-06
Epoch 84: reducing lr to 5.351295533538515e-06
Epoch 87: reducing lr to 3.4791442548038526e-06
Epoch 90: reducing lr to 1.987721930088188e-06
Epoch 93: reducing lr to 9.005417072216145e-07
Epoch 96: reducing lr to 2.347522708923437e-07
Epoch 99: reducing lr to 8.515478877265704e-10
[I 2024-06-21 01:57:06,033] Trial 550 finished with value: 0.9722517728805542 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.42536716211148434, 'bidirectional': True, 'fc_dropout': 0.0322272484239377, 'learning_rate_model': 0.0005524044238774512}. Best is trial 502 with value: 0.9691120386123657.
Epoch 65: reducing lr to 1.3629156178700177e-05
Epoch 73: reducing lr to 8.585916786331675e-06
Epoch 79: reducing lr to 5.307810104571945e-06
Epoch 86: reducing lr to 2.3347936664192446e-06
Epoch 89: reducing lr to 1.4029700701827762e-06
Epoch 92: reducing lr to 6.993788205988819e-07
Epoch 95: reducing lr to 2.351180449464878e-07
Epoch 98: reducing lr to 1.7508177354896892e-08
[I 2024-06-21 01:57:37,351] Trial 551 finished with value: 0.9757602214813232 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.43801124469708935, 'bidirectional': True, 'fc_dropout': 0.03249101102426472, 'learning_rate_model': 0.00031749821576755144}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 4.030506938110092e-05
Epoch 61: reducing lr to 2.748420797143611e-05
Epoch 65: reducing lr to 2.3007041897238417e-05
Epoch 72: reducing lr to 1.5501350297911357e-05
Epoch 75: reducing lr to 1.254480666668218e-05
Epoch 78: reducing lr to 9.813053463336777e-06
Epoch 81: reducing lr to 7.349163983372253e-06
Epoch 84: reducing lr to 5.192002495021058e-06
Epoch 87: reducing lr to 3.375579901776656e-06
Epoch 90: reducing lr to 1.928553030896012e-06
Epoch 93: reducing lr to 8.737351098367483e-07
Epoch 96: reducing lr to 2.277643551072857e-07
Epoch 99: reducing lr to 8.261996987490687e-10
[I 2024-06-21 01:58:08,571] Trial 552 finished with value: 0.9710349440574646 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.4135723150439893, 'bidirectional': True, 'fc_dropout': 0.00025865514109208593, 'learning_rate_model': 0.000535960895647991}. Best is trial 502 with value: 0.9691120386123657.
Epoch 49: reducing lr to 4.1091153869358685e-05
Epoch 65: reducing lr to 2.345575663793542e-05
Epoch 72: reducing lr to 1.580367879413649e-05
Epoch 75: reducing lr to 1.278947261268587e-05
Epoch 78: reducing lr to 1.000444102893145e-05
Epoch 81: reducing lr to 7.4924974125835254e-06
Epoch 84: reducing lr to 5.2932640158918175e-06
Epoch 87: reducing lr to 3.4414150694219877e-06
Epoch 90: reducing lr to 1.9661663049989615e-06
Epoch 93: reducing lr to 8.907758847872724e-07
Epoch 96: reducing lr to 2.322065265084809e-07
Epoch 99: reducing lr to 8.423133732171628e-10
[I 2024-06-21 01:58:40,731] Trial 553 finished with value: 0.9722784757614136 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.4220720648809053, 'bidirectional': True, 'fc_dropout': 0.0006388721986777743, 'learning_rate_model': 0.0005464139367381318}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 4.9793425343169014e-05
Epoch 47: reducing lr to 4.388356264406327e-05
Epoch 50: reducing lr to 4.0898247625731755e-05
Epoch 61: reducing lr to 2.8584468392018276e-05
Epoch 65: reducing lr to 2.3928070351851886e-05
Epoch 72: reducing lr to 1.61219074635425e-05
Epoch 75: reducing lr to 1.3047006121494576e-05
Epoch 78: reducing lr to 1.0205894120851344e-05
Epoch 81: reducing lr to 7.643369087033088e-06
Epoch 84: reducing lr to 5.399851120485263e-06
Epoch 87: reducing lr to 3.5107126647908674e-06
Epoch 90: reducing lr to 2.0057577504605528e-06
Epoch 93: reducing lr to 9.087128745380313e-07
Epoch 96: reducing lr to 2.3688232224696727e-07
Epoch 99: reducing lr to 8.592745040676697e-10
[I 2024-06-21 01:59:11,959] Trial 554 finished with value: 0.9709065556526184 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.44871511388704105, 'bidirectional': True, 'fc_dropout': 0.0002673204670878074, 'learning_rate_model': 0.0005574167280690714}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.375604478873408e-05
Epoch 49: reducing lr to 4.525451041552265e-05
Epoch 53: reducing lr to 4.070829590448121e-05
Epoch 65: reducing lr to 2.5832294377767482e-05
Epoch 72: reducing lr to 1.7404907851130787e-05
Epoch 75: reducing lr to 1.4085302238042692e-05
Epoch 78: reducing lr to 1.10180911975526e-05
Epoch 81: reducing lr to 8.25163740288342e-06
Epoch 84: reducing lr to 5.829577633689015e-06
Epoch 87: reducing lr to 3.7900993142815383e-06
Epoch 90: reducing lr to 2.1653783150288926e-06
Epoch 93: reducing lr to 9.810293155593837e-07
Epoch 96: reducing lr to 2.5573369649925127e-07
Epoch 99: reducing lr to 9.276565813252924e-10
[I 2024-06-21 01:59:44,157] Trial 555 finished with value: 0.971847414970398 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.436062507529089, 'bidirectional': True, 'fc_dropout': 0.0005286634161550749, 'learning_rate_model': 0.0006017766079268401}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.7311905203574485e-05
Epoch 47: reducing lr to 5.050969209124128e-05
Epoch 50: reducing lr to 4.70736141320676e-05
Epoch 61: reducing lr to 3.290053519089074e-05
Epoch 65: reducing lr to 2.7541051660104938e-05
Epoch 72: reducing lr to 1.855620949720635e-05
Epoch 75: reducing lr to 1.5017018268419528e-05
Epoch 78: reducing lr to 1.1746917034543668e-05
Epoch 81: reducing lr to 8.797467567915921e-06
Epoch 84: reducing lr to 6.2151931384074335e-06
Epoch 87: reducing lr to 4.040807196026437e-06
Epoch 90: reducing lr to 2.3086139839443806e-06
Epoch 93: reducing lr to 1.0459225442689137e-06
Epoch 96: reducing lr to 2.726499955256435e-07
Epoch 99: reducing lr to 9.890193048865334e-10
[I 2024-06-21 02:00:15,379] Trial 556 finished with value: 0.9702138900756836 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.44996822454779084, 'bidirectional': True, 'fc_dropout': 0.03167957915564193, 'learning_rate_model': 0.0006415829892763929}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.433185910716424e-05
Epoch 49: reducing lr to 4.57392595292868e-05
Epoch 65: reducing lr to 2.6109000095963084e-05
Epoch 72: reducing lr to 1.759134260820836e-05
Epoch 75: reducing lr to 1.4236178641616582e-05
Epoch 78: reducing lr to 1.1136112802346145e-05
Epoch 81: reducing lr to 8.340025806192249e-06
Epoch 84: reducing lr to 5.892021853405528e-06
Epoch 87: reducing lr to 3.830697417471843e-06
Epoch 90: reducing lr to 2.188573024452037e-06
Epoch 93: reducing lr to 9.91537728686103e-07
Epoch 99: reducing lr to 9.375932860178696e-10
[I 2024-06-21 02:00:46,588] Trial 557 finished with value: 0.9703385829925537 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.4337605960152685, 'bidirectional': True, 'fc_dropout': 0.0013803731911507977, 'learning_rate_model': 0.0006082226102081171}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.882824003643908e-05
Epoch 47: reducing lr to 5.184605676526781e-05
Epoch 50: reducing lr to 4.8319068467667485e-05
Epoch 61: reducing lr to 3.3771004028955527e-05
Epoch 65: reducing lr to 2.8269721485642968e-05
Epoch 72: reducing lr to 1.904716206154008e-05
Epoch 75: reducing lr to 1.5414332365818415e-05
Epoch 78: reducing lr to 1.2057712137497916e-05
Epoch 81: reducing lr to 9.030227349096522e-06
Epoch 84: reducing lr to 6.379632164038726e-06
Epoch 87: reducing lr to 4.14771721205994e-06
Epoch 90: reducing lr to 2.369694343898538e-06
Epoch 93: reducing lr to 1.0735951330743238e-06
Epoch 96: reducing lr to 2.7986365704897163e-07
Epoch 99: reducing lr to 1.0151863711759142e-09
[I 2024-06-21 02:01:17,778] Trial 558 finished with value: 0.9701637625694275 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.4508396324731576, 'bidirectional': True, 'fc_dropout': 0.0010823293385823808, 'learning_rate_model': 0.0006585577283180906}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.699507823808509e-05
Epoch 43: reducing lr to 5.432827831374465e-05
Epoch 49: reducing lr to 4.7981289767425454e-05
Epoch 52: reducing lr to 4.440119927529261e-05
Epoch 55: reducing lr to 4.0623985348518884e-05
Epoch 65: reducing lr to 2.7388801481143632e-05
Epoch 68: reducing lr to 2.3466282250601247e-05
Epoch 72: reducing lr to 1.845362858447796e-05
Epoch 76: reducing lr to 1.3817633943668898e-05
Epoch 79: reducing lr to 1.0666438578268144e-05
Epoch 83: reducing lr to 6.993637574121325e-06
Epoch 86: reducing lr to 4.691941261112385e-06
Epoch 89: reducing lr to 2.8193725445948246e-06
Epoch 92: reducing lr to 1.4054536778612226e-06
Epoch 95: reducing lr to 4.724871718571897e-07
Epoch 98: reducing lr to 3.5183982601894644e-08
[I 2024-06-21 02:01:51,068] Trial 559 finished with value: 0.9740060567855835 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.4391040523048637, 'bidirectional': True, 'fc_dropout': 0.029429633739152628, 'learning_rate_model': 0.0006380362429087748}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 6.042234272639066e-05
Epoch 49: reducing lr to 5.086653136383249e-05
Epoch 58: reducing lr to 3.891665403286315e-05
Epoch 65: reducing lr to 2.9035762404707203e-05
Epoch 72: reducing lr to 1.9563293978106638e-05
Epoch 75: reducing lr to 1.5832023404559984e-05
Epoch 78: reducing lr to 1.238444690537711e-05
Epoch 81: reducing lr to 9.274924618624776e-06
Epoch 84: reducing lr to 6.552504729787697e-06
Epoch 87: reducing lr to 4.260110293355726e-06
Epoch 90: reducing lr to 2.433907315859481e-06
Epoch 93: reducing lr to 1.1026869585053189e-06
Epoch 96: reducing lr to 2.8744728369231935e-07
Epoch 99: reducing lr to 1.0426954607568353e-09
[I 2024-06-21 02:02:23,236] Trial 560 finished with value: 0.9713010191917419 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.45226228862151585, 'bidirectional': True, 'fc_dropout': 0.0010123876695879545, 'learning_rate_model': 0.0006764030462393815}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.397489298084023e-05
Epoch 43: reducing lr to 5.144940753600396e-05
Epoch 49: reducing lr to 4.543874770135738e-05
Epoch 53: reducing lr to 4.0874024930815484e-05
Epoch 65: reducing lr to 2.593746117239954e-05
Epoch 68: reducing lr to 2.2222797341262983e-05
Epoch 72: reducing lr to 1.7475765605490583e-05
Epoch 75: reducing lr to 1.4142645425068052e-05
Epoch 78: reducing lr to 1.1062947349981997e-05
Epoch 81: reducing lr to 8.285230944495958e-06
Epoch 84: reducing lr to 5.853310639547039e-06
Epoch 87: reducing lr to 3.8055293256615228e-06
Epoch 90: reducing lr to 2.174193865565248e-06
Epoch 93: reducing lr to 9.850232197418136e-07
Epoch 96: reducing lr to 2.5677482326664493e-07
Epoch 99: reducing lr to 9.314331978231213e-10
[I 2024-06-21 02:02:56,040] Trial 561 finished with value: 0.9717766046524048 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.4564529137633649, 'bidirectional': True, 'fc_dropout': 0.0030580691866943337, 'learning_rate_model': 0.000604226522596235}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.89141309411785e-05
Epoch 43: reducing lr to 5.6157538533668944e-05
Epoch 47: reducing lr to 5.1921753483714285e-05
Epoch 53: reducing lr to 4.461440354725408e-05
Epoch 65: reducing lr to 2.831099608358486e-05
Epoch 72: reducing lr to 1.907497146024333e-05
Epoch 75: reducing lr to 1.543683772977345e-05
Epoch 78: reducing lr to 1.2075316740388223e-05
Epoch 81: reducing lr to 9.043411738031773e-06
Epoch 84: reducing lr to 6.388946608566293e-06
Epoch 87: reducing lr to 4.153772997236014e-06
Epoch 90: reducing lr to 2.373154165083522e-06
Epoch 93: reducing lr to 1.0751626125237628e-06
Epoch 96: reducing lr to 2.802722659533551e-07
Epoch 99: reducing lr to 1.0166685721697994e-09
[I 2024-06-21 02:03:29,143] Trial 562 finished with value: 0.9705192446708679 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.457649488791385, 'bidirectional': True, 'fc_dropout': 0.028925149465201017, 'learning_rate_model': 0.0006595192413443741}. Best is trial 502 with value: 0.9691120386123657.
Epoch 36: reducing lr to 6.304748764075793e-05
Epoch 40: reducing lr to 5.996104616621323e-05
Epoch 43: reducing lr to 5.715546859140115e-05
Epoch 49: reducing lr to 5.0478188991002834e-05
Epoch 53: reducing lr to 4.540720991786996e-05
Epoch 65: reducing lr to 2.8814087826810205e-05
Epoch 68: reducing lr to 2.46874445456497e-05
Epoch 72: reducing lr to 1.9413937302899492e-05
Epoch 75: reducing lr to 1.571115324945457e-05
Epoch 78: reducing lr to 1.2289897397705426e-05
Epoch 81: reducing lr to 9.204114871279323e-06
Epoch 84: reducing lr to 6.502479395515499e-06
Epoch 87: reducing lr to 4.227586327292378e-06
Epoch 90: reducing lr to 2.415325562456108e-06
Epoch 93: reducing lr to 1.0942684550518273e-06
Epoch 96: reducing lr to 2.852527570120205e-07
Epoch 99: reducing lr to 1.034734964562318e-09
[I 2024-06-21 02:04:01,897] Trial 563 finished with value: 0.9712450504302979 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.4639690695088777, 'bidirectional': True, 'fc_dropout': 0.0025884811531540868, 'learning_rate_model': 0.0006712390227268095}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.8364792510838786e-05
Epoch 47: reducing lr to 5.143761471931994e-05
Epoch 53: reducing lr to 4.419840137555675e-05
Epoch 65: reducing lr to 2.8047013268232156e-05
Epoch 72: reducing lr to 1.8897108955724565e-05
Epoch 75: reducing lr to 1.5292898609015657e-05
Epoch 78: reducing lr to 1.1962721757859466e-05
Epoch 81: reducing lr to 8.959087425176404e-06
Epoch 84: reducing lr to 6.329373568186934e-06
Epoch 87: reducing lr to 4.1150415910040024e-06
Epoch 90: reducing lr to 2.3510259462135505e-06
Epoch 93: reducing lr to 1.065137375242189e-06
Epoch 96: reducing lr to 2.776588975782765e-07
Epoch 99: reducing lr to 1.0071887562358197e-09
[I 2024-06-21 02:04:35,000] Trial 564 finished with value: 0.9705194234848022 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.4573724967311623, 'bidirectional': True, 'fc_dropout': 0.00138872991965543, 'learning_rate_model': 0.0006533696256404493}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.458508576134393e-05
Epoch 43: reducing lr to 5.203104939402128e-05
Epoch 49: reducing lr to 4.595243831325613e-05
Epoch 53: reducing lr to 4.133611079232456e-05
Epoch 65: reducing lr to 2.6230687349941224e-05
Epoch 68: reducing lr to 2.2474028788911225e-05
Epoch 72: reducing lr to 1.7673331277552785e-05
Epoch 75: reducing lr to 1.4302529764971496e-05
Epoch 78: reducing lr to 1.1188015325687819e-05
Epoch 81: reducing lr to 8.378896495790937e-06
Epoch 84: reducing lr to 5.919483033729745e-06
Epoch 87: reducing lr to 3.848551301107372e-06
Epoch 90: reducing lr to 2.198773393692426e-06
Epoch 93: reducing lr to 9.961590279689665e-07
Epoch 96: reducing lr to 2.5967769411492205e-07
Epoch 99: reducing lr to 9.419631642943772e-10
[I 2024-06-21 02:05:07,757] Trial 565 finished with value: 0.9717620611190796 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.4612519813992148, 'bidirectional': True, 'fc_dropout': 0.0037660328107966897, 'learning_rate_model': 0.000611057377490342}. Best is trial 502 with value: 0.9691120386123657.
Epoch 36: reducing lr to 6.458281892420182e-05
Epoch 40: reducing lr to 6.142121648246065e-05
Epoch 43: reducing lr to 5.8547317533079576e-05
Epoch 47: reducing lr to 5.413127903144754e-05
Epoch 53: reducing lr to 4.651296547593506e-05
Epoch 65: reducing lr to 2.95157679745819e-05
Epoch 72: reducing lr to 1.988670515442433e-05
Epoch 75: reducing lr to 1.6093750970402833e-05
Epoch 79: reducing lr to 1.1494775571250951e-05
Epoch 82: reducing lr to 8.459766232682766e-06
Epoch 85: reducing lr to 5.8335321022764105e-06
Epoch 88: reducing lr to 3.657479799812661e-06
Epoch 91: reducing lr to 1.965939857826067e-06
Epoch 94: reducing lr to 7.855802782959329e-07
Epoch 97: reducing lr to 1.3501951455900142e-07
[I 2024-06-21 02:05:40,843] Trial 566 finished with value: 0.9704346656799316 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.4626824226778755, 'bidirectional': True, 'fc_dropout': 0.0025040618075712045, 'learning_rate_model': 0.0006875850233181882}. Best is trial 502 with value: 0.9691120386123657.
Epoch 36: reducing lr to 6.343818016106062e-05
Epoch 40: reducing lr to 6.033261263338391e-05
Epoch 43: reducing lr to 5.750964946218052e-05
Epoch 49: reducing lr to 5.079099211155828e-05
Epoch 53: reducing lr to 4.568858920745892e-05
Epoch 65: reducing lr to 2.899264289719499e-05
Epoch 68: reducing lr to 2.4840427642840352e-05
Epoch 72: reducing lr to 1.9534241543047594e-05
Epoch 75: reducing lr to 1.5808512086254953e-05
Epoch 78: reducing lr to 1.2366055404443613e-05
Epoch 81: reducing lr to 9.261150908253626e-06
Epoch 84: reducing lr to 6.5427739442487556e-06
Epoch 87: reducing lr to 4.253783824112835e-06
Epoch 90: reducing lr to 2.4302928461125825e-06
Epoch 93: reducing lr to 1.1010494151913932e-06
Epoch 96: reducing lr to 2.87020410612987e-07
Epoch 99: reducing lr to 1.0411470077135305e-09
[I 2024-06-21 02:06:13,559] Trial 567 finished with value: 0.9711921215057373 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.47039196427604285, 'bidirectional': True, 'fc_dropout': 0.003931997230005712, 'learning_rate_model': 0.0006753985550940455}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 5.939805455119013e-05
Epoch 49: reducing lr to 5.000423466631162e-05
Epoch 52: reducing lr to 4.627320355057912e-05
Epoch 55: reducing lr to 4.233673805549187e-05
Epoch 58: reducing lr to 3.8256933360913414e-05
Epoch 61: reducing lr to 3.4098112374564635e-05
Epoch 64: reducing lr to 2.9925883726900305e-05
Epoch 67: reducing lr to 2.5806018254959606e-05
Epoch 72: reducing lr to 1.9231654227220347e-05
Epoch 75: reducing lr to 1.5563636684829146e-05
Epoch 78: reducing lr to 1.2174503994374496e-05
Epoch 81: reducing lr to 9.117694773106322e-06
Epoch 84: reducing lr to 6.4414257346706276e-06
Epoch 87: reducing lr to 4.187892295813101e-06
Epoch 90: reducing lr to 2.39264737176136e-06
Epoch 93: reducing lr to 1.083994051848938e-06
Epoch 96: reducing lr to 2.825744363250319e-07
Epoch 99: reducing lr to 1.025019538529199e-09
[I 2024-06-21 02:06:42,078] Trial 568 finished with value: 0.9764052033424377 and parameters: {'hidden_size': 120, 'n_layers': 2, 'rnn_dropout': 0.4609255424339913, 'bidirectional': True, 'fc_dropout': 0.004406082219331059, 'learning_rate_model': 0.0006649365652876256}. Best is trial 502 with value: 0.9691120386123657.
Epoch 36: reducing lr to 6.768030486004688e-05
Epoch 40: reducing lr to 6.436706736642727e-05
Epoch 43: reducing lr to 6.135533204314629e-05
Epoch 53: reducing lr to 4.874379495653188e-05
Epoch 64: reducing lr to 3.242936807280781e-05
Epoch 67: reducing lr to 2.7964850499349474e-05
Epoch 72: reducing lr to 2.0840500460238135e-05
Epoch 75: reducing lr to 1.686563067643305e-05
Epoch 78: reducing lr to 1.3192976178763408e-05
Epoch 81: reducing lr to 9.880446053688084e-06
Epoch 84: reducing lr to 6.980290639688549e-06
Epoch 87: reducing lr to 4.538235259803514e-06
Epoch 90: reducing lr to 2.592807049421836e-06
Epoch 93: reducing lr to 1.174676825484828e-06
Epoch 96: reducing lr to 3.0621352696473365e-07
Epoch 99: reducing lr to 1.1107687311823558e-09
[I 2024-06-21 02:07:15,165] Trial 569 finished with value: 0.9703775644302368 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.4615468832810267, 'bidirectional': True, 'fc_dropout': 0.001278058230613896, 'learning_rate_model': 0.0007205626011771762}. Best is trial 502 with value: 0.9691120386123657.
Epoch 36: reducing lr to 6.588809997180073e-05
Epoch 40: reducing lr to 6.266259849598222e-05
Epoch 49: reducing lr to 5.275249002126077e-05
Epoch 52: reducing lr to 4.8816399747804535e-05
Epoch 65: reducing lr to 3.0112310107370574e-05
Epoch 72: reducing lr to 2.0288634642323004e-05
Epoch 75: reducing lr to 1.641902119670092e-05
Epoch 78: reducing lr to 1.2843620240621743e-05
Epoch 81: reducing lr to 9.618807401910594e-06
Epoch 86: reducing lr to 5.158502111070979e-06
Epoch 89: reducing lr to 3.0997274718097935e-06
Epoch 92: reducing lr to 1.5452102575002637e-06
Epoch 95: reducing lr to 5.194707132589711e-07
Epoch 98: reducing lr to 3.868263442085608e-08
[I 2024-06-21 02:07:45,778] Trial 570 finished with value: 0.9722316265106201 and parameters: {'hidden_size': 119, 'n_layers': 2, 'rnn_dropout': 0.48319004084825923, 'bidirectional': True, 'fc_dropout': 0.024504450886496422, 'learning_rate_model': 0.0007014817796769255}. Best is trial 502 with value: 0.9691120386123657.
Epoch 40: reducing lr to 6.219865114869601e-05
Epoch 65: reducing lr to 2.9889361702256645e-05
Epoch 72: reducing lr to 2.0138419706327844e-05
Epoch 75: reducing lr to 1.6297456475286857e-05
Epoch 78: reducing lr to 1.2748527415185063e-05
Epoch 81: reducing lr to 9.547590754575755e-06
Epoch 84: reducing lr to 6.7451366075588625e-06
Epoch 87: reducing lr to 4.385349889382388e-06
Epoch 90: reducing lr to 2.505459822253514e-06
Epoch 93: reducing lr to 1.135103975839942e-06
Epoch 96: reducing lr to 2.958977178852437e-07
Epoch 99: reducing lr to 1.0733488357448847e-09
[I 2024-06-21 02:08:19,670] Trial 571 finished with value: 0.9682753086090088 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.4578720478162112, 'bidirectional': True, 'fc_dropout': 0.029560797109292804, 'learning_rate_model': 0.0006962880817029804}. Best is trial 571 with value: 0.9682753086090088.
Epoch 40: reducing lr to 5.994343467463711e-05
Epoch 49: reducing lr to 5.0463362762025295e-05
Epoch 65: reducing lr to 2.880562468119349e-05
Epoch 72: reducing lr to 1.9408235127651894e-05
Epoch 75: reducing lr to 1.5706538639456983e-05
Epoch 78: reducing lr to 1.2286287663747632e-05
Epoch 81: reducing lr to 9.201411479629482e-06
Epoch 84: reducing lr to 6.500569516211844e-06
Epoch 87: reducing lr to 4.2263446194545245e-06
Epoch 90: reducing lr to 2.4146161437832026e-06
Epoch 93: reducing lr to 1.0939470513921076e-06
Epoch 96: reducing lr to 2.851689738419703e-07
Epoch 99: reducing lr to 1.0344310468206503e-09
[I 2024-06-21 02:08:53,565] Trial 572 finished with value: 0.9686476588249207 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.4636108697346346, 'bidirectional': True, 'fc_dropout': 0.02784329348504267, 'learning_rate_model': 0.0006710418693889317}. Best is trial 571 with value: 0.9682753086090088.
Epoch 40: reducing lr to 6.309364665045038e-05
Epoch 61: reducing lr to 3.621960803032772e-05
Epoch 65: reducing lr to 3.0319448911220007e-05
Epoch 72: reducing lr to 2.0428197614959903e-05
Epoch 75: reducing lr to 1.653196558386139e-05
Epoch 78: reducing lr to 1.2931969893114454e-05
Epoch 81: reducing lr to 9.684973971416067e-06
Epoch 84: reducing lr to 6.8421944506309145e-06
Epoch 87: reducing lr to 4.448452036328189e-06
Epoch 90: reducing lr to 2.541511653432005e-06
Epoch 93: reducing lr to 1.1514373357060775e-06
Epoch 96: reducing lr to 3.001554810617025e-07
Epoch 99: reducing lr to 1.0887935819213143e-09
[I 2024-06-21 02:09:27,462] Trial 573 finished with value: 0.9682168960571289 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.4674886875381635, 'bidirectional': True, 'fc_dropout': 0.08419589555219706, 'learning_rate_model': 0.0007063071848433289}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 6.970411270198523e-05
Epoch 40: reducing lr to 6.562000447320501e-05
Epoch 61: reducing lr to 3.766989177429122e-05
Epoch 65: reducing lr to 3.153348203507533e-05
Epoch 72: reducing lr to 2.124617120801047e-05
Epoch 75: reducing lr to 1.71939285990868e-05
Epoch 78: reducing lr to 1.3449784047748732e-05
Epoch 81: reducing lr to 1.0072773869738918e-05
Epoch 84: reducing lr to 7.1161654824675525e-06
Epoch 87: reducing lr to 4.6265742752184064e-06
Epoch 90: reducing lr to 2.6432773333084915e-06
Epoch 93: reducing lr to 1.1975424964457727e-06
Epoch 96: reducing lr to 3.121741261690944e-07
Epoch 99: reducing lr to 1.1323903991691982e-09
[I 2024-06-21 02:10:01,568] Trial 574 finished with value: 0.9694216847419739 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.4706351716002535, 'bidirectional': True, 'fc_dropout': 0.08662594496367886, 'learning_rate_model': 0.0007345887120085369}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 6.29686742084363e-05
Epoch 61: reducing lr to 3.614786621312987e-05
Epoch 65: reducing lr to 3.0259393806274997e-05
Epoch 72: reducing lr to 2.038773455920991e-05
Epoch 75: reducing lr to 1.6499219971268193e-05
Epoch 78: reducing lr to 1.2906354954948827e-05
Epoch 81: reducing lr to 9.665790505056032e-06
Epoch 84: reducing lr to 6.82864180635331e-06
Epoch 87: reducing lr to 4.439640786009394e-06
Epoch 90: reducing lr to 2.5364775662521028e-06
Epoch 93: reducing lr to 1.1491566316525117e-06
Epoch 99: reducing lr to 1.0866369591785129e-09
[I 2024-06-21 02:10:35,493] Trial 575 finished with value: 0.9685122966766357 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.4834595655144972, 'bidirectional': True, 'fc_dropout': 0.08256830572700266, 'learning_rate_model': 0.0007049081702295281}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 6.140538173529707e-05
Epoch 61: reducing lr to 3.525044082056127e-05
Epoch 65: reducing lr to 2.9508158637776437e-05
Epoch 72: reducing lr to 1.988157824606804e-05
Epoch 75: reducing lr to 1.6089601907715304e-05
Epoch 78: reducing lr to 1.2585935193688683e-05
Epoch 81: reducing lr to 9.4258226522555e-06
Epoch 84: reducing lr to 6.659110456490368e-06
Epoch 87: reducing lr to 4.329419995887057e-06
Epoch 90: reducing lr to 2.473505678445107e-06
Epoch 93: reducing lr to 1.1206270820740441e-06
Epoch 96: reducing lr to 2.9212389635119775e-07
Epoch 99: reducing lr to 1.0596595549270359e-09
[I 2024-06-21 02:11:09,611] Trial 576 finished with value: 0.9697533845901489 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.49244674643496633, 'bidirectional': True, 'fc_dropout': 0.07867220557474364, 'learning_rate_model': 0.0006874077598965037}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 6.684933485618774e-05
Epoch 40: reducing lr to 6.293249396988426e-05
Epoch 49: reducing lr to 5.297970144618618e-05
Epoch 52: reducing lr to 4.9026657950629785e-05
Epoch 58: reducing lr to 4.053338524693663e-05
Epoch 65: reducing lr to 3.0242007509038895e-05
Epoch 68: reducing lr to 2.5910863040885296e-05
Epoch 72: reducing lr to 2.0376020272556104e-05
Epoch 79: reducing lr to 1.1777606106669922e-05
Epoch 82: reducing lr to 8.667919945495839e-06
Epoch 85: reducing lr to 5.977066962756597e-06
Epoch 88: reducing lr to 3.747472593813124e-06
Epoch 91: reducing lr to 2.0143120786792598e-06
Epoch 94: reducing lr to 8.049095891944223e-07
Epoch 97: reducing lr to 1.3834168830289487e-07
[I 2024-06-21 02:11:44,854] Trial 577 finished with value: 0.970413327217102 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.48650002279154847, 'bidirectional': True, 'fc_dropout': 0.08142146761967356, 'learning_rate_model': 0.0007045031474769168}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.214007363327901e-05
Epoch 40: reducing lr to 6.791323741186198e-05
Epoch 49: reducing lr to 5.7172738840545684e-05
Epoch 53: reducing lr to 5.142923321941943e-05
Epoch 58: reducing lr to 4.3741368595672925e-05
Epoch 65: reducing lr to 3.263548774589345e-05
Epoch 68: reducing lr to 2.796155820686157e-05
Epoch 71: reducing lr to 2.3446166091600155e-05
Epoch 79: reducing lr to 1.2709735610484987e-05
Epoch 82: reducing lr to 9.353935749108774e-06
Epoch 85: reducing lr to 6.450117293341912e-06
Epoch 88: reducing lr to 4.0440634067333565e-06
Epoch 91: reducing lr to 2.173733246395564e-06
Epoch 94: reducing lr to 8.686135345629823e-07
Epoch 97: reducing lr to 1.492906339635577e-07
[I 2024-06-21 02:12:20,036] Trial 578 finished with value: 0.9704570770263672 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5009996606860664, 'bidirectional': True, 'fc_dropout': 0.08356194152041667, 'learning_rate_model': 0.000760260502863587}. Best is trial 573 with value: 0.9682168960571289.
Epoch 36: reducing lr to 6.817940740632619e-05
Epoch 40: reducing lr to 6.484173672977525e-05
Epoch 49: reducing lr to 5.458699689924294e-05
Epoch 52: reducing lr to 5.0514026173771956e-05
Epoch 55: reducing lr to 4.621679352521246e-05
Epoch 65: reducing lr to 3.115948797483503e-05
Epoch 72: reducing lr to 2.099418692584949e-05
Epoch 75: reducing lr to 1.6990004809094253e-05
Epoch 78: reducing lr to 1.3290266638926653e-05
Epoch 81: reducing lr to 9.953308547348068e-06
Epoch 84: reducing lr to 7.031766188435589e-06
Epoch 87: reducing lr to 4.571702082662333e-06
Epoch 90: reducing lr to 2.6119274804401794e-06
Epoch 93: reducing lr to 1.1833393779935247e-06
Epoch 96: reducing lr to 3.0847167209762826e-07
Epoch 99: reducing lr to 1.118959998974451e-09
[I 2024-06-21 02:12:55,705] Trial 579 finished with value: 0.9737194776535034 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.4927693104062305, 'bidirectional': True, 'fc_dropout': 0.08288286422996696, 'learning_rate_model': 0.0007258763276703538}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 6.892232336197863e-05
Epoch 40: reducing lr to 6.488402178868723e-05
Epoch 61: reducing lr to 3.7247392746805514e-05
Epoch 65: reducing lr to 3.11798079238545e-05
Epoch 72: reducing lr to 2.100787780576328e-05
Epoch 75: reducing lr to 1.7001084453016526e-05
Epoch 78: reducing lr to 1.329893358302973e-05
Epoch 81: reducing lr to 9.959799370381507e-06
Epoch 84: reducing lr to 7.036351794289619e-06
Epoch 87: reducing lr to 4.574683413848738e-06
Epoch 90: reducing lr to 2.613630789342046e-06
Epoch 93: reducing lr to 1.184111065764934e-06
Epoch 96: reducing lr to 3.086728348592393e-07
Epoch 99: reducing lr to 1.1196897032163603e-09
[I 2024-06-21 02:13:29,845] Trial 580 finished with value: 0.9695117473602295 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.49595166354423387, 'bidirectional': True, 'fc_dropout': 0.08692177623562267, 'learning_rate_model': 0.0007263496913528576}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 6.305714342064696e-05
Epoch 61: reducing lr to 3.61986529461709e-05
Epoch 65: reducing lr to 3.0301907401577193e-05
Epoch 72: reducing lr to 2.0416378751546608e-05
Epoch 75: reducing lr to 1.652240091022391e-05
Epoch 78: reducing lr to 1.2924488019837505e-05
Epoch 81: reducing lr to 9.679370668242338e-06
Epoch 84: reducing lr to 6.838235855595595e-06
Epoch 87: reducing lr to 4.445878356162719e-06
Epoch 90: reducing lr to 2.540041245730775e-06
Epoch 93: reducing lr to 1.1507711643258981e-06
[I 2024-06-21 02:14:03,756] Trial 581 finished with value: 0.9686193466186523 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5044237785433412, 'bidirectional': True, 'fc_dropout': 0.0923541204836106, 'learning_rate_model': 0.0007058985463377281}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.093227287871786e-05
Epoch 40: reducing lr to 6.677620420327793e-05
Epoch 61: reducing lr to 3.833362106006254e-05
Epoch 65: reducing lr to 3.2089090095603596e-05
Epoch 72: reducing lr to 2.1620520731650253e-05
Epoch 75: reducing lr to 1.7496879136270546e-05
Epoch 78: reducing lr to 1.3686764170051167e-05
Epoch 81: reducing lr to 1.0250252346352446e-05
Epoch 84: reducing lr to 7.2415496343894415e-06
Epoch 87: reducing lr to 4.708092769023975e-06
Epoch 90: reducing lr to 2.689850883002892e-06
Epoch 93: reducing lr to 1.2186427435770774e-06
Epoch 96: reducing lr to 3.176745165349589e-07
Epoch 99: reducing lr to 1.1523426909194021e-09
[I 2024-06-21 02:14:37,858] Trial 582 finished with value: 0.969504714012146 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5236204410958933, 'bidirectional': True, 'fc_dropout': 0.09830148673570878, 'learning_rate_model': 0.0007475318880622004}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 6.342020907281233e-05
Epoch 61: reducing lr to 3.64070748128536e-05
Epoch 65: reducing lr to 3.047637743266024e-05
Epoch 72: reducing lr to 2.0533930633287228e-05
Epoch 75: reducing lr to 1.6617532340802546e-05
Epoch 78: reducing lr to 1.2998903659640955e-05
Epoch 81: reducing lr to 9.73510181674636e-06
Epoch 84: reducing lr to 6.877608532914804e-06
Epoch 87: reducing lr to 4.471476498375728e-06
Epoch 90: reducing lr to 2.554666103143943e-06
Epoch 93: reducing lr to 1.1573969875174388e-06
[I 2024-06-21 02:15:11,774] Trial 583 finished with value: 0.9688892364501953 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5229326351644865, 'bidirectional': True, 'fc_dropout': 0.1291960377794493, 'learning_rate_model': 0.0007099629156095651}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 6.471648019005182e-05
Epoch 65: reducing lr to 3.109929634764979e-05
Epoch 72: reducing lr to 2.095363188612874e-05
Epoch 75: reducing lr to 1.6957184756461483e-05
Epoch 78: reducing lr to 1.3264593470761353e-05
Epoch 81: reducing lr to 9.93408147154293e-06
Epoch 84: reducing lr to 7.018182735164151e-06
Epoch 87: reducing lr to 4.562870801879293e-06
Epoch 90: reducing lr to 2.606881949356217e-06
Epoch 93: reducing lr to 1.1810534892545632e-06
Epoch 96: reducing lr to 3.078757890106359e-07
Epoch 99: reducing lr to 1.116798473642898e-09
[I 2024-06-21 02:15:45,669] Trial 584 finished with value: 0.9687919616699219 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5279490618228814, 'bidirectional': True, 'fc_dropout': 0.11546308259394884, 'learning_rate_model': 0.0007244741327006223}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 6.865688900072921e-05
Epoch 40: reducing lr to 6.463413977602899e-05
Epoch 61: reducing lr to 3.7103945204417394e-05
Epoch 65: reducing lr to 3.105972792659901e-05
Epoch 72: reducing lr to 2.0926972050493065e-05
Epoch 75: reducing lr to 1.693560970155416e-05
Epoch 78: reducing lr to 1.324771659310948e-05
Epoch 81: reducing lr to 9.921442088515586e-06
Epoch 84: reducing lr to 7.009253323824035e-06
Epoch 87: reducing lr to 4.557065345991453e-06
Epoch 90: reducing lr to 2.603565147540226e-06
Epoch 93: reducing lr to 1.17955080503868e-06
Epoch 96: reducing lr to 3.074840708600004e-07
Epoch 99: reducing lr to 1.1153775426918713e-09
[I 2024-06-21 02:16:19,760] Trial 585 finished with value: 0.9696823358535767 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5200020336959054, 'bidirectional': True, 'fc_dropout': 0.1141903151381185, 'learning_rate_model': 0.0007235523659441451}. Best is trial 573 with value: 0.9682168960571289.
Epoch 36: reducing lr to 6.813473145789768e-05
Epoch 40: reducing lr to 6.479924785818842e-05
Epoch 43: reducing lr to 6.176729080807838e-05
Epoch 49: reducing lr to 5.4551227655256944e-05
Epoch 52: reducing lr to 5.048092582699413e-05
Epoch 55: reducing lr to 4.6186509027845714e-05
Epoch 65: reducing lr to 3.113907008429037e-05
Epoch 72: reducing lr to 2.098043005631838e-05
Epoch 75: reducing lr to 1.69788717616313e-05
Epoch 78: reducing lr to 1.3281557920421334e-05
Epoch 81: reducing lr to 9.946786438748563e-06
Epoch 84: reducing lr to 7.027158480103312e-06
Epoch 87: reducing lr to 4.568706381551904e-06
Epoch 90: reducing lr to 2.6102159616421494e-06
Epoch 93: reducing lr to 1.182563970710958e-06
Epoch 96: reducing lr to 3.0826953973776566e-07
Epoch 99: reducing lr to 1.1182267776023782e-09
[I 2024-06-21 02:16:55,374] Trial 586 finished with value: 0.973932683467865 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5452112229886894, 'bidirectional': True, 'fc_dropout': 0.11795914138063185, 'learning_rate_model': 0.000725400682389555}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 6.931257653944985e-05
Epoch 40: reducing lr to 6.525140922478918e-05
Epoch 61: reducing lr to 3.7458295581517236e-05
Epoch 65: reducing lr to 3.13563547743041e-05
Epoch 72: reducing lr to 2.1126828976671562e-05
Epoch 75: reducing lr to 1.7097348288949634e-05
Epoch 78: reducing lr to 1.3374235035949397e-05
Epoch 81: reducing lr to 1.0016193919515522e-05
Epoch 84: reducing lr to 7.076193147737692e-06
Epoch 87: reducing lr to 4.600586265799962e-06
Epoch 90: reducing lr to 2.628429735032265e-06
Epoch 93: reducing lr to 1.190815760025851e-06
Epoch 96: reducing lr to 3.1042060755067865e-07
Epoch 99: reducing lr to 1.126029629716814e-09
[I 2024-06-21 02:17:29,464] Trial 587 finished with value: 0.9695988297462463 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5264832924097851, 'bidirectional': True, 'fc_dropout': 0.12780090565912583, 'learning_rate_model': 0.0007304624412019444}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.014894577647453e-05
Epoch 40: reducing lr to 6.603877385719555e-05
Epoch 61: reducing lr to 3.791029098639051e-05
Epoch 65: reducing lr to 3.173472031527535e-05
Epoch 72: reducing lr to 2.1381758611582942e-05
Epoch 75: reducing lr to 1.7303655670055793e-05
Epoch 78: reducing lr to 1.353561698582452e-05
Epoch 81: reducing lr to 1.0137055628668526e-05
Epoch 84: reducing lr to 7.16157895446274e-06
Epoch 87: reducing lr to 4.656099839484582e-06
Epoch 90: reducing lr to 2.660146024944111e-06
Epoch 93: reducing lr to 1.2051848935709392e-06
Epoch 96: reducing lr to 3.141663382629986e-07
Epoch 99: reducing lr to 1.1396170129752623e-09
[I 2024-06-21 02:18:03,569] Trial 588 finished with value: 0.9695791602134705 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5238658612644334, 'bidirectional': True, 'fc_dropout': 0.10005336193265689, 'learning_rate_model': 0.0007392766614362122}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.304523732109469e-05
Epoch 40: reducing lr to 6.876536568580465e-05
Epoch 61: reducing lr to 3.947552128347639e-05
Epoch 65: reducing lr to 3.304497524644556e-05
Epoch 72: reducing lr to 2.2264563135448014e-05
Epoch 75: reducing lr to 1.801808453357563e-05
Epoch 78: reducing lr to 1.4094472041924428e-05
Epoch 81: reducing lr to 1.055559176174471e-05
Epoch 84: reducing lr to 7.457264375567037e-06
Epoch 87: reducing lr to 4.8483396863808734e-06
Epoch 90: reducing lr to 2.7699774465601953e-06
Epoch 93: reducing lr to 1.2549442560006483e-06
Epoch 96: reducing lr to 3.271375568471616e-07
Epoch 99: reducing lr to 1.1866692256981038e-09
[I 2024-06-21 02:18:37,685] Trial 589 finished with value: 0.9693305492401123 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5247158973641777, 'bidirectional': True, 'fc_dropout': 0.12687338388732436, 'learning_rate_model': 0.0007697997251822507}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 6.964517988741737e-05
Epoch 40: reducing lr to 6.556452465421505e-05
Epoch 49: reducing lr to 5.5195475699782045e-05
Epoch 53: reducing lr to 4.96506035916504e-05
Epoch 58: reducing lr to 4.222861623143872e-05
Epoch 65: reducing lr to 3.1506821386550945e-05
Epoch 68: reducing lr to 2.6994535119950504e-05
Epoch 72: reducing lr to 2.122820818374199e-05
Epoch 79: reducing lr to 1.2270181860549168e-05
Epoch 82: reducing lr to 9.030439048533225e-06
Epoch 85: reducing lr to 6.227046308177173e-06
Epoch 88: reducing lr to 3.904203437188325e-06
Epoch 91: reducing lr to 2.0985568124322165e-06
Epoch 94: reducing lr to 8.385733867532141e-07
Epoch 97: reducing lr to 1.4412756369994738e-07
[I 2024-06-21 02:19:12,904] Trial 590 finished with value: 0.9706850051879883 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5267751474727412, 'bidirectional': True, 'fc_dropout': 0.12778243136757766, 'learning_rate_model': 0.0007339676384639451}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 6.763643859514287e-05
Epoch 61: reducing lr to 3.882744815901234e-05
Epoch 65: reducing lr to 3.250247296504402e-05
Epoch 72: reducing lr to 2.1899043833184703e-05
Epoch 75: reducing lr to 1.772228004611419e-05
Epoch 78: reducing lr to 1.3863081847776326e-05
Epoch 81: reducing lr to 1.0382299678165206e-05
Epoch 84: reducing lr to 7.334837806729025e-06
Epoch 87: reducing lr to 4.768744064920867e-06
Epoch 90: reducing lr to 2.7245024818194878e-06
Epoch 93: reducing lr to 1.2343417251518656e-06
Epoch 96: reducing lr to 3.217669106415585e-07
Epoch 99: reducing lr to 1.1671875720488736e-09
[I 2024-06-21 02:19:46,991] Trial 591 finished with value: 0.9694564342498779 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5292155610970998, 'bidirectional': True, 'fc_dropout': 0.10063858533367358, 'learning_rate_model': 0.0007571618550062525}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.398470740960712e-05
Epoch 40: reducing lr to 6.964979027742378e-05
Epoch 61: reducing lr to 3.9983235035041566e-05
Epoch 65: reducing lr to 3.346998263855247e-05
Epoch 72: reducing lr to 2.25509184389102e-05
Epoch 75: reducing lr to 1.8249823824080935e-05
Epoch 78: reducing lr to 1.4275747856507046e-05
Epoch 81: reducing lr to 1.0691352327257223e-05
Epoch 84: reducing lr to 7.553175855629427e-06
Epoch 87: reducing lr to 4.910696525530845e-06
Epoch 90: reducing lr to 2.8056034647967918e-06
Epoch 93: reducing lr to 1.2710847003951222e-06
Epoch 96: reducing lr to 3.3134503102012507e-07
Epoch 99: reducing lr to 1.2019315519316668e-09
[I 2024-06-21 02:20:21,116] Trial 592 finished with value: 0.9693379402160645 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5224486356705126, 'bidirectional': True, 'fc_dropout': 0.10584361027819211, 'learning_rate_model': 0.000779700491371492}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.061422337155896e-05
Epoch 65: reducing lr to 3.3933437859137865e-05
Epoch 72: reducing lr to 2.286317856143175e-05
Epoch 75: reducing lr to 1.8502527155820704e-05
Epoch 78: reducing lr to 1.4473422589216304e-05
Epoch 81: reducing lr to 1.0839394323714012e-05
Epoch 84: reducing lr to 7.657763862742978e-06
Epoch 87: reducing lr to 4.978694407873523e-06
Epoch 90: reducing lr to 2.844452351773955e-06
Epoch 93: reducing lr to 1.2886852724230891e-06
Epoch 96: reducing lr to 3.3593312973831694e-07
Epoch 99: reducing lr to 1.2185745678250545e-09
[I 2024-06-21 02:20:55,266] Trial 593 finished with value: 0.9689255356788635 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5522820048303185, 'bidirectional': True, 'fc_dropout': 0.11093368729752329, 'learning_rate_model': 0.0007904969195358403}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.106250001889472e-05
Epoch 65: reducing lr to 3.414885575980683e-05
Epoch 72: reducing lr to 2.30083197035927e-05
Epoch 75: reducing lr to 1.8619985798635575e-05
Epoch 78: reducing lr to 1.4565303474454224e-05
Epoch 81: reducing lr to 1.0908205493965324e-05
Epoch 84: reducing lr to 7.706377251754087e-06
Epoch 87: reducing lr to 5.010300397866886e-06
Epoch 90: reducing lr to 2.8625096425417027e-06
Epoch 93: reducing lr to 1.2968661669484424e-06
Epoch 96: reducing lr to 3.3806571677159543e-07
Epoch 99: reducing lr to 1.2263103821579142e-09
[I 2024-06-21 02:21:29,368] Trial 594 finished with value: 0.9689570665359497 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5555852892442996, 'bidirectional': True, 'fc_dropout': 0.10275613533949088, 'learning_rate_model': 0.0007955151905285578}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.273403294561717e-05
Epoch 40: reducing lr to 6.847239541878144e-05
Epoch 49: reducing lr to 5.764346584338804e-05
Epoch 52: reducing lr to 5.3342438818066145e-05
Epoch 65: reducing lr to 3.290418932717974e-05
Epoch 68: reducing lr to 2.8191777376984363e-05
Epoch 71: reducing lr to 2.3639208155287608e-05
Epoch 74: reducing lr to 1.9318257820928412e-05
Epoch 79: reducing lr to 1.2814380164378547e-05
Epoch 82: reducing lr to 9.430950603202758e-06
Epoch 85: reducing lr to 6.5032237990481406e-06
Epoch 88: reducing lr to 4.0773598673431164e-06
Epoch 91: reducing lr to 2.1916304987715392e-06
Epoch 94: reducing lr to 8.75765192049488e-07
Epoch 97: reducing lr to 1.5051980601484212e-07
[I 2024-06-21 02:22:04,553] Trial 595 finished with value: 0.9707068800926208 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5503922551231922, 'bidirectional': True, 'fc_dropout': 0.10357256094424859, 'learning_rate_model': 0.000766520044651335}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.006401016315713e-05
Epoch 65: reducing lr to 3.3669034672001766e-05
Epoch 72: reducing lr to 2.2685032824922577e-05
Epoch 75: reducing lr to 1.8358358823381784e-05
Epoch 78: reducing lr to 1.4360648308070891e-05
Epoch 81: reducing lr to 1.0754935731050569e-05
Epoch 84: reducing lr to 7.598095957002035e-06
Epoch 87: reducing lr to 4.939901324936188e-06
Epoch 90: reducing lr to 2.822288895463164e-06
Epoch 93: reducing lr to 1.2786440707429516e-06
Epoch 96: reducing lr to 3.3331559978050634e-07
Epoch 99: reducing lr to 1.209079656012968e-09
[I 2024-06-21 02:22:38,631] Trial 596 finished with value: 0.9693437814712524 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5320771017386761, 'bidirectional': True, 'fc_dropout': 0.14381964582797463, 'learning_rate_model': 0.0007843375110546197}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.57760843046207e-05
Epoch 40: reducing lr to 7.133620669257226e-05
Epoch 61: reducing lr to 4.0951341092866837e-05
Epoch 65: reducing lr to 3.428038462126577e-05
Epoch 72: reducing lr to 2.309693930818452e-05
Epoch 75: reducing lr to 1.8691703151325283e-05
Epoch 78: reducing lr to 1.4621403678697482e-05
Epoch 81: reducing lr to 1.09502198987604e-05
Epoch 84: reducing lr to 7.736059389071531e-06
Epoch 87: reducing lr to 5.029598236468971e-06
Epoch 90: reducing lr to 2.873534979286427e-06
Epoch 93: reducing lr to 1.3018612195382878e-06
Epoch 96: reducing lr to 3.3936782185933466e-07
Epoch 99: reducing lr to 1.2310336797573088e-09
[I 2024-06-21 02:23:12,765] Trial 597 finished with value: 0.9692474007606506 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5254115691296489, 'bidirectional': True, 'fc_dropout': 0.15440050729213584, 'learning_rate_model': 0.0007985792231280531}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.477173940573154e-05
Epoch 40: reducing lr to 7.039070843998679e-05
Epoch 65: reducing lr to 3.382602847786939e-05
Epoch 72: reducing lr to 2.2790809829642524e-05
Epoch 75: reducing lr to 1.844396117727294e-05
Epoch 78: reducing lr to 1.4427609919966621e-05
Epoch 81: reducing lr to 1.0805084430255284e-05
Epoch 84: reducing lr to 7.633524772032166e-06
Epoch 87: reducing lr to 4.9629353654772275e-06
Epoch 90: reducing lr to 2.835448817607451e-06
Epoch 93: reducing lr to 1.2846062018516034e-06
Epoch 96: reducing lr to 3.3486980188563886e-07
Epoch 99: reducing lr to 1.2147174183944724e-09
[I 2024-06-21 02:23:46,885] Trial 598 finished with value: 0.9693479537963867 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5289767510555485, 'bidirectional': True, 'fc_dropout': 0.1545376192349466, 'learning_rate_model': 0.0007879947626552305}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.364810048461487e-05
Epoch 40: reducing lr to 6.93329058488372e-05
Epoch 61: reducing lr to 3.980132401224228e-05
Epoch 65: reducing lr to 3.331770484588469e-05
Epoch 72: reducing lr to 2.2448318921020017e-05
Epoch 75: reducing lr to 1.8166792920882743e-05
Epoch 78: reducing lr to 1.4210797737000062e-05
Epoch 81: reducing lr to 1.0642710069189447e-05
Epoch 84: reducing lr to 7.518811303984811e-06
Epoch 87: reducing lr to 4.88835441572324e-06
Epoch 90: reducing lr to 2.792838859946704e-06
Epoch 93: reducing lr to 1.26530166863918e-06
Epoch 96: reducing lr to 3.2983751634705966e-07
Epoch 99: reducing lr to 1.196463145052007e-09
[I 2024-06-21 02:24:21,008] Trial 599 finished with value: 0.9694134593009949 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5177851010819996, 'bidirectional': True, 'fc_dropout': 0.16078592896037747, 'learning_rate_model': 0.0007761531017283532}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.146638297866591e-05
Epoch 65: reducing lr to 3.434294041674126e-05
Epoch 72: reducing lr to 2.313908724285432e-05
Epoch 75: reducing lr to 1.8725812289024327e-05
Epoch 78: reducing lr to 1.4648085221165415e-05
Epoch 81: reducing lr to 1.0970202163369346e-05
Epoch 84: reducing lr to 7.750176364545286e-06
Epoch 87: reducing lr to 5.038776386658386e-06
Epoch 90: reducing lr to 2.8787786855179055e-06
Epoch 93: reducing lr to 1.3042368919552353e-06
Epoch 96: reducing lr to 3.399871096616781e-07
Epoch 99: reducing lr to 1.2332801041254476e-09
[I 2024-06-21 02:24:55,121] Trial 600 finished with value: 0.9693385362625122 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5305107027959324, 'bidirectional': True, 'fc_dropout': 0.1550166965870693, 'learning_rate_model': 0.0008000364926162718}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.310166721140195e-05
Epoch 65: reducing lr to 3.5128770993700164e-05
Epoch 72: reducing lr to 2.3668552747488e-05
Epoch 75: reducing lr to 1.9154293825449055e-05
Epoch 78: reducing lr to 1.4983260751303784e-05
Epoch 81: reducing lr to 1.1221220864470264e-05
Epoch 84: reducing lr to 7.927514865272931e-06
Epoch 87: reducing lr to 5.154072995132962e-06
Epoch 90: reducing lr to 2.9446505150096694e-06
Epoch 93: reducing lr to 1.3340802663681212e-06
Epoch 96: reducing lr to 3.477666493080121e-07
Epoch 99: reducing lr to 1.261499854782796e-09
[I 2024-06-21 02:25:29,243] Trial 601 finished with value: 0.9693262577056885 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5309232333833523, 'bidirectional': True, 'fc_dropout': 0.1559791824290816, 'learning_rate_model': 0.0008183428208150754}. Best is trial 573 with value: 0.9682168960571289.
Epoch 35: reducing lr to 7.707108216262103e-05
Epoch 40: reducing lr to 7.255532794583446e-05
Epoch 65: reducing lr to 3.486622941732975e-05
Epoch 72: reducing lr to 2.34916613000233e-05
Epoch 75: reducing lr to 1.901114055384449e-05
Epoch 78: reducing lr to 1.4871280491660718e-05
Epoch 81: reducing lr to 1.1137356928123413e-05
Epoch 84: reducing lr to 7.868267069504556e-06
Epoch 87: reducing lr to 5.115553046652172e-06
Epoch 90: reducing lr to 2.9226431072296166e-06
Epoch 93: reducing lr to 1.324109762811381e-06
Epoch 96: reducing lr to 3.451675488631194e-07
Epoch 99: reducing lr to 1.2520717947899288e-09
[I 2024-06-21 02:26:03,360] Trial 602 finished with value: 0.9692258238792419 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5359370312516302, 'bidirectional': True, 'fc_dropout': 0.15794575868187313, 'learning_rate_model': 0.0008122267795158585}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.233418238071344e-05
Epoch 43: reducing lr to 6.894966571605843e-05
Epoch 65: reducing lr to 3.475995862748658e-05
Epoch 72: reducing lr to 2.3420059711815976e-05
Epoch 75: reducing lr to 1.895319540301403e-05
Epoch 78: reducing lr to 1.4825953458877448e-05
Epoch 81: reducing lr to 1.110341073614061e-05
Epoch 84: reducing lr to 7.844284924886488e-06
Epoch 87: reducing lr to 5.099961057732311e-06
Epoch 90: reducing lr to 2.9137350148827946e-06
Epoch 93: reducing lr to 1.320073932362131e-06
Epoch 96: reducing lr to 3.4411549280031214e-07
Epoch 99: reducing lr to 1.248255533019358e-09
[I 2024-06-21 02:26:37,283] Trial 603 finished with value: 0.9685123562812805 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5681352428337189, 'bidirectional': True, 'fc_dropout': 0.15717810717756076, 'learning_rate_model': 0.0008097511467091742}. Best is trial 573 with value: 0.9682168960571289.
Epoch 36: reducing lr to 7.924513535911583e-05
Epoch 40: reducing lr to 7.536575044497108e-05
Epoch 43: reducing lr to 7.183938669922912e-05
Epoch 49: reducing lr to 6.34466347345631e-05
Epoch 53: reducing lr to 5.707286096353853e-05
Epoch 62: reducing lr to 4.149770039151932e-05
Epoch 65: reducing lr to 3.621676890751957e-05
Epoch 68: reducing lr to 3.102994199924248e-05
Epoch 71: reducing lr to 2.6019049744818127e-05
Epoch 74: reducing lr to 2.126309425950578e-05
Epoch 77: reducing lr to 1.6837093814695373e-05
Epoch 80: reducing lr to 1.2810835982996394e-05
Epoch 83: reducing lr to 9.247829118016657e-06
Epoch 88: reducing lr to 4.487841915812882e-06
Epoch 91: reducing lr to 2.4122695902164573e-06
Epoch 94: reducing lr to 9.639315304907597e-07
Epoch 97: reducing lr to 1.656731602240431e-07
[I 2024-06-21 02:27:10,065] Trial 604 finished with value: 0.9699022769927979 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5625981860098023, 'bidirectional': True, 'fc_dropout': 0.15202265096301165, 'learning_rate_model': 0.0008436882928213567}. Best is trial 573 with value: 0.9682168960571289.
Epoch 30: reducing lr to 8.78527363494364e-05
Epoch 36: reducing lr to 8.37855419056314e-05
Epoch 40: reducing lr to 7.96838848661276e-05
Epoch 49: reducing lr to 6.70818549206064e-05
Epoch 52: reducing lr to 6.207658907302299e-05
Epoch 55: reducing lr to 5.67957281818689e-05
Epoch 61: reducing lr to 4.574341838528528e-05
Epoch 65: reducing lr to 3.8291834511182864e-05
Epoch 72: reducing lr to 2.579971571133379e-05
Epoch 75: reducing lr to 2.0878984051967007e-05
Epoch 78: reducing lr to 1.6332382969780432e-05
Epoch 81: reducing lr to 1.2231601624571053e-05
Epoch 84: reducing lr to 8.641323869839105e-06
Epoch 87: reducing lr to 5.618155847911136e-06
Epoch 90: reducing lr to 3.2097926293590145e-06
Epoch 93: reducing lr to 1.4542034730894469e-06
Epoch 96: reducing lr to 3.7908024127001304e-07
Epoch 99: reducing lr to 1.3750877787236e-09
[I 2024-06-21 02:27:45,657] Trial 605 finished with value: 0.9727264642715454 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5398525197458426, 'bidirectional': True, 'fc_dropout': 0.15516318080241534, 'learning_rate_model': 0.0008920280152609117}. Best is trial 573 with value: 0.9682168960571289.
Epoch 36: reducing lr to 7.577613269716742e-05
Epoch 40: reducing lr to 7.206657015171326e-05
Epoch 53: reducing lr to 5.457446272482884e-05
Epoch 62: reducing lr to 3.968111401721946e-05
Epoch 65: reducing lr to 3.4631358431810126e-05
Epoch 72: reducing lr to 2.333341334108893e-05
Epoch 75: reducing lr to 1.8883074932974287e-05
Epoch 78: reducing lr to 1.477110240061428e-05
Epoch 81: reducing lr to 1.1062331838186614e-05
Epoch 84: reducing lr to 7.815263700002614e-06
Epoch 87: reducing lr to 5.081092911282681e-06
Epoch 90: reducing lr to 2.902955171987147e-06
Epoch 93: reducing lr to 1.315190101290045e-06
Epoch 96: reducing lr to 3.428423808215608e-07
Epoch 99: reducing lr to 1.2436374059471983e-09
[I 2024-06-21 02:28:18,749] Trial 606 finished with value: 0.9700591564178467 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.552888082503937, 'bidirectional': True, 'fc_dropout': 0.16893986605172798, 'learning_rate_model': 0.0008067553388881116}. Best is trial 573 with value: 0.9682168960571289.
Epoch 40: reducing lr to 7.20805483631187e-05
Epoch 52: reducing lr to 5.6153318684359584e-05
Epoch 61: reducing lr to 4.137863868904468e-05
Epoch 65: reducing lr to 3.463807561632992e-05
Epoch 72: reducing lr to 2.3337939148044997e-05
Epoch 77: reducing lr to 1.6103162880208512e-05
Epoch 80: reducing lr to 1.2252410109265621e-05
Epoch 83: reducing lr to 8.84471513995966e-06
Epoch 86: reducing lr to 5.933805329221106e-06
Epoch 89: reducing lr to 3.5656047037149255e-06
Epoch 92: reducing lr to 1.777449473374087e-06
Epoch 95: reducing lr to 5.975451827566433e-07
Epoch 98: reducing lr to 4.449648702909218e-08
[I 2024-06-21 02:28:55,128] Trial 607 finished with value: 0.9666209816932678 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5358749094999582, 'bidirectional': True, 'fc_dropout': 0.13275835994188576, 'learning_rate_model': 0.0008069118191626094}. Best is trial 607 with value: 0.9666209816932678.
Epoch 36: reducing lr to 7.617322120946491e-05
Epoch 40: reducing lr to 7.244421949998878e-05
Epoch 43: reducing lr to 6.905455419811053e-05
Epoch 49: reducing lr to 6.098714477211181e-05
Epoch 53: reducing lr to 5.486044844937672e-05
Epoch 62: reducing lr to 3.988905435371532e-05
Epoch 65: reducing lr to 3.4812836611139826e-05
Epoch 68: reducing lr to 2.9827075508342418e-05
Epoch 71: reducing lr to 2.5010429004764292e-05
Epoch 74: reducing lr to 2.0438836722117102e-05
Epoch 77: reducing lr to 1.6184408870768243e-05
Epoch 80: reducing lr to 1.2314227728790208e-05
Epoch 83: reducing lr to 8.889339767314565e-06
Epoch 88: reducing lr to 4.313872056084501e-06
Epoch 91: reducing lr to 2.3187586755921445e-06
Epoch 94: reducing lr to 9.265650108376583e-07
Epoch 97: reducing lr to 1.5925088934517053e-07
[I 2024-06-21 02:29:27,914] Trial 608 finished with value: 0.9698081016540527 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.535391471113744, 'bidirectional': True, 'fc_dropout': 0.13459013701739772, 'learning_rate_model': 0.0008109829665843862}. Best is trial 607 with value: 0.9666209816932678.
Epoch 40: reducing lr to 7.126065819588966e-05
Epoch 61: reducing lr to 4.0907971639960045e-05
Epoch 65: reducing lr to 3.424408003424169e-05
Epoch 72: reducing lr to 2.3072478531201764e-05
Epoch 77: reducing lr to 1.591999522670706e-05
Epoch 80: reducing lr to 1.2113043375776875e-05
Epoch 83: reducing lr to 8.744109704237073e-06
Epoch 86: reducing lr to 5.866310439765342e-06
Epoch 89: reducing lr to 3.5250472398333467e-06
Epoch 92: reducing lr to 1.757231628489995e-06
Epoch 95: reducing lr to 5.907483224254863e-07
[I 2024-06-21 02:30:04,324] Trial 609 finished with value: 0.9664782285690308 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.574024194981266, 'bidirectional': True, 'fc_dropout': 0.14240294980476184, 'learning_rate_model': 0.0007977334890669572}. Best is trial 609 with value: 0.9664782285690308.
Epoch 40: reducing lr to 7.267530086485505e-05
Epoch 52: reducing lr to 5.661665210130414e-05
Epoch 61: reducing lr to 4.172006296282762e-05
Epoch 65: reducing lr to 3.492388201758485e-05
Epoch 72: reducing lr to 2.3530505631082082e-05
Epoch 77: reducing lr to 1.6236033628647107e-05
Epoch 80: reducing lr to 1.2353507447316865e-05
Epoch 83: reducing lr to 8.917694835260202e-06
Epoch 86: reducing lr to 5.982766465679068e-06
Epoch 89: reducing lr to 3.5950252945108523e-06
Epoch 92: reducing lr to 1.7921156009911213e-06
Epoch 95: reducing lr to 6.024756598467236e-07
Epoch 98: reducing lr to 4.486363735716837e-08
[I 2024-06-21 02:30:40,770] Trial 610 finished with value: 0.9665331840515137 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5780478410253818, 'bidirectional': True, 'fc_dropout': 0.14733553036597327, 'learning_rate_model': 0.0008135698265449887}. Best is trial 609 with value: 0.9664782285690308.
Epoch 36: reducing lr to 7.912247322522396e-05
Epoch 40: reducing lr to 7.524909314190701e-05
Epoch 43: reducing lr to 7.172818779181257e-05
Epoch 49: reducing lr to 6.33484268184616e-05
Epoch 52: reducing lr to 5.8621728106449956e-05
Epoch 55: reducing lr to 5.363477254152584e-05
Epoch 65: reducing lr to 3.6160709615845285e-05
Epoch 72: reducing lr to 2.4363837353795244e-05
Epoch 75: reducing lr to 1.9716968095549237e-05
Epoch 78: reducing lr to 1.542340724711242e-05
Epoch 83: reducing lr to 9.233514568002418e-06
Epoch 86: reducing lr to 6.194645851681252e-06
Epoch 89: reducing lr to 3.722342942029432e-06
Epoch 92: reducing lr to 1.8555832886171016e-06
Epoch 95: reducing lr to 6.23812306299815e-07
Epoch 98: reducing lr to 4.645248091165619e-08
[I 2024-06-21 02:31:16,431] Trial 611 finished with value: 0.9728875160217285 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5828886200186202, 'bidirectional': True, 'fc_dropout': 0.14606175611537298, 'learning_rate_model': 0.0008423823627365878}. Best is trial 609 with value: 0.9664782285690308.
Epoch 36: reducing lr to 7.833356543928078e-05
Epoch 40: reducing lr to 7.449880573246477e-05
Epoch 43: reducing lr to 7.101300633306972e-05
Epoch 52: reducing lr to 5.8037227447617164e-05
Epoch 55: reducing lr to 5.3099995405138084e-05
Epoch 61: reducing lr to 4.276686616810324e-05
Epoch 65: reducing lr to 3.580016141508357e-05
Epoch 72: reducing lr to 2.412091242740738e-05
Epoch 75: reducing lr to 1.952037578729953e-05
Epoch 78: reducing lr to 1.5269624818846004e-05
Epoch 81: reducing lr to 1.1435683824360997e-05
Epoch 84: reducing lr to 8.079027639428146e-06
Epoch 87: reducing lr to 5.252579010064809e-06
Epoch 90: reducing lr to 3.0009294594240283e-06
Epoch 93: reducing lr to 1.3595775635083215e-06
Epoch 96: reducing lr to 3.5441325807391037e-07
Epoch 99: reducing lr to 1.2856099757788293e-09
[I 2024-06-21 02:31:50,824] Trial 612 finished with value: 0.972770094871521 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5740981981217784, 'bidirectional': True, 'fc_dropout': 0.17314498902807926, 'learning_rate_model': 0.0008339832066230974}. Best is trial 609 with value: 0.9664782285690308.
Epoch 40: reducing lr to 7.670679632858453e-05
Epoch 61: reducing lr to 4.403438767259273e-05
Epoch 65: reducing lr to 3.6861204192439485e-05
Epoch 71: reducing lr to 2.6482028476533927e-05
Epoch 76: reducing lr to 1.859645543835221e-05
Epoch 79: reducing lr to 1.435542079891109e-05
Epoch 82: reducing lr to 1.056510441441907e-05
Epoch 85: reducing lr to 7.285293005770347e-06
Epoch 88: reducing lr to 4.567697843631236e-06
Epoch 91: reducing lr to 2.4551931222588885e-06
Epoch 94: reducing lr to 9.810835710850311e-07
Epoch 97: reducing lr to 1.6862112144293934e-07
[I 2024-06-21 02:32:27,214] Trial 613 finished with value: 0.9663827419281006 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5573698188710522, 'bidirectional': True, 'fc_dropout': 0.1597622009243039, 'learning_rate_model': 0.0008587007448365071}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.615657272133601e-05
Epoch 43: reducing lr to 7.259320639831943e-05
Epoch 49: reducing lr to 6.411238823415992e-05
Epoch 52: reducing lr to 5.932868707361146e-05
Epoch 65: reducing lr to 3.6596796008171073e-05
Epoch 68: reducing lr to 3.1355543074298856e-05
Epoch 72: reducing lr to 2.465765730499921e-05
Epoch 75: reducing lr to 1.995474831545463e-05
Epoch 78: reducing lr to 1.560940852018527e-05
Epoch 81: reducing lr to 1.1690153663881302e-05
Epoch 84: reducing lr to 8.25880428404871e-06
Epoch 87: reducing lr to 5.369460777547011e-06
Epoch 90: reducing lr to 3.067706929812345e-06
Epoch 93: reducing lr to 1.3898312404825242e-06
Epoch 96: reducing lr to 3.6229975496304646e-07
Epoch 99: reducing lr to 1.3142177065681403e-09
[I 2024-06-21 02:32:58,613] Trial 614 finished with value: 0.970524251461029 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5544152925705655, 'bidirectional': True, 'fc_dropout': 0.16150315462683879, 'learning_rate_model': 0.0008525412199445144}. Best is trial 613 with value: 0.9663827419281006.
Epoch 30: reducing lr to 8.46552359591613e-05
Epoch 36: reducing lr to 8.073607168905134e-05
Epoch 40: reducing lr to 7.678369912866066e-05
Epoch 43: reducing lr to 7.31909895586953e-05
Epoch 52: reducing lr to 5.981724091796451e-05
Epoch 55: reducing lr to 5.47285829730373e-05
Epoch 61: reducing lr to 4.407853457839171e-05
Epoch 65: reducing lr to 3.689815958560222e-05
Epoch 72: reducing lr to 2.486070567608741e-05
Epoch 75: reducing lr to 2.011906964942443e-05
Epoch 78: reducing lr to 1.5737947291507682e-05
Epoch 81: reducing lr to 1.1786418553520283e-05
Epoch 84: reducing lr to 8.326813046449354e-06
Epoch 87: reducing lr to 5.4136766676056935e-06
Epoch 90: reducing lr to 3.092968571150338e-06
Epoch 93: reducing lr to 1.4012760815709032e-06
Epoch 96: reducing lr to 3.652831841745481e-07
Epoch 99: reducing lr to 1.325039893009406e-09
[I 2024-06-21 02:33:32,961] Trial 615 finished with value: 0.9726821780204773 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5625383738140433, 'bidirectional': True, 'fc_dropout': 0.13434154120870248, 'learning_rate_model': 0.0008595616397619386}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.36872173854804e-05
Epoch 43: reducing lr to 7.023939220788206e-05
Epoch 49: reducing lr to 6.203356217459233e-05
Epoch 53: reducing lr to 5.5801743999116306e-05
Epoch 56: reducing lr to 5.085029923367612e-05
Epoch 65: reducing lr to 3.541015524600508e-05
Epoch 68: reducing lr to 3.0338848456455606e-05
Epoch 71: reducing lr to 2.5439557934342517e-05
Epoch 74: reducing lr to 2.0789526273372524e-05
Epoch 77: reducing lr to 1.646210095086995e-05
Epoch 80: reducing lr to 1.2525515242604207e-05
Epoch 83: reducing lr to 9.041863055030986e-06
Epoch 86: reducing lr to 6.066069323095802e-06
Epoch 89: reducing lr to 3.6450817159400087e-06
Epoch 92: reducing lr to 1.8170686643005662e-06
Epoch 95: reducing lr to 6.108644118191079e-07
Epoch 98: reducing lr to 4.54883097737507e-08
[I 2024-06-21 02:34:04,420] Trial 616 finished with value: 0.9708659052848816 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5115841333958937, 'bidirectional': True, 'fc_dropout': 0.14678698133864593, 'learning_rate_model': 0.0008248978119590992}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.353683783998819e-05
Epoch 61: reducing lr to 4.2214637824157706e-05
Epoch 65: reducing lr to 3.5337890839224095e-05
Epoch 72: reducing lr to 2.380945047759126e-05
Epoch 77: reducing lr to 1.642850539187511e-05
Epoch 80: reducing lr to 1.2499953396790633e-05
Epoch 83: reducing lr to 9.023410583830926e-06
Epoch 86: reducing lr to 6.053689798123931e-06
Epoch 89: reducing lr to 3.6376428988537585e-06
Epoch 92: reducing lr to 1.8133604233116704e-06
Epoch 95: reducing lr to 6.096177707344531e-07
Epoch 98: reducing lr to 4.539547805079963e-08
[I 2024-06-21 02:34:40,822] Trial 617 finished with value: 0.9665506482124329 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5450167908212942, 'bidirectional': True, 'fc_dropout': 0.16009742365620147, 'learning_rate_model': 0.0008232143753680416}. Best is trial 613 with value: 0.9663827419281006.
Epoch 30: reducing lr to 8.53983877213374e-05
Epoch 35: reducing lr to 8.227862499739856e-05
Epoch 40: reducing lr to 7.745775006794799e-05
Epoch 43: reducing lr to 7.383350165200749e-05
Epoch 52: reducing lr to 6.034235064677259e-05
Epoch 55: reducing lr to 5.520902157104008e-05
Epoch 61: reducing lr to 4.446548100025125e-05
Epoch 65: reducing lr to 3.722207259590112e-05
Epoch 68: reducing lr to 3.189126994436537e-05
Epoch 72: reducing lr to 2.5078947076312657e-05
Epoch 75: reducing lr to 2.029568627442004e-05
Epoch 78: reducing lr to 1.5876103935101006e-05
Epoch 81: reducing lr to 1.1889886432601266e-05
Epoch 84: reducing lr to 8.399910542648716e-06
Epoch 87: reducing lr to 5.4612009974336136e-06
Epoch 90: reducing lr to 3.12012040668613e-06
Epoch 93: reducing lr to 1.4135772792170507e-06
Epoch 96: reducing lr to 3.6848984751836845e-07
Epoch 99: reducing lr to 1.3366718460815913e-09
[I 2024-06-21 02:35:15,170] Trial 618 finished with value: 0.9725161790847778 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5438495814715416, 'bidirectional': True, 'fc_dropout': 0.16030365652841957, 'learning_rate_model': 0.0008671073602368804}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.310343732471265e-05
Epoch 40: reducing lr to 7.903517219259966e-05
Epoch 43: reducing lr to 7.533711618953555e-05
Epoch 52: reducing lr to 6.157121875719678e-05
Epoch 55: reducing lr to 5.633334976324982e-05
Epoch 61: reducing lr to 4.537101767607188e-05
Epoch 65: reducing lr to 3.798009772297568e-05
Epoch 72: reducing lr to 2.558967822905645e-05
Epoch 75: reducing lr to 2.070900662694997e-05
Epoch 78: reducing lr to 1.6199419776040478e-05
Epoch 81: reducing lr to 1.213202320912689e-05
Epoch 84: reducing lr to 8.570974183452187e-06
Epoch 87: reducing lr to 5.57241800635737e-06
Epoch 90: reducing lr to 3.1836614591536447e-06
Epoch 93: reducing lr to 1.4423646900724564e-06
Epoch 96: reducing lr to 3.7599411968835486e-07
Epoch 99: reducing lr to 1.3638930827070648e-09
[I 2024-06-21 02:35:49,476] Trial 619 finished with value: 0.9727095365524292 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5752122173653943, 'bidirectional': True, 'fc_dropout': 0.185884090049385, 'learning_rate_model': 0.0008847659461535394}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 7.925355958476418e-05
Epoch 40: reducing lr to 7.537376226910425e-05
Epoch 43: reducing lr to 7.184702365008106e-05
Epoch 49: reducing lr to 6.345337948634136e-05
Epoch 53: reducing lr to 5.707892814554251e-05
Epoch 62: reducing lr to 4.1502111842019116e-05
Epoch 65: reducing lr to 3.62206189638309e-05
Epoch 68: reducing lr to 3.103324066523722e-05
Epoch 71: reducing lr to 2.6021815723388465e-05
Epoch 74: reducing lr to 2.1265354651935075e-05
Epoch 79: reducing lr to 1.4105947925853695e-05
Epoch 82: reducing lr to 1.0381500813428398e-05
Epoch 88: reducing lr to 4.488318999898888e-06
Epoch 91: reducing lr to 2.4125260287127847e-06
Epoch 94: reducing lr to 9.64034002102251e-07
Epoch 97: reducing lr to 1.656907722588988e-07
[I 2024-06-21 02:36:22,280] Trial 620 finished with value: 0.9699232578277588 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5592041989711316, 'bidirectional': True, 'fc_dropout': 0.12369963738733049, 'learning_rate_model': 0.0008437779818669163}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.097697107198012e-05
Epoch 52: reducing lr to 5.52935926038927e-05
Epoch 61: reducing lr to 4.0745117898311865e-05
Epoch 65: reducing lr to 3.4107755099533164e-05
Epoch 72: reducing lr to 2.298062749808349e-05
Epoch 77: reducing lr to 1.585661807341017e-05
Epoch 80: reducing lr to 1.206482161465282e-05
Epoch 83: reducing lr to 8.70929959448022e-06
Epoch 86: reducing lr to 5.842956786027782e-06
Epoch 89: reducing lr to 3.511014103760367e-06
Epoch 92: reducing lr to 1.750236127755797e-06
Epoch 95: reducing lr to 5.883965662561454e-07
Epoch 98: reducing lr to 4.3815230938012636e-08
[I 2024-06-21 02:36:58,653] Trial 621 finished with value: 0.966623067855835 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5377568985383434, 'bidirectional': True, 'fc_dropout': 0.14698697002950137, 'learning_rate_model': 0.0007945577294698788}. Best is trial 613 with value: 0.9663827419281006.
Epoch 35: reducing lr to 8.028139619452071e-05
Epoch 40: reducing lr to 7.557754303426566e-05
Epoch 43: reducing lr to 7.204126951247695e-05
Epoch 52: reducing lr to 5.8877602290210976e-05
Epoch 55: reducing lr to 5.3868879485973984e-05
Epoch 61: reducing lr to 4.338612728729935e-05
Epoch 65: reducing lr to 3.631854515492001e-05
Epoch 72: reducing lr to 2.4470181489281453e-05
Epoch 75: reducing lr to 1.9803029412413345e-05
Epoch 78: reducing lr to 1.5490727878346672e-05
Epoch 81: reducing lr to 1.1601271696430246e-05
Epoch 84: reducing lr to 8.196011373479248e-06
Epoch 87: reducing lr to 5.328635973033595e-06
Epoch 90: reducing lr to 3.0443827002662864e-06
Epoch 93: reducing lr to 1.3792641479847748e-06
Epoch 96: reducing lr to 3.595451363366288e-07
Epoch 99: reducing lr to 1.3042255149454083e-09
[I 2024-06-21 02:37:32,983] Trial 622 finished with value: 0.9725304841995239 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5433575793016459, 'bidirectional': True, 'fc_dropout': 0.13812710997120825, 'learning_rate_model': 0.0008460592229459713}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 6.392245209773372e-05
Epoch 47: reducing lr to 5.633564896691445e-05
Epoch 61: reducing lr to 3.669539299486328e-05
Epoch 65: reducing lr to 3.071772870245584e-05
Epoch 72: reducing lr to 2.0696544783974568e-05
Epoch 77: reducing lr to 1.4280602481637334e-05
Epoch 80: reducing lr to 1.0865678967171403e-05
Epoch 83: reducing lr to 7.84366785063828e-06
Epoch 86: reducing lr to 5.2622156119512936e-06
Epoch 89: reducing lr to 3.1620485838214277e-06
Epoch 92: reducing lr to 1.576277253684613e-06
[I 2024-06-21 02:38:09,434] Trial 623 finished with value: 0.9672915935516357 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5381879870750564, 'bidirectional': True, 'fc_dropout': 0.14524291069719777, 'learning_rate_model': 0.0007155853177985642}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 6.230144406576292e-05
Epoch 40: reducing lr to 5.925152458309841e-05
Epoch 43: reducing lr to 5.647914552581909e-05
Epoch 50: reducing lr to 4.866673678102631e-05
Epoch 53: reducing lr to 4.486990449166963e-05
Epoch 56: reducing lr to 4.0888472410898166e-05
Epoch 61: reducing lr to 3.401399522029106e-05
Epoch 65: reducing lr to 2.8473129512737164e-05
Epoch 72: reducing lr to 1.9184211365639134e-05
Epoch 75: reducing lr to 1.5525242511753095e-05
Epoch 78: reducing lr to 1.2144470524502337e-05
Epoch 81: reducing lr to 9.09520219259559e-06
Epoch 84: reducing lr to 6.425535283131455e-06
Epoch 87: reducing lr to 4.1775611203375105e-06
Epoch 90: reducing lr to 2.3867449134117056e-06
Epoch 93: reducing lr to 1.0813199303641657e-06
Epoch 96: reducing lr to 2.818773491316704e-07
Epoch 99: reducing lr to 1.022490902179412e-09
[I 2024-06-21 02:38:43,772] Trial 624 finished with value: 0.973502516746521 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5628783188374393, 'bidirectional': True, 'fc_dropout': 0.1175185069336599, 'learning_rate_model': 0.0006632962231176537}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 6.003973500037398e-05
Epoch 43: reducing lr to 5.723047557471664e-05
Epoch 49: reducing lr to 5.0544433162781234e-05
Epoch 53: reducing lr to 4.546679927861968e-05
Epoch 65: reducing lr to 2.8851901492906223e-05
Epoch 68: reducing lr to 2.4719842683339292e-05
Epoch 72: reducing lr to 1.9439414845245905e-05
Epoch 75: reducing lr to 1.573177150766662e-05
Epoch 78: reducing lr to 1.2306025830413213e-05
Epoch 81: reducing lr to 9.216193731055957e-06
Epoch 84: reducing lr to 6.51101281104945e-06
Epoch 87: reducing lr to 4.2331343265465255e-06
Epoch 90: reducing lr to 2.4184952728727845e-06
Epoch 93: reducing lr to 1.0957044991920152e-06
Epoch 96: reducing lr to 2.856271034973627e-07
Epoch 99: reducing lr to 1.036092880956664e-09
[I 2024-06-21 02:39:16,532] Trial 625 finished with value: 0.9712971448898315 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5827069711756719, 'bidirectional': True, 'fc_dropout': 0.14018087976816967, 'learning_rate_model': 0.0006721199115624571}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.93400435125392e-05
Epoch 52: reducing lr to 6.180872439186597e-05
Epoch 61: reducing lr to 4.554603244053897e-05
Epoch 64: reducing lr to 3.997304179377178e-05
Epoch 67: reducing lr to 3.4469994458645976e-05
Epoch 71: reducing lr to 2.739112297987527e-05
Epoch 76: reducing lr to 1.923484820481338e-05
Epoch 79: reducing lr to 1.484822421663289e-05
Epoch 82: reducing lr to 1.0927791070348254e-05
Epoch 85: reducing lr to 7.535387889273909e-06
Epoch 88: reducing lr to 4.7245011265160255e-06
Epoch 91: reducing lr to 2.539476793128907e-06
Epoch 94: reducing lr to 1.0147629277318251e-06
Epoch 97: reducing lr to 1.7440967101672475e-07
[I 2024-06-21 02:39:52,986] Trial 626 finished with value: 0.9667447805404663 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.544208735674858, 'bidirectional': True, 'fc_dropout': 0.1714674291993975, 'learning_rate_model': 0.0008881788540318705}. Best is trial 613 with value: 0.9663827419281006.
Epoch 30: reducing lr to 8.643508089108838e-05
Epoch 36: reducing lr to 8.243351764607142e-05
Epoch 40: reducing lr to 7.83980479187897e-05
Epoch 43: reducing lr to 7.472980296288468e-05
Epoch 52: reducing lr to 6.107487621817278e-05
Epoch 55: reducing lr to 5.587923112766654e-05
Epoch 61: reducing lr to 4.5005269416316805e-05
Epoch 65: reducing lr to 3.7673929702970415e-05
Epoch 68: reducing lr to 3.2278413807477764e-05
Epoch 72: reducing lr to 2.5383392790479962e-05
Epoch 75: reducing lr to 2.0542065625336556e-05
Epoch 78: reducing lr to 1.6068831795086862e-05
Epoch 81: reducing lr to 1.2034223631261428e-05
Epoch 84: reducing lr to 8.501881201796271e-06
Epoch 87: reducing lr to 5.527497211258532e-06
Epoch 90: reducing lr to 3.157997087976266e-06
Epoch 93: reducing lr to 1.4307373913618028e-06
Epoch 96: reducing lr to 3.7296312761461075e-07
Epoch 99: reducing lr to 1.3528983652225275e-09
[I 2024-06-21 02:40:27,364] Trial 627 finished with value: 0.9727545976638794 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5527404242622217, 'bidirectional': True, 'fc_dropout': 0.17590027725423465, 'learning_rate_model': 0.0008776336043708053}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 8.092409071398358e-05
Epoch 43: reducing lr to 7.713765220622549e-05
Epoch 49: reducing lr to 6.812592184702866e-05
Epoch 52: reducing lr to 6.304275367346459e-05
Epoch 55: reducing lr to 5.7679701074791145e-05
Epoch 61: reducing lr to 4.6455372315213085e-05
Epoch 65: reducing lr to 3.888781144134494e-05
Epoch 68: reducing lr to 3.3318448053268256e-05
Epoch 71: reducing lr to 2.793799477096312e-05
Epoch 74: reducing lr to 2.2831280237468206e-05
Epoch 77: reducing lr to 1.8078855437326537e-05
Epoch 80: reducing lr to 1.3755654884202668e-05
Epoch 83: reducing lr to 9.929870770678906e-06
Epoch 86: reducing lr to 6.661822248104667e-06
Epoch 89: reducing lr to 4.003067749152581e-06
Epoch 92: reducing lr to 1.995524252926533e-06
Epoch 95: reducing lr to 6.70857834370259e-07
Epoch 98: reducing lr to 4.99557485974752e-08
[I 2024-06-21 02:40:58,706] Trial 628 finished with value: 0.971994161605835 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5101547859115025, 'bidirectional': True, 'fc_dropout': 0.19727084280808072, 'learning_rate_model': 0.0009059116049332348}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 6.68173585056611e-05
Epoch 40: reducing lr to 6.354636588996049e-05
Epoch 43: reducing lr to 6.057303119183883e-05
Epoch 49: reducing lr to 5.349648934064654e-05
Epoch 52: reducing lr to 4.95048860765559e-05
Epoch 55: reducing lr to 4.5293501064805426e-05
Epoch 64: reducing lr to 3.2015882864498947e-05
Epoch 67: reducing lr to 2.7608289372161016e-05
Epoch 72: reducing lr to 2.0574777161075602e-05
Epoch 75: reducing lr to 1.6650588286527444e-05
Epoch 78: reducing lr to 1.3024761352891918e-05
Epoch 83: reducing lr to 7.797519819701217e-06
Epoch 86: reducing lr to 5.231255493103569e-06
Epoch 89: reducing lr to 3.1434447471152575e-06
Epoch 92: reducing lr to 1.5670032644166197e-06
Epoch 95: reducing lr to 5.267971135284266e-07
Epoch 98: reducing lr to 3.9228198311198286e-08
[I 2024-06-21 02:41:34,494] Trial 629 finished with value: 0.9730339050292969 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5433378195457705, 'bidirectional': True, 'fc_dropout': 0.11974543179428701, 'learning_rate_model': 0.0007113751888113722}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.461268805456644e-05
Epoch 40: reducing lr to 8.047053870878504e-05
Epoch 43: reducing lr to 7.670532190104978e-05
Epoch 49: reducing lr to 6.774409922551873e-05
Epoch 52: reducing lr to 6.268942048072107e-05
Epoch 55: reducing lr to 5.735642596782472e-05
Epoch 65: reducing lr to 3.866985848442703e-05
Epoch 72: reducing lr to 2.605441520970649e-05
Epoch 75: reducing lr to 2.108510519004727e-05
Epoch 78: reducing lr to 1.6493619232853155e-05
Epoch 83: reducing lr to 9.874217222277333e-06
Epoch 86: reducing lr to 6.624485000169633e-06
Epoch 89: reducing lr to 3.9806319159098975e-06
Epoch 92: reducing lr to 1.9843400181905916e-06
Epoch 95: reducing lr to 6.670979043753908e-07
Epoch 98: reducing lr to 4.967576361712515e-08
[I 2024-06-21 02:42:10,109] Trial 630 finished with value: 0.9726662635803223 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5712512825794698, 'bidirectional': True, 'fc_dropout': 0.1679940449350914, 'learning_rate_model': 0.0009008342784989805}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 6.084048809528176e-05
Epoch 65: reducing lr to 2.923670081645879e-05
Epoch 72: reducing lr to 1.969867934067431e-05
Epoch 77: reducing lr to 1.3592075972759982e-05
Epoch 83: reducing lr to 7.465492402582933e-06
Epoch 86: reducing lr to 5.008502580661786e-06
Epoch 89: reducing lr to 3.0095932322269358e-06
Epoch 92: reducing lr to 1.5002784521005848e-06
[I 2024-06-21 02:42:46,509] Trial 631 finished with value: 0.9676995277404785 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5105426102977189, 'bidirectional': True, 'fc_dropout': 0.13032571773521237, 'learning_rate_model': 0.0006810840100770398}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.7988536507406434e-05
Epoch 43: reducing lr to 5.527525266692161e-05
Epoch 49: reducing lr to 4.8817632317795e-05
Epoch 65: reducing lr to 2.786620465628413e-05
Epoch 72: reducing lr to 1.8775286357095668e-05
Epoch 75: reducing lr to 1.519431100741568e-05
Epoch 78: reducing lr to 1.1885602561762778e-05
Epoch 81: reducing lr to 8.901331537011943e-06
Epoch 84: reducing lr to 6.2885704624009465e-06
Epoch 87: reducing lr to 4.088513455866678e-06
Epoch 90: reducing lr to 2.335869760636017e-06
Epoch 93: reducing lr to 1.0582708326798881e-06
Epoch 96: reducing lr to 2.75868934440811e-07
Epoch 99: reducing lr to 1.0006957867599378e-09
[I 2024-06-21 02:43:22,752] Trial 632 finished with value: 0.9721577167510986 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5119758893127866, 'bidirectional': True, 'fc_dropout': 0.1312740212042443, 'learning_rate_model': 0.0006491575958613335}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.861762959057594e-05
Epoch 43: reducing lr to 5.5874910482372175e-05
Epoch 53: reducing lr to 4.438986945506326e-05
Epoch 56: reducing lr to 4.045103222525684e-05
Epoch 65: reducing lr to 2.8168513313465283e-05
Epoch 72: reducing lr to 1.8978971490281728e-05
Epoch 75: reducing lr to 1.535914765503603e-05
Epoch 78: reducing lr to 1.201454443219525e-05
Epoch 81: reducing lr to 8.997898314485588e-06
Epoch 84: reducing lr to 6.356792501086396e-06
Epoch 87: reducing lr to 4.132868007480559e-06
Epoch 90: reducing lr to 2.3612106227806177e-06
Epoch 93: reducing lr to 1.0697515649255442e-06
Epoch 96: reducing lr to 2.7886171972167163e-07
Epoch 99: reducing lr to 1.0115519116999165e-09
[I 2024-06-21 02:43:54,187] Trial 633 finished with value: 0.970962643623352 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5526251550511436, 'bidirectional': True, 'fc_dropout': 0.12848131539055763, 'learning_rate_model': 0.0006562000317984975}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 6.0201754984809115e-05
Epoch 65: reducing lr to 2.892975967517127e-05
Epoch 72: reducing lr to 1.9491872999677052e-05
Epoch 77: reducing lr to 1.3449379731561756e-05
Epoch 83: reducing lr to 7.387116023089657e-06
Epoch 86: reducing lr to 4.9559208783725674e-06
Epoch 89: reducing lr to 2.977997054966371e-06
Epoch 92: reducing lr to 1.4845277973592045e-06
[I 2024-06-21 02:44:30,599] Trial 634 finished with value: 0.9678410291671753 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5092695853404668, 'bidirectional': True, 'fc_dropout': 0.11058192102981346, 'learning_rate_model': 0.0006739336580356755}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.7318727265594964e-05
Epoch 49: reducing lr to 4.825375360556646e-05
Epoch 61: reducing lr to 3.2904451471300214e-05
Epoch 65: reducing lr to 2.7544329980060745e-05
Epoch 72: reducing lr to 1.855841831597803e-05
Epoch 75: reducing lr to 1.5018805803306489e-05
Epoch 78: reducing lr to 1.174831531638885e-05
Epoch 81: reducing lr to 8.798514765163305e-06
Epoch 84: reducing lr to 6.215932957349219e-06
Epoch 87: reducing lr to 4.041288189237291e-06
Epoch 90: reducing lr to 2.3088887873682663e-06
Epoch 93: reducing lr to 1.046047044552759e-06
Epoch 96: reducing lr to 2.726824501294916e-07
Epoch 99: reducing lr to 9.891370317542842e-10
[I 2024-06-21 02:45:06,737] Trial 635 finished with value: 0.9715064764022827 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5413455372698847, 'bidirectional': True, 'fc_dropout': 0.11159087080721954, 'learning_rate_model': 0.0006416593594289568}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 8.141574850581488e-05
Epoch 52: reducing lr to 6.342577263343795e-05
Epoch 61: reducing lr to 4.673761392669963e-05
Epoch 71: reducing lr to 2.8107733259169726e-05
Epoch 76: reducing lr to 1.973807291576682e-05
Epoch 79: reducing lr to 1.523668547507511e-05
Epoch 82: reducing lr to 1.1213685424396732e-05
Epoch 85: reducing lr to 7.732529730588418e-06
Epoch 88: reducing lr to 4.848104166606313e-06
Epoch 91: reducing lr to 2.6059149298684245e-06
Epoch 94: reducing lr to 1.0413112932586376e-06
Epoch 97: reducing lr to 1.7897260051584225e-07
[I 2024-06-21 02:45:43,154] Trial 636 finished with value: 0.9666458368301392 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5846588691536871, 'bidirectional': True, 'fc_dropout': 0.14308519893549002, 'learning_rate_model': 0.0009114155098315924}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.228608715445222e-05
Epoch 36: reducing lr to 8.851134282429068e-05
Epoch 40: reducing lr to 8.417833781991789e-05
Epoch 52: reducing lr to 6.557792826587493e-05
Epoch 55: reducing lr to 5.9999208141694275e-05
Epoch 61: reducing lr to 4.832350897980905e-05
Epoch 65: reducing lr to 4.045162941844638e-05
Epoch 68: reducing lr to 3.465830201016709e-05
Epoch 72: reducing lr to 2.7254910932808812e-05
Epoch 75: reducing lr to 2.2056632602889926e-05
Epoch 78: reducing lr to 1.7253587138029617e-05
Epoch 81: reducing lr to 1.2921507220206846e-05
Epoch 84: reducing lr to 9.128725101050493e-06
Epoch 87: reducing lr to 5.935039709534099e-06
Epoch 90: reducing lr to 3.3908362869105235e-06
Epoch 93: reducing lr to 1.5362256925886591e-06
Epoch 96: reducing lr to 4.0046170771033345e-07
Epoch 99: reducing lr to 1.452647593229786e-09
[I 2024-06-21 02:46:17,560] Trial 637 finished with value: 0.9727830290794373 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5818005939861446, 'bidirectional': True, 'fc_dropout': 0.1478234032081334, 'learning_rate_model': 0.0009423415504856149}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 6.066552886703154e-05
Epoch 65: reducing lr to 2.915262472220689e-05
Epoch 72: reducing lr to 1.964203177187776e-05
Epoch 79: reducing lr to 1.1353351157362772e-05
Epoch 86: reducing lr to 4.99409961031037e-06
Epoch 89: reducing lr to 3.000938533263429e-06
[I 2024-06-21 02:46:54,013] Trial 638 finished with value: 0.968633770942688 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.595155850245259, 'bidirectional': True, 'fc_dropout': 0.1818575617723655, 'learning_rate_model': 0.0006791254141402348}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.6802720572568664e-05
Epoch 49: reducing lr to 4.781935352356988e-05
Epoch 61: reducing lr to 3.260823210287503e-05
Epoch 65: reducing lr to 2.72963646238382e-05
Epoch 72: reducing lr to 1.839134782226917e-05
Epoch 75: reducing lr to 1.4883600353264683e-05
Epoch 78: reducing lr to 1.164255216315361e-05
Epoch 81: reducing lr to 8.719306926397493e-06
Epoch 84: reducing lr to 6.159974579304079e-06
Epoch 87: reducing lr to 4.004906855359586e-06
Epoch 90: reducing lr to 2.2881032234771698e-06
Epoch 93: reducing lr to 1.0366301000049798e-06
Epoch 96: reducing lr to 2.7022765086841566e-07
Epoch 99: reducing lr to 9.802324144872129e-10
[I 2024-06-21 02:47:30,161] Trial 639 finished with value: 0.9724404215812683 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.605263513834046, 'bidirectional': True, 'fc_dropout': 0.18505842489316296, 'learning_rate_model': 0.0006358828786886552}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.788528899006145e-05
Epoch 47: reducing lr to 5.101502232590533e-05
Epoch 61: reducing lr to 3.322969251654921e-05
Epoch 65: reducing lr to 2.7816589393994815e-05
Epoch 72: reducing lr to 1.8741857306794256e-05
Epoch 75: reducing lr to 1.516725781753076e-05
Epoch 78: reducing lr to 1.1864440466104534e-05
Epoch 86: reducing lr to 4.765225072406339e-06
Epoch 89: reducing lr to 2.8634085531522876e-06
[I 2024-06-21 02:48:06,614] Trial 640 finished with value: 0.9685863256454468 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5676327765846053, 'bidirectional': True, 'fc_dropout': 0.17103090755145434, 'learning_rate_model': 0.0006480017827614501}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.7253591903089566e-05
Epoch 43: reducing lr to 5.4574696123394804e-05
Epoch 47: reducing lr to 5.045829985708232e-05
Epoch 50: reducing lr to 4.702571801352042e-05
Epoch 53: reducing lr to 4.335691306801343e-05
Epoch 56: reducing lr to 3.950973294655235e-05
Epoch 61: reducing lr to 3.2867059793625775e-05
Epoch 65: reducing lr to 2.7513029391164103e-05
Epoch 72: reducing lr to 1.8537329060123888e-05
Epoch 75: reducing lr to 1.5001738861888492e-05
Epoch 78: reducing lr to 1.1734964866833074e-05
Epoch 81: reducing lr to 8.788516384597694e-06
Epoch 84: reducing lr to 6.208869348895221e-06
Epoch 87: reducing lr to 4.036695784908858e-06
Epoch 90: reducing lr to 2.306265032178223e-06
Epoch 93: reducing lr to 1.044858346605426e-06
Epoch 96: reducing lr to 2.723725815911328e-07
Epoch 99: reducing lr to 9.88013004721809e-10
[I 2024-06-21 02:48:40,954] Trial 641 finished with value: 0.9737445116043091 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5938886268549342, 'bidirectional': True, 'fc_dropout': 0.2119580834262747, 'learning_rate_model': 0.0006409301960826089}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 6.444309401803249e-05
Epoch 40: reducing lr to 6.128833170206897e-05
Epoch 43: reducing lr to 5.842065043206039e-05
Epoch 65: reducing lr to 2.945191062088533e-05
Epoch 72: reducing lr to 1.9843680274773588e-05
Epoch 75: reducing lr to 1.6058932145803505e-05
Epoch 78: reducing lr to 1.2561944069604834e-05
Epoch 87: reducing lr to 4.32116731932914e-06
Epoch 90: reducing lr to 2.4687907183932376e-06
Epoch 93: reducing lr to 1.1184909592540848e-06
Epoch 99: reducing lr to 1.0576396475086256e-09
[I 2024-06-21 02:49:17,954] Trial 642 finished with value: 0.9686768054962158 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5683183550037751, 'bidirectional': True, 'fc_dropout': 0.17289261649727256, 'learning_rate_model': 0.0006860974333605661}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.717454315905602e-05
Epoch 65: reducing lr to 2.747504277153662e-05
Epoch 72: reducing lr to 1.851173498766081e-05
Epoch 75: reducing lr to 1.4981026299131568e-05
Epoch 78: reducing lr to 1.171876266530883e-05
Epoch 87: reducing lr to 4.031122410012443e-06
Epoch 90: reducing lr to 2.3030808240239006e-06
Epoch 93: reducing lr to 1.0434157342339274e-06
Epoch 96: reducing lr to 2.71996522906116e-07
Epoch 99: reducing lr to 9.866488774319496e-10
[I 2024-06-21 02:49:54,954] Trial 643 finished with value: 0.9694210290908813 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.560776503468277, 'bidirectional': True, 'fc_dropout': 0.1751064720323452, 'learning_rate_model': 0.0006400452782053294}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.495291925054122e-05
Epoch 43: reducing lr to 5.238167195288062e-05
Epoch 56: reducing lr to 3.792207776059508e-05
Epoch 65: reducing lr to 2.6407448549770734e-05
Epoch 72: reducing lr to 1.779242686967083e-05
Epoch 75: reducing lr to 1.4398910476926393e-05
Epoch 78: reducing lr to 1.1263408203743059e-05
Epoch 81: reducing lr to 8.435359514776462e-06
Epoch 87: reducing lr to 3.874485602275838e-06
Epoch 90: reducing lr to 2.2135903071052243e-06
Epoch 93: reducing lr to 1.002871862545334e-06
Epoch 99: reducing lr to 9.483107882291008e-10
[I 2024-06-21 02:50:31,998] Trial 644 finished with value: 0.9703330993652344 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6175599251206941, 'bidirectional': True, 'fc_dropout': 0.1877383293482891, 'learning_rate_model': 0.0006151751207186799}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 5.796135397392061e-05
Epoch 43: reducing lr to 5.524934200427944e-05
Epoch 47: reducing lr to 5.1082059338547025e-05
Epoch 50: reducing lr to 4.760704432785722e-05
Epoch 53: reducing lr to 4.389288605342542e-05
Epoch 56: reducing lr to 3.999814755040054e-05
Epoch 61: reducing lr to 3.3273358464651325e-05
Epoch 65: reducing lr to 2.785314217726993e-05
Epoch 72: reducing lr to 1.876648530983967e-05
Epoch 75: reducing lr to 1.5187188567488254e-05
Epoch 78: reducing lr to 1.1880031102141734e-05
Epoch 81: reducing lr to 8.897158975379146e-06
Epoch 84: reducing lr to 6.285622650859782e-06
Epoch 87: reducing lr to 4.086596936488628e-06
Epoch 90: reducing lr to 2.334774805291199e-06
Epoch 93: reducing lr to 1.0577747607994938e-06
Epoch 96: reducing lr to 2.7573961894158427e-07
Epoch 99: reducing lr to 1.0002267035783516e-09
[I 2024-06-21 02:51:06,326] Trial 645 finished with value: 0.9735966324806213 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5860173511441816, 'bidirectional': True, 'fc_dropout': 0.13779330167729995, 'learning_rate_model': 0.0006488532986821}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 6.642725560994259e-05
Epoch 40: reducing lr to 6.3175360182754e-05
Epoch 43: reducing lr to 6.021938484306283e-05
Epoch 65: reducing lr to 3.035871608627745e-05
Epoch 72: reducing lr to 2.0454654481448594e-05
Epoch 75: reducing lr to 1.6553376381548574e-05
Epoch 78: reducing lr to 1.2948718282147428e-05
Epoch 86: reducing lr to 5.200713610548752e-06
Epoch 89: reducing lr to 3.1250922272640445e-06
Epoch 92: reducing lr to 1.557854556285675e-06
Epoch 99: reducing lr to 1.0902036948982202e-09
[I 2024-06-21 02:51:43,377] Trial 646 finished with value: 0.9683738946914673 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5817618391420963, 'bidirectional': True, 'fc_dropout': 0.1151503881780213, 'learning_rate_model': 0.0007072219339191706}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 8.413825374477375e-05
Epoch 52: reducing lr to 6.55467013413183e-05
Epoch 62: reducing lr to 4.6327994145240134e-05
Epoch 65: reducing lr to 4.0432367145095476e-05
Epoch 72: reducing lr to 2.72419326782344e-05
Epoch 75: reducing lr to 2.2046129666605146e-05
Epoch 78: reducing lr to 1.7245371317888026e-05
Epoch 81: reducing lr to 1.2915354251642666e-05
Epoch 84: reducing lr to 9.1243781810186e-06
Epoch 87: reducing lr to 5.932213559911028e-06
Epoch 90: reducing lr to 3.389221637108131e-06
Epoch 93: reducing lr to 1.5354941720134682e-06
Epoch 96: reducing lr to 4.002710156914631e-07
Epoch 99: reducing lr to 1.4519558708993296e-09
[I 2024-06-21 02:52:20,433] Trial 647 finished with value: 0.968040406703949 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5742551984785194, 'bidirectional': True, 'fc_dropout': 0.11571535057575023, 'learning_rate_model': 0.0009418928259027903}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 8.491593769088567e-05
Epoch 52: reducing lr to 6.615254487960189e-05
Epoch 62: reducing lr to 4.6756200528172806e-05
Epoch 65: reducing lr to 4.080608066341291e-05
Epoch 72: reducing lr to 2.749372794093607e-05
Epoch 75: reducing lr to 2.224990048846732e-05
Epoch 78: reducing lr to 1.7404769068872288e-05
Epoch 81: reducing lr to 1.303472996022724e-05
Epoch 84: reducing lr to 9.208714165113984e-06
Epoch 87: reducing lr to 5.98704459151819e-06
Epoch 90: reducing lr to 3.4205479736992147e-06
Epoch 93: reducing lr to 1.5496866363656097e-06
Epoch 96: reducing lr to 4.039706924632391e-07
Epoch 99: reducing lr to 1.46537619663328e-09
[I 2024-06-21 02:52:57,449] Trial 648 finished with value: 0.9681844711303711 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5745101531260481, 'bidirectional': True, 'fc_dropout': 0.11605431708607095, 'learning_rate_model': 0.0009505986748723274}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.997799756974795e-05
Epoch 52: reducing lr to 6.230571336176055e-05
Epoch 65: reducing lr to 3.843316942467997e-05
Epoch 72: reducing lr to 2.5894942295143633e-05
Epoch 75: reducing lr to 2.0956048247050964e-05
Epoch 78: reducing lr to 1.6392665689678892e-05
Epoch 81: reducing lr to 1.2276748387049486e-05
Epoch 84: reducing lr to 8.673218940347875e-06
Epoch 87: reducing lr to 5.638892424805774e-06
Epoch 90: reducing lr to 3.22163995319203e-06
Epoch 93: reducing lr to 1.4595709287023813e-06
Epoch 96: reducing lr to 3.8047942398852784e-07
Epoch 99: reducing lr to 1.3801632188243511e-09
[I 2024-06-21 02:53:34,471] Trial 649 finished with value: 0.9672420024871826 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5782755360641584, 'bidirectional': True, 'fc_dropout': 0.1395735006601761, 'learning_rate_model': 0.0008953204848953211}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 8.075086683182653e-05
Epoch 52: reducing lr to 6.290780608941592e-05
Epoch 65: reducing lr to 3.880456926207512e-05
Epoch 72: reducing lr to 2.6145178679541247e-05
Epoch 75: reducing lr to 2.1158557512560602e-05
Epoch 78: reducing lr to 1.6551076600430122e-05
Epoch 81: reducing lr to 1.2395385034064176e-05
Epoch 84: reducing lr to 8.757032795732636e-06
Epoch 87: reducing lr to 5.693383994484047e-06
Epoch 90: reducing lr to 3.252772346712331e-06
Epoch 93: reducing lr to 1.473675526728036e-06
Epoch 96: reducing lr to 3.841561958581752e-07
Epoch 99: reducing lr to 1.393500458589641e-09
[I 2024-06-21 02:54:11,493] Trial 650 finished with value: 0.9675776362419128 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5715402497902338, 'bidirectional': True, 'fc_dropout': 0.11418610897587324, 'learning_rate_model': 0.0009039724354756219}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.796363981289625e-05
Epoch 40: reducing lr to 8.365744719000539e-05
Epoch 52: reducing lr to 6.517213588214026e-05
Epoch 65: reducing lr to 4.020131710206591e-05
Epoch 72: reducing lr to 2.7086259138396205e-05
Epoch 75: reducing lr to 2.1920147450678363e-05
Epoch 78: reducing lr to 1.714682295017164e-05
Epoch 81: reducing lr to 1.2841549689449331e-05
Epoch 84: reducing lr to 9.072237084164765e-06
Epoch 87: reducing lr to 5.898314031017274e-06
Epoch 90: reducing lr to 3.3698539903344445e-06
Epoch 93: reducing lr to 1.5267196178736682e-06
Epoch 96: reducing lr to 3.979836740904412e-07
Epoch 99: reducing lr to 1.4436586948996833e-09
[I 2024-06-21 02:54:48,511] Trial 651 finished with value: 0.9673452377319336 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6025141608150987, 'bidirectional': True, 'fc_dropout': 0.1727269159357429, 'learning_rate_model': 0.0009365103960991354}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 9.064829007374461e-05
Epoch 40: reducing lr to 8.621067245328774e-05
Epoch 43: reducing lr to 8.217687476613693e-05
Epoch 52: reducing lr to 6.716118944982108e-05
Epoch 56: reducing lr to 5.9492523220858226e-05
Epoch 65: reducing lr to 4.1428261288028144e-05
Epoch 72: reducing lr to 2.7912931759219713e-05
Epoch 75: reducing lr to 2.2589150344333866e-05
Epoch 78: reducing lr to 1.767014398149565e-05
Epoch 83: reducing lr to 1.0578566023560507e-05
Epoch 86: reducing lr to 7.097023527928678e-06
Epoch 89: reducing lr to 4.264578810656678e-06
Epoch 92: reducing lr to 2.125887189139753e-06
Epoch 95: reducing lr to 7.146834089990193e-07
Epoch 98: reducing lr to 5.321924091451768e-08
[I 2024-06-21 02:55:25,500] Trial 652 finished with value: 0.9679046869277954 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6286138355070169, 'bidirectional': True, 'fc_dropout': 0.1397264983681055, 'learning_rate_model': 0.000965092693108702}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 0.00010065478579848333
Epoch 36: reducing lr to 9.653773962487579e-05
Epoch 39: reducing lr to 9.310517096210664e-05
Epoch 52: reducing lr to 7.15246742627933e-05
Epoch 61: reducing lr to 5.2705587541633135e-05
Epoch 65: reducing lr to 4.411986920085729e-05
Epoch 72: reducing lr to 2.9726444218046617e-05
Epoch 75: reducing lr to 2.4056774954215004e-05
Epoch 78: reducing lr to 1.881817911217027e-05
Epoch 81: reducing lr to 1.4093256974550682e-05
Epoch 84: reducing lr to 9.956537306882098e-06
Epoch 87: reducing lr to 6.473241732189185e-06
Epoch 90: reducing lr to 3.6983245325537426e-06
Epoch 93: reducing lr to 1.675533905417929e-06
Epoch 96: reducing lr to 4.367764270102629e-07
Epoch 99: reducing lr to 1.5843767662638077e-09
[I 2024-06-21 02:56:02,719] Trial 653 finished with value: 0.9713469743728638 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6276783244588374, 'bidirectional': True, 'fc_dropout': 0.140097818037496, 'learning_rate_model': 0.001027795086321029}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.602067737080365e-05
Epoch 40: reducing lr to 8.180960098630959e-05
Epoch 52: reducing lr to 6.373259776662688e-05
Epoch 65: reducing lr to 3.931334055382265e-05
Epoch 72: reducing lr to 2.6487971205852335e-05
Epoch 75: reducing lr to 2.143596985966017e-05
Epoch 78: reducing lr to 1.6768079720988992e-05
Epoch 81: reducing lr to 1.2557902391566504e-05
Epoch 84: reducing lr to 8.871847287223833e-06
Epoch 87: reducing lr to 5.7680306246199136e-06
Epoch 90: reducing lr to 3.2954198292142007e-06
Epoch 93: reducing lr to 1.4929970606506522e-06
Epoch 96: reducing lr to 3.891929131241028e-07
Epoch 99: reducing lr to 1.4117708077121364e-09
[I 2024-06-21 02:56:39,718] Trial 654 finished with value: 0.9672916531562805 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5995787290548983, 'bidirectional': True, 'fc_dropout': 0.19643989762030115, 'learning_rate_model': 0.0009158245248673367}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 9.445819302427508e-05
Epoch 40: reducing lr to 8.983406452256772e-05
Epoch 45: reducing lr to 8.251433846669345e-05
Epoch 52: reducing lr to 6.998394113810827e-05
Epoch 65: reducing lr to 4.3169470689641797e-05
Epoch 68: reducing lr to 3.6986904465667504e-05
Epoch 71: reducing lr to 3.101404789034192e-05
Epoch 74: reducing lr to 2.5345069482889225e-05
Epoch 77: reducing lr to 2.006938912160461e-05
Epoch 80: reducing lr to 1.5270191824399506e-05
Epoch 83: reducing lr to 1.102317793927074e-05
Epoch 86: reducing lr to 7.395307928627717e-06
Epoch 89: reducing lr to 4.443816956023483e-06
Epoch 92: reducing lr to 2.215237179832466e-06
Epoch 95: reducing lr to 7.447212004060844e-07
Epoch 98: reducing lr to 5.545601937796511e-08
[I 2024-06-21 02:57:16,689] Trial 655 finished with value: 0.9685662984848022 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6047955553857708, 'bidirectional': True, 'fc_dropout': 0.1400606357477719, 'learning_rate_model': 0.001005655063298134}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.789453000684773e-05
Epoch 40: reducing lr to 8.359172060158644e-05
Epoch 43: reducing lr to 7.968046368139086e-05
Epoch 52: reducing lr to 6.512093252493309e-05
Epoch 65: reducing lr to 4.0169732401458614e-05
Epoch 72: reducing lr to 2.7064978457883025e-05
Epoch 75: reducing lr to 2.1902925594669518e-05
Epoch 78: reducing lr to 1.7133351320178204e-05
Epoch 83: reducing lr to 1.0257204940444494e-05
Epoch 86: reducing lr to 6.881426521419907e-06
Epoch 89: reducing lr to 4.13502725682849e-06
Epoch 92: reducing lr to 2.0613059020198397e-06
Epoch 95: reducing lr to 6.929723912779575e-07
Epoch 98: reducing lr to 5.1602519624991894e-08
[I 2024-06-21 02:57:53,746] Trial 656 finished with value: 0.9677063226699829 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6281899446675904, 'bidirectional': True, 'fc_dropout': 0.14357763238080173, 'learning_rate_model': 0.0009357746142240958}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.226357851930726e-05
Epoch 36: reducing lr to 8.848975485168013e-05
Epoch 40: reducing lr to 8.415780666997404e-05
Epoch 43: reducing lr to 8.022006258051728e-05
Epoch 52: reducing lr to 6.556193376760968e-05
Epoch 56: reducing lr to 5.807587535339993e-05
Epoch 65: reducing lr to 4.044176324039426e-05
Epoch 72: reducing lr to 2.7248263442759e-05
Epoch 75: reducing lr to 2.2051252976219284e-05
Epoch 78: reducing lr to 1.7249378977191877e-05
Epoch 83: reducing lr to 1.0326667092624097e-05
Epoch 86: reducing lr to 6.928027783558962e-06
Epoch 89: reducing lr to 4.163029806670128e-06
Epoch 92: reducing lr to 2.0752651379994505e-06
Epoch 95: reducing lr to 6.976652246549532e-07
Epoch 98: reducing lr to 5.1951973700629406e-08
[I 2024-06-21 02:58:30,733] Trial 657 finished with value: 0.9677965044975281 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6313769081732473, 'bidirectional': True, 'fc_dropout': 0.13773335268141726, 'learning_rate_model': 0.0009421117127842251}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.578043793963392e-05
Epoch 40: reducing lr to 8.158112229251306e-05
Epoch 52: reducing lr to 6.35546047130681e-05
Epoch 65: reducing lr to 3.920354585258681e-05
Epoch 72: reducing lr to 2.6413995327844493e-05
Epoch 75: reducing lr to 2.1376103262894634e-05
Epoch 78: reducing lr to 1.6721249655740676e-05
Epoch 83: reducing lr to 1.0010492481834262e-05
Epoch 86: reducing lr to 6.715910314450998e-06
Epoch 89: reducing lr to 4.03556909577236e-06
Epoch 92: reducing lr to 2.0117261334583473e-06
Epoch 95: reducing lr to 6.763046027923134e-07
Epoch 98: reducing lr to 5.036134480581982e-08
[I 2024-06-21 02:59:07,729] Trial 658 finished with value: 0.9674491882324219 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6120601823776431, 'bidirectional': True, 'fc_dropout': 0.12325224149669511, 'learning_rate_model': 0.0009132668007290227}. Best is trial 613 with value: 0.9663827419281006.
Epoch 30: reducing lr to 0.00010077041970530205
Epoch 36: reducing lr to 9.610519346243238e-05
Epoch 40: reducing lr to 9.14004373155778e-05
Epoch 43: reducing lr to 8.712381051107258e-05
Epoch 52: reducing lr to 7.120420142499324e-05
Epoch 55: reducing lr to 6.51468537484194e-05
Epoch 58: reducing lr to 5.8868938822346354e-05
Epoch 61: reducing lr to 5.24694353412704e-05
Epoch 64: reducing lr to 4.604930044192864e-05
Epoch 67: reducing lr to 3.970974086103013e-05
Epoch 70: reducing lr to 3.3550777574977784e-05
Epoch 73: reducing lr to 2.766952186540404e-05
Epoch 76: reducing lr to 2.215871657974131e-05
Epoch 79: reducing lr to 1.7105286645645865e-05
Epoch 82: reducing lr to 1.2588912716757478e-05
Epoch 85: reducing lr to 8.6808340143309e-06
Epoch 88: reducing lr to 5.44266740909031e-06
Epoch 91: reducing lr to 2.9254999010437786e-06
Epoch 94: reducing lr to 1.1690159377296914e-06
Epoch 97: reducing lr to 2.009214955935078e-07
[I 2024-06-21 02:59:43,892] Trial 659 finished with value: 0.9737579822540283 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.6179647204222071, 'bidirectional': True, 'fc_dropout': 0.11972938191481683, 'learning_rate_model': 0.0010231899565335092}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.914380293144081e-05
Epoch 40: reducing lr to 8.47798363268719e-05
Epoch 43: reducing lr to 8.081298746743794e-05
Epoch 52: reducing lr to 6.604651706155115e-05
Epoch 65: reducing lr to 4.0740677590805235e-05
Epoch 72: reducing lr to 2.7449661609263378e-05
Epoch 75: reducing lr to 2.2214238846047803e-05
Epoch 78: reducing lr to 1.7376873094629615e-05
Epoch 83: reducing lr to 1.0402993858288432e-05
Epoch 86: reducing lr to 6.979234426361409e-06
Epoch 89: reducing lr to 4.193799715069158e-06
Epoch 92: reducing lr to 2.0906038987495313e-06
Epoch 95: reducing lr to 7.028218283913583e-07
Epoch 98: reducing lr to 5.233596265726568e-08
[I 2024-06-21 03:00:20,905] Trial 660 finished with value: 0.968014657497406 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.641200508544955, 'bidirectional': True, 'fc_dropout': 0.12054328265654612, 'learning_rate_model': 0.0009490750765962209}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.872212491900073e-05
Epoch 40: reducing lr to 8.437880123860216e-05
Epoch 43: reducing lr to 8.043071681245115e-05
Epoch 52: reducing lr to 6.573409642066237e-05
Epoch 65: reducing lr to 4.0547961469358566e-05
Epoch 72: reducing lr to 2.7319816141951978e-05
Epoch 75: reducing lr to 2.2109158562545924e-05
Epoch 78: reducing lr to 1.7294675061025384e-05
Epoch 83: reducing lr to 1.0353784450238339e-05
Epoch 86: reducing lr to 6.946220469086935e-06
Epoch 89: reducing lr to 4.17396173340057e-06
Epoch 92: reducing lr to 2.080714689765449e-06
Epoch 95: reducing lr to 6.994972617703466e-07
Epoch 98: reducing lr to 5.208839721819675e-08
[I 2024-06-21 03:00:57,933] Trial 661 finished with value: 0.9680190086364746 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6389030079593088, 'bidirectional': True, 'fc_dropout': 0.12176000826928228, 'learning_rate_model': 0.0009445856552478484}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.540422484316756e-05
Epoch 36: reducing lr to 9.150194045876747e-05
Epoch 39: reducing lr to 8.824842846830855e-05
Epoch 49: reducing lr to 7.325989371438854e-05
Epoch 52: reducing lr to 6.779365780841662e-05
Epoch 61: reducing lr to 4.9956250807390665e-05
Epoch 67: reducing lr to 3.780772102934699e-05
Epoch 72: reducing lr to 2.8175792591166907e-05
Epoch 75: reducing lr to 2.2801876220057422e-05
Epoch 78: reducing lr to 1.7836546736593858e-05
Epoch 81: reducing lr to 1.3358095658406663e-05
Epoch 84: reducing lr to 9.437164029010121e-06
Epoch 87: reducing lr to 6.1355712476341835e-06
Epoch 90: reducing lr to 3.5054049586192322e-06
Epoch 93: reducing lr to 1.5881312763893593e-06
Epoch 96: reducing lr to 4.139924010380263e-07
Epoch 99: reducing lr to 1.501729262514198e-09
[I 2024-06-21 03:01:35,134] Trial 662 finished with value: 0.9706639647483826 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6394396692905965, 'bidirectional': True, 'fc_dropout': 0.1345168194636246, 'learning_rate_model': 0.0009741811353549348}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 9.055777010208566e-05
Epoch 39: reducing lr to 8.733782974476478e-05
Epoch 49: reducing lr to 7.250395542901465e-05
Epoch 52: reducing lr to 6.709412333130264e-05
Epoch 61: reducing lr to 4.944077309285461e-05
Epoch 67: reducing lr to 3.7417598926245897e-05
Epoch 72: reducing lr to 2.7885058339989098e-05
Epoch 75: reducing lr to 2.2566593170367256e-05
Epoch 78: reducing lr to 1.765249885072579e-05
Epoch 81: reducing lr to 1.322025904118135e-05
Epoch 84: reducing lr to 9.339785869785713e-06
Epoch 87: reducing lr to 6.072260847174026e-06
Epoch 90: reducing lr to 3.469234147011294e-06
Epoch 93: reducing lr to 1.5717440121830634e-06
Epoch 96: reducing lr to 4.0972058613451127e-07
Epoch 99: reducing lr to 1.4862335446434553e-09
[I 2024-06-21 03:02:12,357] Trial 663 finished with value: 0.9706118106842041 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.639955899517322, 'bidirectional': True, 'fc_dropout': 0.14174783121245033, 'learning_rate_model': 0.0009641289665656267}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.911701310698622e-05
Epoch 40: reducing lr to 8.475435797775735e-05
Epoch 61: reducing lr to 4.86541797436991e-05
Epoch 65: reducing lr to 4.0728434052107896e-05
Epoch 72: reducing lr to 2.744141233571133e-05
Epoch 75: reducing lr to 2.220756294105487e-05
Epoch 78: reducing lr to 1.737165093263442e-05
Epoch 83: reducing lr to 1.0399867512203781e-05
Epoch 86: reducing lr to 6.977137001089404e-06
Epoch 89: reducing lr to 4.192539378910392e-06
Epoch 92: reducing lr to 2.089975622754901e-06
Epoch 95: reducing lr to 7.026106137832068e-07
Epoch 98: reducing lr to 5.2320234460731875e-08
[I 2024-06-21 03:02:49,570] Trial 664 finished with value: 0.9705499410629272 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6227472087588791, 'bidirectional': True, 'fc_dropout': 0.12677781812386232, 'learning_rate_model': 0.0009487898570536375}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 9.098750980435588e-05
Epoch 40: reducing lr to 8.653328594176754e-05
Epoch 52: reducing lr to 6.741251686674386e-05
Epoch 55: reducing lr to 6.16777281289625e-05
Epoch 63: reducing lr to 4.5619468002656734e-05
Epoch 66: reducing lr to 3.958146989269352e-05
Epoch 69: reducing lr to 3.368309066203578e-05
Epoch 72: reducing lr to 2.8017386208214057e-05
Epoch 75: reducing lr to 2.26736823910863e-05
Epoch 79: reducing lr to 1.6194415518089636e-05
Epoch 82: reducing lr to 1.191854235941893e-05
Epoch 85: reducing lr to 8.21857218671196e-06
Epoch 88: reducing lr to 5.152840719685285e-06
Epoch 91: reducing lr to 2.769714531951036e-06
Epoch 94: reducing lr to 1.1067648403122664e-06
Epoch 97: reducing lr to 1.9022225429854737e-07
[I 2024-06-21 03:03:28,046] Trial 665 finished with value: 0.9716936349868774 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6272871287555377, 'bidirectional': True, 'fc_dropout': 0.20276490341153047, 'learning_rate_model': 0.0009687042171992821}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.428698089801792e-05
Epoch 36: reducing lr to 9.043039473723308e-05
Epoch 40: reducing lr to 8.600344401610693e-05
Epoch 52: reducing lr to 6.699975110427945e-05
Epoch 55: reducing lr to 6.130007638620757e-05
Epoch 63: reducing lr to 4.534014072979198e-05
Epoch 66: reducing lr to 3.933911318567403e-05
Epoch 71: reducing lr to 2.9691575747191192e-05
Epoch 74: reducing lr to 2.426432863681029e-05
Epoch 77: reducing lr to 1.9213608923636978e-05
Epoch 80: reducing lr to 1.4619054527528785e-05
Epoch 83: reducing lr to 1.0553137852751789e-05
Epoch 86: reducing lr to 7.079964096045406e-06
Epoch 89: reducing lr to 4.254327852428201e-06
Epoch 92: reducing lr to 2.120777099318019e-06
Epoch 95: reducing lr to 7.129654926238057e-07
Epoch 98: reducing lr to 5.309131545228052e-08
[I 2024-06-21 03:04:06,483] Trial 666 finished with value: 0.9712700247764587 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6103032011368464, 'bidirectional': True, 'fc_dropout': 0.1425018200706044, 'learning_rate_model': 0.0009627728567724769}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 9.593583639332294e-05
Epoch 40: reducing lr to 9.12393709920899e-05
Epoch 43: reducing lr to 8.697028048146424e-05
Epoch 52: reducing lr to 7.107872501287481e-05
Epoch 55: reducing lr to 6.503205162571367e-05
Epoch 58: reducing lr to 5.8765199673801464e-05
Epoch 61: reducing lr to 5.2376973430866995e-05
Epoch 65: reducing lr to 4.38447861923792e-05
Epoch 72: reducing lr to 2.9541102786737537e-05
Epoch 75: reducing lr to 2.39067833484249e-05
Epoch 78: reducing lr to 1.8700849631869936e-05
Epoch 81: reducing lr to 1.4005386915141286e-05
Epoch 84: reducing lr to 9.894459284303823e-06
Epoch 87: reducing lr to 6.4328817120315965e-06
Epoch 90: reducing lr to 3.675265845908232e-06
Epoch 93: reducing lr to 1.6650871177039578e-06
Epoch 96: reducing lr to 4.340531693091277e-07
Epoch 99: reducing lr to 1.5744983342721313e-09
[I 2024-06-21 03:04:42,561] Trial 667 finished with value: 0.9731245040893555 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.6692351455160334, 'bidirectional': True, 'fc_dropout': 0.11847804063042003, 'learning_rate_model': 0.001021386885898742}. Best is trial 613 with value: 0.9663827419281006.
Epoch 30: reducing lr to 9.629598226583848e-05
Epoch 36: reducing lr to 9.183790275338541e-05
Epoch 40: reducing lr to 8.734204855522369e-05
Epoch 49: reducing lr to 7.352887775857769e-05
Epoch 52: reducing lr to 6.804257179563554e-05
Epoch 55: reducing lr to 6.225418422964942e-05
Epoch 58: reducing lr to 5.6255023105229036e-05
Epoch 63: reducing lr to 4.604583942452942e-05
Epoch 66: reducing lr to 3.9951408612649126e-05
Epoch 72: reducing lr to 2.8279243992133343e-05
Epoch 75: reducing lr to 2.288559652826155e-05
Epoch 78: reducing lr to 1.7902036136486814e-05
Epoch 83: reducing lr to 1.071739264967565e-05
Epoch 86: reducing lr to 7.19016052113246e-06
Epoch 89: reducing lr to 4.320544532926299e-06
Epoch 92: reducing lr to 2.1537860315076415e-06
Epoch 95: reducing lr to 7.240624766525019e-07
Epoch 98: reducing lr to 5.391765766061228e-08
[I 2024-06-21 03:05:18,284] Trial 668 finished with value: 0.9727110862731934 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6335910220585487, 'bidirectional': True, 'fc_dropout': 0.1092024971793033, 'learning_rate_model': 0.0009777579789493594}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.208629442350725e-05
Epoch 40: reducing lr to 7.806782274317632e-05
Epoch 49: reducing lr to 6.57213735000954e-05
Epoch 52: reducing lr to 6.081761902542222e-05
Epoch 65: reducing lr to 3.7515241057239134e-05
Epoch 71: reducing lr to 2.6951905227925554e-05
Epoch 74: reducing lr to 2.2025435477280663e-05
Epoch 79: reducing lr to 1.4610132347757474e-05
Epoch 82: reducing lr to 1.0752563503693875e-05
Epoch 85: reducing lr to 7.414557643240275e-06
Epoch 88: reducing lr to 4.648743562089157e-06
Epoch 91: reducing lr to 2.4987561812348347e-06
Epoch 94: reducing lr to 9.984911636202391e-07
Epoch 97: reducing lr to 1.716130049699005e-07
[I 2024-06-21 03:05:56,738] Trial 669 finished with value: 0.9706147909164429 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6039340653181702, 'bidirectional': True, 'fc_dropout': 0.14613262266289836, 'learning_rate_model': 0.0008739368706022839}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.56800938599768e-05
Epoch 40: reducing lr to 8.148569048043023e-05
Epoch 43: reducing lr to 7.767297471749558e-05
Epoch 49: reducing lr to 6.85987044444072e-05
Epoch 52: reducing lr to 6.348025992687811e-05
Epoch 61: reducing lr to 4.6777764893396325e-05
Epoch 64: reducing lr to 4.1054060055485145e-05
Epoch 72: reducing lr to 2.638309681397697e-05
Epoch 75: reducing lr to 2.1351097964949195e-05
Epoch 78: reducing lr to 1.6701689503709236e-05
Epoch 81: reducing lr to 1.250818161958625e-05
Epoch 84: reducing lr to 8.83672079218849e-06
Epoch 87: reducing lr to 5.7451931373932e-06
Epoch 90: reducing lr to 3.2823722028831113e-06
Epoch 93: reducing lr to 1.4870858054023612e-06
Epoch 96: reducing lr to 3.8765197328508203e-07
Epoch 99: reducing lr to 1.4061811533109051e-09
[I 2024-06-21 03:06:33,934] Trial 670 finished with value: 0.9705914258956909 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6467340667795074, 'bidirectional': True, 'fc_dropout': 0.13049894765363498, 'learning_rate_model': 0.0009121984812053448}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.348178519919006e-05
Epoch 36: reducing lr to 8.965813366585057e-05
Epoch 40: reducing lr to 8.526898839406242e-05
Epoch 52: reducing lr to 6.642758397263638e-05
Epoch 55: reducing lr to 6.077658356276696e-05
Epoch 61: reducing lr to 4.894960904519947e-05
Epoch 65: reducing lr to 4.097573804297985e-05
Epoch 68: reducing lr to 3.510734980518448e-05
Epoch 72: reducing lr to 2.7608037224286778e-05
Epoch 75: reducing lr to 2.2342407775399277e-05
Epoch 78: reducing lr to 1.747713199773452e-05
Epoch 81: reducing lr to 1.3088923798313681e-05
Epoch 84: reducing lr to 9.247000770664792e-06
Epoch 87: reducing lr to 6.0119366242798214e-06
Epoch 90: reducing lr to 3.4347694131628044e-06
Epoch 93: reducing lr to 1.5561296901850743e-06
Epoch 96: reducing lr to 4.05650260997903e-07
Epoch 99: reducing lr to 1.4714687172076926e-09
[I 2024-06-21 03:07:08,345] Trial 671 finished with value: 0.9727945923805237 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5861188931983075, 'bidirectional': True, 'fc_dropout': 0.16794305824718883, 'learning_rate_model': 0.0009545509309473208}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 9.138925579853261e-05
Epoch 40: reducing lr to 8.691536476846373e-05
Epoch 52: reducing lr to 6.771016990359276e-05
Epoch 55: reducing lr to 6.195005979579253e-05
Epoch 63: reducing lr to 4.5820896073014405e-05
Epoch 66: reducing lr to 3.975623780322552e-05
Epoch 71: reducing lr to 3.0006404582289546e-05
Epoch 74: reducing lr to 2.452161071520902e-05
Epoch 77: reducing lr to 1.941733668018881e-05
Epoch 80: reducing lr to 1.4774064822244333e-05
Epoch 83: reducing lr to 1.0665035992651905e-05
Epoch 86: reducing lr to 7.1550351151073366e-06
Epoch 89: reducing lr to 4.299437788435323e-06
Epoch 92: reducing lr to 2.143264345847698e-06
Epoch 95: reducing lr to 7.205252832330738e-07
Epoch 98: reducing lr to 5.365425886559539e-08
[I 2024-06-21 03:07:46,794] Trial 672 finished with value: 0.9712740182876587 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6087473642228813, 'bidirectional': True, 'fc_dropout': 0.1011657802441676, 'learning_rate_model': 0.0009729814310678531}. Best is trial 613 with value: 0.9663827419281006.
Epoch 30: reducing lr to 9.341167143797331e-05
Epoch 36: reducing lr to 8.908712280299502e-05
Epoch 40: reducing lr to 8.472593093070751e-05
Epoch 49: reducing lr to 7.132649990968299e-05
Epoch 52: reducing lr to 6.600452283483722e-05
Epoch 55: reducing lr to 6.038951227316229e-05
Epoch 63: reducing lr to 4.4666648827940786e-05
Epoch 66: reducing lr to 3.875476179791687e-05
Epoch 72: reducing lr to 2.7432208345047084e-05
Epoch 75: reducing lr to 2.220011441036394e-05
Epoch 78: reducing lr to 1.7365824391673244e-05
Epoch 83: reducing lr to 1.0396379343215948e-05
Epoch 86: reducing lr to 6.974796833497611e-06
Epoch 89: reducing lr to 4.191133179665585e-06
Epoch 92: reducing lr to 2.089274634194799e-06
Epoch 95: reducing lr to 7.023749545740244e-07
Epoch 98: reducing lr to 5.2302685985320364e-08
[I 2024-06-21 03:08:22,552] Trial 673 finished with value: 0.9726147651672363 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6250439492289881, 'bidirectional': True, 'fc_dropout': 0.14750865505358798, 'learning_rate_model': 0.0009484716280616374}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.67080145147793e-05
Epoch 36: reducing lr to 9.275240169461616e-05
Epoch 40: reducing lr to 8.821177890112634e-05
Epoch 52: reducing lr to 6.872012276315728e-05
Epoch 55: reducing lr to 6.287409587678357e-05
Epoch 61: reducing lr to 5.06389505928791e-05
Epoch 65: reducing lr to 4.238988655352105e-05
Epoch 68: reducing lr to 3.63189693831888e-05
Epoch 71: reducing lr to 3.0453974779739685e-05
Epoch 74: reducing lr to 2.488737070219791e-05
Epoch 77: reducing lr to 1.9706962222898352e-05
Epoch 80: reducing lr to 1.4994432147209878e-05
Epoch 83: reducing lr to 1.082411377392869e-05
Epoch 86: reducing lr to 7.2617583471576475e-06
Epoch 89: reducing lr to 4.3635674383111515e-06
Epoch 92: reducing lr to 2.1752328958893426e-06
Epoch 95: reducing lr to 7.312725102925606e-07
Epoch 98: reducing lr to 5.445455625438175e-08
[I 2024-06-21 03:08:56,919] Trial 674 finished with value: 0.9729875922203064 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5866298645494263, 'bidirectional': True, 'fc_dropout': 0.12617146373934937, 'learning_rate_model': 0.000987494249157208}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.62350856773698e-05
Epoch 40: reducing lr to 8.201351309842555e-05
Epoch 52: reducing lr to 6.389145257663146e-05
Epoch 55: reducing lr to 5.8456201087635174e-05
Epoch 65: reducing lr to 3.94113298632682e-05
Epoch 71: reducing lr to 2.831410374681165e-05
Epoch 74: reducing lr to 2.3138641216587217e-05
Epoch 79: reducing lr to 1.5348555122568365e-05
Epoch 82: reducing lr to 1.129601770313148e-05
Epoch 85: reducing lr to 7.78930292949753e-06
Epoch 88: reducing lr to 4.8836995528217825e-06
Epoch 91: reducing lr to 2.625047882706406e-06
Epoch 94: reducing lr to 1.0489567308111973e-06
Epoch 97: reducing lr to 1.8028663969866728e-07
[I 2024-06-21 03:09:35,329] Trial 675 finished with value: 0.9708535671234131 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6003298595942513, 'bidirectional': True, 'fc_dropout': 0.11502747883538895, 'learning_rate_model': 0.0009181072363210272}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.405587292480557e-05
Epoch 36: reducing lr to 9.020873968957469e-05
Epoch 40: reducing lr to 8.579263992155922e-05
Epoch 52: reducing lr to 6.683552719408563e-05
Epoch 55: reducing lr to 6.114982301849498e-05
Epoch 61: reducing lr to 4.925021701568965e-05
Epoch 65: reducing lr to 4.122737709981202e-05
Epoch 72: reducing lr to 2.777758293059766e-05
Epoch 75: reducing lr to 2.247961634535996e-05
Epoch 78: reducing lr to 1.758446207211727e-05
Epoch 81: reducing lr to 1.3169305131191707e-05
Epoch 84: reducing lr to 9.303788193261442e-06
Epoch 87: reducing lr to 6.048856961389591e-06
Epoch 90: reducing lr to 3.4558629230504795e-06
Epoch 93: reducing lr to 1.5656861503307347e-06
Epoch 96: reducing lr to 4.0814142903919946e-07
Epoch 99: reducing lr to 1.4805052597505892e-09
[I 2024-06-21 03:10:09,759] Trial 676 finished with value: 0.9723765254020691 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6164900691489192, 'bidirectional': True, 'fc_dropout': 0.16431452920261685, 'learning_rate_model': 0.0009604129924362414}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.177781423547153e-05
Epoch 40: reducing lr to 7.777444396650617e-05
Epoch 43: reducing lr to 7.413537744186535e-05
Epoch 52: reducing lr to 6.058906649196209e-05
Epoch 65: reducing lr to 3.737425882997674e-05
Epoch 72: reducing lr to 2.518148490518535e-05
Epoch 75: reducing lr to 2.0378667254432e-05
Epoch 78: reducing lr to 1.5941015002679277e-05
Epoch 83: reducing lr to 9.54339024430166e-06
Epoch 86: reducing lr to 6.402537446868208e-06
Epoch 89: reducing lr to 3.847264338761316e-06
Epoch 92: reducing lr to 1.9178564482309324e-06
Epoch 95: reducing lr to 6.447473748346353e-07
Epoch 98: reducing lr to 4.8011420774945415e-08
[I 2024-06-21 03:10:46,756] Trial 677 finished with value: 0.9676931500434875 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6521387641050179, 'bidirectional': True, 'fc_dropout': 0.14786923196224347, 'learning_rate_model': 0.0008706526169754379}. Best is trial 613 with value: 0.9663827419281006.
Epoch 27: reducing lr to 0.00010234047581719451
Epoch 31: reducing lr to 0.00010059262785282763
Epoch 36: reducing lr to 9.64781241031127e-05
Epoch 39: reducing lr to 9.304767517478744e-05
Epoch 52: reducing lr to 7.148050520733705e-05
Epoch 55: reducing lr to 6.539965234370243e-05
Epoch 63: reducing lr to 4.837236127180603e-05
Epoch 66: reducing lr to 4.197000195633542e-05
Epoch 69: reducing lr to 3.5715686780041006e-05
Epoch 72: reducing lr to 2.9708087070996085e-05
Epoch 75: reducing lr to 2.4041919031584063e-05
Epoch 78: reducing lr to 1.8806558210637228e-05
Epoch 81: reducing lr to 1.4084553882152375e-05
Epoch 84: reducing lr to 9.950388787465653e-06
Epoch 87: reducing lr to 6.469244272906802e-06
Epoch 90: reducing lr to 3.6960406843145487e-06
Epoch 93: reducing lr to 1.6744992030477256e-06
Epoch 96: reducing lr to 4.36506701877973e-07
Epoch 99: reducing lr to 1.5833983567012377e-09
[I 2024-06-21 03:11:25,210] Trial 678 finished with value: 0.97178053855896 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6497309587308239, 'bidirectional': True, 'fc_dropout': 0.14155781674279835, 'learning_rate_model': 0.001027160385937794}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.113214359051624e-05
Epoch 40: reducing lr to 7.716038163350838e-05
Epoch 43: reducing lr to 7.355004708772964e-05
Epoch 52: reducing lr to 6.011069002757701e-05
Epoch 65: reducing lr to 3.707917315143325e-05
Epoch 72: reducing lr to 2.4982666365564723e-05
Epoch 75: reducing lr to 2.021776900406288e-05
Epoch 78: reducing lr to 1.5815153905335874e-05
Epoch 83: reducing lr to 9.468041117014458e-06
Epoch 86: reducing lr to 6.351986689045726e-06
Epoch 89: reducing lr to 3.816888549555599e-06
Epoch 92: reducing lr to 1.9027141554044872e-06
Epoch 95: reducing lr to 6.396568199300528e-07
Epoch 98: reducing lr to 4.763235017607665e-08
[I 2024-06-21 03:12:02,256] Trial 679 finished with value: 0.9678278565406799 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6350286678798809, 'bidirectional': True, 'fc_dropout': 0.19610595776576617, 'learning_rate_model': 0.0008637784440473639}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.20027971380863e-05
Epoch 40: reducing lr to 7.798841300341941e-05
Epoch 43: reducing lr to 7.433933486673961e-05
Epoch 52: reducing lr to 6.075575600516954e-05
Epoch 65: reducing lr to 3.7477080962278196e-05
Epoch 72: reducing lr to 2.5250762907038098e-05
Epoch 75: reducing lr to 2.0434731992199637e-05
Epoch 83: reducing lr to 9.569645527082466e-06
Epoch 86: reducing lr to 6.4201517775074285e-06
Epoch 89: reducing lr to 3.857848733883058e-06
Epoch 92: reducing lr to 1.9231327559258127e-06
Epoch 95: reducing lr to 6.465211705419349e-07
Epoch 98: reducing lr to 4.814350731828683e-08
[I 2024-06-21 03:12:39,279] Trial 680 finished with value: 0.9682042598724365 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6772255383276088, 'bidirectional': True, 'fc_dropout': 0.19231159936370634, 'learning_rate_model': 0.0008730479115277264}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 9.406675422343724e-05
Epoch 36: reducing lr to 9.021917591439751e-05
Epoch 40: reducing lr to 8.580256524898808e-05
Epoch 52: reducing lr to 6.68432593782437e-05
Epoch 55: reducing lr to 6.11568974250668e-05
Epoch 63: reducing lr to 4.5234239487405805e-05
Epoch 66: reducing lr to 3.924722857981096e-05
Epoch 69: reducing lr to 3.3398657555449714e-05
Epoch 72: reducing lr to 2.7780796511692965e-05
Epoch 75: reducing lr to 2.2482217006126514e-05
Epoch 79: reducing lr to 1.6057663580407524e-05
Epoch 82: reducing lr to 1.1817897556265879e-05
Epoch 85: reducing lr to 8.149171369482237e-06
Epoch 88: reducing lr to 5.109328130286977e-06
Epoch 91: reducing lr to 2.7463259861497045e-06
Epoch 94: reducing lr to 1.0974188879188623e-06
Epoch 97: reducing lr to 1.886159436640853e-07
[I 2024-06-21 03:13:17,703] Trial 681 finished with value: 0.9710079431533813 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6367541724759295, 'bidirectional': True, 'fc_dropout': 0.23526508682802852, 'learning_rate_model': 0.0009605241023569243}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 0.00010317519109232125
Epoch 36: reducing lr to 9.895505369569217e-05
Epoch 40: reducing lr to 9.411078482359297e-05
Epoch 45: reducing lr to 8.644258938489081e-05
Epoch 49: reducing lr to 7.922713638531803e-05
Epoch 52: reducing lr to 7.331565882673534e-05
Epoch 55: reducing lr to 6.707868928332406e-05
Epoch 61: reducing lr to 5.402534040585085e-05
Epoch 65: reducing lr to 4.522463487111469e-05
Epoch 68: reducing lr to 3.874773590573513e-05
Epoch 71: reducing lr to 3.2490530212883095e-05
Epoch 76: reducing lr to 2.2815801206758522e-05
Epoch 79: reducing lr to 1.761251913156757e-05
Epoch 82: reducing lr to 1.2962218679098527e-05
Epoch 85: reducing lr to 8.938251566470175e-06
Epoch 88: reducing lr to 5.604061823410805e-06
Epoch 91: reducing lr to 3.0122513608766894e-06
Epoch 94: reducing lr to 1.203681411185973e-06
Epoch 97: reducing lr to 2.0687953136318272e-07
[I 2024-06-21 03:13:56,612] Trial 682 finished with value: 0.9701533317565918 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.6519809448367274, 'bidirectional': True, 'fc_dropout': 0.2084972755326574, 'learning_rate_model': 0.0010535311718533192}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.261096725107022e-05
Epoch 40: reducing lr to 7.85668106142699e-05
Epoch 43: reducing lr to 7.489066925121094e-05
Epoch 52: reducing lr to 6.120634837864513e-05
Epoch 65: reducing lr to 3.775502807333534e-05
Epoch 72: reducing lr to 2.543803407175513e-05
Epoch 75: reducing lr to 2.0586285276943885e-05
Epoch 78: reducing lr to 1.6103422189094714e-05
Epoch 83: reducing lr to 9.640618379284352e-06
Epoch 86: reducing lr to 6.467766548810003e-06
Epoch 89: reducing lr to 3.886460298149709e-06
Epoch 92: reducing lr to 1.937395584832556e-06
Epoch 95: reducing lr to 6.513160661681642e-07
Epoch 98: reducing lr to 4.850056150799541e-08
[I 2024-06-21 03:14:33,579] Trial 683 finished with value: 0.967925488948822 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6604886319678009, 'bidirectional': True, 'fc_dropout': 0.17833457736685648, 'learning_rate_model': 0.0008795228326953555}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 7.874059147969994e-05
Epoch 40: reducing lr to 7.488590612478316e-05
Epoch 43: reducing lr to 7.138199429658093e-05
Epoch 52: reducing lr to 5.833879246323261e-05
Epoch 55: reducing lr to 5.337590625835698e-05
Epoch 61: reducing lr to 4.2989085444092066e-05
Epoch 65: reducing lr to 3.5986181263221905e-05
Epoch 72: reducing lr to 2.424624617701483e-05
Epoch 75: reducing lr to 1.962180486460047e-05
Epoch 78: reducing lr to 1.5348966731777562e-05
Epoch 81: reducing lr to 1.149510434327155e-05
Epoch 84: reducing lr to 8.121006765643998e-06
Epoch 87: reducing lr to 5.279871734766755e-06
Epoch 90: reducing lr to 3.016522474099119e-06
Epoch 93: reducing lr to 1.3666420124353452e-06
Epoch 96: reducing lr to 3.56254811235664e-07
Epoch 99: reducing lr to 1.2922900845561621e-09
[I 2024-06-21 03:15:07,982] Trial 684 finished with value: 0.9731265902519226 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6860067403401423, 'bidirectional': True, 'fc_dropout': 0.1818801511381154, 'learning_rate_model': 0.0008383166348344178}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 7.935091275010352e-05
Epoch 40: reducing lr to 7.546634958478922e-05
Epoch 43: reducing lr to 7.193527880491817e-05
Epoch 52: reducing lr to 5.879097862618709e-05
Epoch 55: reducing lr to 5.3789624904665865e-05
Epoch 61: reducing lr to 4.332229545367766e-05
Epoch 65: reducing lr to 3.62651115005088e-05
Epoch 72: reducing lr to 2.443417973823387e-05
Epoch 75: reducing lr to 1.97738942081973e-05
Epoch 78: reducing lr to 1.5467937147151384e-05
Epoch 81: reducing lr to 1.1584203327091295e-05
Epoch 84: reducing lr to 8.183952992907762e-06
Epoch 87: reducing lr to 5.320796218113587e-06
Epoch 90: reducing lr to 3.0399036526500538e-06
Epoch 93: reducing lr to 1.377234905802565e-06
Epoch 96: reducing lr to 3.5901615560576326e-07
Epoch 99: reducing lr to 1.3023066733515442e-09
[I 2024-06-21 03:15:42,374] Trial 685 finished with value: 0.9727820754051208 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.657667972746162, 'bidirectional': True, 'fc_dropout': 0.16479871524331058, 'learning_rate_model': 0.0008448144584341363}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 0.00010023048920729687
Epoch 40: reducing lr to 9.53237823664787e-05
Epoch 45: reducing lr to 8.755675126039847e-05
Epoch 52: reducing lr to 7.426062718693526e-05
Epoch 55: reducing lr to 6.794326910202587e-05
Epoch 60: reducing lr to 5.6954032533843936e-05
Epoch 65: reducing lr to 4.580753693785864e-05
Epoch 68: reducing lr to 3.924715697138818e-05
Epoch 71: reducing lr to 3.2909301912525775e-05
Epoch 74: reducing lr to 2.689389487484761e-05
Epoch 77: reducing lr to 2.1295820143765342e-05
Epoch 80: reducing lr to 1.620334613489261e-05
Epoch 83: reducing lr to 1.1696799209235861e-05
Epoch 86: reducing lr to 7.847231751876368e-06
Epoch 89: reducing lr to 4.715376540555348e-06
Epoch 92: reducing lr to 2.3506092921737106e-06
Epoch 95: reducing lr to 7.902307661185543e-07
Epoch 98: reducing lr to 5.884491089422437e-08
[I 2024-06-21 03:16:21,247] Trial 686 finished with value: 0.9708257913589478 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.6333609218690999, 'bidirectional': True, 'fc_dropout': 0.19629047213788842, 'learning_rate_model': 0.001067110175845338}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 0.00010356674516722075
Epoch 36: reducing lr to 9.933059217636991e-05
Epoch 40: reducing lr to 9.446793910554332e-05
Epoch 49: reducing lr to 7.952780661200588e-05
Epoch 52: reducing lr to 7.359389475403169e-05
Epoch 55: reducing lr to 6.733325565581859e-05
Epoch 58: reducing lr to 6.0844646822379534e-05
Epoch 61: reducing lr to 5.4230368784861224e-05
Epoch 64: reducing lr to 4.759476691540471e-05
Epoch 67: reducing lr to 4.104244456297967e-05
Epoch 72: reducing lr to 3.058643509730092e-05
Epoch 75: reducing lr to 2.4752741377011658e-05
Epoch 78: reducing lr to 1.9362592102904482e-05
Epoch 83: reducing lr to 1.1591782113509925e-05
Epoch 86: reducing lr to 7.776777136615483e-06
Epoch 89: reducing lr to 4.673040586873905e-06
Epoch 92: reducing lr to 2.3295048723546897e-06
Epoch 95: reducing lr to 7.831358559190395e-07
Epoch 98: reducing lr to 5.831658502234924e-08
[I 2024-06-21 03:16:57,009] Trial 687 finished with value: 0.9726268649101257 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6731226582366473, 'bidirectional': True, 'fc_dropout': 0.15665938695873477, 'learning_rate_model': 0.0010575293657893372}. Best is trial 613 with value: 0.9663827419281006.
Epoch 31: reducing lr to 8.277246239196102e-05
Epoch 36: reducing lr to 7.938685040274914e-05
Epoch 40: reducing lr to 7.550052793717128e-05
Epoch 47: reducing lr to 6.65395506446161e-05
Epoch 53: reducing lr to 5.717492505801036e-05
Epoch 65: reducing lr to 3.628153579778644e-05
Epoch 68: reducing lr to 3.1085433223586214e-05
Epoch 72: reducing lr to 2.4445245862537676e-05
Epoch 79: reducing lr to 1.4129671697345365e-05
Epoch 88: reducing lr to 4.495867578335093e-06
Epoch 91: reducing lr to 2.4165834813932973e-06
Epoch 94: reducing lr to 9.656553410222736e-07
Epoch 97: reducing lr to 1.659694355603714e-07
[I 2024-06-21 03:17:36,770] Trial 688 finished with value: 0.9681780934333801 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.6618331692899487, 'bidirectional': True, 'fc_dropout': 0.17279939637425334, 'learning_rate_model': 0.0008451970709020335}. Best is trial 613 with value: 0.9663827419281006.
Epoch 36: reducing lr to 8.164065651301809e-05
Epoch 40: reducing lr to 7.764400069531756e-05
Epoch 43: reducing lr to 7.401103761182448e-05
Epoch 52: reducing lr to 6.048744653007744e-05
Epoch 65: reducing lr to 3.7311574735672807e-05
Epoch 72: reducing lr to 2.5139250527195666e-05
Epoch 75: reducing lr to 2.0344488160586254e-05
Epoch 78: reducing lr to 1.591427873769343e-05
Epoch 83: reducing lr to 9.527384073402753e-06
Epoch 86: reducing lr to 6.3917991132218e-06
Epoch 89: reducing lr to 3.84081170830997e-06
Epoch 92: reducing lr to 1.9146398200428195e-06
Epoch 95: reducing lr to 6.436660047550325e-07
Epoch 98: reducing lr to 4.7930896036212956e-08
[I 2024-06-21 03:18:13,744] Trial 689 finished with value: 0.9674851894378662 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6176662794109707, 'bidirectional': True, 'fc_dropout': 0.22138994293907466, 'learning_rate_model': 0.0008691923586999033}. Best is trial 613 with value: 0.9663827419281006.
Epoch 40: reducing lr to 7.438553816189166e-05
Epoch 52: reducing lr to 5.794898796926367e-05
Epoch 61: reducing lr to 4.270184366224882e-05
Epoch 64: reducing lr to 3.747686658789859e-05
Epoch 67: reducing lr to 3.231746511253891e-05
Epoch 72: reducing lr to 2.408423912608668e-05
Epoch 77: reducing lr to 1.661810938116843e-05
Epoch 80: reducing lr to 1.264421734372182e-05
Epoch 83: reducing lr to 9.127551198142192e-06
Epoch 86: reducing lr to 6.123556393306483e-06
Epoch 89: reducing lr to 3.6796255131449028e-06
[I 2024-06-21 03:18:50,162] Trial 690 finished with value: 0.966281533241272 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6193926562216368, 'bidirectional': True, 'fc_dropout': 0.209474313213079, 'learning_rate_model': 0.0008327152231865279}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.482897778987753e-05
Epoch 40: reducing lr to 7.116578248756135e-05
Epoch 43: reducing lr to 6.783593525828532e-05
Epoch 52: reducing lr to 5.544068343257183e-05
Epoch 55: reducing lr to 5.072433961778006e-05
Epoch 61: reducing lr to 4.0853507186727705e-05
Epoch 65: reducing lr to 3.419848781784132e-05
Epoch 72: reducing lr to 2.3041760070287178e-05
Epoch 75: reducing lr to 1.8647048146559015e-05
Epoch 78: reducing lr to 1.4586472733899361e-05
Epoch 81: reducing lr to 1.0924059515310478e-05
Epoch 84: reducing lr to 7.717577725517685e-06
Epoch 87: reducing lr to 5.017582384761736e-06
Epoch 90: reducing lr to 2.8666700233668248e-06
Epoch 93: reducing lr to 1.2987510364536984e-06
Epoch 96: reducing lr to 3.3855706258395036e-07
Epoch 99: reducing lr to 1.2280927056546043e-09
[I 2024-06-21 03:19:24,551] Trial 691 finished with value: 0.9728176593780518 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6191148461276748, 'bidirectional': True, 'fc_dropout': 0.22723237407069205, 'learning_rate_model': 0.000796671394893979}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.366278312580268e-05
Epoch 52: reducing lr to 5.7385936011504185e-05
Epoch 61: reducing lr to 4.2286938113135156e-05
Epoch 72: reducing lr to 2.3850228516647164e-05
Epoch 77: reducing lr to 1.6456642212383858e-05
Epoch 80: reducing lr to 1.2521361853415498e-05
Epoch 83: reducing lr to 9.038864824975678e-06
Epoch 86: reducing lr to 6.064057849215765e-06
Epoch 89: reducing lr to 3.6438730276986307e-06
Epoch 92: reducing lr to 1.8164661347279876e-06
Epoch 99: reducing lr to 1.2711829122527899e-09
[I 2024-06-21 03:20:00,984] Trial 692 finished with value: 0.9663873314857483 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6030167419029788, 'bidirectional': True, 'fc_dropout': 0.218033689798945, 'learning_rate_model': 0.0008246242805643722}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.621692320451132e-05
Epoch 40: reducing lr to 7.24857820973355e-05
Epoch 43: reducing lr to 6.909417208137164e-05
Epoch 52: reducing lr to 5.64690102202299e-05
Epoch 55: reducing lr to 5.166518655518603e-05
Epoch 61: reducing lr to 4.161126761118185e-05
Epoch 65: reducing lr to 3.4832809383578424e-05
Epoch 72: reducing lr to 2.3469144035419644e-05
Epoch 75: reducing lr to 1.8992917965122448e-05
Epoch 78: reducing lr to 1.4857026048198873e-05
Epoch 81: reducing lr to 1.1126681531022586e-05
Epoch 84: reducing lr to 7.86072516562158e-06
Epoch 87: reducing lr to 5.110649678598527e-06
Epoch 90: reducing lr to 2.91984169070367e-06
Epoch 93: reducing lr to 1.3228405750126534e-06
Epoch 96: reducing lr to 3.448366983144365e-07
Epoch 99: reducing lr to 1.2508716569384174e-09
[I 2024-06-21 03:20:35,329] Trial 693 finished with value: 0.9727787971496582 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6076752324232225, 'bidirectional': True, 'fc_dropout': 0.22226372034413117, 'learning_rate_model': 0.0008114482426095519}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.767757684989615e-05
Epoch 40: reducing lr to 7.387493056210553e-05
Epoch 43: reducing lr to 7.041832228426946e-05
Epoch 49: reducing lr to 6.219158845684761e-05
Epoch 52: reducing lr to 5.755120643284965e-05
Epoch 55: reducing lr to 5.265532024083515e-05
Epoch 65: reducing lr to 3.550036020911617e-05
Epoch 72: reducing lr to 2.391891672825604e-05
Epoch 75: reducing lr to 1.9356906351111356e-05
Epoch 78: reducing lr to 1.5141752436308902e-05
Epoch 83: reducing lr to 9.06489658644785e-06
Epoch 86: reducing lr to 6.081522222291464e-06
Epoch 89: reducing lr to 3.6543673137985556e-06
Epoch 92: reducing lr to 1.8216975231884984e-06
Epoch 95: reducing lr to 6.124205473782774e-07
Epoch 98: reducing lr to 4.5604188150349774e-08
[I 2024-06-21 03:21:11,000] Trial 694 finished with value: 0.9732246398925781 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5968632402015024, 'bidirectional': True, 'fc_dropout': 0.21062336975428836, 'learning_rate_model': 0.0008269991830539517}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 9.551527774035934e-05
Epoch 52: reducing lr to 7.440980891488119e-05
Epoch 62: reducing lr to 5.259238254883812e-05
Epoch 68: reducing lr to 3.9326000349309255e-05
Epoch 71: reducing lr to 3.2975413211484305e-05
Epoch 74: reducing lr to 2.6947921858736738e-05
Epoch 77: reducing lr to 2.1338601188949275e-05
Epoch 80: reducing lr to 1.6235896939625558e-05
Epoch 83: reducing lr to 1.1720296838916221e-05
Epoch 86: reducing lr to 7.862996008612148e-06
Epoch 89: reducing lr to 4.724849232167058e-06
Epoch 92: reducing lr to 2.3553314170629774e-06
Epoch 95: reducing lr to 7.918182559584625e-07
Epoch 98: reducing lr to 5.896312408226918e-08
[I 2024-06-21 03:21:47,442] Trial 695 finished with value: 0.9683632850646973 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5943114155591356, 'bidirectional': True, 'fc_dropout': 0.2009827502840752, 'learning_rate_model': 0.0010692538870685218}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.283144999733033e-05
Epoch 52: reducing lr to 5.673829784620051e-05
Epoch 61: reducing lr to 4.180970210516256e-05
Epoch 64: reducing lr to 3.669388704310671e-05
Epoch 67: reducing lr to 3.1642277552146284e-05
Epoch 72: reducing lr to 2.358106294556545e-05
Epoch 77: reducing lr to 1.6270918142860092e-05
Epoch 80: reducing lr to 1.23800500202126e-05
Epoch 83: reducing lr to 8.936855269350385e-06
Epoch 86: reducing lr to 5.995620953824288e-06
Epoch 89: reducing lr to 3.6027495154536513e-06
Epoch 99: reducing lr to 1.2568367740431117e-09
[I 2024-06-21 03:22:23,908] Trial 696 finished with value: 0.9665594100952148 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6209599975807054, 'bidirectional': True, 'fc_dropout': 0.18760476707696835, 'learning_rate_model': 0.000815317851267436}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.210206660405122e-05
Epoch 52: reducing lr to 5.617008216171998e-05
Epoch 61: reducing lr to 4.139099147404688e-05
Epoch 64: reducing lr to 3.632640964364421e-05
Epoch 67: reducing lr to 3.1325390386328355e-05
Epoch 72: reducing lr to 2.3344906234295863e-05
Epoch 77: reducing lr to 1.6107970165204263e-05
Epoch 80: reducing lr to 1.2256067827175958e-05
Epoch 83: reducing lr to 8.847355557044202e-06
Epoch 86: reducing lr to 5.935576750993286e-06
Epoch 89: reducing lr to 3.5666691454100417e-06
[I 2024-06-21 03:23:00,338] Trial 697 finished with value: 0.9665403962135315 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6194361484025459, 'bidirectional': True, 'fc_dropout': 0.2218685418784768, 'learning_rate_model': 0.0008071527069378878}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.8791631663057e-05
Epoch 52: reducing lr to 5.359113524126893e-05
Epoch 61: reducing lr to 3.9490599559200156e-05
Epoch 72: reducing lr to 2.227306742395163e-05
Epoch 77: reducing lr to 1.53684020810297e-05
Epoch 80: reducing lr to 1.1693352816563498e-05
Epoch 83: reducing lr to 8.441145355993002e-06
Epoch 86: reducing lr to 5.6630555654446725e-06
Epoch 89: reducing lr to 3.4029120338868625e-06
[I 2024-06-21 03:23:36,743] Trial 698 finished with value: 0.966787576675415 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6060682656964972, 'bidirectional': True, 'fc_dropout': 0.20769666542834667, 'learning_rate_model': 0.0007700937618949017}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.668054980581283e-05
Epoch 47: reducing lr to 5.876639464702745e-05
Epoch 61: reducing lr to 3.827870959168987e-05
Epoch 64: reducing lr to 3.359494507711208e-05
Epoch 67: reducing lr to 2.8969963722575673e-05
Epoch 72: reducing lr to 2.1589550150016532e-05
Epoch 77: reducing lr to 1.4896775605196036e-05
Epoch 80: reducing lr to 1.1334506480394112e-05
Epoch 83: reducing lr to 8.182102963995666e-06
Epoch 86: reducing lr to 5.489267365168616e-06
[I 2024-06-21 03:24:13,162] Trial 699 finished with value: 0.967428982257843 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6159780051157824, 'bidirectional': True, 'fc_dropout': 0.23362167084520893, 'learning_rate_model': 0.0007464610767875605}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.046863521413503e-05
Epoch 52: reducing lr to 5.489758083549628e-05
Epoch 61: reducing lr to 4.0453302057948194e-05
Epoch 72: reducing lr to 2.2816040635378258e-05
Epoch 77: reducing lr to 1.5743053244859017e-05
Epoch 80: reducing lr to 1.1978413567752462e-05
Epoch 83: reducing lr to 8.646923739132713e-06
Epoch 86: reducing lr to 5.801109629051111e-06
Epoch 89: reducing lr to 3.4858682805534035e-06
[I 2024-06-21 03:24:49,571] Trial 700 finished with value: 0.9666551351547241 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6118017879143638, 'bidirectional': True, 'fc_dropout': 0.2494078345848608, 'learning_rate_model': 0.00078886712054536}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.167461809918603e-05
Epoch 40: reducing lr to 6.816584205986183e-05
Epoch 43: reducing lr to 6.497636205443969e-05
Epoch 52: reducing lr to 5.310362281502389e-05
Epoch 55: reducing lr to 4.8586092952476606e-05
Epoch 61: reducing lr to 3.913135809289601e-05
Epoch 65: reducing lr to 3.2756876096803e-05
Epoch 72: reducing lr to 2.207045187772584e-05
Epoch 75: reducing lr to 1.7860995754007862e-05
Epoch 78: reducing lr to 1.3971590866203884e-05
Epoch 81: reducing lr to 1.0463563942451373e-05
Epoch 84: reducing lr to 7.392248998516889e-06
Epoch 87: reducing lr to 4.80606994550779e-06
Epoch 90: reducing lr to 2.745827688815439e-06
Epoch 93: reducing lr to 1.2440031561721074e-06
Epoch 96: reducing lr to 3.242854423807077e-07
Epoch 99: reducing lr to 1.1763233745474937e-09
[I 2024-06-21 03:25:23,937] Trial 701 finished with value: 0.9731018543243408 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6146240582784558, 'bidirectional': True, 'fc_dropout': 0.257080027391084, 'learning_rate_model': 0.0007630883070448158}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.840636248876646e-05
Epoch 52: reducing lr to 5.329099680982678e-05
Epoch 65: reducing lr to 3.287245741510404e-05
Epoch 72: reducing lr to 2.2148326578475345e-05
Epoch 77: reducing lr to 1.528233097853101e-05
Epoch 80: reducing lr to 1.1627863915146085e-05
Epoch 83: reducing lr to 8.393870520046409e-06
Epoch 86: reducing lr to 5.631339487646909e-06
Epoch 89: reducing lr to 3.3838539438578235e-06
[I 2024-06-21 03:26:00,377] Trial 702 finished with value: 0.967121958732605 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5926585558083564, 'bidirectional': True, 'fc_dropout': 0.24217856323696701, 'learning_rate_model': 0.0007657808334092865}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.213927013152687e-05
Epoch 47: reducing lr to 4.595098471863791e-05
Epoch 50: reducing lr to 4.282502692208627e-05
Epoch 53: reducing lr to 3.9483947249085907e-05
Epoch 56: reducing lr to 3.5980426213462016e-05
Epoch 59: reducing lr to 3.236973197092501e-05
Epoch 62: reducing lr to 2.870879408452794e-05
Epoch 65: reducing lr to 2.505535851777945e-05
Epoch 72: reducing lr to 1.6881435299619107e-05
Epoch 75: reducing lr to 1.3661670629968286e-05
Epoch 78: reducing lr to 1.0686709476873363e-05
Epoch 81: reducing lr to 8.003459950731271e-06
Epoch 84: reducing lr to 5.6542464050353514e-06
Epoch 87: reducing lr to 3.676107733544692e-06
Epoch 90: reducing lr to 2.1002520804488837e-06
Epoch 93: reducing lr to 9.515237345292488e-07
Epoch 96: reducing lr to 2.480422124788266e-07
Epoch 99: reducing lr to 8.9975624645083e-10
[I 2024-06-21 03:26:36,093] Trial 703 finished with value: 0.9757499694824219 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5961197096456369, 'bidirectional': True, 'fc_dropout': 0.24672748099666564, 'learning_rate_model': 0.0005836774867429815}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.164916102352333e-05
Epoch 53: reducing lr to 3.911279816859643e-05
Epoch 61: reducing lr to 2.9649774053025707e-05
Epoch 65: reducing lr to 2.4819838162721155e-05
Epoch 72: reducing lr to 1.6722749817915118e-05
Epoch 75: reducing lr to 1.3533250934230288e-05
Epoch 79: reducing lr to 9.665968022276768e-06
Epoch 86: reducing lr to 4.251855374174654e-06
Epoch 89: reducing lr to 2.5549263382495752e-06
Epoch 92: reducing lr to 1.2736275756255024e-06
[I 2024-06-21 03:27:12,541] Trial 704 finished with value: 0.9708434343338013 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.591076566955053, 'bidirectional': True, 'fc_dropout': 0.23740315899428435, 'learning_rate_model': 0.0005781909187172359}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.84337693545825e-05
Epoch 43: reducing lr to 6.523175303003637e-05
Epoch 49: reducing lr to 5.761094850265972e-05
Epoch 53: reducing lr to 5.182342085794976e-05
Epoch 56: reducing lr to 4.72249838281257e-05
Epoch 65: reducing lr to 3.28856276962978e-05
Epoch 72: reducing lr to 2.215720025911705e-05
Epoch 75: reducing lr to 1.793119877840807e-05
Epoch 78: reducing lr to 1.4026506501815622e-05
Epoch 81: reducing lr to 1.0504691203488883e-05
Epoch 84: reducing lr to 7.421304390722492e-06
Epoch 87: reducing lr to 4.8249603058382304e-06
Epoch 90: reducing lr to 2.756620223055503e-06
Epoch 93: reducing lr to 1.2488927370851504e-06
Epoch 96: reducing lr to 3.255600532219949e-07
Epoch 99: reducing lr to 1.1809469386441819e-09
[I 2024-06-21 03:27:43,921] Trial 705 finished with value: 0.9704186320304871 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.579466006462878, 'bidirectional': True, 'fc_dropout': 0.21913680871838534, 'learning_rate_model': 0.0007660876418958391}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.080633185468306e-05
Epoch 52: reducing lr to 5.5160658565979155e-05
Epoch 61: reducing lr to 4.0647160561983806e-05
Epoch 72: reducing lr to 2.292537864440542e-05
Epoch 77: reducing lr to 1.5818496400194773e-05
Epoch 80: reducing lr to 1.2035815985276709e-05
Epoch 83: reducing lr to 8.688361140168042e-06
Epoch 86: reducing lr to 5.828909447044308e-06
Epoch 89: reducing lr to 3.502573102552001e-06
[I 2024-06-21 03:28:20,311] Trial 706 finished with value: 0.9665791988372803 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6057715495439651, 'bidirectional': True, 'fc_dropout': 0.2392623731122068, 'learning_rate_model': 0.0007926474942625102}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.243013840784489e-05
Epoch 40: reducing lr to 6.888437645040144e-05
Epoch 43: reducing lr to 6.566127621815335e-05
Epoch 49: reducing lr to 5.7990291953078906e-05
Epoch 52: reducing lr to 5.366338673932676e-05
Epoch 55: reducing lr to 4.909823771051614e-05
Epoch 65: reducing lr to 3.310216548648777e-05
Epoch 72: reducing lr to 2.2303095944162645e-05
Epoch 75: reducing lr to 1.8049268051550472e-05
Epoch 78: reducing lr to 1.4118865046710607e-05
Epoch 83: reducing lr to 8.452525697061514e-06
Epoch 86: reducing lr to 5.670690489510861e-06
Epoch 89: reducing lr to 3.40749983541598e-06
Epoch 92: reducing lr to 1.6986343947976414e-06
Epoch 95: reducing lr to 5.710490312556098e-07
Epoch 98: reducing lr to 4.2523438470399784e-08
[I 2024-06-21 03:28:56,005] Trial 707 finished with value: 0.9736229181289673 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6044635607638554, 'bidirectional': True, 'fc_dropout': 0.2680087094003162, 'learning_rate_model': 0.0007711320012919849}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.997337379570851e-05
Epoch 43: reducing lr to 6.669931937359266e-05
Epoch 49: reducing lr to 5.890706404632644e-05
Epoch 53: reducing lr to 5.298933016938678e-05
Epoch 61: reducing lr to 4.0168991744624045e-05
Epoch 65: reducing lr to 3.362547965722221e-05
Epoch 72: reducing lr to 2.2655686960106742e-05
Epoch 75: reducing lr to 1.83346100406302e-05
Epoch 78: reducing lr to 1.4342071052874978e-05
Epoch 81: reducing lr to 1.0741022906127544e-05
Epoch 84: reducing lr to 7.588266890474658e-06
Epoch 87: reducing lr to 4.933510958318515e-06
Epoch 90: reducing lr to 2.818637919551573e-06
Epoch 93: reducing lr to 1.2769899882323753e-06
Epoch 96: reducing lr to 3.3288441528068255e-07
Epoch 99: reducing lr to 1.2075155635918495e-09
[I 2024-06-21 03:29:27,320] Trial 708 finished with value: 0.9701641201972961 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.6143611657172652, 'bidirectional': True, 'fc_dropout': 0.2323363653202786, 'learning_rate_model': 0.0007833228745430908}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.152888542707222e-05
Epoch 47: reducing lr to 4.541304511656575e-05
Epoch 50: reducing lr to 4.232368232452747e-05
Epoch 53: reducing lr to 3.9021716047698915e-05
Epoch 56: reducing lr to 3.5559210078961954e-05
Epoch 62: reducing lr to 2.8372705590225477e-05
Epoch 65: reducing lr to 2.4762040111800598e-05
Epoch 72: reducing lr to 1.668380748722099e-05
Epoch 75: reducing lr to 1.3501736001639309e-05
Epoch 78: reducing lr to 1.0561602163533935e-05
Epoch 81: reducing lr to 7.90976493880804e-06
Epoch 84: reducing lr to 5.588053197647808e-06
Epoch 87: reducing lr to 3.633072226395806e-06
Epoch 90: reducing lr to 2.0756648213221085e-06
Epoch 93: reducing lr to 9.403844237560562e-07
Epoch 96: reducing lr to 2.4513842859051744e-07
Epoch 99: reducing lr to 8.892229680021458e-10
[I 2024-06-21 03:30:02,925] Trial 709 finished with value: 0.975742757320404 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5891009396860282, 'bidirectional': True, 'fc_dropout': 0.2539802703324682, 'learning_rate_model': 0.0005768444833399089}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.455682253369177e-05
Epoch 40: reducing lr to 7.090695038352073e-05
Epoch 43: reducing lr to 6.75892139093627e-05
Epoch 49: reducing lr to 5.969299522081809e-05
Epoch 52: reducing lr to 5.523904398955987e-05
Epoch 55: reducing lr to 5.0539853659916314e-05
Epoch 65: reducing lr to 3.407410688296652e-05
Epoch 72: reducing lr to 2.295795649177901e-05
Epoch 75: reducing lr to 1.8579228268280234e-05
Epoch 78: reducing lr to 1.4533421291249851e-05
Epoch 83: reducing lr to 8.700707636491672e-06
Epoch 86: reducing lr to 5.837192552211976e-06
Epoch 89: reducing lr to 3.5075503940384184e-06
Epoch 92: reducing lr to 1.748509472803046e-06
Epoch 95: reducing lr to 5.878160972387398e-07
Epoch 98: reducing lr to 4.3772006035774395e-08
[I 2024-06-21 03:30:38,620] Trial 710 finished with value: 0.9731836915016174 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6076258692879324, 'bidirectional': True, 'fc_dropout': 0.24207776197462272, 'learning_rate_model': 0.0007937738769273155}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.814775366674076e-05
Epoch 43: reducing lr to 6.495912001730054e-05
Epoch 49: reducing lr to 5.7370166280394186e-05
Epoch 53: reducing lr to 5.160682733251938e-05
Epoch 56: reducing lr to 4.702760925179729e-05
Epoch 65: reducing lr to 3.274818377768316e-05
Epoch 72: reducing lr to 2.2064595293286186e-05
Epoch 75: reducing lr to 1.785625618499545e-05
Epoch 78: reducing lr to 1.396788338426751e-05
Epoch 81: reducing lr to 1.0460787345664496e-05
Epoch 84: reducing lr to 7.390287401595416e-06
Epoch 87: reducing lr to 4.804794613465907e-06
Epoch 90: reducing lr to 2.745099059795733e-06
Epoch 93: reducing lr to 1.2436730492233435e-06
Epoch 96: reducing lr to 3.241993904463795e-07
Epoch 99: reducing lr to 1.1760112270174475e-09
[I 2024-06-21 03:31:09,951] Trial 711 finished with value: 0.9704318642616272 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5905115875788886, 'bidirectional': True, 'fc_dropout': 0.22925114969280128, 'learning_rate_model': 0.0007628858149920983}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.15116320459178e-05
Epoch 47: reducing lr to 4.5397839498002777e-05
Epoch 50: reducing lr to 4.2309511115176595e-05
Epoch 53: reducing lr to 3.900865043343827e-05
Epoch 56: reducing lr to 3.5547303813185815e-05
Epoch 59: reducing lr to 3.198007410738645e-05
Epoch 62: reducing lr to 2.836320557678887e-05
Epoch 65: reducing lr to 2.4753749054994902e-05
Epoch 72: reducing lr to 1.6678221259471274e-05
Epoch 75: reducing lr to 1.3497215224689594e-05
Epoch 78: reducing lr to 1.055806582956865e-05
Epoch 81: reducing lr to 7.907116517671031e-06
Epoch 84: reducing lr to 5.586182153651193e-06
Epoch 87: reducing lr to 3.63185576732893e-06
Epoch 90: reducing lr to 2.0749698279021112e-06
Epoch 93: reducing lr to 9.400695554882799e-07
Epoch 96: reducing lr to 2.450563490595959e-07
Epoch 99: reducing lr to 8.889252300827571e-10
[I 2024-06-21 03:31:45,606] Trial 712 finished with value: 0.9754906892776489 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6154307820923687, 'bidirectional': True, 'fc_dropout': 0.21213215836478466, 'learning_rate_model': 0.0005766513388995545}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010272182625164456
Epoch 40: reducing lr to 9.76931579137314e-05
Epoch 45: reducing lr to 8.973306886218751e-05
Epoch 52: reducing lr to 7.610645526689873e-05
Epoch 55: reducing lr to 6.963207242491374e-05
Epoch 60: reducing lr to 5.836968651497024e-05
Epoch 65: reducing lr to 4.6946132734269276e-05
Epoch 68: reducing lr to 4.0222687439426925e-05
Epoch 71: reducing lr to 3.372729814906741e-05
Epoch 74: reducing lr to 2.756237167365806e-05
Epoch 77: reducing lr to 2.1825150749986345e-05
Epoch 80: reducing lr to 1.6606097800453727e-05
Epoch 83: reducing lr to 1.1987535784510834e-05
Epoch 86: reducing lr to 8.042283170996903e-06
Epoch 89: reducing lr to 4.832582316426949e-06
Epoch 92: reducing lr to 2.4090362244644267e-06
Epoch 95: reducing lr to 8.098728051506021e-07
Epoch 98: reducing lr to 6.03075646988735e-08
[I 2024-06-21 03:32:24,505] Trial 713 finished with value: 0.9722740650177002 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5845111639439211, 'bidirectional': True, 'fc_dropout': 0.21885445737582382, 'learning_rate_model': 0.001093634351597743}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.54565129791426e-05
Epoch 40: reducing lr to 7.176259717221376e-05
Epoch 43: reducing lr to 6.840482498160638e-05
Epoch 49: reducing lr to 6.041332121695671e-05
Epoch 52: reducing lr to 5.590562336357742e-05
Epoch 55: reducing lr to 5.114972706797045e-05
Epoch 65: reducing lr to 3.448528519446206e-05
Epoch 72: reducing lr to 2.3234994238303095e-05
Epoch 75: reducing lr to 1.8803427122104347e-05
Epoch 78: reducing lr to 1.4708798672300935e-05
Epoch 83: reducing lr to 8.805700623896355e-06
Epoch 86: reducing lr to 5.907630993511016e-06
Epoch 89: reducing lr to 3.5498766288377605e-06
Epoch 92: reducing lr to 1.769609047771526e-06
Epoch 95: reducing lr to 5.949093786903367e-07
Epoch 98: reducing lr to 4.430021062217003e-08
[I 2024-06-21 03:33:00,175] Trial 714 finished with value: 0.9732572436332703 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6035489606323506, 'bidirectional': True, 'fc_dropout': 0.24289845012295774, 'learning_rate_model': 0.0008033524875581165}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.969071022257344e-05
Epoch 43: reducing lr to 6.642988163010381e-05
Epoch 49: reducing lr to 5.866910380083585e-05
Epoch 53: reducing lr to 5.2775275128287254e-05
Epoch 65: reducing lr to 3.348964658654534e-05
Epoch 72: reducing lr to 2.256416732798681e-05
Epoch 75: reducing lr to 1.8260545777254053e-05
Epoch 78: reducing lr to 1.4284135000487403e-05
Epoch 81: reducing lr to 1.0697633603181592e-05
Epoch 84: reducing lr to 7.557613421636258e-06
Epoch 87: reducing lr to 4.913581608625441e-06
Epoch 90: reducing lr to 2.807251784762066e-06
Epoch 93: reducing lr to 1.2718314753102266e-06
Epoch 96: reducing lr to 3.315396995243907e-07
Epoch 99: reducing lr to 1.2026376986938245e-09
[I 2024-06-21 03:33:31,530] Trial 715 finished with value: 0.9702661037445068 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5731584240504971, 'bidirectional': True, 'fc_dropout': 0.2824228824770745, 'learning_rate_model': 0.0007801585731720688}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.0001081891502567758
Epoch 36: reducing lr to 0.0001037639287081227
Epoch 40: reducing lr to 9.868424504251359e-05
Epoch 47: reducing lr to 8.69716477518721e-05
Epoch 52: reducing lr to 7.687854749774737e-05
Epoch 55: reducing lr to 7.033848270179008e-05
Epoch 61: reducing lr to 5.665078599769191e-05
Epoch 64: reducing lr to 4.971902304096638e-05
Epoch 68: reducing lr to 4.0630742503441214e-05
Epoch 71: reducing lr to 3.406945815083192e-05
Epoch 74: reducing lr to 2.784198912474503e-05
Epoch 77: reducing lr to 2.2046564679620448e-05
Epoch 83: reducing lr to 1.210914811310747e-05
Epoch 86: reducing lr to 8.123871313984728e-06
Epoch 89: reducing lr to 4.881608371422752e-06
Epoch 92: reducing lr to 2.4334756513989677e-06
Epoch 95: reducing lr to 8.180888822053277e-07
Epoch 98: reducing lr to 6.091937879537223e-08
[I 2024-06-21 03:34:11,311] Trial 716 finished with value: 0.9696855545043945 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.6152487776587724, 'bidirectional': True, 'fc_dropout': 0.20612614156826276, 'learning_rate_model': 0.0011047291606161977}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.2432488373643584e-05
Epoch 47: reducing lr to 4.620940158808663e-05
Epoch 50: reducing lr to 4.3065864185074766e-05
Epoch 53: reducing lr to 3.970599511387189e-05
Epoch 56: reducing lr to 3.618277115036481e-05
Epoch 65: reducing lr to 2.5196263600678656e-05
Epoch 72: reducing lr to 1.6976372278415975e-05
Epoch 75: reducing lr to 1.3738500455863262e-05
Epoch 78: reducing lr to 1.0746808863744638e-05
Epoch 86: reducing lr to 4.316340344256346e-06
Epoch 89: reducing lr to 2.593675151175729e-06
Epoch 92: reducing lr to 1.292943810276431e-06
Epoch 95: reducing lr to 4.346634641259916e-07
Epoch 98: reducing lr to 3.2367422165928676e-08
[I 2024-06-21 03:34:47,040] Trial 717 finished with value: 0.9744430780410767 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5972182318763746, 'bidirectional': True, 'fc_dropout': 0.22490496950342156, 'learning_rate_model': 0.0005869599432521376}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 9.314568057639055e-05
Epoch 52: reducing lr to 7.256380818758958e-05
Epoch 68: reducing lr to 3.835037863619159e-05
Epoch 71: reducing lr to 3.215734046464022e-05
Epoch 74: reducing lr to 2.627938253474614e-05
Epoch 77: reducing lr to 2.080922107242128e-05
Epoch 80: reducing lr to 1.583310760316772e-05
Epoch 83: reducing lr to 1.1429533069942405e-05
Epoch 86: reducing lr to 7.667926345589737e-06
Epoch 89: reducing lr to 4.607632493592987e-06
Epoch 92: reducing lr to 2.2968990198788364e-06
Epoch 95: reducing lr to 7.721743797316736e-07
Epoch 98: reducing lr to 5.7500333470040905e-08
[I 2024-06-21 03:35:23,458] Trial 718 finished with value: 0.9681203365325928 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5778520066652169, 'bidirectional': True, 'fc_dropout': 0.20290528325423537, 'learning_rate_model': 0.0010427272304089706}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.917837734001148e-05
Epoch 40: reducing lr to 7.530226051356748e-05
Epoch 49: reducing lr to 6.339318575457811e-05
Epoch 52: reducing lr to 5.866314738574596e-05
Epoch 65: reducing lr to 3.6186259025245164e-05
Epoch 72: reducing lr to 2.4381051663517508e-05
Epoch 76: reducing lr to 1.8255946005737556e-05
Epoch 79: reducing lr to 1.4092566611058669e-05
Epoch 83: reducing lr to 9.240038523046555e-06
Epoch 86: reducing lr to 6.199022688989852e-06
Epoch 89: reducing lr to 3.724972969613603e-06
Epoch 92: reducing lr to 1.8568943540696477e-06
Epoch 95: reducing lr to 6.242530619202601e-07
Epoch 98: reducing lr to 4.6485301989152624e-08
[I 2024-06-21 03:36:02,353] Trial 719 finished with value: 0.9687763452529907 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.6224429020020825, 'bidirectional': True, 'fc_dropout': 0.2543987493731021, 'learning_rate_model': 0.0008429775493931949}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.719457868090087e-05
Epoch 40: reducing lr to 6.390511954040343e-05
Epoch 43: reducing lr to 6.0914998757633183e-05
Epoch 49: reducing lr to 5.379850599522712e-05
Epoch 52: reducing lr to 4.978436796896624e-05
Epoch 55: reducing lr to 4.554920740805215e-05
Epoch 62: reducing lr to 3.518727656149602e-05
Epoch 65: reducing lr to 3.070939959779356e-05
Epoch 72: reducing lr to 2.0690932920893054e-05
Epoch 75: reducing lr to 1.674458987491345e-05
Epoch 78: reducing lr to 1.309829318458768e-05
Epoch 81: reducing lr to 9.80953633601114e-06
Epoch 85: reducing lr to 6.0694429008127764e-06
Epoch 88: reducing lr to 3.8053900135693977e-06
Epoch 91: reducing lr to 2.04544339592322e-06
Epoch 94: reducing lr to 8.173495164723993e-07
Epoch 97: reducing lr to 1.4047976761652352e-07
[I 2024-06-21 03:36:37,974] Trial 720 finished with value: 0.9744707345962524 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6008451973019686, 'bidirectional': True, 'fc_dropout': 0.19158488117736455, 'learning_rate_model': 0.0007153912870137863}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.225694032912719e-05
Epoch 53: reducing lr to 3.957305635750111e-05
Epoch 56: reducing lr to 3.606162842154052e-05
Epoch 65: reducing lr to 2.511190455266441e-05
Epoch 72: reducing lr to 1.691953406514597e-05
Epoch 75: reducing lr to 1.3692502889003044e-05
Epoch 78: reducing lr to 1.071082771275715e-05
Epoch 81: reducing lr to 8.021522511091484e-06
Epoch 84: reducing lr to 5.667007156961545e-06
Epoch 87: reducing lr to 3.6844041351305804e-06
Epoch 90: reducing lr to 2.104992021700327e-06
Epoch 93: reducing lr to 9.536711751355429e-07
Epoch 96: reducing lr to 2.4860200505132274e-07
Epoch 99: reducing lr to 9.017868559096226e-10
[I 2024-06-21 03:37:09,283] Trial 721 finished with value: 0.9715473055839539 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5635097990072503, 'bidirectional': True, 'fc_dropout': 0.21783657203235707, 'learning_rate_model': 0.0005849947557616433}. Best is trial 690 with value: 0.966281533241272.
[I 2024-06-21 03:37:45,371] Trial 722 finished with value: 1.0529987812042236 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5829781569672385, 'bidirectional': True, 'fc_dropout': 0.22944119231309387, 'learning_rate_model': 3.904138063797538e-05}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 8.153522326892544e-05
Epoch 40: reducing lr to 7.675790578535367e-05
Epoch 43: reducing lr to 7.316640308601831e-05
Epoch 49: reducing lr to 6.461862029635376e-05
Epoch 52: reducing lr to 5.9797146983338e-05
Epoch 61: reducing lr to 4.4063727623429256e-05
Epoch 65: reducing lr to 3.6885764677459126e-05
Epoch 68: reducing lr to 3.160312675771643e-05
Epoch 71: reducing lr to 2.64996733548806e-05
Epoch 74: reducing lr to 2.165586591044347e-05
Epoch 79: reducing lr to 1.43649857630843e-05
Epoch 82: reducing lr to 1.0572143904701221e-05
Epoch 85: reducing lr to 7.290147169752555e-06
Epoch 88: reducing lr to 4.570741283934406e-06
Epoch 91: reducing lr to 2.4568290084221385e-06
Epoch 94: reducing lr to 9.817372634664626e-07
Epoch 97: reducing lr to 1.6873347307706613e-07
[I 2024-06-21 03:38:20,577] Trial 723 finished with value: 0.9706874489784241 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.6128788216166442, 'bidirectional': True, 'fc_dropout': 0.19075031707713125, 'learning_rate_model': 0.0008592728939901213}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010189725506080248
Epoch 52: reducing lr to 7.549553164199685e-05
Epoch 55: reducing lr to 6.907312012650389e-05
Epoch 58: reducing lr to 6.241684822875109e-05
Epoch 61: reducing lr to 5.563165988480802e-05
Epoch 64: reducing lr to 4.882459670961435e-05
Epoch 67: reducing lr to 4.210296496095672e-05
Epoch 70: reducing lr to 3.557281367299e-05
Epoch 73: reducing lr to 2.9337106823801893e-05
Epoch 76: reducing lr to 2.3494176680769633e-05
Epoch 79: reducing lr to 1.8136186957480595e-05
Epoch 82: reducing lr to 1.334762049606668e-05
Epoch 85: reducing lr to 9.204009958572512e-06
Epoch 88: reducing lr to 5.770685737311199e-06
Epoch 91: reducing lr to 3.1018137403109136e-06
Epoch 94: reducing lr to 1.2394701148335778e-06
Epoch 97: reducing lr to 2.1303061932540554e-07
[I 2024-06-21 03:39:00,314] Trial 724 finished with value: 0.9706985950469971 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.5659351552264408, 'bidirectional': True, 'fc_dropout': 0.2704286981288918, 'learning_rate_model': 0.0010848555028121537}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.803082643962329e-05
Epoch 40: reducing lr to 6.470042943051129e-05
Epoch 43: reducing lr to 6.167309609500258e-05
Epoch 49: reducing lr to 5.446803739112693e-05
Epoch 52: reducing lr to 5.0403942746437e-05
Epoch 55: reducing lr to 4.611607490472109e-05
Epoch 64: reducing lr to 3.259732230033457e-05
Epoch 67: reducing lr to 2.8109682642022585e-05
Epoch 72: reducing lr to 2.094843503818649e-05
Epoch 75: reducing lr to 1.6952979093634784e-05
Epoch 78: reducing lr to 1.3261303631765522e-05
Epoch 85: reducing lr to 6.144978131811121e-06
Epoch 88: reducing lr to 3.852748728102359e-06
Epoch 91: reducing lr to 2.070899280743276e-06
Epoch 94: reducing lr to 8.275215677696919e-07
Epoch 97: reducing lr to 1.4222806179621907e-07
[I 2024-06-21 03:39:36,087] Trial 725 finished with value: 0.9733571410179138 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5950271336128192, 'bidirectional': True, 'fc_dropout': 0.23653909278491195, 'learning_rate_model': 0.000724294451110042}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010500589484768831
Epoch 35: reducing lr to 0.00010174192282218617
Epoch 40: reducing lr to 9.57806529902817e-05
Epoch 43: reducing lr to 9.129907587794288e-05
Epoch 46: reducing lr to 8.622314368268166e-05
Epoch 51: reducing lr to 7.666361461946969e-05
Epoch 61: reducing lr to 5.4983946758006525e-05
Epoch 65: reducing lr to 4.602708464627014e-05
Epoch 68: reducing lr to 3.943526189801683e-05
Epoch 71: reducing lr to 3.306703058128434e-05
Epoch 74: reducing lr to 2.7022792724082388e-05
Epoch 77: reducing lr to 2.139788737601251e-05
Epoch 80: reducing lr to 1.6281006008143156e-05
Epoch 83: reducing lr to 1.1752859972023018e-05
Epoch 86: reducing lr to 7.88484219469139e-06
Epoch 89: reducing lr to 4.737976535730375e-06
Epoch 92: reducing lr to 2.361875361427059e-06
Epoch 95: reducing lr to 7.940182073436089e-07
Epoch 98: reducing lr to 5.9126944511431366e-08
[I 2024-06-21 03:40:29,066] Trial 726 finished with value: 0.9784318208694458 and parameters: {'hidden_size': 130, 'n_layers': 3, 'rnn_dropout': 0.6242927620757054, 'bidirectional': True, 'fc_dropout': 0.20464505373043096, 'learning_rate_model': 0.001072224652837351}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.049209911391251e-05
Epoch 40: reducing lr to 5.753075472773668e-05
Epoch 49: reducing lr to 4.843224886189679e-05
Epoch 65: reducing lr to 2.7646219094852245e-05
Epoch 72: reducing lr to 1.862706768285369e-05
Epoch 79: reducing lr to 1.0766688644612871e-05
Epoch 82: reducing lr to 7.92391886809083e-06
Epoch 86: reducing lr to 4.736039149949469e-06
Epoch 89: reducing lr to 2.8458708254007504e-06
Epoch 92: reducing lr to 1.4186630376129568e-06
[I 2024-06-21 03:41:07,981] Trial 727 finished with value: 0.9695636034011841 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5808949384213525, 'bidirectional': True, 'fc_dropout': 0.24428067948491186, 'learning_rate_model': 0.0006440329188576022}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.291590999317196e-05
Epoch 40: reducing lr to 7.885682511796273e-05
Epoch 43: reducing lr to 7.51671139751892e-05
Epoch 49: reducing lr to 6.63855949160863e-05
Epoch 52: reducing lr to 6.143228002343382e-05
Epoch 55: reducing lr to 5.6206230559785683e-05
Epoch 61: reducing lr to 4.5268635558701277e-05
Epoch 65: reducing lr to 3.789439361004211e-05
Epoch 72: reducing lr to 2.553193375749471e-05
Epoch 75: reducing lr to 2.066227565075176e-05
Epoch 78: reducing lr to 1.616286492270464e-05
Epoch 83: reducing lr to 9.676204896449806e-06
Epoch 86: reducing lr to 6.491641084265725e-06
Epoch 89: reducing lr to 3.90080643038642e-06
Epoch 92: reducing lr to 1.9445471137618704e-06
Epoch 95: reducing lr to 6.53720276090901e-07
Epoch 98: reducing lr to 4.867959214655167e-08
[I 2024-06-21 03:41:43,653] Trial 728 finished with value: 0.972195029258728 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6069948665229765, 'bidirectional': True, 'fc_dropout': 0.18584611537695503, 'learning_rate_model': 0.0008827694246826892}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.534830286846956e-05
Epoch 43: reducing lr to 6.229065553237097e-05
Epoch 65: reducing lr to 3.1402916703045756e-05
Epoch 72: reducing lr to 2.1158200796273343e-05
Epoch 75: reducing lr to 1.7122736619909387e-05
Epoch 78: reducing lr to 1.3394094811844913e-05
Epoch 81: reducing lr to 1.003106724617929e-05
Epoch 84: reducing lr to 7.086700784976735e-06
Epoch 87: reducing lr to 4.607417805097816e-06
Epoch 90: reducing lr to 2.632332763904911e-06
Epoch 93: reducing lr to 1.1925840356740227e-06
Epoch 96: reducing lr to 3.108815597982527e-07
Epoch 99: reducing lr to 1.1277017026244638e-09
[I 2024-06-21 03:42:19,747] Trial 729 finished with value: 0.971635103225708 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.557398437689611, 'bidirectional': True, 'fc_dropout': 0.22033395506191408, 'learning_rate_model': 0.0007315471253235685}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.969717633803932e-05
Epoch 56: reducing lr to 3.4295178696163115e-05
Epoch 65: reducing lr to 2.3881818202091817e-05
Epoch 72: reducing lr to 1.6090744362320514e-05
Epoch 75: reducing lr to 1.3021786700447317e-05
Epoch 78: reducing lr to 1.0186166473098241e-05
Epoch 81: reducing lr to 7.628594713400512e-06
Epoch 84: reducing lr to 5.389413391114115e-06
Epoch 87: reducing lr to 3.5039265760863134e-06
Epoch 90: reducing lr to 2.0018806886460325e-06
Epoch 93: reducing lr to 9.069563633215517e-07
Epoch 96: reducing lr to 2.3642443673916983e-07
Epoch 99: reducing lr to 8.576135555513585e-10
[I 2024-06-21 03:42:51,129] Trial 730 finished with value: 0.9721118211746216 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5929896773983112, 'bidirectional': True, 'fc_dropout': 0.1800064833673073, 'learning_rate_model': 0.0005563392604084406}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.136304409862415e-05
Epoch 40: reducing lr to 7.737997858406966e-05
Epoch 43: reducing lr to 7.375936909615125e-05
Epoch 49: reducing lr to 6.51423120980729e-05
Epoch 52: reducing lr to 6.028176388629492e-05
Epoch 55: reducing lr to 5.5153588931603956e-05
Epoch 65: reducing lr to 3.718469975953199e-05
Epoch 72: reducing lr to 2.505376654981246e-05
Epoch 75: reducing lr to 2.0275308382775806e-05
Epoch 78: reducing lr to 1.586016352681189e-05
Epoch 83: reducing lr to 9.494986978518364e-06
Epoch 86: reducing lr to 6.370064320044828e-06
Epoch 89: reducing lr to 3.827751340386464e-06
Epoch 92: reducing lr to 1.9081292430112353e-06
Epoch 95: reducing lr to 6.41477270841369e-07
Epoch 98: reducing lr to 4.776791092143138e-08
[I 2024-06-21 03:43:26,745] Trial 731 finished with value: 0.9720968008041382 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6225052208042373, 'bidirectional': True, 'fc_dropout': 0.20514664895398607, 'learning_rate_model': 0.0008662367407568633}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 0.00010268350632648845
Epoch 40: reducing lr to 9.313563852155053e-05
Epoch 43: reducing lr to 8.877782164611544e-05
Epoch 52: reducing lr to 7.25559850686139e-05
Epoch 55: reducing lr to 6.638364103860801e-05
Epoch 58: reducing lr to 5.998654237697903e-05
Epoch 61: reducing lr to 5.346554684964896e-05
Epoch 64: reducing lr to 4.6923528224725216e-05
Epoch 67: reducing lr to 4.0463614608844025e-05
Epoch 70: reducing lr to 3.418772583714439e-05
Epoch 73: reducing lr to 2.819481681058867e-05
Epoch 76: reducing lr to 2.257939106294121e-05
Epoch 79: reducing lr to 1.743002375728084e-05
Epoch 82: reducing lr to 1.2827908252988692e-05
Epoch 85: reducing lr to 8.845636219800758e-06
Epoch 88: reducing lr to 5.545994300397796e-06
Epoch 91: reducing lr to 2.98103936130739e-06
Epoch 94: reducing lr to 1.191209243632009e-06
Epoch 97: reducing lr to 2.047359108381299e-07
[I 2024-06-21 03:44:02,811] Trial 732 finished with value: 0.974463701248169 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5717843400628289, 'bidirectional': True, 'fc_dropout': 0.25262204843591773, 'learning_rate_model': 0.001042614813773372}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.810579839877466e-05
Epoch 40: reducing lr to 6.477173119481731e-05
Epoch 49: reducing lr to 5.45280627603317e-05
Epoch 65: reducing lr to 3.1125847040234764e-05
Epoch 72: reducing lr to 2.0971520825882496e-05
Epoch 79: reducing lr to 1.2121813211863501e-05
Epoch 83: reducing lr to 7.947879484131767e-06
Epoch 86: reducing lr to 5.332129852987362e-06
Epoch 89: reducing lr to 3.2040598283540436e-06
Epoch 92: reducing lr to 1.5972198064001345e-06
Epoch 95: reducing lr to 5.369553483318825e-07
Epoch 98: reducing lr to 3.998463611074943e-08
[I 2024-06-21 03:44:41,648] Trial 733 finished with value: 0.9687053561210632 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.6059276765816263, 'bidirectional': True, 'fc_dropout': 0.17121620933382248, 'learning_rate_model': 0.0007250926447649491}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.700935260816753e-05
Epoch 43: reducing lr to 7.340608471105045e-05
Epoch 65: reducing lr to 3.7006596639200533e-05
Epoch 72: reducing lr to 2.4933766818002508e-05
Epoch 75: reducing lr to 2.0178195976006164e-05
Epoch 78: reducing lr to 1.5784198287577482e-05
Epoch 81: reducing lr to 1.1821056717449714e-05
Epoch 84: reducing lr to 8.351284052124484e-06
Epoch 87: reducing lr to 5.429586489492813e-06
Epoch 90: reducing lr to 3.102058249402446e-06
Epoch 93: reducing lr to 1.4053941799061572e-06
Epoch 96: reducing lr to 3.663566857438805e-07
Epoch 99: reducing lr to 1.328933946903848e-09
[I 2024-06-21 03:45:15,563] Trial 734 finished with value: 0.9699751138687134 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.6481176983689166, 'bidirectional': True, 'fc_dropout': 0.19024163698360574, 'learning_rate_model': 0.0008620877367989921}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010220650577935997
Epoch 40: reducing lr to 9.720306456052576e-05
Epoch 49: reducing lr to 8.183037116780081e-05
Epoch 52: reducing lr to 7.572465506042688e-05
Epoch 55: reducing lr to 6.92827519955801e-05
Epoch 58: reducing lr to 6.260627879931277e-05
Epoch 61: reducing lr to 5.580049790486713e-05
Epoch 64: reducing lr to 4.8972775790657495e-05
Epoch 67: reducing lr to 4.223074438111684e-05
Epoch 70: reducing lr to 3.5680774561464566e-05
Epoch 73: reducing lr to 2.942614279793334e-05
Epoch 76: reducing lr to 2.3565479789141978e-05
Epoch 79: reducing lr to 1.8191228958809742e-05
Epoch 82: reducing lr to 1.3388129548317198e-05
Epoch 85: reducing lr to 9.2319434558154e-06
Epoch 88: reducing lr to 5.788199346581317e-06
Epoch 91: reducing lr to 3.1112275182134697e-06
Epoch 94: reducing lr to 1.243231816004187e-06
Epoch 97: reducing lr to 2.136771516786243e-07
[I 2024-06-21 03:45:51,317] Trial 735 finished with value: 0.9741079211235046 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5896151486635627, 'bidirectional': True, 'fc_dropout': 0.2142810957395529, 'learning_rate_model': 0.0010881479599404202}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 5.800319963456987e-05
Epoch 40: reducing lr to 5.5163697416360936e-05
Epoch 49: reducing lr to 4.6439542363999036e-05
Epoch 65: reducing lr to 2.6508737319233955e-05
Epoch 72: reducing lr to 1.7860671744596812e-05
Epoch 75: reducing lr to 1.445413913413915e-05
Epoch 78: reducing lr to 1.1306610285715074e-05
Epoch 86: reducing lr to 4.5411785723348885e-06
Epoch 89: reducing lr to 2.728779725581613e-06
Epoch 92: reducing lr to 1.3602932711905794e-06
[I 2024-06-21 03:46:30,201] Trial 736 finished with value: 0.9698323011398315 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5525147780637002, 'bidirectional': True, 'fc_dropout': 0.23324340489509482, 'learning_rate_model': 0.0006175346947935682}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.259524778490333e-05
Epoch 40: reducing lr to 7.855186068253096e-05
Epoch 43: reducing lr to 7.487641882683409e-05
Epoch 49: reducing lr to 6.61288606962632e-05
Epoch 52: reducing lr to 6.119470184847422e-05
Epoch 55: reducing lr to 5.5988863181061364e-05
Epoch 65: reducing lr to 3.774784393173554e-05
Epoch 72: reducing lr to 2.5433193645244586e-05
Epoch 75: reducing lr to 2.0582368055953967e-05
Epoch 78: reducing lr to 1.610035798093088e-05
Epoch 83: reducing lr to 9.638783933090531e-06
Epoch 86: reducing lr to 6.466535842514904e-06
Epoch 89: reducing lr to 3.88572077066082e-06
Epoch 92: reducing lr to 1.9370269313067507e-06
Epoch 95: reducing lr to 6.511921317656666e-07
Epoch 98: reducing lr to 4.8491332673599504e-08
[I 2024-06-21 03:47:05,876] Trial 737 finished with value: 0.9720497131347656 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6243957615567913, 'bidirectional': True, 'fc_dropout': 0.1702515894355864, 'learning_rate_model': 0.0008793554744150736}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.110286386345746e-05
Epoch 43: reducing lr to 5.824386063431253e-05
Epoch 53: reducing lr to 4.627188388841722e-05
Epoch 56: reducing lr to 4.2166050255870045e-05
Epoch 65: reducing lr to 2.9362784647732924e-05
Epoch 72: reducing lr to 1.978363027196799e-05
Epoch 75: reducing lr to 1.6010335368035603e-05
Epoch 78: reducing lr to 1.2523929711069615e-05
Epoch 81: reducing lr to 9.379385683238857e-06
Epoch 84: reducing lr to 6.626303886989244e-06
Epoch 87: reducing lr to 4.3080908080139e-06
Epoch 90: reducing lr to 2.4613197811722667e-06
Epoch 93: reducing lr to 1.115106235034032e-06
Epoch 96: reducing lr to 2.906847277158158e-07
Epoch 99: reducing lr to 1.0544390686441882e-09
[I 2024-06-21 03:47:37,306] Trial 738 finished with value: 0.9706660509109497 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5691272730897288, 'bidirectional': True, 'fc_dropout': 0.1995074280425243, 'learning_rate_model': 0.0006840211978927635}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.990799508886017e-05
Epoch 50: reducing lr to 4.099235044748786e-05
Epoch 61: reducing lr to 2.8650238425949867e-05
Epoch 64: reducing lr to 2.514460901719913e-05
Epoch 67: reducing lr to 2.1682976691123875e-05
Epoch 72: reducing lr to 1.6159002377688794e-05
Epoch 75: reducing lr to 1.3077025991850328e-05
Epoch 78: reducing lr to 1.0229376873562544e-05
Epoch 81: reducing lr to 7.660955723150008e-06
Epoch 84: reducing lr to 5.412275643710586e-06
Epoch 87: reducing lr to 3.5187904673205816e-06
Epoch 90: reducing lr to 2.0103728006163756e-06
Epoch 93: reducing lr to 9.108037329641191e-07
Epoch 96: reducing lr to 2.374273650358966e-07
Epoch 99: reducing lr to 8.612516096996592e-10
[I 2024-06-21 03:48:13,429] Trial 739 finished with value: 0.9733544588088989 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.6009221604148051, 'bidirectional': True, 'fc_dropout': 0.22089937606755633, 'learning_rate_model': 0.0005586992888155703}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010629175834144863
Epoch 36: reducing lr to 0.00010194414512570151
Epoch 40: reducing lr to 9.69535475717466e-05
Epoch 43: reducing lr to 9.24170907175458e-05
Epoch 49: reducing lr to 8.162031536455238e-05
Epoch 52: reducing lr to 7.553027242452499e-05
Epoch 55: reducing lr to 6.910490550760743e-05
Epoch 65: reducing lr to 4.659071536392368e-05
Epoch 71: reducing lr to 3.347195725262222e-05
Epoch 74: reducing lr to 2.735370388591528e-05
Epoch 77: reducing lr to 2.1659918019723706e-05
Epoch 80: reducing lr to 1.6480377208187982e-05
Epoch 83: reducing lr to 1.1896781164325824e-05
Epoch 86: reducing lr to 7.981397066652826e-06
Epoch 89: reducing lr to 4.795996050448365e-06
Epoch 92: reducing lr to 2.3907980167549216e-06
Epoch 95: reducing lr to 8.037414617160068e-07
Epoch 98: reducing lr to 5.985099128572557e-08
[I 2024-06-21 03:48:49,143] Trial 740 finished with value: 0.973437488079071 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6371623702852336, 'bidirectional': True, 'fc_dropout': 0.19362405993634135, 'learning_rate_model': 0.001085354720822517}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.286806278954676e-05
Epoch 40: reducing lr to 6.930086257936752e-05
Epoch 65: reducing lr to 3.330230655583154e-05
Epoch 72: reducing lr to 2.243794408495161e-05
Epoch 78: reducing lr to 1.4204229998122095e-05
Epoch 81: reducing lr to 1.063779137693994e-05
Epoch 84: reducing lr to 7.515336369626365e-06
Epoch 87: reducing lr to 4.886095187498364e-06
Epoch 90: reducing lr to 2.7915481064858624e-06
Epoch 93: reducing lr to 1.264716890000061e-06
Epoch 96: reducing lr to 3.296850768626952e-07
Epoch 99: reducing lr to 1.1959101811924096e-09
[I 2024-06-21 03:49:28,000] Trial 741 finished with value: 0.9683598279953003 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5834214506184308, 'bidirectional': True, 'fc_dropout': 0.166735458412007, 'learning_rate_model': 0.000775794390627413}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.617343165295224e-05
Epoch 40: reducing lr to 8.19548772995576e-05
Epoch 52: reducing lr to 6.384577319745197e-05
Epoch 63: reducing lr to 4.320577754489533e-05
Epoch 66: reducing lr to 3.7487245203826016e-05
Epoch 72: reducing lr to 2.6535008164299914e-05
Epoch 75: reducing lr to 2.1474035546750227e-05
Epoch 78: reducing lr to 1.679785623588144e-05
Epoch 81: reducing lr to 1.2580202534087442e-05
Epoch 84: reducing lr to 8.8876017860852e-06
Epoch 87: reducing lr to 5.7782734104756625e-06
Epoch 90: reducing lr to 3.3012717883684045e-06
Epoch 93: reducing lr to 1.495648303366016e-06
Epoch 96: reducing lr to 3.8988403630377383e-07
Epoch 99: reducing lr to 1.4142778100218779e-09
[I 2024-06-21 03:50:01,147] Trial 742 finished with value: 0.9694676995277405 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6089979333069189, 'bidirectional': True, 'fc_dropout': 0.184114277235294, 'learning_rate_model': 0.0009174508328917102}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.904857677269558e-05
Epoch 40: reducing lr to 6.500287775076545e-05
Epoch 65: reducing lr to 3.1236923774044445e-05
Epoch 72: reducing lr to 2.1046360493164462e-05
Epoch 75: reducing lr to 1.703222740922256e-05
Epoch 78: reducing lr to 1.332329485876527e-05
Epoch 81: reducing lr to 9.978043947453636e-06
Epoch 84: reducing lr to 7.0492411365186615e-06
Epoch 87: reducing lr to 4.583063418407143e-06
Epoch 90: reducing lr to 2.6184184950579e-06
Epoch 93: reducing lr to 1.186280146164842e-06
Epoch 96: reducing lr to 3.092382684705217e-07
Epoch 99: reducing lr to 1.1217407751588569e-09
[I 2024-06-21 03:50:37,192] Trial 743 finished with value: 0.9721564650535583 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5768144426111436, 'bidirectional': True, 'fc_dropout': 0.15622208857934775, 'learning_rate_model': 0.0007276802345126532}. Best is trial 690 with value: 0.966281533241272.
Epoch 28: reducing lr to 0.00010338697692591839
Epoch 36: reducing lr to 9.775537620126617e-05
Epoch 40: reducing lr to 9.296983662217183e-05
Epoch 49: reducing lr to 7.826662947920713e-05
Epoch 52: reducing lr to 7.242681947393248e-05
Epoch 55: reducing lr to 6.626546357242433e-05
Epoch 65: reducing lr to 4.4676355883612385e-05
Epoch 68: reducing lr to 3.827797933454451e-05
Epoch 71: reducing lr to 3.209663261571509e-05
Epoch 74: reducing lr to 2.6229771318093453e-05
Epoch 77: reducing lr to 2.076993663437818e-05
Epoch 80: reducing lr to 1.5803217261164914e-05
Epoch 83: reducing lr to 1.1407955963226816e-05
Epoch 86: reducing lr to 7.65345054294464e-06
Epoch 89: reducing lr to 4.598934029936396e-06
Epoch 92: reducing lr to 2.292562846654294e-06
Epoch 95: reducing lr to 7.707166396041937e-07
Epoch 98: reducing lr to 5.739178215618309e-08
[I 2024-06-21 03:51:12,885] Trial 744 finished with value: 0.974422037601471 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5515516102979965, 'bidirectional': True, 'fc_dropout': 0.2437822474452924, 'learning_rate_model': 0.0010407587303321874}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.380943222922991e-05
Epoch 43: reducing lr to 5.129168869358153e-05
Epoch 49: reducing lr to 4.529945461652316e-05
Epoch 52: reducing lr to 4.1919467384885226e-05
Epoch 55: reducing lr to 3.835337460825985e-05
Epoch 61: reducing lr to 3.0889901711890975e-05
Epoch 65: reducing lr to 2.5857949540538184e-05
Epoch 68: reducing lr to 2.2154673060733624e-05
Epoch 72: reducing lr to 1.7422193413822197e-05
Epoch 75: reducing lr to 1.4099290957600739e-05
Epoch 78: reducing lr to 1.1029033737884554e-05
Epoch 81: reducing lr to 8.259832458947739e-06
Epoch 84: reducing lr to 5.8353672380071464e-06
Epoch 87: reducing lr to 3.793863424948719e-06
Epoch 90: reducing lr to 2.1675288453813214e-06
Epoch 93: reducing lr to 9.820036179734674e-07
Epoch 96: reducing lr to 2.559876766341021e-07
Epoch 99: reducing lr to 9.285778769930856e-10
[I 2024-06-21 03:51:46,303] Trial 745 finished with value: 0.9756715297698975 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.6221108684241029, 'bidirectional': True, 'fc_dropout': 0.20779326958610675, 'learning_rate_model': 0.0006023742581627112}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.021724653103548e-05
Epoch 40: reducing lr to 7.629027265892975e-05
Epoch 52: reducing lr to 5.94328440948089e-05
Epoch 65: reducing lr to 3.666104508303855e-05
Epoch 72: reducing lr to 2.4700946112847088e-05
Epoch 75: reducing lr to 1.9989780729718296e-05
Epoch 78: reducing lr to 1.563681228679988e-05
Epoch 83: reducing lr to 9.361273532754405e-06
Epoch 86: reducing lr to 6.280357693600965e-06
Epoch 89: reducing lr to 3.773846914565911e-06
Epoch 92: reducing lr to 1.8812579543382571e-06
Epoch 95: reducing lr to 6.324436474717415e-07
Epoch 98: reducing lr to 4.709521784869095e-08
[I 2024-06-21 03:52:23,318] Trial 746 finished with value: 0.9670869708061218 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5962597014494029, 'bidirectional': True, 'fc_dropout': 0.266789185207853, 'learning_rate_model': 0.0008540379352486518}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.812481077009422e-05
Epoch 47: reducing lr to 6.003923972775079e-05
Epoch 65: reducing lr to 3.27371586425122e-05
Epoch 72: reducing lr to 2.205716693795349e-05
Epoch 75: reducing lr to 1.7850244626020965e-05
Epoch 78: reducing lr to 1.3963180900508134e-05
Epoch 81: reducing lr to 1.0457265574953068e-05
Epoch 84: reducing lr to 7.387799357736011e-06
Epoch 87: reducing lr to 4.803177012000068e-06
Epoch 90: reducing lr to 2.7441748837132504e-06
Epoch 93: reducing lr to 1.2432543492560681e-06
Epoch 96: reducing lr to 3.240902441765687e-07
Epoch 99: reducing lr to 1.175615306351404e-09
[I 2024-06-21 03:52:57,446] Trial 747 finished with value: 0.9695095419883728 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5927924365058123, 'bidirectional': True, 'fc_dropout': 0.2762992570893623, 'learning_rate_model': 0.0007626289787874589}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010252183777029716
Epoch 40: reducing lr to 9.750295971533356e-05
Epoch 45: reducing lr to 8.955836811140034e-05
Epoch 52: reducing lr to 7.595828408493129e-05
Epoch 55: reducing lr to 6.949650617842571e-05
Epoch 60: reducing lr to 5.825604693720168e-05
Epoch 65: reducing lr to 4.685473360194019e-05
Epoch 68: reducing lr to 4.014437814070966e-05
Epoch 71: reducing lr to 3.366163468315259e-05
Epoch 74: reducing lr to 2.7508710664557196e-05
Epoch 77: reducing lr to 2.178265950043462e-05
Epoch 80: reducing lr to 1.6573767492461706e-05
Epoch 83: reducing lr to 1.196419732603397e-05
Epoch 86: reducing lr to 8.026625700168909e-06
Epoch 89: reducing lr to 4.823173792126763e-06
Epoch 92: reducing lr to 2.4043460868994945e-06
Epoch 95: reducing lr to 8.082960688493135e-07
Epoch 98: reducing lr to 6.019015227818041e-08
[I 2024-06-21 03:53:36,339] Trial 748 finished with value: 0.9729851484298706 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5677843452069038, 'bidirectional': True, 'fc_dropout': 0.2610365618411301, 'learning_rate_model': 0.0010915051617156474}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.153392518819413e-05
Epoch 40: reducing lr to 5.792852515556316e-05
Epoch 43: reducing lr to 5.521804924645596e-05
Epoch 47: reducing lr to 5.105312689421343e-05
Epoch 50: reducing lr to 4.758008010249635e-05
Epoch 53: reducing lr to 4.38680254957497e-05
Epoch 56: reducing lr to 3.997549293951693e-05
Epoch 61: reducing lr to 3.32545127171612e-05
Epoch 65: reducing lr to 2.7837366394226053e-05
Epoch 72: reducing lr to 1.8755856132030584e-05
Epoch 75: reducing lr to 1.517858667293853e-05
Epoch 78: reducing lr to 1.1873302353477438e-05
Epoch 81: reducing lr to 8.892119700140136e-06
Epoch 84: reducing lr to 6.282062527603147e-06
Epoch 87: reducing lr to 4.084282322710131e-06
Epoch 90: reducing lr to 2.333452408681503e-06
Epoch 93: reducing lr to 1.0571756461635428e-06
Epoch 96: reducing lr to 2.7558344236454236e-07
Epoch 99: reducing lr to 9.996601836787834e-10
[I 2024-06-21 03:54:10,779] Trial 749 finished with value: 0.97381591796875 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6008440487943438, 'bidirectional': True, 'fc_dropout': 0.25143895308036984, 'learning_rate_model': 0.0006484857936874332}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.899467995081968e-05
Epoch 43: reducing lr to 7.52985185799002e-05
Epoch 52: reducing lr to 6.153967385626925e-05
Epoch 55: reducing lr to 5.630448838981812e-05
Epoch 58: reducing lr to 5.087867320874467e-05
Epoch 61: reducing lr to 4.534777265532256e-05
Epoch 65: reducing lr to 3.796063930646108e-05
Epoch 72: reducing lr to 2.5576567819992018e-05
Epoch 75: reducing lr to 2.069839674175458e-05
Epoch 78: reducing lr to 1.6191120296149823e-05
Epoch 81: reducing lr to 1.2125807586342302e-05
Epoch 84: reducing lr to 8.566582999763992e-06
Epoch 87: reducing lr to 5.569563078722597e-06
Epoch 90: reducing lr to 3.1820303677550557e-06
Epoch 93: reducing lr to 1.4416257205966547e-06
Epoch 96: reducing lr to 3.758014858978604e-07
Epoch 99: reducing lr to 1.3631943167379325e-09
[I 2024-06-21 03:54:46,877] Trial 750 finished with value: 0.9729194641113281 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5852263171558031, 'bidirectional': True, 'fc_dropout': 0.22482297919531583, 'learning_rate_model': 0.0008843126523146517}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.763890752395798e-05
Epoch 43: reducing lr to 6.447408278744539e-05
Epoch 46: reducing lr to 6.088953311447551e-05
Epoch 49: reducing lr to 5.6941794305503515e-05
Epoch 52: reducing lr to 5.269312201290258e-05
Epoch 61: reducing lr to 3.882886547499716e-05
Epoch 65: reducing lr to 3.2503659400843014e-05
Epoch 68: reducing lr to 2.784860981239775e-05
Epoch 72: reducing lr to 2.189984321265331e-05
Epoch 75: reducing lr to 1.7722926961427654e-05
Epoch 78: reducing lr to 1.3863587891012058e-05
Epoch 81: reducing lr to 1.0382678662620547e-05
Epoch 84: reducing lr to 7.335105549868543e-06
Epoch 87: reducing lr to 4.768918138096196e-06
Epoch 90: reducing lr to 2.724601934168307e-06
Epoch 93: reducing lr to 1.2343867822529822e-06
Epoch 96: reducing lr to 3.217786560795791e-07
Epoch 99: reducing lr to 1.1672301778332255e-09
[I 2024-06-21 03:55:39,850] Trial 751 finished with value: 0.9742388129234314 and parameters: {'hidden_size': 130, 'n_layers': 3, 'rnn_dropout': 0.6126270161994956, 'bidirectional': True, 'fc_dropout': 0.23436978666513708, 'learning_rate_model': 0.0007571894936395152}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.888289769458239e-05
Epoch 49: reducing lr to 4.115205297477583e-05
Epoch 52: reducing lr to 3.808152122581974e-05
Epoch 55: reducing lr to 3.48419228664363e-05
Epoch 61: reducing lr to 2.806176989092684e-05
Epoch 64: reducing lr to 2.4628145209390955e-05
Epoch 67: reducing lr to 2.1237613921758326e-05
Epoch 72: reducing lr to 1.582710062122664e-05
Epoch 75: reducing lr to 1.2808427238378446e-05
Epoch 78: reducing lr to 1.0019268101220478e-05
Epoch 81: reducing lr to 7.503601661230751e-06
Epoch 84: reducing lr to 5.301108892780148e-06
Epoch 87: reducing lr to 3.446515415344686e-06
Epoch 90: reducing lr to 1.969080259896802e-06
Epoch 93: reducing lr to 8.920960583380894e-07
Epoch 96: reducing lr to 2.3255066796971838e-07
Epoch 99: reducing lr to 8.435617229480413e-10
[I 2024-06-21 03:56:15,521] Trial 752 finished with value: 0.9756611585617065 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5640085615996299, 'bidirectional': True, 'fc_dropout': 0.2191837213986875, 'learning_rate_model': 0.0005472237489921417}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00010946636421351678
Epoch 31: reducing lr to 0.0001075968149434997
Epoch 36: reducing lr to 0.00010319582147119359
Epoch 39: reducing lr to 9.952651302988155e-05
Epoch 52: reducing lr to 7.645763765227571e-05
Epoch 55: reducing lr to 6.995337969388544e-05
Epoch 58: reducing lr to 6.321228106454213e-05
Epoch 63: reducing lr to 5.1740491477739266e-05
Epoch 66: reducing lr to 4.4892340821248404e-05
Epoch 69: reducing lr to 3.8202542503158666e-05
Epoch 72: reducing lr to 3.1776638260012295e-05
Epoch 75: reducing lr to 2.5715939310310397e-05
Epoch 78: reducing lr to 2.011604435341539e-05
Epoch 81: reducing lr to 1.5065250505602536e-05
Epoch 84: reducing lr to 1.0643226684038947e-05
Epoch 87: reducing lr to 6.919692761925213e-06
Epoch 90: reducing lr to 3.9533931464209165e-06
Epoch 93: reducing lr to 1.7910932910209257e-06
Epoch 96: reducing lr to 4.669003268537465e-07
Epoch 99: reducing lr to 1.693649163015537e-09
[I 2024-06-21 03:56:53,890] Trial 753 finished with value: 0.9735935926437378 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6135298069867484, 'bidirectional': True, 'fc_dropout': 0.2819352099061774, 'learning_rate_model': 0.0010986807713656511}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 9.216934070446886e-05
Epoch 36: reducing lr to 8.790230650633839e-05
Epoch 40: reducing lr to 8.35991164084999e-05
Epoch 43: reducing lr to 7.968751343847113e-05
Epoch 49: reducing lr to 7.037789143724022e-05
Epoch 52: reducing lr to 6.512669412236778e-05
Epoch 55: reducing lr to 5.9586360526454605e-05
Epoch 58: reducing lr to 5.3844285804258584e-05
Epoch 61: reducing lr to 4.799100050077692e-05
Epoch 65: reducing lr to 4.017328643267688e-05
Epoch 68: reducing lr to 3.4419822240578084e-05
Epoch 71: reducing lr to 2.8861512764260448e-05
Epoch 74: reducing lr to 2.3585990741288263e-05
Epoch 77: reducing lr to 1.8676469848506295e-05
Epoch 80: reducing lr to 1.4210361633892258e-05
Epoch 83: reducing lr to 1.025811245026324e-05
Epoch 86: reducing lr to 6.8820353580544e-06
Epoch 89: reducing lr to 4.135393104821015e-06
Epoch 92: reducing lr to 2.0614882767853354e-06
Epoch 95: reducing lr to 6.930337022542838e-07
Epoch 98: reducing lr to 5.1607085176086626e-08
[I 2024-06-21 03:57:27,231] Trial 754 finished with value: 0.9754443168640137 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5466669165467929, 'bidirectional': True, 'fc_dropout': 0.2436794648480875, 'learning_rate_model': 0.0009358574072125822}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.858786107277425e-05
Epoch 40: reducing lr to 6.523019485978358e-05
Epoch 47: reducing lr to 5.7488178864691574e-05
Epoch 53: reducing lr to 4.939742283300477e-05
Epoch 65: reducing lr to 3.1346160279452106e-05
Epoch 72: reducing lr to 2.111996027810019e-05
Epoch 79: reducing lr to 1.2207613155892049e-05
Epoch 85: reducing lr to 6.195293052539176e-06
Epoch 88: reducing lr to 3.884294934237097e-06
Epoch 91: reducing lr to 2.087855750060395e-06
Epoch 94: reducing lr to 8.342972927909944e-07
Epoch 97: reducing lr to 1.4339262145795455e-07
[I 2024-06-21 03:58:06,923] Trial 755 finished with value: 0.9682924151420593 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.5837879414979642, 'bidirectional': True, 'fc_dropout': 0.2637156775667046, 'learning_rate_model': 0.0007302249551915327}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.179810786455425e-05
Epoch 40: reducing lr to 5.817722882214037e-05
Epoch 43: reducing lr to 5.5455116067542035e-05
Epoch 47: reducing lr to 5.127231251674958e-05
Epoch 50: reducing lr to 4.7784354945429956e-05
Epoch 53: reducing lr to 4.4056363430419995e-05
Epoch 56: reducing lr to 4.0147119122655384e-05
Epoch 61: reducing lr to 3.3397283816905e-05
Epoch 65: reducing lr to 2.79568801410633e-05
Epoch 72: reducing lr to 1.8836380367324057e-05
Epoch 75: reducing lr to 1.524375267101776e-05
Epoch 78: reducing lr to 1.1924277824055358e-05
Epoch 81: reducing lr to 8.930296104029745e-06
Epoch 84: reducing lr to 6.30903321225439e-06
Epoch 87: reducing lr to 4.101817310633047e-06
Epoch 90: reducing lr to 2.3434705848436686e-06
Epoch 93: reducing lr to 1.0617144024793811e-06
Epoch 96: reducing lr to 2.7676660061654555e-07
Epoch 99: reducing lr to 1.0039520097204767e-09
[I 2024-06-21 03:58:41,329] Trial 756 finished with value: 0.9737875461578369 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6008341328195289, 'bidirectional': True, 'fc_dropout': 0.18169372927362717, 'learning_rate_model': 0.0006512699280008857}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.898106694321999e-05
Epoch 43: reducing lr to 7.52855425249771e-05
Epoch 49: reducing lr to 6.64901878599542e-05
Epoch 52: reducing lr to 6.152906883769826e-05
Epoch 65: reducing lr to 3.79540976194552e-05
Epoch 72: reducing lr to 2.557216025720014e-05
Epoch 75: reducing lr to 2.069482982519358e-05
Epoch 78: reducing lr to 1.6188330110231273e-05
Epoch 81: reducing lr to 1.2123717968270205e-05
Epoch 84: reducing lr to 8.565106736304826e-06
Epoch 87: reducing lr to 5.568603286182578e-06
Epoch 90: reducing lr to 3.1814820143266254e-06
Epoch 93: reducing lr to 1.4413772878933065e-06
Epoch 96: reducing lr to 3.7573672472043924e-07
Epoch 99: reducing lr to 1.3629593999752409e-09
[I 2024-06-21 03:59:15,282] Trial 757 finished with value: 0.9693681597709656 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.626226761315553, 'bidirectional': True, 'fc_dropout': 0.20400444817558577, 'learning_rate_model': 0.0008841602603451676}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.752916886684997e-05
Epoch 65: reducing lr to 2.283998918678637e-05
Epoch 68: reducing lr to 1.9568933428021976e-05
Epoch 73: reducing lr to 1.4388436377663219e-05
Epoch 76: reducing lr to 1.1522760865518248e-05
Epoch 87: reducing lr to 3.3510700245634964e-06
Epoch 90: reducing lr to 1.9145499264334447e-06
Epoch 93: reducing lr to 8.673909731603691e-07
Epoch 96: reducing lr to 2.26110572190089e-07
Epoch 99: reducing lr to 8.202007137597493e-10
[I 2024-06-21 03:59:52,300] Trial 758 finished with value: 0.9716024994850159 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5757140500631366, 'bidirectional': True, 'fc_dropout': 0.16298175885057004, 'learning_rate_model': 0.0005320693166821158}. Best is trial 690 with value: 0.966281533241272.
Epoch 28: reducing lr to 0.00010591794888000094
Epoch 36: reducing lr to 0.00010014848336894627
Epoch 40: reducing lr to 9.5245791061143e-05
Epoch 49: reducing lr to 8.018264105090042e-05
Epoch 52: reducing lr to 7.419986917770005e-05
Epoch 55: reducing lr to 6.788767978197906e-05
Epoch 65: reducing lr to 4.5770058466995375e-05
Epoch 68: reducing lr to 3.9215046023554125e-05
Epoch 71: reducing lr to 3.288237642394226e-05
Epoch 74: reducing lr to 2.6871891027383953e-05
Epoch 77: reducing lr to 2.1278396487569846e-05
Epoch 80: reducing lr to 1.619008900131593e-05
Epoch 83: reducing lr to 1.1687229208802278e-05
Epoch 86: reducing lr to 7.84081136199666e-06
Epoch 89: reducing lr to 4.711518548746602e-06
Epoch 92: reducing lr to 2.348686088095146e-06
Epoch 95: reducing lr to 7.895842209706656e-07
Epoch 98: reducing lr to 5.8796765601415443e-08
[I 2024-06-21 04:00:27,970] Trial 759 finished with value: 0.9752142429351807 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5591157327260702, 'bidirectional': True, 'fc_dropout': 0.2976895389670301, 'learning_rate_model': 0.001066237095555351}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.452284169189255e-05
Epoch 40: reducing lr to 7.087463304780866e-05
Epoch 47: reducing lr to 6.246269216855935e-05
Epoch 53: reducing lr to 5.367183440617232e-05
Epoch 65: reducing lr to 3.4058576891262325e-05
Epoch 68: reducing lr to 2.9180837976222194e-05
Epoch 72: reducing lr to 2.2947492919686366e-05
Epoch 79: reducing lr to 1.326395091526668e-05
Epoch 85: reducing lr to 6.731378354245341e-06
Epoch 88: reducing lr to 4.220407109089392e-06
Epoch 91: reducing lr to 2.268520130291991e-06
Epoch 94: reducing lr to 9.064899255083842e-07
Epoch 97: reducing lr to 1.5580053761057303e-07
[I 2024-06-21 04:01:07,696] Trial 760 finished with value: 0.9677417278289795 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.5939039157503084, 'bidirectional': True, 'fc_dropout': 0.230374447276961, 'learning_rate_model': 0.0007934120977685533}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.584746404554558e-05
Epoch 47: reducing lr to 4.9219061955756475e-05
Epoch 50: reducing lr to 4.587078310163325e-05
Epoch 53: reducing lr to 4.229208270094686e-05
Epoch 56: reducing lr to 3.853938795519693e-05
Epoch 65: reducing lr to 2.683731917305542e-05
Epoch 72: reducing lr to 1.808205885035225e-05
Epoch 75: reducing lr to 1.4633301490116132e-05
Epoch 78: reducing lr to 1.1446758302701983e-05
Epoch 86: reducing lr to 4.597467517970539e-06
Epoch 89: reducing lr to 2.7626035735493447e-06
Epoch 92: reducing lr to 1.37715441698589e-06
Epoch 95: reducing lr to 4.629734910100358e-07
Epoch 98: reducing lr to 3.447554182012952e-08
[I 2024-06-21 04:01:43,434] Trial 761 finished with value: 0.9740046262741089 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.610647278811752, 'bidirectional': True, 'fc_dropout': 0.17985096610132106, 'learning_rate_model': 0.000625189178384045}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 8.48515351700169e-05
Epoch 40: reducing lr to 7.98799081084385e-05
Epoch 53: reducing lr to 6.0491335418757846e-05
Epoch 63: reducing lr to 4.2111874897023424e-05
Epoch 66: reducing lr to 3.6538126842345826e-05
Epoch 72: reducing lr to 2.5863183298700202e-05
Epoch 75: reducing lr to 2.093034658476646e-05
Epoch 78: reducing lr to 1.6372560813390584e-05
Epoch 81: reducing lr to 1.2261691500499319e-05
Epoch 84: reducing lr to 8.66258162259139e-06
Epoch 87: reducing lr to 5.631976573732499e-06
Epoch 90: reducing lr to 3.217688754898198e-06
Epoch 93: reducing lr to 1.4577808297940617e-06
Epoch 96: reducing lr to 3.800127828763292e-07
Epoch 99: reducing lr to 1.3784705099498078e-09
[I 2024-06-21 04:02:16,552] Trial 762 finished with value: 0.9692518711090088 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6387222072607831, 'bidirectional': True, 'fc_dropout': 0.2019004326455112, 'learning_rate_model': 0.0008942224140917088}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.94713719893106e-05
Epoch 40: reducing lr to 6.540090052058599e-05
Epoch 49: reducing lr to 5.505772877125245e-05
Epoch 52: reducing lr to 5.094963471526046e-05
Epoch 65: reducing lr to 3.142819233555166e-05
Epoch 68: reducing lr to 2.6927167020433025e-05
Epoch 72: reducing lr to 2.1175230644579143e-05
Epoch 79: reducing lr to 1.223956015030278e-05
Epoch 82: reducing lr to 9.007902504977104e-06
Epoch 85: reducing lr to 6.2115059673813265e-06
Epoch 88: reducing lr to 3.894460029327273e-06
Epoch 91: reducing lr to 2.093319612252436e-06
Epoch 94: reducing lr to 8.364806263066798e-07
Epoch 97: reducing lr to 1.4376787608126042e-07
[I 2024-06-21 04:02:51,765] Trial 763 finished with value: 0.9708982110023499 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5483575973350129, 'bidirectional': True, 'fc_dropout': 0.15606939274675127, 'learning_rate_model': 0.0007321359342063637}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 9.813591547648274e-05
Epoch 45: reducing lr to 9.013975030965086e-05
Epoch 52: reducing lr to 7.645137920387878e-05
Epoch 62: reducing lr to 5.40353515230244e-05
Epoch 65: reducing lr to 4.715889845659737e-05
Epoch 68: reducing lr to 4.0404981670039434e-05
Epoch 71: reducing lr to 3.388015446618899e-05
Epoch 74: reducing lr to 2.7687287775936708e-05
Epoch 77: reducing lr to 2.1924065052268174e-05
Epoch 80: reducing lr to 1.6681358704553407e-05
Epoch 83: reducing lr to 1.2041864790151451e-05
Epoch 86: reducing lr to 8.078731800274407e-06
Epoch 89: reducing lr to 4.854484181551484e-06
Epoch 92: reducing lr to 2.4199542767630893e-06
Epoch 95: reducing lr to 8.135432496014306e-07
Epoch 98: reducing lr to 6.05808860954871e-08
[I 2024-06-21 04:03:28,770] Trial 764 finished with value: 0.9717075824737549 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5819562487371931, 'bidirectional': True, 'fc_dropout': 0.19012615398212915, 'learning_rate_model': 0.0010985908387294433}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 8.351753252612844e-05
Epoch 43: reducing lr to 7.960974686626153e-05
Epoch 49: reducing lr to 7.030921006998136e-05
Epoch 52: reducing lr to 6.506313736745473e-05
Epoch 55: reducing lr to 5.952821055027087e-05
Epoch 58: reducing lr to 5.379173948477399e-05
Epoch 61: reducing lr to 4.794416636773879e-05
Epoch 64: reducing lr to 4.207774120582634e-05
Epoch 67: reducing lr to 3.628494207871777e-05
Epoch 70: reducing lr to 3.0657163572646674e-05
Epoch 73: reducing lr to 2.5283141528059713e-05
Epoch 76: reducing lr to 2.024762011035141e-05
Epoch 79: reducing lr to 1.5630027336346214e-05
Epoch 82: reducing lr to 1.150317173713556e-05
Epoch 85: reducing lr to 7.932148449603069e-06
Epoch 88: reducing lr to 4.9732601475214935e-06
Epoch 91: reducing lr to 2.6731877911810414e-06
Epoch 94: reducing lr to 1.0681932107808597e-06
Epoch 97: reducing lr to 1.8359285837432626e-07
[I 2024-06-21 04:04:02,174] Trial 765 finished with value: 0.9754592180252075 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.6238614656979174, 'bidirectional': True, 'fc_dropout': 0.2144076624128607, 'learning_rate_model': 0.0009349441095139149}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.809395716270326e-05
Epoch 61: reducing lr to 2.760886982347295e-05
Epoch 64: reducing lr to 2.423066177659423e-05
Epoch 67: reducing lr to 2.0894851622191685e-05
Epoch 72: reducing lr to 1.557166074816038e-05
Epoch 79: reducing lr to 9.000623491013015e-06
Epoch 86: reducing lr to 3.959184354116265e-06
Epoch 89: reducing lr to 2.37906125541272e-06
Epoch 92: reducing lr to 1.1859590523739956e-06
Epoch 95: reducing lr to 3.98697194664815e-07
Epoch 98: reducing lr to 2.9689176756636514e-08
[I 2024-06-21 04:04:38,613] Trial 766 finished with value: 0.9716856479644775 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6008051551148204, 'bidirectional': True, 'fc_dropout': 0.2513939446814501, 'learning_rate_model': 0.0005383918872174131}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.904575220492499e-05
Epoch 47: reducing lr to 5.2037752934105624e-05
Epoch 53: reducing lr to 4.471407749783902e-05
Epoch 65: reducing lr to 2.8374246258422606e-05
Epoch 72: reducing lr to 1.91175872437475e-05
Epoch 75: reducing lr to 1.5471325484371255e-05
Epoch 79: reducing lr to 1.1050215363694774e-05
Epoch 85: reducing lr to 5.6079203688329336e-06
Epoch 88: reducing lr to 3.5160268441756787e-06
Epoch 91: reducing lr to 1.8899071744717329e-06
Epoch 94: reducing lr to 7.551979772752251e-07
Epoch 97: reducing lr to 1.2979763762505048e-07
[I 2024-06-21 04:05:18,403] Trial 767 finished with value: 0.9690622091293335 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.562340767204528, 'bidirectional': True, 'fc_dropout': 0.1677439954330177, 'learning_rate_model': 0.0006609926867576239}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.633353110049314e-05
Epoch 40: reducing lr to 7.259668154306994e-05
Epoch 45: reducing lr to 6.668146637068595e-05
Epoch 48: reducing lr to 6.25671631612406e-05
Epoch 51: reducing lr to 5.810697508018643e-05
Epoch 54: reducing lr to 5.337124554136322e-05
Epoch 57: reducing lr to 4.843464528257769e-05
Epoch 60: reducing lr to 4.337504932984175e-05
Epoch 65: reducing lr to 3.4886101755437305e-05
Epoch 68: reducing lr to 2.988984789932854e-05
Epoch 71: reducing lr to 2.506306455154388e-05
Epoch 74: reducing lr to 2.0481851152065606e-05
Epoch 77: reducing lr to 1.6218469670367274e-05
Epoch 80: reducing lr to 1.2340143562123166e-05
Epoch 83: reducing lr to 8.908047773445644e-06
Epoch 86: reducing lr to 5.976294376312623e-06
Epoch 89: reducing lr to 3.59113623664537e-06
Epoch 92: reducing lr to 1.790176910522229e-06
Epoch 95: reducing lr to 6.018239084648125e-07
Epoch 98: reducing lr to 4.481510437966173e-08
[I 2024-06-21 04:06:17,690] Trial 768 finished with value: 0.9769010543823242 and parameters: {'hidden_size': 139, 'n_layers': 3, 'rnn_dropout': 0.5772057201620554, 'bidirectional': True, 'fc_dropout': 0.23667596198177818, 'learning_rate_model': 0.0008126897158715325}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010485237278412045
Epoch 40: reducing lr to 9.971940517232107e-05
Epoch 45: reducing lr to 9.159421644580237e-05
Epoch 52: reducing lr to 7.768497416872099e-05
Epoch 55: reducing lr to 7.107630658495074e-05
Epoch 60: reducing lr to 5.958032828161411e-05
Epoch 63: reducing lr to 5.257105591210307e-05
Epoch 66: reducing lr to 4.561297529140046e-05
Epoch 69: reducing lr to 3.8815788960607695e-05
Epoch 72: reducing lr to 3.228673286539048e-05
Epoch 75: reducing lr to 2.6128744522966544e-05
Epoch 78: reducing lr to 2.0438957231179985e-05
Epoch 81: reducing lr to 1.5307085496097707e-05
Epoch 84: reducing lr to 1.0814077120479778e-05
Epoch 87: reducing lr to 7.030771156053979e-06
Epoch 90: reducing lr to 4.016855004797107e-06
Epoch 93: reducing lr to 1.8198448228224587e-06
Epoch 96: reducing lr to 4.7439524610947813e-07
Epoch 99: reducing lr to 1.7208364725806309e-09
[I 2024-06-21 04:06:52,930] Trial 769 finished with value: 0.9749029278755188 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5914724807143126, 'bidirectional': True, 'fc_dropout': 0.21288163562136087, 'learning_rate_model': 0.0011163173485869615}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.4851804276836e-05
Epoch 40: reducing lr to 8.069794917951292e-05
Epoch 49: reducing lr to 6.793554466308023e-05
Epoch 52: reducing lr to 6.28665813504e-05
Epoch 55: reducing lr to 5.751851574674139e-05
Epoch 65: reducing lr to 3.877913985450397e-05
Epoch 72: reducing lr to 2.6128045222906997e-05
Epoch 75: reducing lr to 2.1144691888155116e-05
Epoch 78: reducing lr to 1.654023035008854e-05
Epoch 83: reducing lr to 9.90212184951879e-06
Epoch 86: reducing lr to 6.643205854738174e-06
Epoch 89: reducing lr to 3.991881217732876e-06
Epoch 92: reducing lr to 1.9899477810422176e-06
Epoch 95: reducing lr to 6.689831290910609e-07
Epoch 98: reducing lr to 4.981614777473326e-08
[I 2024-06-21 04:07:28,716] Trial 770 finished with value: 0.9722888469696045 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.614693411719217, 'bidirectional': True, 'fc_dropout': 0.18382857655029383, 'learning_rate_model': 0.0009033800443234474}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.0410719181825725e-05
Epoch 43: reducing lr to 5.7584101404928043e-05
Epoch 61: reducing lr to 3.467944374364846e-05
Epoch 65: reducing lr to 2.9030176747762115e-05
Epoch 72: reducing lr to 1.9559530555353876e-05
Epoch 75: reducing lr to 1.5828977772409816e-05
Epoch 78: reducing lr to 1.2382064489137996e-05
Epoch 81: reducing lr to 9.273140386256804e-06
Epoch 87: reducing lr to 4.25929076899417e-06
Epoch 90: reducing lr to 2.433439101141616e-06
Epoch 93: reducing lr to 1.102474832817622e-06
[I 2024-06-21 04:08:02,627] Trial 771 finished with value: 0.9694904088973999 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5441762796782751, 'bidirectional': True, 'fc_dropout': 0.1585500859698017, 'learning_rate_model': 0.0006762729254827694}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.27274160080486e-05
Epoch 40: reducing lr to 6.916710105334776e-05
Epoch 49: reducing lr to 5.8228303576498045e-05
Epoch 53: reducing lr to 5.237875717234557e-05
Epoch 56: reducing lr to 4.773104359863755e-05
Epoch 63: reducing lr to 3.6464192004255135e-05
Epoch 66: reducing lr to 3.163794715654656e-05
Epoch 72: reducing lr to 2.239463533626018e-05
Epoch 75: reducing lr to 1.8123348306120544e-05
Epoch 79: reducing lr to 1.294439200417505e-05
Epoch 82: reducing lr to 9.52663492216504e-06
Epoch 85: reducing lr to 6.569204055594056e-06
Epoch 88: reducing lr to 4.118727850114482e-06
Epoch 91: reducing lr to 2.2138662924380535e-06
Epoch 94: reducing lr to 8.846505101364885e-07
Epoch 97: reducing lr to 1.5204694635677102e-07
[I 2024-06-21 04:08:41,015] Trial 772 finished with value: 0.9717836380004883 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6392893505336023, 'bidirectional': True, 'fc_dropout': 0.26800078828367835, 'learning_rate_model': 0.0007742969858664109}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 0.00010696849337471406
Epoch 40: reducing lr to 9.702219264373009e-05
Epoch 43: reducing lr to 9.248252388635393e-05
Epoch 49: reducing lr to 8.167810419811071e-05
Epoch 52: reducing lr to 7.558374938454727e-05
Epoch 55: reducing lr to 6.915383317793812e-05
Epoch 58: reducing lr to 6.248978331945241e-05
Epoch 61: reducing lr to 5.569666637383632e-05
Epoch 64: reducing lr to 4.8881649035877616e-05
Epoch 67: reducing lr to 4.215216295245043e-05
Epoch 70: reducing lr to 3.561438107771111e-05
Epoch 73: reducing lr to 2.9371387704811242e-05
Epoch 76: reducing lr to 2.3521630003963474e-05
Epoch 79: reducing lr to 1.815737938353633e-05
Epoch 82: reducing lr to 1.3363217405221086e-05
Epoch 85: reducing lr to 9.214764992192346e-06
Epoch 88: reducing lr to 5.777428876377073e-06
Epoch 91: reducing lr to 3.1054382595378315e-06
Epoch 94: reducing lr to 1.2409184555911252e-06
Epoch 97: reducing lr to 2.132795490292303e-07
[I 2024-06-21 04:09:14,405] Trial 773 finished with value: 0.9774060249328613 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5710100878270407, 'bidirectional': True, 'fc_dropout': 0.1964762848774839, 'learning_rate_model': 0.0010861231739096343}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.1887601121875014e-05
Epoch 61: reducing lr to 2.97866532375315e-05
Epoch 65: reducing lr to 2.493441978486786e-05
Epoch 72: reducing lr to 1.6799950957919655e-05
Epoch 79: reducing lr to 9.710591290500734e-06
Epoch 86: reducing lr to 4.2714842082835654e-06
Epoch 89: reducing lr to 2.5667212420834663e-06
Epoch 92: reducing lr to 1.279507320395344e-06
[I 2024-06-21 04:09:50,891] Trial 774 finished with value: 0.9706563949584961 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6000356035615291, 'bidirectional': True, 'fc_dropout': 0.22693111552559078, 'learning_rate_model': 0.0005808601566446864}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 9.331687278478952e-05
Epoch 36: reducing lr to 8.94999666040858e-05
Epoch 40: reducing lr to 8.511856427967914e-05
Epoch 47: reducing lr to 7.501604523080796e-05
Epoch 52: reducing lr to 6.631039822107656e-05
Epoch 58: reducing lr to 5.482292755258369e-05
Epoch 61: reducing lr to 4.886325641303262e-05
Epoch 65: reducing lr to 4.0903452218761655e-05
Epoch 68: reducing lr to 3.504541647980735e-05
Epoch 71: reducing lr to 2.9386082472801164e-05
Epoch 74: reducing lr to 2.4014675695880206e-05
Epoch 78: reducing lr to 1.7446300365364622e-05
Epoch 84: reducing lr to 9.230688018187878e-06
Epoch 87: reducing lr to 6.001330889891885e-06
Epoch 90: reducing lr to 3.428710092455282e-06
Epoch 93: reducing lr to 1.553384501870776e-06
Epoch 96: reducing lr to 4.0493464817770813e-07
Epoch 99: reducing lr to 1.4688728803917938e-09
[I 2024-06-21 04:10:30,673] Trial 775 finished with value: 0.9680190682411194 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.629653424080862, 'bidirectional': True, 'fc_dropout': 0.1679929718984806, 'learning_rate_model': 0.0009528669954260278}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.366414993694923e-05
Epoch 43: reducing lr to 6.0685304122827574e-05
Epoch 49: reducing lr to 5.3595645805787934e-05
Epoch 65: reducing lr to 3.0593602430107816e-05
Epoch 72: reducing lr to 2.0612912788282346e-05
Epoch 75: reducing lr to 1.6681450376682625e-05
Epoch 78: reducing lr to 1.3048902923880606e-05
Epoch 81: reducing lr to 9.772547122972203e-06
Epoch 84: reducing lr to 6.904062715158016e-06
Epoch 87: reducing lr to 4.488675682309828e-06
Epoch 90: reducing lr to 2.5644924261077625e-06
Epoch 93: reducing lr to 1.1618488243280138e-06
Epoch 96: reducing lr to 3.0286953703242275e-07
Epoch 99: reducing lr to 1.0986386352637538e-09
[I 2024-06-21 04:11:04,588] Trial 776 finished with value: 0.9699015021324158 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.6126781569728774, 'bidirectional': True, 'fc_dropout': 0.149935749268562, 'learning_rate_model': 0.0007126937323266798}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 7.840240412176165e-05
Epoch 40: reducing lr to 7.380864499597164e-05
Epoch 43: reducing lr to 7.035513822002337e-05
Epoch 52: reducing lr to 5.749956761206073e-05
Epoch 55: reducing lr to 5.260807433907162e-05
Epoch 65: reducing lr to 3.546850689356632e-05
Epoch 72: reducing lr to 2.3897455064271e-05
Epoch 75: reducing lr to 1.933953803027069e-05
Epoch 78: reducing lr to 1.5128166235619926e-05
Epoch 81: reducing lr to 1.1329743066077347e-05
Epoch 84: reducing lr to 8.004183115264898e-06
Epoch 87: reducing lr to 5.203918850181261e-06
Epoch 90: reducing lr to 2.973128695834366e-06
Epoch 93: reducing lr to 1.3469823676078517e-06
Epoch 96: reducing lr to 3.5112995557247224e-07
Epoch 99: reducing lr to 1.273700019380989e-09
[I 2024-06-21 04:11:40,729] Trial 777 finished with value: 0.9727382659912109 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5852072507661816, 'bidirectional': True, 'fc_dropout': 0.24125259038977936, 'learning_rate_model': 0.0008262571436554185}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.0001136171077404632
Epoch 30: reducing lr to 0.00011230817199322188
Epoch 36: reducing lr to 0.0001071087986770866
Epoch 40: reducing lr to 0.00010186536946370892
Epoch 49: reducing lr to 8.575533117240411e-05
Epoch 52: reducing lr to 7.935675691005783e-05
Epoch 55: reducing lr to 7.260587062147365e-05
Epoch 65: reducing lr to 4.895107557165468e-05
Epoch 68: reducing lr to 4.194049004392564e-05
Epoch 71: reducing lr to 3.5167700178156655e-05
Epoch 74: reducing lr to 2.8739486303765097e-05
Epoch 77: reducing lr to 2.2757244132815718e-05
Epoch 80: reducing lr to 1.731529949402875e-05
Epoch 83: reducing lr to 1.2499491138641882e-05
Epoch 86: reducing lr to 8.385747416096386e-06
Epoch 89: reducing lr to 5.038968886248124e-06
Epoch 92: reducing lr to 2.5119196707022945e-06
Epoch 95: reducing lr to 8.444602905366937e-07
Epoch 98: reducing lr to 6.288313829441728e-08
[I 2024-06-21 04:12:16,395] Trial 778 finished with value: 0.9777870178222656 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5590806458113388, 'bidirectional': True, 'fc_dropout': 0.1778925112390838, 'learning_rate_model': 0.0011403405280652654}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.282396437335257e-05
Epoch 40: reducing lr to 8.82798382837292e-05
Epoch 45: reducing lr to 8.1086751385925e-05
Epoch 52: reducing lr to 6.877314345025732e-05
Epoch 61: reducing lr to 5.067802083673931e-05
Epoch 65: reducing lr to 4.2422592270868296e-05
Epoch 72: reducing lr to 2.858287763691734e-05
Epoch 75: reducing lr to 2.313131869427367e-05
Epoch 78: reducing lr to 1.809424991994894e-05
Epoch 81: reducing lr to 1.3551093990739143e-05
Epoch 84: reducing lr to 9.573512574949805e-06
Epoch 87: reducing lr to 6.22421824111153e-06
Epoch 90: reducing lr to 3.5560511980581322e-06
Epoch 93: reducing lr to 1.6110766643927113e-06
Epoch 96: reducing lr to 4.1997378079766927e-07
Epoch 99: reducing lr to 1.5234263105569704e-09
[I 2024-06-21 04:12:53,495] Trial 779 finished with value: 0.9716306924819946 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.5949279093932694, 'bidirectional': True, 'fc_dropout': 0.2141227384913634, 'learning_rate_model': 0.000988256145694822}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.767927452634543e-05
Epoch 53: reducing lr to 3.6106488555055226e-05
Epoch 56: reducing lr to 3.290265887264987e-05
Epoch 61: reducing lr to 2.7370816654204182e-05
Epoch 65: reducing lr to 2.2912121978532764e-05
Epoch 72: reducing lr to 1.5437396534681923e-05
Epoch 75: reducing lr to 1.24930506854353e-05
Epoch 78: reducing lr to 9.772567848490916e-06
Epoch 81: reducing lr to 7.318843612288826e-06
Epoch 84: reducing lr to 5.170581903145397e-06
Epoch 87: reducing lr to 3.3616533061155422e-06
Epoch 90: reducing lr to 1.920596419275537e-06
Epoch 93: reducing lr to 8.701303497825677e-07
Epoch 96: reducing lr to 2.2682467002445825e-07
Epoch 99: reducing lr to 8.227910550584124e-10
[I 2024-06-21 04:13:26,609] Trial 780 finished with value: 0.9714367389678955 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5716147890064309, 'bidirectional': True, 'fc_dropout': 0.19704718169506297, 'learning_rate_model': 0.0005337496872331264}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.7267812152454845e-05
Epoch 43: reducing lr to 5.458825100723901e-05
Epoch 50: reducing lr to 4.703739793463115e-05
Epoch 53: reducing lr to 4.336768175684224e-05
Epoch 56: reducing lr to 3.951954609951237e-05
Epoch 61: reducing lr to 3.287522308558063e-05
Epoch 65: reducing lr to 2.75198628862468e-05
Epoch 72: reducing lr to 1.854193323312054e-05
Epoch 75: reducing lr to 1.5005464889556592e-05
Epoch 78: reducing lr to 1.1737879515873466e-05
Epoch 81: reducing lr to 8.790699215235659e-06
Epoch 84: reducing lr to 6.210411464725567e-06
Epoch 87: reducing lr to 4.037698391361405e-06
Epoch 90: reducing lr to 2.306837846263255e-06
Epoch 93: reducing lr to 1.045117861262e-06
Epoch 96: reducing lr to 2.724402316005444e-07
Epoch 99: reducing lr to 9.882584005261805e-10
[I 2024-06-21 04:14:01,017] Trial 781 finished with value: 0.9739541411399841 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6174556808306222, 'bidirectional': True, 'fc_dropout': 0.14998150638081217, 'learning_rate_model': 0.0006410893858715999}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.527014173619551e-05
Epoch 40: reducing lr to 7.158534959073867e-05
Epoch 43: reducing lr to 6.823587081513272e-05
Epoch 53: reducing lr to 5.4210044749725516e-05
Epoch 65: reducing lr to 3.44001094394301e-05
Epoch 71: reducing lr to 2.4713915286514737e-05
Epoch 74: reducing lr to 2.019652198725126e-05
Epoch 77: reducing lr to 1.5992532943688836e-05
Epoch 80: reducing lr to 1.2168235133039849e-05
Epoch 83: reducing lr to 8.783951283706864e-06
Epoch 86: reducing lr to 5.893039641649339e-06
Epoch 89: reducing lr to 3.541108732025341e-06
Epoch 92: reducing lr to 1.7652382622058766e-06
Epoch 95: reducing lr to 5.934400025428012e-07
Epoch 98: reducing lr to 4.4190792826523555e-08
[I 2024-06-21 04:14:38,327] Trial 782 finished with value: 0.9702649116516113 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.5420427301400547, 'bidirectional': True, 'fc_dropout': 0.2634011781753853, 'learning_rate_model': 0.000801368274456827}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.298029091980137e-05
Epoch 40: reducing lr to 7.891805432563308e-05
Epoch 43: reducing lr to 7.522547826800322e-05
Epoch 61: reducing lr to 4.53037848648448e-05
Epoch 65: reducing lr to 3.792381710879978e-05
Epoch 72: reducing lr to 2.555175829483722e-05
Epoch 75: reducing lr to 2.0678319091061077e-05
Epoch 78: reducing lr to 1.617541474843526e-05
Epoch 81: reducing lr to 1.2114045432387447e-05
Epoch 84: reducing lr to 8.558273329056049e-06
Epoch 87: reducing lr to 5.564160547144657e-06
Epoch 90: reducing lr to 3.178943766651745e-06
Epoch 93: reducing lr to 1.4402273293101205e-06
Epoch 96: reducing lr to 3.754369547190285e-07
Epoch 99: reducing lr to 1.361872004686411e-09
[I 2024-06-21 04:15:15,446] Trial 783 finished with value: 0.9709923267364502 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6034021899701159, 'bidirectional': True, 'fc_dropout': 0.22614810872092356, 'learning_rate_model': 0.0008834548602470564}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 0.00011383879887308669
Epoch 40: reducing lr to 0.00010325367335879789
Epoch 43: reducing lr to 9.842243359541218e-05
Epoch 49: reducing lr to 8.692407439611282e-05
Epoch 52: reducing lr to 8.043829517276557e-05
Epoch 55: reducing lr to 7.359540232906765e-05
Epoch 58: reducing lr to 6.650333804371888e-05
Epoch 61: reducing lr to 5.9273917031081245e-05
Epoch 64: reducing lr to 5.20211890213973e-05
Epoch 67: reducing lr to 4.4859485714172874e-05
Epoch 70: reducing lr to 3.7901799273667286e-05
Epoch 73: reducing lr to 3.125783482654757e-05
Epoch 76: reducing lr to 2.5032362546309624e-05
Epoch 79: reducing lr to 1.9323580191635564e-05
Epoch 82: reducing lr to 1.4221501775866823e-05
Epoch 85: reducing lr to 9.806605155541201e-06
Epoch 88: reducing lr to 6.148497965260914e-06
Epoch 91: reducing lr to 3.304892406046393e-06
Epoch 94: reducing lr to 1.3206193901328134e-06
Epoch 97: reducing lr to 2.2697793452724006e-07
[I 2024-06-21 04:15:48,871] Trial 784 finished with value: 0.9776873588562012 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.6306677225635244, 'bidirectional': True, 'fc_dropout': 0.1790561336631744, 'learning_rate_model': 0.0011558820139026564}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.421104113174718e-05
Epoch 40: reducing lr to 6.0448783335196155e-05
Epoch 43: reducing lr to 5.762038453641965e-05
Epoch 49: reducing lr to 5.088879038253975e-05
Epoch 52: reducing lr to 4.7091758758592896e-05
Epoch 61: reducing lr to 3.4701294893301e-05
Epoch 65: reducing lr to 2.9048468354203216e-05
Epoch 68: reducing lr to 2.4888257991745022e-05
Epoch 72: reducing lr to 1.957185480808578e-05
Epoch 75: reducing lr to 1.5838951443403745e-05
Epoch 79: reducing lr to 1.1312787954821331e-05
Epoch 82: reducing lr to 8.325829499190699e-06
Epoch 85: reducing lr to 5.741174439781969e-06
Epoch 88: reducing lr to 3.5995738383798803e-06
Epoch 91: reducing lr to 1.9348146995703024e-06
Epoch 94: reducing lr to 7.731428121205341e-07
Epoch 97: reducing lr to 1.3288185824080053e-07
[I 2024-06-21 04:16:24,091] Trial 785 finished with value: 0.971634566783905 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5863306570660636, 'bidirectional': True, 'fc_dropout': 0.2070871054375607, 'learning_rate_model': 0.0006766990378797825}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.72182793499309e-05
Epoch 40: reducing lr to 7.343811761951273e-05
Epoch 43: reducing lr to 7.000194782089747e-05
Epoch 52: reducing lr to 5.72109135670504e-05
Epoch 55: reducing lr to 5.234397611905315e-05
Epoch 61: reducing lr to 4.215796638606351e-05
Epoch 65: reducing lr to 3.5290451154880086e-05
Epoch 72: reducing lr to 2.3777487256577306e-05
Epoch 75: reducing lr to 1.9242451458790155e-05
Epoch 78: reducing lr to 1.5052221205790102e-05
Epoch 81: reducing lr to 1.1272866531161199e-05
Epoch 84: reducing lr to 7.96400125078876e-06
Epoch 87: reducing lr to 5.177794615019273e-06
Epoch 90: reducing lr to 2.9582032683915256e-06
Epoch 93: reducing lr to 1.3402203705161386e-06
Epoch 96: reducing lr to 3.4936724523900244e-07
Epoch 99: reducing lr to 1.2673059076001406e-09
[I 2024-06-21 04:16:58,511] Trial 786 finished with value: 0.9727407097816467 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5665601788894585, 'bidirectional': True, 'fc_dropout': 0.13684426166806768, 'learning_rate_model': 0.0008221092434773867}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.0573912164911684e-05
Epoch 43: reducing lr to 4.8207558625197364e-05
Epoch 53: reducing lr to 3.829853534700409e-05
Epoch 56: reducing lr to 3.4900199223836465e-05
Epoch 61: reducing lr to 2.9032515513354765e-05
Epoch 66: reducing lr to 2.3133176556571308e-05
Epoch 72: reducing lr to 1.6374610229619148e-05
Epoch 75: reducing lr to 1.3251511360305576e-05
Epoch 78: reducing lr to 1.0365866362377775e-05
Epoch 81: reducing lr to 7.763175041434318e-06
Epoch 84: reducing lr to 5.4844910626034396e-06
Epoch 87: reducing lr to 3.5657413146760756e-06
Epoch 90: reducing lr to 2.037196991305182e-06
Epoch 93: reducing lr to 9.229564904057373e-07
Epoch 96: reducing lr to 2.4059533314235975e-07
Epoch 99: reducing lr to 8.727431984240953e-10
[I 2024-06-21 04:17:35,829] Trial 787 finished with value: 0.9740232229232788 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.6426677424269053, 'bidirectional': True, 'fc_dropout': 0.16217580691886774, 'learning_rate_model': 0.000566153954067855}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.158671653817255e-05
Epoch 40: reducing lr to 8.71031589688136e-05
Epoch 49: reducing lr to 7.332776863087229e-05
Epoch 52: reducing lr to 6.7856468285317e-05
Epoch 55: reducing lr to 6.208391256131874e-05
Epoch 58: reducing lr to 5.610115976648904e-05
Epoch 63: reducing lr to 4.591989926491574e-05
Epoch 66: reducing lr to 3.984213735512943e-05
Epoch 72: reducing lr to 2.8201897318761012e-05
Epoch 75: reducing lr to 2.2823002041644636e-05
Epoch 83: reducing lr to 1.0688079466165229e-05
Epoch 86: reducing lr to 7.170494684327297e-06
Epoch 89: reducing lr to 4.308727394290214e-06
Epoch 92: reducing lr to 2.1478952027167724e-06
Epoch 95: reducing lr to 7.220820904760457e-07
Epoch 98: reducing lr to 5.377018725944392e-08
[I 2024-06-21 04:18:11,643] Trial 788 finished with value: 0.9723924398422241 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6157962284757864, 'bidirectional': True, 'fc_dropout': 0.24115602389804816, 'learning_rate_model': 0.0009750837091896721}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.611595100163782e-05
Epoch 65: reducing lr to 3.17718075437406e-05
Epoch 72: reducing lr to 2.1406746705340773e-05
Epoch 75: reducing lr to 1.732387783129572e-05
Epoch 78: reducing lr to 1.3551435575513801e-05
Epoch 81: reducing lr to 1.0148902441696353e-05
Epoch 84: reducing lr to 7.1699484347107825e-06
Epoch 87: reducing lr to 4.661541256229028e-06
Epoch 90: reducing lr to 2.663254842113377e-06
Epoch 93: reducing lr to 1.2065933498940597e-06
Epoch 96: reducing lr to 3.145334931850346e-07
Epoch 99: reducing lr to 1.1409488424678726e-09
[I 2024-06-21 04:18:45,788] Trial 789 finished with value: 0.9693065881729126 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5503734477488141, 'bidirectional': True, 'fc_dropout': 0.19285867409759616, 'learning_rate_model': 0.0007401406275329457}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010741318214198845
Epoch 39: reducing lr to 0.00010359391804463538
Epoch 45: reducing lr to 9.38312218694557e-05
Epoch 52: reducing lr to 7.958227418716295e-05
Epoch 61: reducing lr to 5.864312647580535e-05
Epoch 65: reducing lr to 4.9090185506386496e-05
Epoch 68: reducing lr to 4.205967718669006e-05
Epoch 71: reducing lr to 3.5267640300397494e-05
Epoch 74: reducing lr to 2.8821158626941942e-05
Epoch 77: reducing lr to 2.2821916026313922e-05
Epoch 80: reducing lr to 1.7364506383853887e-05
Epoch 83: reducing lr to 1.2535012388709891e-05
Epoch 86: reducing lr to 8.409578164698082e-06
Epoch 89: reducing lr to 5.053288707103885e-06
Epoch 92: reducing lr to 2.5190580834411837e-06
Epoch 95: reducing lr to 8.468600910421873e-07
Epoch 98: reducing lr to 6.306184058362495e-08
[I 2024-06-21 04:19:22,938] Trial 790 finished with value: 0.974917471408844 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.5979350315251424, 'bidirectional': True, 'fc_dropout': 0.14845278164860395, 'learning_rate_model': 0.0011435811656728904}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.666350419901593e-05
Epoch 53: reducing lr to 3.533726754524264e-05
Epoch 56: reducing lr to 3.2201692993762274e-05
Epoch 65: reducing lr to 2.2423996815699423e-05
Epoch 68: reducing lr to 1.921251789079068e-05
Epoch 73: reducing lr to 1.4126374967912616e-05
Epoch 76: reducing lr to 1.131289296344901e-05
Epoch 79: reducing lr to 8.732918995327688e-06
Epoch 82: reducing lr to 6.427133159015385e-06
Epoch 85: reducing lr to 4.431905868022043e-06
Epoch 88: reducing lr to 2.7786949489206355e-06
Epoch 91: reducing lr to 1.4935823167370197e-06
Epoch 94: reducing lr to 5.968284367242069e-07
Epoch 97: reducing lr to 1.025782953415085e-07
[I 2024-06-21 04:19:55,732] Trial 791 finished with value: 0.972756564617157 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5783710498068197, 'bidirectional': True, 'fc_dropout': 0.25274525369605455, 'learning_rate_model': 0.0005223785600526314}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 8.792098416752514e-05
Epoch 52: reducing lr to 6.849358329143491e-05
Epoch 61: reducing lr to 5.047201664901198e-05
Epoch 64: reducing lr to 4.4296284941192275e-05
Epoch 71: reducing lr to 3.0353581662250265e-05
Epoch 76: reducing lr to 2.131517339302785e-05
Epoch 79: reducing lr to 1.6454118607335056e-05
Epoch 82: reducing lr to 1.2109675053685424e-05
Epoch 85: reducing lr to 8.35037000205711e-06
Epoch 88: reducing lr to 5.235474677780106e-06
Epoch 91: reducing lr to 2.81413128904067e-06
Epoch 94: reducing lr to 1.124513566580054e-06
Epoch 97: reducing lr to 1.9327276927572425e-07
[I 2024-06-21 04:20:32,187] Trial 792 finished with value: 0.966809093952179 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6110744675666963, 'bidirectional': True, 'fc_dropout': 0.2143632582990733, 'learning_rate_model': 0.0009842389228199144}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010722250374752256
Epoch 40: reducing lr to 0.00010197350818949909
Epoch 45: reducing lr to 9.366465398290719e-05
Epoch 54: reducing lr to 7.496834605404224e-05
Epoch 60: reducing lr to 6.092710925681109e-05
Epoch 63: reducing lr to 5.3759396090656366e-05
Epoch 66: reducing lr to 4.664403183500035e-05
Epoch 69: reducing lr to 3.969319879732959e-05
Epoch 72: reducing lr to 3.3016556933644124e-05
Epoch 75: reducing lr to 2.671937060785458e-05
Epoch 78: reducing lr to 2.0900968763270057e-05
Epoch 81: reducing lr to 1.565309385366193e-05
Epoch 84: reducing lr to 1.1058523462925836e-05
Epoch 87: reducing lr to 7.189697921095961e-06
Epoch 90: reducing lr to 4.10765383146714e-06
Epoch 93: reducing lr to 1.8609814270654535e-06
Epoch 96: reducing lr to 4.851186931029843e-07
Epoch 99: reducing lr to 1.7597350468077574e-09
[I 2024-06-21 04:21:07,425] Trial 793 finished with value: 0.9738744497299194 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.6269082583049176, 'bidirectional': True, 'fc_dropout': 0.21444839722555425, 'learning_rate_model': 0.0011415510962134111}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.904892034491176e-05
Epoch 43: reducing lr to 5.628602114069642e-05
Epoch 65: reducing lr to 2.8375768697900616e-05
Epoch 72: reducing lr to 1.911861301089138e-05
Epoch 75: reducing lr to 1.547215560886091e-05
Epoch 78: reducing lr to 1.2102943809095255e-05
Epoch 81: reducing lr to 9.06410212345222e-06
Epoch 84: reducing lr to 6.403563853870514e-06
Epoch 87: reducing lr to 4.163276397805562e-06
Epoch 90: reducing lr to 2.37858369497336e-06
Epoch 93: reducing lr to 1.0776224727498756e-06
Epoch 96: reducing lr to 2.80913499745765e-07
Epoch 99: reducing lr to 1.018994604114979e-09
[I 2024-06-21 04:21:38,911] Trial 794 finished with value: 0.971346378326416 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.6510181974461557, 'bidirectional': True, 'fc_dropout': 0.2264379145362702, 'learning_rate_model': 0.0006610281527697192}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 9.361561869660094e-05
Epoch 36: reducing lr to 8.978649302028806e-05
Epoch 40: reducing lr to 8.539106401460336e-05
Epoch 49: reducing lr to 7.188644199975248e-05
Epoch 52: reducing lr to 6.652268523614637e-05
Epoch 61: reducing lr to 4.901968790988369e-05
Epoch 65: reducing lr to 4.1034401089685675e-05
Epoch 68: reducing lr to 3.5157611355060145e-05
Epoch 71: reducing lr to 2.9480159478822877e-05
Epoch 74: reducing lr to 2.4091556606840066e-05
Epoch 77: reducing lr to 1.9076800101663026e-05
Epoch 84: reducing lr to 9.260239268956847e-06
Epoch 87: reducing lr to 6.0205436326175955e-06
Epoch 90: reducing lr to 3.4396868118023663e-06
Epoch 93: reducing lr to 1.5583575282437683e-06
Epoch 96: reducing lr to 4.062310114942612e-07
Epoch 99: reducing lr to 1.4735753500988261e-09
[I 2024-06-21 04:22:11,531] Trial 795 finished with value: 0.970208466053009 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.6163117066928195, 'bidirectional': True, 'fc_dropout': 0.20049242690884975, 'learning_rate_model': 0.0009559175168471651}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.470227664719252e-05
Epoch 40: reducing lr to 7.104528390228644e-05
Epoch 43: reducing lr to 6.772107480226698e-05
Epoch 52: reducing lr to 5.53468107357956e-05
Epoch 55: reducing lr to 5.063845267957328e-05
Epoch 61: reducing lr to 4.078433363663882e-05
Epoch 65: reducing lr to 3.41405826103589e-05
Epoch 72: reducing lr to 2.3002745541202248e-05
Epoch 75: reducing lr to 1.8615474785841648e-05
Epoch 78: reducing lr to 1.4561774778405181e-05
Epoch 81: reducing lr to 1.0905562792994744e-05
Epoch 84: reducing lr to 7.704510248913511e-06
Epoch 87: reducing lr to 5.009086566157207e-06
Epoch 90: reducing lr to 2.8618161501964365e-06
Epoch 93: reducing lr to 1.2965519787458175e-06
Epoch 96: reducing lr to 3.3798381451936257e-07
Epoch 99: reducing lr to 1.2260132873113602e-09
[I 2024-06-21 04:22:45,952] Trial 796 finished with value: 0.9728188514709473 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6054377172442963, 'bidirectional': True, 'fc_dropout': 0.17398597813169966, 'learning_rate_model': 0.0007953224632493296}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.17142020084288e-05
Epoch 40: reducing lr to 8.722440348550413e-05
Epoch 43: reducing lr to 8.314317331955711e-05
Epoch 49: reducing lr to 7.342983829140926e-05
Epoch 52: reducing lr to 6.79509220892775e-05
Epoch 55: reducing lr to 6.217033117187051e-05
Epoch 61: reducing lr to 5.0072136778316325e-05
Epoch 65: reducing lr to 4.1915406474155754e-05
Epoch 71: reducing lr to 3.0113096198896215e-05
Epoch 74: reducing lr to 2.460880044438268e-05
Epoch 77: reducing lr to 1.9486377508953344e-05
Epoch 80: reducing lr to 1.4826595902914598e-05
Epoch 83: reducing lr to 1.0702956894774787e-05
Epoch 86: reducing lr to 7.180475759327627e-06
Epoch 89: reducing lr to 4.314724990435481e-06
Epoch 92: reducing lr to 2.1508849968739324e-06
Epoch 95: reducing lr to 7.230872032079682e-07
Epoch 98: reducing lr to 5.384503345840907e-08
[I 2024-06-21 04:23:21,638] Trial 797 finished with value: 0.9727581143379211 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5931908580514144, 'bidirectional': True, 'fc_dropout': 0.21709547308563346, 'learning_rate_model': 0.0009764409912268924}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.6001101279225954e-05
Epoch 49: reducing lr to 4.714451055842183e-05
Epoch 61: reducing lr to 3.2148053651702667e-05
Epoch 65: reducing lr to 2.6911149051414498e-05
Epoch 68: reducing lr to 2.3057037372126877e-05
Epoch 72: reducing lr to 1.813180287272627e-05
Epoch 75: reducing lr to 1.4673557927878802e-05
Epoch 78: reducing lr to 1.1478248511081014e-05
Epoch 81: reducing lr to 8.596257104376248e-06
Epoch 84: reducing lr to 6.0730429249837114e-06
Epoch 87: reducing lr to 3.948388249015819e-06
Epoch 90: reducing lr to 2.2558127333279335e-06
Epoch 93: reducing lr to 1.0220008238039938e-06
Epoch 96: reducing lr to 2.664141064404827e-07
Epoch 99: reducing lr to 9.663990415851795e-10
[I 2024-06-21 04:24:14,548] Trial 798 finished with value: 0.9755878448486328 and parameters: {'hidden_size': 130, 'n_layers': 3, 'rnn_dropout': 0.6342231430278141, 'bidirectional': True, 'fc_dropout': 0.23549203679019984, 'learning_rate_model': 0.0006269090834421422}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.230827911231242e-05
Epoch 40: reducing lr to 6.87684826833597e-05
Epoch 49: reducing lr to 5.789272681955213e-05
Epoch 52: reducing lr to 5.357310136023335e-05
Epoch 55: reducing lr to 4.901563291656145e-05
Epoch 64: reducing lr to 3.464688586869459e-05
Epoch 67: reducing lr to 2.9877084912995626e-05
Epoch 72: reducing lr to 2.226557234390877e-05
Epoch 75: reducing lr to 1.801890125758891e-05
Epoch 78: reducing lr to 1.4095110916370255e-05
Epoch 85: reducing lr to 6.531344937995068e-06
Epoch 88: reducing lr to 4.094991123303117e-06
Epoch 91: reducing lr to 2.2011075132001806e-06
Epoch 94: reducing lr to 8.7955216223713e-07
Epoch 97: reducing lr to 1.5117068141295162e-07
[I 2024-06-21 04:24:50,312] Trial 799 finished with value: 0.9729123115539551 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6113841918476105, 'bidirectional': True, 'fc_dropout': 0.27676945013282495, 'learning_rate_model': 0.0007698346186760515}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 0.00010827931223443129
Epoch 45: reducing lr to 9.94566578517455e-05
Epoch 52: reducing lr to 8.435344714905583e-05
Epoch 56: reducing lr to 7.472171732491569e-05
Epoch 59: reducing lr to 6.722327156062936e-05
Epoch 65: reducing lr to 5.203327513500832e-05
Epoch 68: reducing lr to 4.4581268792719445e-05
Epoch 71: reducing lr to 3.738203089239396e-05
Epoch 74: reducing lr to 3.054906517618042e-05
Epoch 77: reducing lr to 2.419015172698334e-05
Epoch 80: reducing lr to 1.8405555589866957e-05
Epoch 83: reducing lr to 1.3286520344430656e-05
Epoch 86: reducing lr to 8.913755161022276e-06
Epoch 89: reducing lr to 5.35624705673924e-06
Epoch 92: reducing lr to 2.670082440811095e-06
Epoch 95: reducing lr to 8.97631648027141e-07
Epoch 98: reducing lr to 6.684256878967172e-08
[I 2024-06-21 04:25:24,445] Trial 800 finished with value: 0.9725633859634399 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.6321993929070745, 'bidirectional': True, 'fc_dropout': 0.1892732234614597, 'learning_rate_model': 0.0012121419550335513}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.157914194355905e-05
Epoch 40: reducing lr to 8.709595518267864e-05
Epoch 52: reducing lr to 6.785085627891921e-05
Epoch 55: reducing lr to 6.20787779687987e-05
Epoch 63: reducing lr to 4.591610150215307e-05
Epoch 66: reducing lr to 3.983884224803959e-05
Epoch 71: reducing lr to 3.006875108508628e-05
Epoch 74: reducing lr to 2.4572561060387617e-05
Epoch 77: reducing lr to 1.945768150165239e-05
Epoch 80: reducing lr to 1.4804761977954311e-05
Epoch 83: reducing lr to 1.0687195518446464e-05
Epoch 86: reducing lr to 7.169901655201848e-06
Epoch 89: reducing lr to 4.308371044979477e-06
Epoch 92: reducing lr to 2.1477175630322458e-06
Epoch 95: reducing lr to 7.220223713451466e-07
Epoch 98: reducing lr to 5.3765740245885746e-08
[I 2024-06-21 04:26:02,859] Trial 801 finished with value: 0.9718605875968933 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.5895374996298138, 'bidirectional': True, 'fc_dropout': 0.25569020158368644, 'learning_rate_model': 0.0009750030657940958}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.204155916431961e-05
Epoch 40: reducing lr to 6.851481980616191e-05
Epoch 43: reducing lr to 6.530901113068266e-05
Epoch 49: reducing lr to 5.767918080136396e-05
Epoch 52: reducing lr to 5.33754889293467e-05
Epoch 55: reducing lr to 4.8834831392547404e-05
Epoch 65: reducing lr to 3.292457623004649e-05
Epoch 72: reducing lr to 2.2183442436095963e-05
Epoch 75: reducing lr to 1.7952435833915277e-05
Epoch 78: reducing lr to 1.4043118982711756e-05
Epoch 83: reducing lr to 8.407178882690572e-06
Epoch 86: reducing lr to 5.640267896525163e-06
Epoch 89: reducing lr to 3.3892189962865203e-06
Epoch 92: reducing lr to 1.6895214194165936e-06
Epoch 95: reducing lr to 5.679854198162478e-07
Epoch 98: reducing lr to 4.2295305183409715e-08
[I 2024-06-21 04:26:38,519] Trial 802 finished with value: 0.9735686182975769 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6065091615115795, 'bidirectional': True, 'fc_dropout': 0.20431194183337326, 'learning_rate_model': 0.00076699496805821}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.709802103376642e-05
Epoch 43: reducing lr to 5.442640441563028e-05
Epoch 53: reducing lr to 4.323910259651366e-05
Epoch 56: reducing lr to 3.940237612758413e-05
Epoch 61: reducing lr to 3.277775260967049e-05
Epoch 65: reducing lr to 2.743827031041784e-05
Epoch 72: reducing lr to 1.8486958973270753e-05
Epoch 75: reducing lr to 1.496097576775715e-05
Epoch 78: reducing lr to 1.1703078331418854e-05
Epoch 81: reducing lr to 8.764636011531726e-06
Epoch 84: reducing lr to 6.191998456257493e-06
Epoch 87: reducing lr to 4.025727175751301e-06
Epoch 90: reducing lr to 2.2999983920597778e-06
Epoch 93: reducing lr to 1.0420192317848954e-06
Epoch 96: reducing lr to 2.716324841074767e-07
Epoch 99: reducing lr to 9.85328351462561e-10
[I 2024-06-21 04:27:15,852] Trial 803 finished with value: 0.9725511074066162 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.6460670341767442, 'bidirectional': True, 'fc_dropout': 0.1657947894534441, 'learning_rate_model': 0.0006391886447761197}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.262073089304031e-05
Epoch 40: reducing lr to 7.857609628462452e-05
Epoch 53: reducing lr to 5.950398678222452e-05
Epoch 63: reducing lr to 4.1424518567830124e-05
Epoch 66: reducing lr to 3.5941746063685007e-05
Epoch 72: reducing lr to 2.5441040547352296e-05
Epoch 75: reducing lr to 2.0588718333057682e-05
Epoch 78: reducing lr to 1.6105325423664875e-05
Epoch 81: reducing lr to 1.2061554335386301e-05
Epoch 84: reducing lr to 8.521189667946718e-06
Epoch 87: reducing lr to 5.540050608591075e-06
Epoch 90: reducing lr to 3.1651691571253224e-06
Epoch 93: reducing lr to 1.4339867127573345e-06
Epoch 96: reducing lr to 3.738101573194392e-07
Epoch 99: reducing lr to 1.3559709078226484e-09
[I 2024-06-21 04:27:48,977] Trial 804 finished with value: 0.9697309136390686 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5854473064693695, 'bidirectional': True, 'fc_dropout': 0.22296159019613054, 'learning_rate_model': 0.0008796267819206062}. Best is trial 690 with value: 0.966281533241272.
Epoch 53: reducing lr to 3.5524923736489244e-05
Epoch 56: reducing lr to 3.237269792647716e-05
Epoch 61: reducing lr to 2.69299567240774e-05
Epoch 65: reducing lr to 2.2543077948092382e-05
Epoch 68: reducing lr to 1.931454467956339e-05
Epoch 73: reducing lr to 1.4201392135530517e-05
Epoch 76: reducing lr to 1.1372969323421766e-05
Epoch 79: reducing lr to 8.779294576434245e-06
Epoch 82: reducing lr to 6.461264018955579e-06
Epoch 85: reducing lr to 4.455441207139359e-06
Epoch 88: reducing lr to 2.7934510222385033e-06
Epoch 91: reducing lr to 1.5015138855407916e-06
Epoch 94: reducing lr to 5.999978541422393e-07
Epoch 97: reducing lr to 1.0312303050485482e-07
[I 2024-06-21 04:28:21,545] Trial 805 finished with value: 0.9729570746421814 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.6193845673214888, 'bidirectional': True, 'fc_dropout': 0.18433189603462166, 'learning_rate_model': 0.0005251526163896943}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00010991889319527835
Epoch 40: reducing lr to 0.0001045378979567839
Epoch 43: reducing lr to 9.964656931964598e-05
Epoch 52: reducing lr to 8.14387519499493e-05
Epoch 55: reducing lr to 7.451075016024107e-05
Epoch 58: reducing lr to 6.733047784209821e-05
Epoch 61: reducing lr to 6.00111404130115e-05
Epoch 64: reducing lr to 5.266820610451462e-05
Epoch 67: reducing lr to 4.54174286244164e-05
Epoch 70: reducing lr to 3.8373205484718305e-05
Epoch 73: reducing lr to 3.164660627707548e-05
Epoch 76: reducing lr to 2.534370426115559e-05
Epoch 79: reducing lr to 1.956391853695544e-05
Epoch 82: reducing lr to 1.4398382673240853e-05
Epoch 85: reducing lr to 9.928575475373972e-06
Epoch 88: reducing lr to 6.224970327655365e-06
Epoch 91: reducing lr to 3.345997230538118e-06
Epoch 94: reducing lr to 1.3370446837830587e-06
Epoch 97: reducing lr to 2.2980098805392822e-07
[I 2024-06-21 04:28:57,642] Trial 806 finished with value: 0.9777927994728088 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5588770481206692, 'bidirectional': True, 'fc_dropout': 0.24217264706255265, 'learning_rate_model': 0.001170258375211035}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.657428596503761e-05
Epoch 61: reducing lr to 3.821770765464859e-05
Epoch 65: reducing lr to 3.199205893583424e-05
Epoch 72: reducing lr to 2.1555144487102363e-05
Epoch 77: reducing lr to 1.4873035720093346e-05
Epoch 80: reducing lr to 1.1316443519074717e-05
Epoch 83: reducing lr to 8.169063754074571e-06
Epoch 86: reducing lr to 5.480519527381383e-06
[I 2024-06-21 04:29:34,084] Trial 807 finished with value: 0.9672282934188843 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6059135622104403, 'bidirectional': True, 'fc_dropout': 0.15379449283449287, 'learning_rate_model': 0.0007452714971989149}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.173715011362452e-05
Epoch 40: reducing lr to 5.87148552092404e-05
Epoch 43: reducing lr to 5.596758691397499e-05
Epoch 49: reducing lr to 4.942908350223971e-05
Epoch 52: reducing lr to 4.5740966889723226e-05
Epoch 55: reducing lr to 4.184977881417728e-05
Epoch 65: reducing lr to 2.8215234771717495e-05
Epoch 72: reducing lr to 1.9010450795358483e-05
Epoch 75: reducing lr to 1.538462297096658e-05
Epoch 78: reducing lr to 1.2034472251241337e-05
Epoch 81: reducing lr to 9.012822599831411e-06
Epoch 84: reducing lr to 6.367336139373318e-06
Epoch 87: reducing lr to 4.139722952856086e-06
Epoch 90: reducing lr to 2.3651270241295022e-06
Epoch 93: reducing lr to 1.0715258990029084e-06
Epoch 96: reducing lr to 2.7932425127422676e-07
Epoch 99: reducing lr to 1.0132297134342673e-09
[I 2024-06-21 04:30:09,780] Trial 808 finished with value: 0.9747275710105896 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.540988553304378, 'bidirectional': True, 'fc_dropout': 0.15490982924359778, 'learning_rate_model': 0.0006572884322422697}. Best is trial 690 with value: 0.966281533241272.
Epoch 53: reducing lr to 3.444791139872165e-05
Epoch 56: reducing lr to 3.139125190474152e-05
Epoch 62: reducing lr to 2.504709037191964e-05
Epoch 65: reducing lr to 2.1859637407545516e-05
Epoch 72: reducing lr to 1.4728268777585151e-05
Epoch 75: reducing lr to 1.191917354287724e-05
Epoch 78: reducing lr to 9.323658014251233e-06
Epoch 81: reducing lr to 6.982647340873207e-06
Epoch 84: reducing lr to 4.933067556757702e-06
Epoch 87: reducing lr to 3.2072333776161345e-06
Epoch 90: reducing lr to 1.8323724607842797e-06
Epoch 93: reducing lr to 8.301602951210283e-07
Epoch 96: reducing lr to 2.1640531795642966e-07
Epoch 99: reducing lr to 7.849955644713413e-10
[I 2024-06-21 04:30:42,946] Trial 809 finished with value: 0.9719007015228271 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6011506849705075, 'bidirectional': True, 'fc_dropout': 0.13548363635468957, 'learning_rate_model': 0.0005092315168467925}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.416973420289481e-05
Epoch 43: reducing lr to 6.116723209907498e-05
Epoch 49: reducing lr to 5.402127177062743e-05
Epoch 52: reducing lr to 4.9990512231307e-05
Epoch 55: reducing lr to 4.573781495986826e-05
Epoch 65: reducing lr to 3.083655932252805e-05
Epoch 68: reducing lr to 2.6420265421181088e-05
Epoch 72: reducing lr to 2.0776608752045082e-05
Epoch 75: reducing lr to 1.6813924914581203e-05
Epoch 78: reducing lr to 1.3152529847552703e-05
Epoch 81: reducing lr to 9.850155102792552e-06
Epoch 84: reducing lr to 6.9588908324476054e-06
Epoch 87: reducing lr to 4.524322177270582e-06
Epoch 90: reducing lr to 2.584858158188701e-06
Epoch 93: reducing lr to 1.1710755631687818e-06
Epoch 96: reducing lr to 3.052747536686222e-07
Epoch 99: reducing lr to 1.1073633949263209e-09
[I 2024-06-21 04:31:16,383] Trial 810 finished with value: 0.9744886159896851 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5782258134137502, 'bidirectional': True, 'fc_dropout': 0.1614055789161712, 'learning_rate_model': 0.0007183535383220362}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.358345867968584e-05
Epoch 47: reducing lr to 4.722376597813539e-05
Epoch 61: reducing lr to 3.076017908807352e-05
Epoch 65: reducing lr to 2.5749358678313237e-05
Epoch 72: reducing lr to 1.7349028640966144e-05
Epoch 75: reducing lr to 1.4040080765414153e-05
Epoch 78: reducing lr to 1.0982717138757953e-05
Epoch 86: reducing lr to 4.411090368928235e-06
Epoch 89: reducing lr to 2.650610138911762e-06
Epoch 92: reducing lr to 1.3213258302638269e-06
[I 2024-06-21 04:31:52,853] Trial 811 finished with value: 0.9696090817451477 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5614856494636148, 'bidirectional': True, 'fc_dropout': 0.1752901743445428, 'learning_rate_model': 0.0005998445780744484}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.016079868736533e-05
Epoch 43: reducing lr to 6.68779746538665e-05
Epoch 65: reducing lr to 3.371554608591925e-05
Epoch 72: reducing lr to 2.2716370609380327e-05
Epoch 75: reducing lr to 1.838371961065705e-05
Epoch 78: reducing lr to 1.4380486538186099e-05
Epoch 81: reducing lr to 1.0769792921710053e-05
Epoch 84: reducing lr to 7.608592194553352e-06
Epoch 87: reducing lr to 4.946725452728204e-06
Epoch 90: reducing lr to 2.8261876899575757e-06
Epoch 93: reducing lr to 1.2804104279969257e-06
Epoch 96: reducing lr to 3.3377605194308096e-07
Epoch 99: reducing lr to 1.210749914898595e-09
[I 2024-06-21 04:32:26,777] Trial 812 finished with value: 0.9689123034477234 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5984609951124708, 'bidirectional': True, 'fc_dropout': 0.15460062811605582, 'learning_rate_model': 0.0007854210184073865}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 9.034063814514349e-05
Epoch 52: reducing lr to 7.037858006236129e-05
Epoch 68: reducing lr to 3.71954733452191e-05
Epoch 71: reducing lr to 3.118893587603027e-05
Epoch 76: reducing lr to 2.19017835700226e-05
Epoch 79: reducing lr to 1.6906948769706554e-05
Epoch 82: reducing lr to 1.2442942744996574e-05
Epoch 85: reducing lr to 8.580178689725606e-06
Epoch 88: reducing lr to 5.379559019518963e-06
Epoch 91: reducing lr to 2.8915783744155748e-06
Epoch 94: reducing lr to 1.1554610559652523e-06
Epoch 97: reducing lr to 1.985917864520204e-07
[I 2024-06-21 04:33:03,230] Trial 813 finished with value: 0.9675768613815308 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5849756236885186, 'bidirectional': True, 'fc_dropout': 0.1457214185596241, 'learning_rate_model': 0.0010113259447302953}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.578136389253293e-05
Epoch 40: reducing lr to 6.256108759973455e-05
Epoch 43: reducing lr to 5.963385407650474e-05
Epoch 49: reducing lr to 5.266703310326051e-05
Epoch 53: reducing lr to 4.737616534336062e-05
Epoch 58: reducing lr to 4.029417086761061e-05
Epoch 61: reducing lr to 3.591388659728399e-05
Epoch 65: reducing lr to 3.0063529372762473e-05
Epoch 72: reducing lr to 2.025576786795335e-05
Epoch 75: reducing lr to 1.639242304090798e-05
Epoch 78: reducing lr to 1.282281409097295e-05
Epoch 81: reducing lr to 9.603225319717429e-06
Epoch 84: reducing lr to 6.784441051122588e-06
Epoch 87: reducing lr to 4.410903669426109e-06
Epoch 90: reducing lr to 2.520059334452354e-06
Epoch 93: reducing lr to 1.1417183163274807e-06
Epoch 96: reducing lr to 2.976219372494908e-07
Epoch 99: reducing lr to 1.0796033241434327e-09
[I 2024-06-21 04:33:39,356] Trial 814 finished with value: 0.9728786945343018 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.6277155102854164, 'bidirectional': True, 'fc_dropout': 0.13801523181712, 'learning_rate_model': 0.0007003454073293763}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.494971202424385e-05
Epoch 65: reducing lr to 3.601684298292961e-05
Epoch 72: reducing lr to 2.426690498487237e-05
Epoch 75: reducing lr to 1.9638523456565454e-05
Epoch 78: reducing lr to 1.5362044688349e-05
Epoch 81: reducing lr to 1.150489864916931e-05
Epoch 84: reducing lr to 8.127926200395115e-06
Epoch 87: reducing lr to 5.28437040457669e-06
Epoch 90: reducing lr to 3.019092676419732e-06
Epoch 93: reducing lr to 1.3678064481396935e-06
Epoch 96: reducing lr to 3.565583551178703e-07
Epoch 99: reducing lr to 1.2933911693285664e-09
[I 2024-06-21 04:34:13,521] Trial 815 finished with value: 0.9688276052474976 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.6059714048859207, 'bidirectional': True, 'fc_dropout': 0.17396952153768988, 'learning_rate_model': 0.0008390309153938243}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.814295782560869e-05
Epoch 53: reducing lr to 3.6457625939261205e-05
Epoch 56: reducing lr to 3.322263885498316e-05
Epoch 61: reducing lr to 2.7636999197790044e-05
Epoch 65: reducing lr to 2.3134943496218774e-05
Epoch 68: reducing lr to 1.982164551113146e-05
Epoch 72: reducing lr to 1.5587525978310126e-05
Epoch 75: reducing lr to 1.2614546220282102e-05
Epoch 78: reducing lr to 9.867606553405742e-06
Epoch 81: reducing lr to 7.390019727836949e-06
Epoch 84: reducing lr to 5.220866067486793e-06
Epoch 87: reducing lr to 3.39434555052244e-06
Epoch 90: reducing lr to 1.939274314295746e-06
Epoch 93: reducing lr to 8.78592410402917e-07
Epoch 96: reducing lr to 2.2903055114148388e-07
Epoch 99: reducing lr to 8.307927386948417e-10
[I 2024-06-21 04:34:46,087] Trial 816 finished with value: 0.9727197885513306 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.5431228905507204, 'bidirectional': True, 'fc_dropout': 0.19886269692304032, 'learning_rate_model': 0.0005389404292990578}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.0001141546909101598
Epoch 36: reducing lr to 0.00010761558756337609
Epoch 40: reducing lr to 0.00010234734888817822
Epoch 45: reducing lr to 9.400803394803931e-05
Epoch 52: reducing lr to 7.973223607657562e-05
Epoch 55: reducing lr to 7.294940774227795e-05
Epoch 58: reducing lr to 6.591959510570766e-05
Epoch 63: reducing lr to 5.395648110373401e-05
Epoch 66: reducing lr to 4.68150315167805e-05
Epoch 69: reducing lr to 3.9838716328643985e-05
Epoch 72: reducing lr to 3.31375975149794e-05
Epoch 75: reducing lr to 2.681732534486085e-05
Epoch 78: reducing lr to 2.0977592907170384e-05
Epoch 81: reducing lr to 1.571047898874887e-05
Epoch 84: reducing lr to 1.1099064640837045e-05
Epoch 87: reducing lr to 7.216055763851754e-06
Epoch 90: reducing lr to 4.122712724757686e-06
Epoch 93: reducing lr to 1.8678038911473056e-06
Epoch 96: reducing lr to 4.868971659082724e-07
Epoch 99: reducing lr to 1.7661863358983676e-09
[I 2024-06-21 04:35:24,447] Trial 817 finished with value: 0.9754449129104614 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.5777958987617152, 'bidirectional': True, 'fc_dropout': 0.1841957626508484, 'learning_rate_model': 0.0011457360876583784}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.083977362012827e-05
Epoch 40: reducing lr to 8.639278206929975e-05
Epoch 49: reducing lr to 7.27297380479985e-05
Epoch 52: reducing lr to 6.730305933754576e-05
Epoch 55: reducing lr to 6.157758216139846e-05
Epoch 63: reducing lr to 4.554539579050698e-05
Epoch 66: reducing lr to 3.951720156245083e-05
Epoch 72: reducing lr to 2.797189445072632e-05
Epoch 75: reducing lr to 2.263686719165892e-05
Epoch 78: reducing lr to 1.7707470022967953e-05
Epoch 83: reducing lr to 1.0600911964517551e-05
Epoch 86: reducing lr to 7.112015131551755e-06
Epoch 89: reducing lr to 4.273587217476396e-06
Epoch 92: reducing lr to 2.1303778686424336e-06
Epoch 95: reducing lr to 7.161930912399678e-07
Epoch 98: reducing lr to 5.33316601226074e-08
[I 2024-06-21 04:36:00,216] Trial 818 finished with value: 0.9726462960243225 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5961797628498858, 'bidirectional': True, 'fc_dropout': 0.1310739699634992, 'learning_rate_model': 0.0009671313346684608}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.6790087526772685e-05
Epoch 47: reducing lr to 5.0049807707893605e-05
Epoch 53: reducing lr to 4.3005911178304384e-05
Epoch 56: reducing lr to 3.9189876435910446e-05
Epoch 61: reducing lr to 3.260097996274285e-05
Epoch 65: reducing lr to 2.7290293854324335e-05
Epoch 72: reducing lr to 1.83872575474206e-05
Epoch 75: reducing lr to 1.4880290208909338e-05
Epoch 78: reducing lr to 1.1639962834804959e-05
Epoch 81: reducing lr to 8.717367734003179e-06
Epoch 84: reducing lr to 6.158604587863908e-06
Epoch 87: reducing lr to 4.0040161555620745e-06
Epoch 90: reducing lr to 2.2875943444566504e-06
Epoch 93: reducing lr to 1.0363995512672619e-06
Epoch 96: reducing lr to 2.701675516644602e-07
Epoch 99: reducing lr to 9.800144087088528e-10
[I 2024-06-21 04:36:33,349] Trial 819 finished with value: 0.9705621004104614 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5649054882346485, 'bidirectional': True, 'fc_dropout': 0.31612970158411025, 'learning_rate_model': 0.0006357414569847935}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.321759606121417e-05
Epoch 40: reducing lr to 6.963328471739936e-05
Epoch 43: reducing lr to 6.637514306453202e-05
Epoch 53: reducing lr to 5.273178802899866e-05
Epoch 65: reducing lr to 3.346205094478489e-05
Epoch 72: reducing lr to 2.254557433756522e-05
Epoch 75: reducing lr to 1.8245498993218374e-05
Epoch 78: reducing lr to 1.4272364799469825e-05
Epoch 81: reducing lr to 1.0688818697839574e-05
Epoch 84: reducing lr to 7.551385909141957e-06
Epoch 87: reducing lr to 4.909532792001414e-06
Epoch 90: reducing lr to 2.804938594792037e-06
Epoch 93: reducing lr to 1.2707834796055888e-06
Epoch 96: reducing lr to 3.312665090995839e-07
Epoch 99: reducing lr to 1.2016467190105288e-09
[I 2024-06-21 04:37:10,658] Trial 820 finished with value: 0.9709758758544922 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.6445003886599542, 'bidirectional': True, 'fc_dropout': 0.1669987839525613, 'learning_rate_model': 0.0007795157184782767}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 9.343505383894302e-05
Epoch 40: reducing lr to 8.796050065337063e-05
Epoch 43: reducing lr to 8.384482849818071e-05
Epoch 49: reducing lr to 7.404952147459301e-05
Epoch 52: reducing lr to 6.852436804367886e-05
Epoch 61: reducing lr to 5.049470152623883e-05
Epoch 65: reducing lr to 4.226913559998149e-05
Epoch 68: reducing lr to 3.621551191865755e-05
Epoch 71: reducing lr to 3.036722421745449e-05
Epoch 74: reducing lr to 2.4816477053082127e-05
Epoch 77: reducing lr to 1.96508253781635e-05
Epoch 80: reducing lr to 1.4951719318118583e-05
Epoch 83: reducing lr to 1.0793280427447031e-05
Epoch 86: reducing lr to 7.241072652618543e-06
Epoch 89: reducing lr to 4.351137470414324e-06
Epoch 92: reducing lr to 2.169036572480503e-06
Epoch 95: reducing lr to 7.291894225540818e-07
Epoch 98: reducing lr to 5.429943813243208e-08
[I 2024-06-21 04:37:45,886] Trial 821 finished with value: 0.9712100028991699 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.6204623248669017, 'bidirectional': True, 'fc_dropout': 0.1551925291913862, 'learning_rate_model': 0.00098468129347614}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.59020981706031e-05
Epoch 47: reducing lr to 4.0454087797355055e-05
Epoch 50: reducing lr to 3.7702073407960814e-05
Epoch 53: reducing lr to 3.47606711451559e-05
Epoch 56: reducing lr to 3.167625960440557e-05
Epoch 61: reducing lr to 2.6350608845288547e-05
Epoch 64: reducing lr to 2.312636100716717e-05
Epoch 67: reducing lr to 1.9942578797941412e-05
Epoch 72: reducing lr to 1.4861989790594497e-05
Epoch 75: reducing lr to 1.202739019647426e-05
Epoch 78: reducing lr to 9.408309442972838e-06
Epoch 86: reducing lr to 3.7787464292727375e-06
Epoch 89: reducing lr to 2.2706366816603074e-06
Epoch 92: reducing lr to 1.1319095383276885e-06
Epoch 95: reducing lr to 3.8052676156250355e-07
Epoch 98: reducing lr to 2.8336106789412714e-08
[I 2024-06-21 04:38:21,713] Trial 822 finished with value: 0.975185215473175 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.587976848826413, 'bidirectional': True, 'fc_dropout': 0.18841391767030613, 'learning_rate_model': 0.0005138549356149692}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.885772314536108e-05
Epoch 40: reducing lr to 6.548684604127217e-05
Epoch 43: reducing lr to 6.242271626959896e-05
Epoch 49: reducing lr to 5.5130081982437267e-05
Epoch 52: reducing lr to 5.101658934202428e-05
Epoch 61: reducing lr to 3.759345069873791e-05
Epoch 64: reducing lr to 3.29935341330681e-05
Epoch 67: reducing lr to 2.8451348401392005e-05
Epoch 72: reducing lr to 2.1203057726605722e-05
Epoch 75: reducing lr to 1.7159038071580752e-05
Epoch 78: reducing lr to 1.3422491270675475e-05
Epoch 81: reducing lr to 1.0052333841054432e-05
Epoch 84: reducing lr to 7.101725108002064e-06
Epoch 87: reducing lr to 4.617185867206945e-06
Epoch 90: reducing lr to 2.6379135015365737e-06
Epoch 93: reducing lr to 1.1951124008936753e-06
Epoch 96: reducing lr to 3.115406514007826e-07
Epoch 99: reducing lr to 1.1300925125594732e-09
[I 2024-06-21 04:38:58,829] Trial 823 finished with value: 0.9717232584953308 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6593282555843218, 'bidirectional': True, 'fc_dropout': 0.13194926070103086, 'learning_rate_model': 0.0007330980586355008}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00011975350257233914
Epoch 30: reducing lr to 0.00011837387195603832
Epoch 36: reducing lr to 0.00011289368346883736
Epoch 40: reducing lr to 0.00010736705965064912
Epoch 49: reducing lr to 9.038692743002243e-05
Epoch 52: reducing lr to 8.364276984122389e-05
Epoch 55: reducing lr to 7.652727205569436e-05
Epoch 58: reducing lr to 6.915267641757801e-05
Epoch 65: reducing lr to 5.159489509079644e-05
Epoch 68: reducing lr to 4.420567185914828e-05
Epoch 71: reducing lr to 3.706708749679148e-05
Epoch 74: reducing lr to 3.029168947749881e-05
Epoch 77: reducing lr to 2.3986349837595203e-05
Epoch 80: reducing lr to 1.825048888971603e-05
Epoch 83: reducing lr to 1.3174581486826527e-05
Epoch 86: reducing lr to 8.838656824977816e-06
Epoch 89: reducing lr to 5.311120706044409e-06
Epoch 92: reducing lr to 2.6475870115801874e-06
Epoch 95: reducing lr to 8.900691065471395e-07
Epoch 98: reducing lr to 6.627942053143761e-08
[I 2024-06-21 04:39:34,501] Trial 824 finished with value: 0.9766895771026611 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6067985414149215, 'bidirectional': True, 'fc_dropout': 0.1484915298570405, 'learning_rate_model': 0.0012019296660231071}. Best is trial 690 with value: 0.966281533241272.
Epoch 39: reducing lr to 7.957606624532797e-05
Epoch 45: reducing lr to 7.207681366146125e-05
Epoch 48: reducing lr to 6.762961293366937e-05
Epoch 51: reducing lr to 6.280854101203363e-05
Epoch 54: reducing lr to 5.768963295408344e-05
Epoch 57: reducing lr to 5.235360127482171e-05
Epoch 60: reducing lr to 4.6884622043615876e-05
Epoch 65: reducing lr to 3.7708814644586596e-05
Epoch 68: reducing lr to 3.230830266138853e-05
Epoch 71: reducing lr to 2.7090973426177524e-05
Epoch 74: reducing lr to 2.2139083755635642e-05
Epoch 77: reducing lr to 1.7530742497573817e-05
Epoch 80: reducing lr to 1.3338612308529563e-05
Epoch 83: reducing lr to 9.62881793697771e-06
Epoch 86: reducing lr to 6.459849784240621e-06
Epoch 89: reducing lr to 3.881703139560803e-06
Epoch 92: reducing lr to 1.935024147241683e-06
Epoch 95: reducing lr to 6.505188333185818e-07
Epoch 98: reducing lr to 4.844119518360645e-08
[I 2024-06-21 04:40:27,613] Trial 825 finished with value: 0.9767482876777649 and parameters: {'hidden_size': 127, 'n_layers': 3, 'rnn_dropout': 0.5529559694940029, 'bidirectional': True, 'fc_dropout': 0.20767537763241384, 'learning_rate_model': 0.000878446267060635}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.3710311934306685e-05
Epoch 47: reducing lr to 4.733556332301358e-05
Epoch 53: reducing lr to 4.067366339798071e-05
Epoch 56: reducing lr to 3.706457552204628e-05
Epoch 63: reducing lr to 2.8315530030244073e-05
Epoch 66: reducing lr to 2.45678073081101e-05
Epoch 72: reducing lr to 1.7390100658373097e-05
Epoch 75: reducing lr to 1.4073319193543232e-05
Epoch 78: reducing lr to 1.1008717577101431e-05
Epoch 81: reducing lr to 8.244617337816937e-06
Epoch 84: reducing lr to 5.824618131435044e-06
Epoch 87: reducing lr to 3.7868748943881777e-06
Epoch 90: reducing lr to 2.1635361234827956e-06
Epoch 93: reducing lr to 9.801947067064782e-07
Epoch 96: reducing lr to 2.555161315358971e-07
Epoch 99: reducing lr to 9.268673792340275e-10
[I 2024-06-21 04:41:00,726] Trial 826 finished with value: 0.9709365367889404 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.632407758969493, 'bidirectional': True, 'fc_dropout': 0.16977561906287286, 'learning_rate_model': 0.0006012646476046793}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.695902985440118e-05
Epoch 40: reducing lr to 7.319155948449871e-05
Epoch 52: reducing lr to 5.7018835983790136e-05
Epoch 65: reducing lr to 3.517196843633287e-05
Epoch 72: reducing lr to 2.3697657692539793e-05
Epoch 77: reducing lr to 1.6351368442674238e-05
Epoch 83: reducing lr to 8.981042860948e-06
Epoch 86: reducing lr to 6.025265839200271e-06
Epoch 89: reducing lr to 3.6205630325599856e-06
Epoch 92: reducing lr to 1.8048461313831429e-06
Epoch 95: reducing lr to 6.067554254454949e-07
Epoch 98: reducing lr to 4.51823321436797e-08
[I 2024-06-21 04:41:39,622] Trial 827 finished with value: 0.968542218208313 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.569246766283679, 'bidirectional': True, 'fc_dropout': 0.2872198575726345, 'learning_rate_model': 0.0008193491274992}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.0001023507033451923
Epoch 36: reducing lr to 9.816428967165871e-05
Epoch 40: reducing lr to 9.335873204677466e-05
Epoch 52: reducing lr to 7.27297828837362e-05
Epoch 55: reducing lr to 6.654265385831493e-05
Epoch 58: reducing lr to 6.013023183267316e-05
Epoch 61: reducing lr to 5.359361616354499e-05
Epoch 65: reducing lr to 4.486323832873999e-05
Epoch 68: reducing lr to 3.843809719176677e-05
Epoch 71: reducing lr to 3.2230893726876734e-05
Epoch 74: reducing lr to 2.63394911844996e-05
Epoch 77: reducing lr to 2.085681785972901e-05
Epoch 80: reducing lr to 1.5869322560584217e-05
Epoch 83: reducing lr to 1.1455675761812669e-05
Epoch 86: reducing lr to 7.685465140439007e-06
Epoch 89: reducing lr to 4.618171499499373e-06
Epoch 92: reducing lr to 2.3021527011067838e-06
Epoch 95: reducing lr to 7.739405688450812e-07
Epoch 98: reducing lr to 5.763185358474004e-08
[I 2024-06-21 04:42:14,084] Trial 828 finished with value: 0.9729269742965698 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6129001748745578, 'bidirectional': True, 'fc_dropout': 0.19694570234196065, 'learning_rate_model': 0.0010451122531848361}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.757463477857324e-05
Epoch 40: reducing lr to 6.42665702828681e-05
Epoch 43: reducing lr to 6.125953721850344e-05
Epoch 49: reducing lr to 5.410279319592894e-05
Epoch 61: reducing lr to 3.6892937857830654e-05
Epoch 65: reducing lr to 3.088309358921547e-05
Epoch 72: reducing lr to 2.0807961933909998e-05
Epoch 75: reducing lr to 1.6839298162544885e-05
Epoch 78: reducing lr to 1.3172377824920698e-05
Epoch 81: reducing lr to 9.865019593336766e-06
Epoch 84: reducing lr to 6.969392227186848e-06
Epoch 87: reducing lr to 4.531149658007808e-06
Epoch 90: reducing lr to 2.588758868304387e-06
Epoch 93: reducing lr to 1.1728427882991104e-06
Epoch 96: reducing lr to 3.0573543206830313e-07
Epoch 99: reducing lr to 1.1090344744770982e-09
[I 2024-06-21 04:42:51,184] Trial 829 finished with value: 0.9720768332481384 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.5924177320584864, 'bidirectional': True, 'fc_dropout': 0.23307657498374001, 'learning_rate_model': 0.0007194375780418534}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 0.00010734127212410214
Epoch 45: reducing lr to 9.859504973493153e-05
Epoch 49: reducing lr to 9.036521820842147e-05
Epoch 52: reducing lr to 8.362268043806094e-05
Epoch 56: reducing lr to 7.40743917507405e-05
Epoch 59: reducing lr to 6.664090615979549e-05
Epoch 62: reducing lr to 5.9103981900942715e-05
Epoch 65: reducing lr to 5.1582502977878655e-05
Epoch 68: reducing lr to 4.419505449717243e-05
Epoch 71: reducing lr to 3.705818468706612e-05
Epoch 74: reducing lr to 3.0284413989568948e-05
Epoch 77: reducing lr to 2.3980588772374514e-05
Epoch 80: reducing lr to 1.8246105469249194e-05
Epoch 83: reducing lr to 1.3171417202818553e-05
Epoch 86: reducing lr to 8.836533947642307e-06
Epoch 89: reducing lr to 5.30984507582182e-06
Epoch 92: reducing lr to 2.6469511115139246e-06
Epoch 95: reducing lr to 8.898553288690928e-07
Epoch 98: reducing lr to 6.626350147469641e-08
[I 2024-06-21 04:43:25,268] Trial 830 finished with value: 0.9759496450424194 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5367376393730812, 'bidirectional': True, 'fc_dropout': 0.14595560827433124, 'learning_rate_model': 0.0012016409853675037}. Best is trial 690 with value: 0.966281533241272.
Epoch 53: reducing lr to 2.9289167739213893e-05
Epoch 56: reducing lr to 2.6690257993871045e-05
Epoch 61: reducing lr to 2.2202891287029206e-05
Epoch 64: reducing lr to 1.948615617655954e-05
Epoch 67: reducing lr to 1.6803517202710694e-05
Epoch 72: reducing lr to 1.2522638302853e-05
Epoch 75: reducing lr to 1.0134218854937223e-05
Epoch 78: reducing lr to 7.927394504753755e-06
Epoch 81: reducing lr to 5.9369616596900795e-06
Epoch 84: reducing lr to 4.194316499087146e-06
Epoch 87: reducing lr to 2.7269344515119245e-06
Epoch 90: reducing lr to 1.5579656990936854e-06
Epoch 93: reducing lr to 7.058397199412574e-07
Epoch 96: reducing lr to 1.8399756037223275e-07
Epoch 99: reducing lr to 6.674386291859093e-10
[I 2024-06-21 04:43:59,669] Trial 831 finished with value: 0.9755810499191284 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5786590234358777, 'bidirectional': True, 'fc_dropout': 0.17877077535246844, 'learning_rate_model': 0.0004329716000016629}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 5.9861066463238174e-05
Epoch 40: reducing lr to 5.693061379721988e-05
Epoch 43: reducing lr to 5.426683016430908e-05
Epoch 49: reducing lr to 4.792702039693825e-05
Epoch 53: reducing lr to 4.311232869883068e-05
Epoch 56: reducing lr to 3.928685123229948e-05
Epoch 61: reducing lr to 3.268165062775861e-05
Epoch 65: reducing lr to 2.7357823301482662e-05
Epoch 72: reducing lr to 1.8432756556832636e-05
Epoch 75: reducing lr to 1.491711127711502e-05
Epoch 78: reducing lr to 1.1668765758634722e-05
Epoch 81: reducing lr to 8.738938737485062e-06
Epoch 84: reducing lr to 6.173843968037141e-06
Epoch 87: reducing lr to 4.013924037054358e-06
Epoch 90: reducing lr to 2.2932549643909085e-06
Epoch 93: reducing lr to 1.0389641073363852e-06
Epoch 96: reducing lr to 2.708360774597942e-07
Epoch 99: reducing lr to 9.824394405421807e-10
[I 2024-06-21 04:44:37,003] Trial 832 finished with value: 0.9728156924247742 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.6211700538382011, 'bidirectional': True, 'fc_dropout': 0.2530152630239263, 'learning_rate_model': 0.0006373145902516972}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.518464069541898e-05
Epoch 40: reducing lr to 8.101449184611632e-05
Epoch 49: reducing lr to 6.820202601339273e-05
Epoch 53: reducing lr to 6.135053126739812e-05
Epoch 65: reducing lr to 3.893125341455026e-05
Epoch 68: reducing lr to 3.335566843348318e-05
Epoch 71: reducing lr to 2.7969204591605287e-05
Epoch 74: reducing lr to 2.2856785294902862e-05
Epoch 77: reducing lr to 1.8099051512250314e-05
Epoch 84: reducing lr to 8.785621626868263e-06
Epoch 87: reducing lr to 5.711971020181598e-06
Epoch 90: reducing lr to 3.2633915783073193e-06
Epoch 93: reducing lr to 1.4784865925039789e-06
Epoch 96: reducing lr to 3.854103394555796e-07
Epoch 99: reducing lr to 1.3980497791344845e-09
[I 2024-06-21 04:45:09,559] Trial 833 finished with value: 0.9705209136009216 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.6020508310176202, 'bidirectional': True, 'fc_dropout': 0.16112659212251557, 'learning_rate_model': 0.000906923608083046}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 7.374170148314243e-05
Epoch 40: reducing lr to 6.942102257113645e-05
Epoch 43: reducing lr to 6.617281266488817e-05
Epoch 52: reducing lr to 5.408145321249718e-05
Epoch 65: reducing lr to 3.336004905329399e-05
Epoch 72: reducing lr to 2.2476848985644173e-05
Epoch 75: reducing lr to 1.818988149949168e-05
Epoch 78: reducing lr to 1.4228858553902392e-05
Epoch 81: reducing lr to 1.0656236124620085e-05
Epoch 84: reducing lr to 7.5283671274366485e-06
Epoch 87: reducing lr to 4.894567133382729e-06
Epoch 90: reducing lr to 2.7963883405755005e-06
Epoch 93: reducing lr to 1.2669097684930007e-06
Epoch 96: reducing lr to 3.3025671413598657e-07
Epoch 99: reducing lr to 1.1979837564990956e-09
[I 2024-06-21 04:45:45,650] Trial 834 finished with value: 0.9720684289932251 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5605452212647568, 'bidirectional': True, 'fc_dropout': 0.1320008441700726, 'learning_rate_model': 0.0007771395318583091}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 9.182961166058758e-05
Epoch 40: reducing lr to 8.644912465500496e-05
Epoch 43: reducing lr to 8.240416978844224e-05
Epoch 49: reducing lr to 7.277717003711999e-05
Epoch 52: reducing lr to 6.734695222185994e-05
Epoch 65: reducing lr to 4.154284872640823e-05
Epoch 68: reducing lr to 3.559324106894875e-05
Epoch 71: reducing lr to 2.98454412185137e-05
Epoch 74: reducing lr to 2.4390069432577257e-05
Epoch 77: reducing lr to 1.9313176256068683e-05
Epoch 80: reducing lr to 1.4694812302539438e-05
Epoch 83: reducing lr to 1.0607825537348615e-05
Epoch 86: reducing lr to 7.1166533584092384e-06
Epoch 89: reducing lr to 4.2763743132071094e-06
Epoch 92: reducing lr to 2.1317672323690508e-06
Epoch 95: reducing lr to 7.166601692719268e-07
Epoch 98: reducing lr to 5.336644131102939e-08
[I 2024-06-21 04:46:20,890] Trial 835 finished with value: 0.9708090424537659 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.641039287370162, 'bidirectional': True, 'fc_dropout': 0.20785405553884453, 'learning_rate_model': 0.0009677620665283057}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.296339461376133e-05
Epoch 53: reducing lr to 4.010803898456936e-05
Epoch 56: reducing lr to 3.6549140544309824e-05
Epoch 65: reducing lr to 2.5451388886320763e-05
Epoch 72: reducing lr to 1.7148266885304478e-05
Epoch 75: reducing lr to 1.3877609924975264e-05
Epoch 78: reducing lr to 1.085562589806993e-05
Epoch 81: reducing lr to 8.12996435463531e-06
Epoch 84: reducing lr to 5.743618635970329e-06
Epoch 87: reducing lr to 3.7342130805298553e-06
Epoch 90: reducing lr to 2.13344911512151e-06
Epoch 93: reducing lr to 9.665637226816408e-07
Epoch 96: reducing lr to 2.519628208689259e-07
Epoch 99: reducing lr to 9.139779865883699e-10
[I 2024-06-21 04:46:54,002] Trial 836 finished with value: 0.9710022211074829 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5883735482375483, 'bidirectional': True, 'fc_dropout': 0.18040817952311416, 'learning_rate_model': 0.0005929032182375064}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00011919250106912642
Epoch 31: reducing lr to 0.00011715684148578035
Epoch 36: reducing lr to 0.00011236481771736608
Epoch 39: reducing lr to 0.00010836948953179695
Epoch 45: reducing lr to 9.815674325356083e-05
Epoch 52: reducing lr to 8.325093395662874e-05
Epoch 55: reducing lr to 7.616876968425785e-05
Epoch 58: reducing lr to 6.882872133828497e-05
Epoch 61: reducing lr to 6.134651339273997e-05
Epoch 64: reducing lr to 5.384018348802539e-05
Epoch 67: reducing lr to 4.642806109326118e-05
Epoch 70: reducing lr to 3.9227089303575896e-05
Epoch 73: reducing lr to 3.235080924058639e-05
Epoch 76: reducing lr to 2.5907654515119404e-05
Epoch 79: reducing lr to 1.99992565094062e-05
Epoch 82: reducing lr to 1.4718776704104289e-05
Epoch 85: reducing lr to 1.0149506977854389e-05
Epoch 88: reducing lr to 6.36348889467397e-06
Epoch 91: reducing lr to 3.4204526443355625e-06
Epoch 94: reducing lr to 1.3667967153412889e-06
Epoch 97: reducing lr to 2.3491453910544618e-07
[I 2024-06-21 04:47:32,393] Trial 837 finished with value: 0.9760154485702515 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6104509853551381, 'bidirectional': True, 'fc_dropout': 0.2239153365502698, 'learning_rate_model': 0.0011962990637032493}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.906460482466662e-05
Epoch 40: reducing lr to 6.568359998640106e-05
Epoch 49: reducing lr to 5.529571923298507e-05
Epoch 52: reducing lr to 5.116986768457474e-05
Epoch 61: reducing lr to 3.7706399484381414e-05
Epoch 65: reducing lr to 3.156404265434042e-05
Epoch 72: reducing lr to 2.1266761897881214e-05
Epoch 75: reducing lr to 1.7210592065081555e-05
Epoch 78: reducing lr to 1.3462818882564113e-05
Epoch 81: reducing lr to 1.0082535881014231e-05
Epoch 84: reducing lr to 7.123062101867055e-06
Epoch 87: reducing lr to 4.631058111629792e-06
Epoch 90: reducing lr to 2.6458390609383634e-06
Epoch 93: reducing lr to 1.1987030926732074e-06
Epoch 96: reducing lr to 3.1247666918047354e-07
Epoch 99: reducing lr to 1.1334878533605235e-09
[I 2024-06-21 04:48:03,455] Trial 838 finished with value: 0.9716561436653137 and parameters: {'hidden_size': 123, 'n_layers': 2, 'rnn_dropout': 0.5731810856361028, 'bidirectional': True, 'fc_dropout': 0.2680011825945936, 'learning_rate_model': 0.0007353006373810394}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.4994558892405754e-05
Epoch 61: reducing lr to 2.5829625851381983e-05
Epoch 64: reducing lr to 2.2669125242087977e-05
Epoch 67: reducing lr to 1.9548290207898948e-05
Epoch 72: reducing lr to 1.4568150510372425e-05
Epoch 75: reducing lr to 1.1789594334138343e-05
Epoch 79: reducing lr to 8.420581454021339e-06
Epoch 86: reducing lr to 3.7040361013446416e-06
Epoch 89: reducing lr to 2.2257434838055186e-06
Epoch 92: reducing lr to 1.1095303354951538e-06
Epoch 95: reducing lr to 3.730032932181904e-07
Epoch 98: reducing lr to 2.7775868130877365e-08
[I 2024-06-21 04:48:39,902] Trial 839 finished with value: 0.971576452255249 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5448524700719796, 'bidirectional': True, 'fc_dropout': 0.15084410704697088, 'learning_rate_model': 0.0005036954101041111}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 8.687511314792386e-05
Epoch 36: reducing lr to 8.332169192378598e-05
Epoch 40: reducing lr to 7.924274230491869e-05
Epoch 43: reducing lr to 7.553497409552132e-05
Epoch 52: reducing lr to 6.173292327986051e-05
Epoch 65: reducing lr to 3.807984487265018e-05
Epoch 68: reducing lr to 3.262619536148492e-05
Epoch 72: reducing lr to 2.5656884413807573e-05
Epoch 75: reducing lr to 2.0763394701427704e-05
Epoch 78: reducing lr to 1.624196432031276e-05
Epoch 81: reducing lr to 1.216388554775809e-05
Epoch 84: reducing lr to 8.593484137243479e-06
Epoch 87: reducing lr to 5.587052850558756e-06
Epoch 90: reducing lr to 3.1920227108385543e-06
Epoch 93: reducing lr to 1.4461527731804988e-06
Epoch 96: reducing lr to 3.76981593233237e-07
Epoch 99: reducing lr to 1.3674750757967925e-09
[I 2024-06-21 04:49:17,236] Trial 840 finished with value: 0.9713135361671448 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.6333088895753085, 'bidirectional': True, 'fc_dropout': 0.1899427949152469, 'learning_rate_model': 0.0008870896074011116}. Best is trial 690 with value: 0.966281533241272.
Epoch 14: reducing lr to 6.799672408954401e-05
Epoch 17: reducing lr to 8.43122820262126e-05
Epoch 20: reducing lr to 9.628857359010668e-05
Epoch 23: reducing lr to 0.00010224356408985004
Epoch 26: reducing lr to 0.00010256273616726388
Epoch 29: reducing lr to 0.00010165020934473781
Epoch 32: reducing lr to 9.99446109957096e-05
Epoch 35: reducing lr to 9.747283940006247e-05
Epoch 38: reducing lr to 9.427387521006701e-05
Epoch 41: reducing lr to 9.039816840855863e-05
Epoch 44: reducing lr to 8.59068412270432e-05
Epoch 47: reducing lr to 8.08707265693076e-05
Epoch 50: reducing lr to 7.53692453762487e-05
Epoch 53: reducing lr to 6.948916375588918e-05
Epoch 56: reducing lr to 6.332319596571829e-05
Epoch 59: reducing lr to 5.696861034363589e-05
Epoch 62: reducing lr to 5.0525599195760495e-05
Epoch 65: reducing lr to 4.409579163959593e-05
Epoch 68: reducing lr to 3.778056127760336e-05
Epoch 71: reducing lr to 3.167954046749778e-05
Epoch 74: reducing lr to 2.5888918375752223e-05
Epoch 77: reducing lr to 2.0500033632624386e-05
Epoch 80: reducing lr to 1.5597856221733732e-05
Epoch 83: reducing lr to 1.1259710852394199e-05
Epoch 86: reducing lr to 7.5539948098012e-06
Epoch 89: reducing lr to 4.5391713969829314e-06
Epoch 92: reducing lr to 2.262771249071996e-06
Epoch 95: reducing lr to 7.607012631400018e-07
Epoch 98: reducing lr to 5.6645982371016264e-08
[I 2024-06-21 04:49:33,519] Trial 841 finished with value: 1.092559814453125 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5972071049330442, 'bidirectional': False, 'fc_dropout': 0.1294055622019968, 'learning_rate_model': 0.0010272341871251961}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.918524049828857e-05
Epoch 43: reducing lr to 5.64159628735859e-05
Epoch 47: reducing lr to 5.216068569516289e-05
Epoch 53: reducing lr to 4.4819708980656856e-05
Epoch 56: reducing lr to 4.0842730887923046e-05
Epoch 63: reducing lr to 3.1201856670023834e-05
Epoch 66: reducing lr to 2.7072112070854568e-05
Epoch 72: reducing lr to 1.9162750181271013e-05
Epoch 75: reducing lr to 1.5507874579052915e-05
Epoch 78: reducing lr to 1.2130884627432502e-05
Epoch 81: reducing lr to 9.085027481349961e-06
Epoch 84: reducing lr to 6.418347101415421e-06
Epoch 87: reducing lr to 4.172887724715252e-06
Epoch 90: reducing lr to 2.3840748858744777e-06
Epoch 93: reducing lr to 1.0801102686301364e-06
Epoch 96: reducing lr to 2.8156201577532546e-07
Epoch 99: reducing lr to 1.0213470518871496e-09
[I 2024-06-21 04:50:06,673] Trial 842 finished with value: 0.9703547954559326 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6216204269657469, 'bidirectional': True, 'fc_dropout': 0.16459185319860076, 'learning_rate_model': 0.0006625542003019282}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 7.847942966032389e-05
Epoch 36: reducing lr to 7.526941403089509e-05
Epoch 40: reducing lr to 7.158465750969393e-05
Epoch 43: reducing lr to 6.823521111656267e-05
Epoch 46: reducing lr to 6.444155491986631e-05
Epoch 49: reducing lr to 6.0263522764662e-05
Epoch 52: reducing lr to 5.5767002018388315e-05
Epoch 55: reducing lr to 5.102289825280628e-05
Epoch 58: reducing lr to 4.6106046615583954e-05
Epoch 61: reducing lr to 4.109396704157474e-05
Epoch 64: reducing lr to 3.606572897802355e-05
Epoch 67: reducing lr to 3.110059735842273e-05
Epoch 70: reducing lr to 2.6276908430933386e-05
Epoch 73: reducing lr to 2.167071957602498e-05
Epoch 76: reducing lr to 1.7354666824387408e-05
Epoch 79: reducing lr to 1.3396829622444356e-05
Epoch 82: reducing lr to 9.859613714287155e-06
Epoch 85: reducing lr to 6.798813529401661e-06
Epoch 88: reducing lr to 4.262687289708407e-06
Epoch 91: reducing lr to 2.2912462413915686e-06
Epoch 94: reducing lr to 9.155711721249159e-07
Epoch 97: reducing lr to 1.5736135264580921e-07
[I 2024-06-21 04:51:03,721] Trial 843 finished with value: 0.9797888994216919 and parameters: {'hidden_size': 134, 'n_layers': 3, 'rnn_dropout': 0.5793684516447252, 'bidirectional': True, 'fc_dropout': 0.23629955435644986, 'learning_rate_model': 0.0008013605268968056}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00011700232405704337
Epoch 39: reducing lr to 0.00011284210119922663
Epoch 47: reducing lr to 9.806765261041037e-05
Epoch 51: reducing lr to 8.906506787110488e-05
Epoch 54: reducing lr to 8.18062478720871e-05
Epoch 57: reducing lr to 7.423953773970626e-05
Epoch 60: reducing lr to 6.648430256683881e-05
Epoch 65: reducing lr to 5.347263415998708e-05
Epoch 68: reducing lr to 4.581448833185685e-05
Epoch 71: reducing lr to 3.8416103097101304e-05
Epoch 74: reducing lr to 3.1394121970162924e-05
Epoch 77: reducing lr to 2.4859306476775612e-05
Epoch 80: reducing lr to 1.8914695221752213e-05
Epoch 83: reducing lr to 1.3654055790137255e-05
Epoch 86: reducing lr to 9.160329951945546e-06
Epoch 89: reducing lr to 5.504413062456344e-06
Epoch 92: reducing lr to 2.743943008854267e-06
Epoch 95: reducing lr to 9.224621859923532e-07
Epoch 98: reducing lr to 6.869158664199893e-08
[I 2024-06-21 04:51:40,860] Trial 844 finished with value: 0.9776932001113892 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.5567136583638533, 'bidirectional': True, 'fc_dropout': 0.2077605821564052, 'learning_rate_model': 0.001245672565167279}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 8.898381781273608e-05
Epoch 36: reducing lr to 8.53441450069902e-05
Epoch 40: reducing lr to 8.116618774626587e-05
Epoch 52: reducing lr to 6.323135589854478e-05
Epoch 55: reducing lr to 5.785225889199902e-05
Epoch 61: reducing lr to 4.6594350803223935e-05
Epoch 65: reducing lr to 3.9004150391326814e-05
Epoch 72: reducing lr to 2.6279649552033326e-05
Epoch 75: reducing lr to 2.1267381006339747e-05
Epoch 78: reducing lr to 1.663620272400428e-05
Epoch 81: reducing lr to 1.2459137447495187e-05
Epoch 84: reducing lr to 8.802072298232013e-06
Epoch 87: reducing lr to 5.7226664225199866e-06
Epoch 90: reducing lr to 3.269502128552479e-06
Epoch 93: reducing lr to 1.4812549904707823e-06
Epoch 96: reducing lr to 3.8613200254373256e-07
Epoch 99: reducing lr to 1.4006675628803339e-09
[I 2024-06-21 04:52:15,231] Trial 845 finished with value: 0.9724490642547607 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6052991069141691, 'bidirectional': True, 'fc_dropout': 0.14916972452944088, 'learning_rate_model': 0.0009086217807180895}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.7405893903040886e-05
Epoch 47: reducing lr to 5.059252549650395e-05
Epoch 65: reducing lr to 2.7586217627250432e-05
Epoch 72: reducing lr to 1.858664076609321e-05
Epoch 75: reducing lr to 1.5041645438148026e-05
Epoch 78: reducing lr to 1.1766181399441236e-05
Epoch 81: reducing lr to 8.811894981075e-06
Epoch 84: reducing lr to 6.225385748789803e-06
Epoch 87: reducing lr to 4.047433920644991e-06
Epoch 90: reducing lr to 2.312399997079866e-06
Epoch 93: reducing lr to 1.047637805685008e-06
Epoch 96: reducing lr to 2.730971280785993e-07
Epoch 99: reducing lr to 9.906412477985425e-10
[I 2024-06-21 04:52:49,437] Trial 846 finished with value: 0.9707497954368591 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5897672964751093, 'bidirectional': True, 'fc_dropout': 0.25355162007425713, 'learning_rate_model': 0.0006426351537533486}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.773669930886512e-05
Epoch 40: reducing lr to 9.295207404273517e-05
Epoch 49: reducing lr to 7.825167605695886e-05
Epoch 52: reducing lr to 7.241298179086245e-05
Epoch 55: reducing lr to 6.62528030622699e-05
Epoch 61: reducing lr to 5.3360169623510513e-05
Epoch 65: reducing lr to 4.466782013321037e-05
Epoch 71: reducing lr to 3.209050031509804e-05
Epoch 74: reducing lr to 2.6224759925005438e-05
Epoch 77: reducing lr to 2.0765968383354342e-05
Epoch 80: reducing lr to 1.5800197938854006e-05
Epoch 83: reducing lr to 1.1405776388309098e-05
Epoch 86: reducing lr to 7.65198829423934e-06
Epoch 89: reducing lr to 4.598055369351437e-06
Epoch 92: reducing lr to 2.292124835454566e-06
Epoch 95: reducing lr to 7.705693884522969e-07
Epoch 98: reducing lr to 5.73808170289394e-08
[I 2024-06-21 04:53:22,114] Trial 847 finished with value: 0.9701458215713501 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.6597851167351132, 'bidirectional': True, 'fc_dropout': 0.16911905278693512, 'learning_rate_model': 0.0010405598856284256}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.63855298547941e-05
Epoch 40: reducing lr to 6.313567707999449e-05
Epoch 43: reducing lr to 6.018155851283012e-05
Epoch 53: reducing lr to 4.781128959176748e-05
Epoch 65: reducing lr to 3.0339646498915947e-05
Epoch 72: reducing lr to 2.044180605203956e-05
Epoch 75: reducing lr to 1.6542978509117526e-05
Epoch 78: reducing lr to 1.2940584647187401e-05
Epoch 81: reducing lr to 9.69142570844115e-06
Epoch 84: reducing lr to 6.84675244318725e-06
Epoch 87: reducing lr to 4.451415414731867e-06
Epoch 90: reducing lr to 2.5432047054611113e-06
Epoch 93: reducing lr to 1.1522043765791585e-06
Epoch 96: reducing lr to 3.003554324747032e-07
Epoch 99: reducing lr to 1.089518892065291e-09
[I 2024-06-21 04:53:59,374] Trial 848 finished with value: 0.9714112281799316 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.623828866173388, 'bidirectional': True, 'fc_dropout': 0.19653472157675952, 'learning_rate_model': 0.000706777698055753}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.6831247003040294e-05
Epoch 65: reducing lr to 2.2504605080506465e-05
Epoch 68: reducing lr to 1.928158174869609e-05
Epoch 73: reducing lr to 1.4177155503761597e-05
Epoch 76: reducing lr to 1.1353559784766656e-05
Epoch 87: reducing lr to 3.3018626621572233e-06
Epoch 90: reducing lr to 1.8864365323878517e-06
Epoch 93: reducing lr to 8.546541393576138e-07
Epoch 96: reducing lr to 2.2279034766833775e-07
Epoch 99: reducing lr to 8.081568252495466e-10
[I 2024-06-21 04:54:36,325] Trial 849 finished with value: 0.9717642068862915 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5726077622295238, 'bidirectional': True, 'fc_dropout': 0.12682217138345248, 'learning_rate_model': 0.0005242563711156781}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.82900373218556e-05
Epoch 40: reducing lr to 7.445740850069354e-05
Epoch 43: reducing lr to 7.097354607792818e-05
Epoch 53: reducing lr to 5.6384993156384465e-05
Epoch 62: reducing lr to 4.099755142950239e-05
Epoch 65: reducing lr to 3.5780268108540245e-05
Epoch 68: reducing lr to 3.0655955172600435e-05
Epoch 71: reducing lr to 2.57054564468821e-05
Epoch 74: reducing lr to 2.1006821877594863e-05
Epoch 77: reducing lr to 1.6634165582158353e-05
Epoch 80: reducing lr to 1.2656434021947631e-05
Epoch 83: reducing lr to 9.136370119309557e-06
Epoch 88: reducing lr to 4.433752425197439e-06
Epoch 91: reducing lr to 2.3831958314233506e-06
Epoch 94: reducing lr to 9.523137938479646e-07
Epoch 97: reducing lr to 1.6367639273240235e-07
[I 2024-06-21 04:55:09,107] Trial 850 finished with value: 0.9697140455245972 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5437978517573714, 'bidirectional': True, 'fc_dropout': 0.21451178832154708, 'learning_rate_model': 0.0008335197817969078}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010045992455148373
Epoch 36: reducing lr to 9.635084871674262e-05
Epoch 40: reducing lr to 9.163406670504055e-05
Epoch 43: reducing lr to 8.734650838052686e-05
Epoch 49: reducing lr to 7.714211196931347e-05
Epoch 52: reducing lr to 7.138620705422963e-05
Epoch 55: reducing lr to 6.531337614277198e-05
Epoch 61: reducing lr to 5.2603552884952506e-05
Epoch 65: reducing lr to 4.403445594741148e-05
Epoch 71: reducing lr to 3.16354753431308e-05
Epoch 74: reducing lr to 2.5852907802646697e-05
Epoch 77: reducing lr to 2.0471518808286087e-05
Epoch 80: reducing lr to 1.557616015341563e-05
Epoch 83: reducing lr to 1.124404899140362e-05
Epoch 86: reducing lr to 7.543487469232203e-06
Epoch 89: reducing lr to 4.532857569535323e-06
Epoch 92: reducing lr to 2.2596238140071842e-06
Epoch 95: reducing lr to 7.596431544909521e-07
Epoch 98: reducing lr to 5.656718980581385e-08
[I 2024-06-21 04:55:44,707] Trial 851 finished with value: 0.9724682569503784 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6067848038611523, 'bidirectional': True, 'fc_dropout': 0.14125608056771022, 'learning_rate_model': 0.001025805340571815}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.64243427452606e-05
Epoch 40: reducing lr to 7.268304757331303e-05
Epoch 49: reducing lr to 6.118820211504407e-05
Epoch 52: reducing lr to 5.662268706356058e-05
Epoch 55: reducing lr to 5.1805790095940966e-05
Epoch 63: reducing lr to 3.83177632401209e-05
Epoch 66: reducing lr to 3.324618761349601e-05
Epoch 72: reducing lr to 2.3533013828018642e-05
Epoch 75: reducing lr to 1.9044605991300613e-05
Epoch 78: reducing lr to 1.4897458505850681e-05
Epoch 85: reducing lr to 6.903134056800738e-06
Epoch 88: reducing lr to 4.328093670435962e-06
Epoch 91: reducing lr to 2.3264029662038396e-06
Epoch 94: reducing lr to 9.296196332474909e-07
Epoch 97: reducing lr to 1.597758944227334e-07
[I 2024-06-21 04:56:20,517] Trial 852 finished with value: 0.9733743667602539 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6475644180971222, 'bidirectional': True, 'fc_dropout': 0.1816376015141431, 'learning_rate_model': 0.0008136565477306108}. Best is trial 690 with value: 0.966281533241272.
Epoch 53: reducing lr to 2.8138676827454902e-05
Epoch 61: reducing lr to 2.1330752315108194e-05
Epoch 64: reducing lr to 1.8720731710222354e-05
Epoch 67: reducing lr to 1.6143467931272317e-05
Epoch 73: reducing lr to 1.1248676753419654e-05
Epoch 76: reducing lr to 9.008332030045042e-06
Epoch 79: reducing lr to 6.953927183397924e-06
Epoch 82: reducing lr to 5.117855325316483e-06
Epoch 85: reducing lr to 3.5290778153774484e-06
Epoch 88: reducing lr to 2.21264417430276e-06
Epoch 91: reducing lr to 1.1893231436770372e-06
Epoch 94: reducing lr to 4.7524790876703715e-07
Epoch 97: reducing lr to 8.168196645172497e-08
[I 2024-06-21 04:56:58,926] Trial 853 finished with value: 0.9736760854721069 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.5905569194370063, 'bidirectional': True, 'fc_dropout': 0.23367214669437453, 'learning_rate_model': 0.0004159642922049057}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.349625231203217e-05
Epoch 47: reducing lr to 4.7146909925925364e-05
Epoch 53: reducing lr to 4.051156065253137e-05
Epoch 61: reducing lr to 3.071011730496462e-05
Epoch 65: reducing lr to 2.570745193890005e-05
Epoch 72: reducing lr to 1.7320793327170107e-05
Epoch 75: reducing lr to 1.401723072035762e-05
Epoch 78: reducing lr to 1.096484291241575e-05
Epoch 81: reducing lr to 8.211758849203249e-06
Epoch 84: reducing lr to 5.8014044223314146e-06
Epoch 87: reducing lr to 3.771782503741018e-06
Epoch 90: reducing lr to 2.154913464095967e-06
Epoch 93: reducing lr to 9.762881922753325e-07
Epoch 96: reducing lr to 2.544977854375104e-07
Epoch 99: reducing lr to 9.231733980562262e-10
[I 2024-06-21 04:57:33,083] Trial 854 finished with value: 0.9712477922439575 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.560247970899086, 'bidirectional': True, 'fc_dropout': 0.16008970637624614, 'learning_rate_model': 0.0005988683389868725}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00012644153961265784
Epoch 31: reducing lr to 0.00012428207547240743
Epoch 36: reducing lr to 0.0001191986108441474
Epoch 39: reducing lr to 0.0001149602951572551
Epoch 45: reducing lr to 0.00010412643101722237
Epoch 51: reducing lr to 9.073693578769825e-05
Epoch 54: reducing lr to 8.334185823497549e-05
Epoch 57: reducing lr to 7.563311104803588e-05
Epoch 60: reducing lr to 6.773229995880673e-05
Epoch 65: reducing lr to 5.4476385502737925e-05
Epoch 68: reducing lr to 4.667448625234373e-05
Epoch 71: reducing lr to 3.913722364170737e-05
Epoch 74: reducing lr to 3.198343073673226e-05
Epoch 77: reducing lr to 2.5325948201985224e-05
Epoch 80: reducing lr to 1.9269748811776465e-05
Epoch 83: reducing lr to 1.3910360291470407e-05
Epoch 86: reducing lr to 9.33228133668184e-06
Epoch 89: reducing lr to 5.607738101315774e-06
Epoch 92: reducing lr to 2.7954504111514643e-06
Epoch 95: reducing lr to 9.397780087935402e-07
Epoch 98: reducing lr to 6.998101764554929e-08
[I 2024-06-21 04:58:07,556] Trial 855 finished with value: 0.9769772291183472 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6222318059270193, 'bidirectional': True, 'fc_dropout': 0.21523578896244658, 'learning_rate_model': 0.001269055469891471}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.38386628342852e-05
Epoch 43: reducing lr to 6.0851651561043255e-05
Epoch 46: reducing lr to 5.746849730320109e-05
Epoch 49: reducing lr to 5.374255943682558e-05
Epoch 52: reducing lr to 4.973259582402403e-05
Epoch 55: reducing lr to 4.5501839524032754e-05
Epoch 58: reducing lr to 4.111702796252835e-05
Epoch 61: reducing lr to 3.664729283834406e-05
Epoch 64: reducing lr to 3.2163147693889125e-05
Epoch 67: reducing lr to 2.7735280404747982e-05
Epoch 70: reducing lr to 2.3433550651863907e-05
Epoch 73: reducing lr to 1.932578583899574e-05
Epoch 76: reducing lr to 1.5476762235726176e-05
Epoch 79: reducing lr to 1.1947192583826696e-05
Epoch 82: reducing lr to 8.792729859711015e-06
Epoch 85: reducing lr to 6.063131118813745e-06
Epoch 88: reducing lr to 3.8014326829577566e-06
Epoch 91: reducing lr to 2.0433162826086313e-06
Epoch 94: reducing lr to 8.164995320423044e-07
Epoch 97: reducing lr to 1.4033367881019367e-07
[I 2024-06-21 04:59:00,051] Trial 856 finished with value: 0.9780896902084351 and parameters: {'hidden_size': 129, 'n_layers': 3, 'rnn_dropout': 0.604134657862111, 'bidirectional': True, 'fc_dropout': 0.245541984777485, 'learning_rate_model': 0.0007146473317741664}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 9.02432433792105e-05
Epoch 36: reducing lr to 8.655205674659326e-05
Epoch 40: reducing lr to 8.231496709169737e-05
Epoch 45: reducing lr to 7.560790098474006e-05
Epoch 52: reducing lr to 6.412629599191166e-05
Epoch 61: reducing lr to 4.725382033484543e-05
Epoch 64: reducing lr to 4.147186558184006e-05
Epoch 67: reducing lr to 3.576247672546338e-05
Epoch 70: reducing lr to 3.0215732365149856e-05
Epoch 73: reducing lr to 2.4919090637715686e-05
Epoch 76: reducing lr to 1.995607548088595e-05
Epoch 83: reducing lr to 1.0100539635374609e-05
Epoch 86: reducing lr to 6.7763217885463994e-06
Epoch 89: reducing lr to 4.071870157947792e-06
Epoch 92: reducing lr to 2.0298221674296406e-06
Epoch 95: reducing lr to 6.823881500821433e-07
Epoch 98: reducing lr to 5.081435905625927e-08
[I 2024-06-21 04:59:39,096] Trial 857 finished with value: 0.9689373970031738 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.5765372193097876, 'bidirectional': True, 'fc_dropout': 0.1920614589365441, 'learning_rate_model': 0.0009214818886458041}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 9.540050330234529e-05
Epoch 45: reducing lr to 8.762722093471657e-05
Epoch 52: reducing lr to 7.432039553303296e-05
Epoch 55: reducing lr to 6.79979529496653e-05
Epoch 62: reducing lr to 5.252918573578722e-05
Epoch 65: reducing lr to 4.584440493676057e-05
Epoch 68: reducing lr to 3.9278744876716676e-05
Epoch 71: reducing lr to 3.2935788822494766e-05
Epoch 74: reducing lr to 2.6915540310358804e-05
Epoch 77: reducing lr to 2.1312959992929055e-05
Epoch 80: reducing lr to 1.6216387328273508e-05
Epoch 83: reducing lr to 1.170621332772443e-05
Epoch 86: reducing lr to 7.853547562569358e-06
Epoch 89: reducing lr to 4.7191716910641996e-06
Epoch 92: reducing lr to 2.352501169943107e-06
Epoch 95: reducing lr to 7.90866779948698e-07
Epoch 98: reducing lr to 5.889227196742564e-08
[I 2024-06-21 05:00:16,156] Trial 858 finished with value: 0.9729651808738708 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5391815377511116, 'bidirectional': True, 'fc_dropout': 0.1401713725060682, 'learning_rate_model': 0.0010679690348764333}. Best is trial 690 with value: 0.966281533241272.
Epoch 16: reducing lr to 5.730444755015748e-05
Epoch 19: reducing lr to 6.71710304584641e-05
Epoch 22: reducing lr to 7.302815585896051e-05
Epoch 25: reducing lr to 7.424965526846331e-05
Epoch 28: reducing lr to 7.378375336631399e-05
Epoch 31: reducing lr to 7.273992716649825e-05
Epoch 34: reducing lr to 7.113463981831289e-05
Epoch 37: reducing lr to 6.899320571897864e-05
Epoch 40: reducing lr to 6.63493962179792e-05
Epoch 43: reducing lr to 6.324490772030561e-05
Epoch 46: reducing lr to 5.972869619026774e-05
Epoch 49: reducing lr to 5.58562195937349e-05
Epoch 52: reducing lr to 5.1688546701586546e-05
Epoch 55: reducing lr to 4.7291397488444264e-05
Epoch 58: reducing lr to 4.2734134119838975e-05
Epoch 61: reducing lr to 3.808860719967533e-05
Epoch 64: reducing lr to 3.3428103522449476e-05
Epoch 67: reducing lr to 2.8826091072243918e-05
Epoch 70: reducing lr to 2.435517706613963e-05
Epoch 73: reducing lr to 2.008585651588385e-05
Epoch 76: reducing lr to 1.6085453299910912e-05
Epoch 79: reducing lr to 1.2417067952919252e-05
Epoch 82: reducing lr to 9.138542246945475e-06
Epoch 85: reducing lr to 6.3015901502824355e-06
Epoch 88: reducing lr to 3.950940575498415e-06
Epoch 91: reducing lr to 2.1236785924757347e-06
Epoch 94: reducing lr to 8.486119313604176e-07
Epoch 97: reducing lr to 1.458529117734516e-07
[I 2024-06-21 05:00:32,356] Trial 859 finished with value: 1.0926599502563477 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6268589861040984, 'bidirectional': False, 'fc_dropout': 0.17569597608863136, 'learning_rate_model': 0.000742753949829606}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.507204317482977e-05
Epoch 53: reducing lr to 3.413208835939233e-05
Epoch 56: reducing lr to 3.1103452726725876e-05
Epoch 61: reducing lr to 2.587410656357567e-05
Epoch 64: reducing lr to 2.2708163315708467e-05
Epoch 67: reducing lr to 1.9581953950286058e-05
Epoch 72: reducing lr to 1.4593238047984266e-05
Epoch 75: reducing lr to 1.180989697249147e-05
Epoch 78: reducing lr to 9.238177475891587e-06
Epoch 81: reducing lr to 6.918629500132903e-06
Epoch 84: reducing lr to 4.887840536432455e-06
Epoch 87: reducing lr to 3.177829034884484e-06
Epoch 90: reducing lr to 1.8155730260362547e-06
Epoch 93: reducing lr to 8.225492749781458e-07
Epoch 96: reducing lr to 2.1442128518146634e-07
Epoch 99: reducing lr to 7.777986205841427e-10
[I 2024-06-21 05:01:10,782] Trial 860 finished with value: 0.9730693101882935 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.5912847556274856, 'bidirectional': True, 'fc_dropout': 0.16005933729558564, 'learning_rate_model': 0.0005045628144830609}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.378549917016661e-05
Epoch 65: reducing lr to 3.545738424144112e-05
Epoch 72: reducing lr to 2.3889961005382223e-05
Epoch 75: reducing lr to 1.9333473299256842e-05
Epoch 78: reducing lr to 1.5123422158547954e-05
Epoch 81: reducing lr to 1.1326190145421003e-05
Epoch 84: reducing lr to 8.00167306474023e-06
Epoch 87: reducing lr to 5.202286941084224e-06
Epoch 90: reducing lr to 2.972196345445155e-06
Epoch 93: reducing lr to 1.3465599642532834e-06
Epoch 96: reducing lr to 3.5101984390755815e-07
Epoch 99: reducing lr to 1.2733005968178225e-09
[I 2024-06-21 05:01:44,988] Trial 861 finished with value: 0.9688360691070557 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.6124878271477585, 'bidirectional': True, 'fc_dropout': 0.12508701579386788, 'learning_rate_model': 0.0008259980357430831}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.4298790853093e-05
Epoch 47: reducing lr to 4.7854197084786174e-05
Epoch 50: reducing lr to 4.459876738309729e-05
Epoch 61: reducing lr to 3.117082346049077e-05
Epoch 64: reducing lr to 2.7356776477932326e-05
Epoch 67: reducing lr to 2.359059734472525e-05
Epoch 72: reducing lr to 1.758063590690305e-05
Epoch 75: reducing lr to 1.4227514009482499e-05
Epoch 78: reducing lr to 1.1129334977814357e-05
Epoch 81: reducing lr to 8.334949777194674e-06
Epoch 84: reducing lr to 5.888435764527964e-06
Epoch 87: reducing lr to 3.8283659221475e-06
Epoch 90: reducing lr to 2.1872409829939337e-06
Epoch 93: reducing lr to 9.90934244430783e-07
Epoch 96: reducing lr to 2.5831570300370977e-07
Epoch 99: reducing lr to 9.370226342129798e-10
[I 2024-06-21 05:02:16,294] Trial 862 finished with value: 0.9713476896286011 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.5584166978840844, 'bidirectional': True, 'fc_dropout': 0.2189121264856062, 'learning_rate_model': 0.0006078524248300214}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00011042864322917826
Epoch 40: reducing lr to 0.00010502269356815004
Epoch 45: reducing lr to 9.646539015931044e-05
Epoch 49: reducing lr to 8.841332353644715e-05
Epoch 52: reducing lr to 8.181642502652861e-05
Epoch 55: reducing lr to 7.485629455498505e-05
Epoch 58: reducing lr to 6.764272364775359e-05
Epoch 61: reducing lr to 6.0289442713650195e-05
Epoch 64: reducing lr to 5.29124554693579e-05
Epoch 67: reducing lr to 4.562805243173415e-05
Epoch 70: reducing lr to 3.8551161632456555e-05
Epoch 73: reducing lr to 3.179336774959047e-05
Epoch 76: reducing lr to 2.5461235958671163e-05
Epoch 79: reducing lr to 1.9654646416826928e-05
Epoch 82: reducing lr to 1.4465155325716076e-05
Epoch 85: reducing lr to 9.974619349525428e-06
Epoch 88: reducing lr to 6.253838693623284e-06
Epoch 91: reducing lr to 3.3615143282100237e-06
Epoch 94: reducing lr to 1.3432452426958381e-06
Epoch 97: reducing lr to 2.3086669257519482e-07
[I 2024-06-21 05:02:52,045] Trial 863 finished with value: 0.9742920398712158 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6344237503287439, 'bidirectional': True, 'fc_dropout': 0.10705174799075033, 'learning_rate_model': 0.001175685460847493}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.723284363045951e-05
Epoch 40: reducing lr to 8.296242657502289e-05
Epoch 43: reducing lr to 7.90806143246884e-05
Epoch 52: reducing lr to 6.463069002786202e-05
Epoch 62: reducing lr to 4.5680563139585165e-05
Epoch 65: reducing lr to 3.9867327181575936e-05
Epoch 68: reducing lr to 3.415768130138947e-05
Epoch 71: reducing lr to 2.864170384108978e-05
Epoch 74: reducing lr to 2.3406360128398977e-05
Epoch 77: reducing lr to 1.85342300858313e-05
Epoch 80: reducing lr to 1.4102135695975393e-05
Epoch 83: reducing lr to 1.0179986793099201e-05
Epoch 88: reducing lr to 4.940204976699471e-06
Epoch 91: reducing lr to 2.6554202349994323e-06
Epoch 94: reducing lr to 1.061093379280825e-06
Epoch 97: reducing lr to 1.8237259377621318e-07
[I 2024-06-21 05:03:24,840] Trial 864 finished with value: 0.9693262577056885 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5734101798491219, 'bidirectional': True, 'fc_dropout': 0.1930156897078546, 'learning_rate_model': 0.0009287299288090445}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.2682637819359e-05
Epoch 47: reducing lr to 5.524298528336401e-05
Epoch 53: reducing lr to 4.7468212708957795e-05
Epoch 65: reducing lr to 3.012193993974822e-05
Epoch 72: reducing lr to 2.029512289081939e-05
Epoch 75: reducing lr to 1.6424271953662624e-05
Epoch 79: reducing lr to 1.1730846362401353e-05
Epoch 88: reducing lr to 3.732594285050454e-06
Epoch 91: reducing lr to 2.006314806837859e-06
Epoch 94: reducing lr to 8.017139171529906e-07
Epoch 97: reducing lr to 1.3779244069619989e-07
[I 2024-06-21 05:04:04,705] Trial 865 finished with value: 0.9687158465385437 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.5983148998934996, 'bidirectional': True, 'fc_dropout': 0.14948558495655623, 'learning_rate_model': 0.0007017061115840481}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.240622667876049e-05
Epoch 40: reducing lr to 8.788255061806478e-05
Epoch 43: reducing lr to 8.377052574531894e-05
Epoch 52: reducing lr to 6.846364218526925e-05
Epoch 61: reducing lr to 5.044995344343472e-05
Epoch 64: reducing lr to 4.427692137884913e-05
Epoch 67: reducing lr to 3.818136339108013e-05
Epoch 72: reducing lr to 2.845424549445867e-05
Epoch 76: reducing lr to 2.1305855733783997e-05
Epoch 79: reducing lr to 1.644692589688818e-05
Epoch 82: reducing lr to 1.2104381461950394e-05
Epoch 85: reducing lr to 8.346719743116934e-06
Epoch 88: reducing lr to 5.2331860560491e-06
Epoch 91: reducing lr to 2.8129011270365827e-06
Epoch 99: reducing lr to 1.5165704021812345e-09
[I 2024-06-21 05:04:41,238] Trial 866 finished with value: 0.9676473140716553 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6685339850389322, 'bidirectional': True, 'fc_dropout': 0.2735371957834843, 'learning_rate_model': 0.0009838086751870064}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.422791562845033e-05
Epoch 49: reducing lr to 4.565175474245639e-05
Epoch 61: reducing lr to 3.113013675125931e-05
Epoch 65: reducing lr to 2.6059050391677002e-05
Epoch 72: reducing lr to 1.7557688222439255e-05
Epoch 75: reducing lr to 1.4208943094077462e-05
Epoch 78: reducing lr to 1.1114808059179837e-05
Epoch 81: reducing lr to 8.324070318765442e-06
Epoch 84: reducing lr to 5.8807496963660785e-06
Epoch 87: reducing lr to 3.823368825702379e-06
Epoch 90: reducing lr to 2.1843860170990874e-06
Epoch 93: reducing lr to 9.896407959749981e-07
Epoch 96: reducing lr to 2.5797852821231314e-07
Epoch 99: reducing lr to 9.357995556084387e-10
[I 2024-06-21 05:05:17,347] Trial 867 finished with value: 0.9725191593170166 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5848155966558463, 'bidirectional': True, 'fc_dropout': 0.1722795597832647, 'learning_rate_model': 0.0006070590061095939}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 7.378052209392739e-05
Epoch 40: reducing lr to 6.945756860198907e-05
Epoch 49: reducing lr to 5.847283359646176e-05
Epoch 53: reducing lr to 5.259872199615675e-05
Epoch 65: reducing lr to 3.33776111308433e-05
Epoch 68: reducing lr to 2.859739752345212e-05
Epoch 71: reducing lr to 2.397932644389745e-05
Epoch 74: reducing lr to 1.9596206758380795e-05
Epoch 79: reducing lr to 1.2998752035994288e-05
Epoch 82: reducing lr to 9.566642067910625e-06
Epoch 85: reducing lr to 6.596791457256027e-06
Epoch 88: reducing lr to 4.136024466047824e-06
Epoch 91: reducing lr to 2.223163433784023e-06
Epoch 94: reducing lr to 8.883656038901591e-07
Epoch 97: reducing lr to 1.526854681845437e-07
[I 2024-06-21 05:05:52,595] Trial 868 finished with value: 0.9703158140182495 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5361999274884363, 'bidirectional': True, 'fc_dropout': 0.23547331050540685, 'learning_rate_model': 0.0007775486495038894}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00013387211181136938
Epoch 31: reducing lr to 0.00013158574274530253
Epoch 35: reducing lr to 0.00012749557062783642
Epoch 49: reducing lr to 0.00010104329806878683
Epoch 52: reducing lr to 9.350402281246813e-05
Epoch 55: reducing lr to 8.554962736952443e-05
Epoch 58: reducing lr to 7.730558714837783e-05
Epoch 61: reducing lr to 6.890187911559649e-05
Epoch 64: reducing lr to 6.047107829101957e-05
Epoch 67: reducing lr to 5.214608746449138e-05
Epoch 70: reducing lr to 4.4058252307645545e-05
Epoch 73: reducing lr to 3.6335097535476213e-05
Epoch 76: reducing lr to 2.909841131705866e-05
Epoch 79: reducing lr to 2.2462341838256618e-05
Epoch 82: reducing lr to 1.65315242400666e-05
Epoch 85: reducing lr to 1.139950853268519e-05
Epoch 88: reducing lr to 7.147208835932891e-06
Epoch 91: reducing lr to 3.841711640755158e-06
Epoch 94: reducing lr to 1.5351298198991795e-06
Epoch 97: reducing lr to 2.638463423718356e-07
[I 2024-06-21 05:06:24,017] Trial 869 finished with value: 0.978422999382019 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.6119034440722989, 'bidirectional': True, 'fc_dropout': 0.20283066672571648, 'learning_rate_model': 0.001343633874441793}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.161098149187314e-05
Epoch 40: reducing lr to 7.76157783921071e-05
Epoch 53: reducing lr to 5.8776758707975285e-05
Epoch 63: reducing lr to 4.091824874468302e-05
Epoch 66: reducing lr to 3.5502483953892104e-05
Epoch 72: reducing lr to 2.5130112827637314e-05
Epoch 75: reducing lr to 2.0337093277422226e-05
Epoch 79: reducing lr to 1.4525533756889835e-05
Epoch 82: reducing lr to 1.0690301800721214e-05
Epoch 85: reducing lr to 7.371624347798824e-06
Epoch 88: reducing lr to 4.621825451746504e-06
Epoch 91: reducing lr to 2.4842873696715526e-06
Epoch 94: reducing lr to 9.927094948834022e-07
Epoch 97: reducing lr to 1.7061929608010652e-07
[I 2024-06-21 05:07:02,416] Trial 870 finished with value: 0.9709553718566895 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6476951707396267, 'bidirectional': True, 'fc_dropout': 0.13485682882887448, 'learning_rate_model': 0.0008688764217301984}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010821867159422716
Epoch 36: reducing lr to 0.00010379224254502223
Epoch 45: reducing lr to 9.066813536625805e-05
Epoch 52: reducing lr to 7.68995251793177e-05
Epoch 55: reducing lr to 7.035767581014453e-05
Epoch 61: reducing lr to 5.666624417410194e-05
Epoch 64: reducing lr to 4.973258976233779e-05
Epoch 67: reducing lr to 4.288595554889698e-05
Epoch 70: reducing lr to 3.623436276622684e-05
Epoch 73: reducing lr to 2.988269021779175e-05
Epoch 76: reducing lr to 2.3931098860229134e-05
Epoch 79: reducing lr to 1.8473466379535557e-05
Epoch 82: reducing lr to 1.359584674876671e-05
Epoch 85: reducing lr to 9.375177314020137e-06
Epoch 88: reducing lr to 5.878003419628017e-06
Epoch 91: reducing lr to 3.1594982992596633e-06
Epoch 94: reducing lr to 1.2625205920350698e-06
Epoch 97: reducing lr to 2.1699235860030315e-07
[I 2024-06-21 05:07:41,360] Trial 871 finished with value: 0.9727230072021484 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.5638147291917133, 'bidirectional': True, 'fc_dropout': 0.2613661779498196, 'learning_rate_model': 0.00110503060565265}. Best is trial 690 with value: 0.966281533241272.
Epoch 65: reducing lr to 2.1378296042269215e-05
Epoch 72: reducing lr to 1.4403957588456637e-05
Epoch 75: reducing lr to 1.1656717621988388e-05
Epoch 78: reducing lr to 9.11835441317686e-06
Epoch 81: reducing lr to 6.828891954100932e-06
Epoch 84: reducing lr to 4.824443180767351e-06
Epoch 87: reducing lr to 3.13661125045273e-06
Epoch 90: reducing lr to 1.7920242772565767e-06
Epoch 93: reducing lr to 8.118804635573983e-07
Epoch 96: reducing lr to 2.1164015057252918e-07
Epoch 99: reducing lr to 7.67710244052415e-10
[I 2024-06-21 05:08:01,212] Trial 872 finished with value: 0.974820077419281 and parameters: {'hidden_size': 85, 'n_layers': 2, 'rnn_dropout': 0.596379741186786, 'bidirectional': True, 'fc_dropout': 0.15640158016369293, 'learning_rate_model': 0.0004980184217258209}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.357874110138854e-05
Epoch 40: reducing lr to 6.0466292520522196e-05
Epoch 49: reducing lr to 5.09035304850309e-05
Epoch 52: reducing lr to 4.71053990386108e-05
Epoch 55: reducing lr to 4.3098138600177484e-05
Epoch 64: reducing lr to 3.0464082587190266e-05
Epoch 67: reducing lr to 2.627012383460391e-05
Epoch 72: reducing lr to 1.9577523859042665e-05
Epoch 75: reducing lr to 1.58435392468448e-05
Epoch 78: reducing lr to 1.2393455061422785e-05
Epoch 85: reducing lr to 5.7428373895008805e-06
Epoch 88: reducing lr to 3.6006164665678006e-06
Epoch 91: reducing lr to 1.9353751248969555e-06
Epoch 94: reducing lr to 7.733667554331151e-07
Epoch 97: reducing lr to 1.3292034790022025e-07
[I 2024-06-21 05:08:37,027] Trial 873 finished with value: 0.9735333919525146 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6170180429347608, 'bidirectional': True, 'fc_dropout': 0.17584707672624295, 'learning_rate_model': 0.0006768950459416568}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 8.034487735092755e-05
Epoch 40: reducing lr to 7.563730469833261e-05
Epoch 65: reducing lr to 3.6347263430320475e-05
Epoch 72: reducing lr to 2.4489530871480302e-05
Epoch 75: reducing lr to 1.9818688323033342e-05
Epoch 78: reducing lr to 1.5502976909453687e-05
Epoch 81: reducing lr to 1.1610445205835777e-05
Epoch 84: reducing lr to 8.202492230870552e-06
Epoch 87: reducing lr to 5.332849501817028e-06
Epoch 90: reducing lr to 3.0467899944031463e-06
Epoch 93: reducing lr to 1.3803547777851457e-06
Epoch 96: reducing lr to 3.598294405729452e-07
Epoch 99: reducing lr to 1.305256809222815e-09
[I 2024-06-21 05:09:11,230] Trial 874 finished with value: 0.9689077138900757 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5508826493977743, 'bidirectional': True, 'fc_dropout': 0.12472630886853536, 'learning_rate_model': 0.0008467282299688578}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010072814389442757
Epoch 36: reducing lr to 9.660809718124494e-05
Epoch 40: reducing lr to 9.187872176796867e-05
Epoch 49: reducing lr to 7.734807476172138e-05
Epoch 52: reducing lr to 7.157680207644196e-05
Epoch 61: reducing lr to 5.274399983884289e-05
Epoch 65: reducing lr to 4.415202415079045e-05
Epoch 71: reducing lr to 3.1719939336590233e-05
Epoch 74: reducing lr to 2.592193283899799e-05
Epoch 77: reducing lr to 2.0526175999681927e-05
Epoch 80: reducing lr to 1.5617747158986163e-05
Epoch 83: reducing lr to 1.1274069633425486e-05
Epoch 86: reducing lr to 7.56362793082957e-06
Epoch 89: reducing lr to 4.544959908695813e-06
Epoch 92: reducing lr to 2.2656568149018305e-06
Epoch 95: reducing lr to 7.616713362627435e-07
Epoch 98: reducing lr to 5.67182193287729e-08
[I 2024-06-21 05:09:43,859] Trial 875 finished with value: 0.9702184200286865 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.6336053036816529, 'bidirectional': True, 'fc_dropout': 0.22235512936067553, 'learning_rate_model': 0.0010285441524479422}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.461011239879909e-05
Epoch 61: reducing lr to 3.13495410486394e-05
Epoch 65: reducing lr to 2.6242713820053905e-05
Epoch 72: reducing lr to 1.768143429778879e-05
Epoch 75: reducing lr to 1.4309087311270584e-05
Epoch 78: reducing lr to 1.1193144902741574e-05
Epoch 86: reducing lr to 4.495606420041632e-06
Epoch 89: reducing lr to 2.701395564565234e-06
[I 2024-06-21 05:10:20,419] Trial 876 finished with value: 0.969847559928894 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5789262374647713, 'bidirectional': True, 'fc_dropout': 0.10019611319978353, 'learning_rate_model': 0.0006113375403083985}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.923116183744953e-05
Epoch 40: reducing lr to 6.584200332817556e-05
Epoch 43: reducing lr to 6.276125574571725e-05
Epoch 49: reducing lr to 5.5429071039435125e-05
Epoch 61: reducing lr to 3.779733268058027e-05
Epoch 65: reducing lr to 3.1640162870610474e-05
Epoch 72: reducing lr to 2.131804907084439e-05
Epoch 75: reducing lr to 1.7252097331199602e-05
Epoch 78: reducing lr to 1.3495285974823748e-05
Epoch 81: reducing lr to 1.0106851043055368e-05
Epoch 86: reducing lr to 5.420236653405207e-06
Epoch 89: reducing lr to 3.257002923816256e-06
Epoch 92: reducing lr to 1.6236118730950403e-06
Epoch 95: reducing lr to 5.458278662586997e-07
Epoch 98: reducing lr to 4.06453323898489e-08
[I 2024-06-21 05:10:57,604] Trial 877 finished with value: 0.9718574285507202 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.5981228026722887, 'bidirectional': True, 'fc_dropout': 0.20210147601030204, 'learning_rate_model': 0.0007370738970408965}. Best is trial 690 with value: 0.966281533241272.
Epoch 13: reducing lr to 7.808349388040974e-05
Epoch 16: reducing lr to 9.984640727171828e-05
Epoch 19: reducing lr to 0.00011703779288939968
Epoch 22: reducing lr to 0.0001272431600673594
Epoch 25: reducing lr to 0.00012937148226114078
Epoch 28: reducing lr to 0.00012855970179628326
Epoch 31: reducing lr to 0.00012674095472998498
Epoch 34: reducing lr to 0.00012394392620589424
Epoch 37: reducing lr to 0.00012021272364887588
Epoch 40: reducing lr to 0.00011560618975018371
Epoch 43: reducing lr to 0.00011019697569855586
Epoch 46: reducing lr to 0.00010407038162966831
Epoch 49: reducing lr to 9.732303666889218e-05
Epoch 52: reducing lr to 9.006134612383427e-05
Epoch 55: reducing lr to 8.239981948952485e-05
Epoch 58: reducing lr to 7.44593123596381e-05
Epoch 61: reducing lr to 6.636501614543238e-05
Epoch 64: reducing lr to 5.824462465504526e-05
Epoch 67: reducing lr to 5.022614739862366e-05
Epoch 70: reducing lr to 4.2436094098841144e-05
Epoch 73: reducing lr to 3.499729420357572e-05
Epoch 76: reducing lr to 2.802705182573034e-05
Epoch 79: reducing lr to 2.1635312387622337e-05
Epoch 82: reducing lr to 1.5922858522624636e-05
Epoch 85: reducing lr to 1.0979795871059227e-05
Epoch 88: reducing lr to 6.884059417243057e-06
Epoch 91: reducing lr to 3.7002656289979346e-06
Epoch 94: reducing lr to 1.478608661920869e-06
Epoch 97: reducing lr to 2.5413191913161747e-07
[I 2024-06-21 05:11:12,940] Trial 878 finished with value: 1.0926153659820557 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.6143023730315352, 'bidirectional': False, 'fc_dropout': 0.18427290346379263, 'learning_rate_model': 0.001294163307524294}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.250962207928195e-05
Epoch 40: reducing lr to 7.847042671775456e-05
Epoch 43: reducing lr to 7.479879515757474e-05
Epoch 49: reducing lr to 6.606030553708745e-05
Epoch 52: reducing lr to 6.113126188471612e-05
Epoch 55: reducing lr to 5.593082006059877e-05
Epoch 63: reducing lr to 4.136881064720496e-05
Epoch 66: reducing lr to 3.5893411405707824e-05
Epoch 72: reducing lr to 2.5406827295963198e-05
Epoch 75: reducing lr to 2.056103051129595e-05
Epoch 78: reducing lr to 1.6083666893370197e-05
Epoch 83: reducing lr to 9.628791497717587e-06
Epoch 86: reducing lr to 6.459832046481952e-06
Epoch 89: reducing lr to 3.881692480998174e-06
Epoch 92: reducing lr to 1.9350188339615723e-06
Epoch 95: reducing lr to 6.505170470934426e-07
Epoch 98: reducing lr to 4.844106217150469e-08
[I 2024-06-21 05:11:48,749] Trial 879 finished with value: 0.9726775288581848 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5704834584194935, 'bidirectional': True, 'fc_dropout': 0.29411384810624647, 'learning_rate_model': 0.0008784438549816542}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.122054506330382e-05
Epoch 53: reducing lr to 3.8788216525609625e-05
Epoch 56: reducing lr to 3.534642962232697e-05
Epoch 61: reducing lr to 2.9403722304571524e-05
Epoch 65: reducing lr to 2.461386814199298e-05
Epoch 72: reducing lr to 1.6583974330982195e-05
Epoch 75: reducing lr to 1.3420943837094191e-05
Epoch 78: reducing lr to 1.0498403275646282e-05
Epoch 81: reducing lr to 7.862434208124832e-06
Epoch 84: reducing lr to 5.554615207645902e-06
Epoch 87: reducing lr to 3.611332520547339e-06
Epoch 90: reducing lr to 2.0632443848860474e-06
Epoch 93: reducing lr to 9.347573182423227e-07
Epoch 96: reducing lr to 2.436715606072637e-07
Epoch 99: reducing lr to 8.83902004215377e-10
[I 2024-06-21 05:12:26,092] Trial 880 finished with value: 0.9734410643577576 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.5885394913270117, 'bidirectional': True, 'fc_dropout': 0.1456392904838249, 'learning_rate_model': 0.0005733927409558716}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010943040244739751
Epoch 36: reducing lr to 0.00010495441041087001
Epoch 39: reducing lr to 0.00010122257225518428
Epoch 45: reducing lr to 9.168335182940842e-05
Epoch 52: reducing lr to 7.776057370154795e-05
Epoch 61: reducing lr to 5.7300739454699224e-05
Epoch 65: reducing lr to 4.796647277400594e-05
Epoch 68: reducing lr to 4.1096898287263205e-05
Epoch 71: reducing lr to 3.4460336436340625e-05
Epoch 74: reducing lr to 2.8161419769225852e-05
Epoch 77: reducing lr to 2.229950452284237e-05
Epoch 80: reducing lr to 1.6967019254527357e-05
Epoch 83: reducing lr to 1.2248076153361796e-05
Epoch 86: reducing lr to 8.217076344626704e-06
Epoch 89: reducing lr to 4.93761497717207e-06
Epoch 92: reducing lr to 2.461394874130225e-06
Epoch 95: reducing lr to 8.274748013547758e-07
Epoch 98: reducing lr to 6.161830574137866e-08
[I 2024-06-21 05:13:03,216] Trial 881 finished with value: 0.9730626940727234 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6293112291383292, 'bidirectional': True, 'fc_dropout': 0.24594395827827717, 'learning_rate_model': 0.0011174036985657427}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.837158142242397e-05
Epoch 40: reducing lr to 6.502450301408532e-05
Epoch 49: reducing lr to 5.474085864166506e-05
Epoch 52: reducing lr to 5.065640762952777e-05
Epoch 55: reducing lr to 4.6347062578006804e-05
Epoch 65: reducing lr to 3.12473157247594e-05
Epoch 72: reducing lr to 2.105336223067729e-05
Epoch 75: reducing lr to 1.7037893718398288e-05
Epoch 78: reducing lr to 1.3327727274214843e-05
Epoch 81: reducing lr to 9.981363459377524e-06
Epoch 84: reducing lr to 7.051586289549686e-06
Epoch 87: reducing lr to 4.584588119415213e-06
Epoch 90: reducing lr to 2.6192895947906547e-06
Epoch 93: reducing lr to 1.1866747997774206e-06
Epoch 96: reducing lr to 3.093411463617104e-07
Epoch 99: reducing lr to 1.1221139577092567e-09
[I 2024-06-21 05:13:38,888] Trial 882 finished with value: 0.973649263381958 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.523399558795757, 'bidirectional': True, 'fc_dropout': 0.16290146906812777, 'learning_rate_model': 0.0007279223203591342}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.841466873838604e-05
Epoch 40: reducing lr to 8.408639633985396e-05
Epoch 49: reducing lr to 7.078810790341443e-05
Epoch 52: reducing lr to 6.550630257284702e-05
Epoch 55: reducing lr to 5.993367565267065e-05
Epoch 61: reducing lr to 4.827072895287396e-05
Epoch 64: reducing lr to 4.2364345749941836e-05
Epoch 67: reducing lr to 3.653209007156887e-05
Epoch 72: reducing lr to 2.722514250407678e-05
Epoch 77: reducing lr to 1.8785330675470615e-05
Epoch 80: reducing lr to 1.4293190548107609e-05
Epoch 83: reducing lr to 1.0317904617278024e-05
Epoch 86: reducing lr to 6.922149151846921e-06
Epoch 89: reducing lr to 4.159497355617073e-06
Epoch 92: reducing lr to 2.073504215579353e-06
Epoch 95: reducing lr to 6.970732355575946e-07
Epoch 98: reducing lr to 5.190789095013692e-08
[I 2024-06-21 05:14:09,987] Trial 883 finished with value: 0.9725596904754639 and parameters: {'hidden_size': 123, 'n_layers': 2, 'rnn_dropout': 0.6049253436875259, 'bidirectional': True, 'fc_dropout': 0.2255518519208197, 'learning_rate_model': 0.0009413123037800938}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.011644262779466e-05
Epoch 40: reducing lr to 7.619440353357465e-05
Epoch 43: reducing lr to 7.262926710671167e-05
Epoch 49: reducing lr to 6.414423609226053e-05
Epoch 53: reducing lr to 5.770038211517106e-05
Epoch 62: reducing lr to 4.195397127579169e-05
Epoch 65: reducing lr to 3.6614975483282e-05
Epoch 68: reducing lr to 3.137111895462405e-05
Epoch 71: reducing lr to 2.6305131496890863e-05
Epoch 74: reducing lr to 2.1496883860582904e-05
Epoch 79: reducing lr to 1.4259528198271271e-05
Epoch 82: reducing lr to 1.0494530702054827e-05
Epoch 88: reducing lr to 4.53718613441014e-06
Epoch 91: reducing lr to 2.4387927076096403e-06
Epoch 94: reducing lr to 9.745300428817046e-07
Epoch 97: reducing lr to 1.6749475126650193e-07
[I 2024-06-21 05:14:42,806] Trial 884 finished with value: 0.9698536396026611 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.556364588127399, 'bidirectional': True, 'fc_dropout': 0.13957009694544012, 'learning_rate_model': 0.0008529647201844141}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.430744182429672e-05
Epoch 40: reducing lr to 6.053953571165014e-05
Epoch 43: reducing lr to 5.770689060883926e-05
Epoch 49: reducing lr to 5.096519024383183e-05
Epoch 52: reducing lr to 4.716245809748682e-05
Epoch 55: reducing lr to 4.315034363989742e-05
Epoch 61: reducing lr to 3.475339230211293e-05
Epoch 65: reducing lr to 2.909207911673708e-05
Epoch 72: reducing lr to 1.9601238233813252e-05
Epoch 75: reducing lr to 1.5862730623144347e-05
Epoch 78: reducing lr to 1.2408467329580138e-05
Epoch 81: reducing lr to 9.292913926140293e-06
Epoch 84: reducing lr to 6.565213730391794e-06
Epoch 87: reducing lr to 4.268373048824732e-06
Epoch 90: reducing lr to 2.438628034244776e-06
Epoch 93: reducing lr to 1.1048256901506518e-06
Epoch 96: reducing lr to 2.8800480602197844e-07
Epoch 99: reducing lr to 1.0447178350712904e-09
[I 2024-06-21 05:15:17,254] Trial 885 finished with value: 0.9730492830276489 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5353416876843112, 'bidirectional': True, 'fc_dropout': 0.20644609234899597, 'learning_rate_model': 0.0006777149730639727}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010992858157236364
Epoch 34: reducing lr to 0.00010750258297604731
Epoch 40: reducing lr to 0.00010027085946526014
Epoch 43: reducing lr to 9.55791855751307e-05
Epoch 47: reducing lr to 8.836995069915313e-05
Epoch 50: reducing lr to 8.235831160022307e-05
Epoch 53: reducing lr to 7.593296407409637e-05
Epoch 56: reducing lr to 6.91952198649728e-05
Epoch 59: reducing lr to 6.22513671019374e-05
Epoch 62: reducing lr to 5.5210889024819034e-05
Epoch 65: reducing lr to 4.818483892180251e-05
Epoch 68: reducing lr to 4.128399087186201e-05
Epoch 72: reducing lr to 3.246528043507191e-05
Epoch 79: reducing lr to 1.8765356531464044e-05
Epoch 82: reducing lr to 1.3810668033065036e-05
Epoch 85: reducing lr to 9.52330987746676e-06
Epoch 88: reducing lr to 5.970878859241891e-06
Epoch 91: reducing lr to 3.209419977175522e-06
Epoch 94: reducing lr to 1.2824690586547622e-06
Epoch 97: reducing lr to 2.2042094808198512e-07
[I 2024-06-21 05:15:39,988] Trial 886 finished with value: 0.9732003808021545 and parameters: {'hidden_size': 98, 'n_layers': 2, 'rnn_dropout': 0.6461755885301039, 'bidirectional': True, 'fc_dropout': 0.12281907965166129, 'learning_rate_model': 0.001122490650494417}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.290500124952866e-05
Epoch 50: reducing lr to 3.524038270900518e-05
Epoch 61: reducing lr to 2.4630092097990292e-05
Epoch 65: reducing lr to 2.061786031528468e-05
Epoch 68: reducing lr to 1.766504934124406e-05
Epoch 73: reducing lr to 1.2988568819535482e-05
Epoch 76: reducing lr to 1.040169818071228e-05
Epoch 79: reducing lr to 8.029527718462018e-06
Epoch 82: reducing lr to 5.909460957804748e-06
Epoch 85: reducing lr to 4.074938864306063e-06
Epoch 88: reducing lr to 2.554885499962364e-06
Epoch 91: reducing lr to 1.37328201698207e-06
Epoch 94: reducing lr to 5.487570053503733e-07
Epoch 97: reducing lr to 9.431614631922572e-08
[I 2024-06-21 05:16:34,538] Trial 887 finished with value: 0.9769246578216553 and parameters: {'hidden_size': 132, 'n_layers': 3, 'rnn_dropout': 0.5772526095814591, 'bidirectional': True, 'fc_dropout': 0.1907841063461297, 'learning_rate_model': 0.00048030367964216857}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.319558409389178e-05
Epoch 40: reducing lr to 6.961235032908007e-05
Epoch 43: reducing lr to 6.635518819632064e-05
Epoch 49: reducing lr to 5.8603136547661044e-05
Epoch 61: reducing lr to 3.996174221721162e-05
Epoch 65: reducing lr to 3.345199099182876e-05
Epoch 72: reducing lr to 2.2538796288676974e-05
Epoch 75: reducing lr to 1.824001370895305e-05
Epoch 78: reducing lr to 1.426807398900241e-05
Epoch 81: reducing lr to 1.0685605236314639e-05
Epoch 84: reducing lr to 7.549115678093515e-06
Epoch 87: reducing lr to 4.9080568015127946e-06
Epoch 90: reducing lr to 2.8040953245945514e-06
Epoch 93: reducing lr to 1.2704014342239954e-06
Epoch 96: reducing lr to 3.3116691790887515e-07
Epoch 99: reducing lr to 1.2012854587483725e-09
[I 2024-06-21 05:17:11,741] Trial 888 finished with value: 0.9715487360954285 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6144338299483992, 'bidirectional': True, 'fc_dropout': 0.1709216672583368, 'learning_rate_model': 0.0007792813666906533}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.277526629735731e-05
Epoch 40: reducing lr to 8.823352418475044e-05
Epoch 49: reducing lr to 7.427936625378145e-05
Epoch 52: reducing lr to 6.873706311487429e-05
Epoch 55: reducing lr to 6.288959511129037e-05
Epoch 58: reducing lr to 5.682920224307085e-05
Epoch 63: reducing lr to 4.651581630699419e-05
Epoch 66: reducing lr to 4.035918136051322e-05
Epoch 72: reducing lr to 2.856788225122442e-05
Epoch 75: reducing lr to 2.311918335052653e-05
Epoch 78: reducing lr to 1.8084757164886966e-05
Epoch 83: reducing lr to 1.0826782050511773e-05
Epoch 86: reducing lr to 7.263548459508122e-06
Epoch 89: reducing lr to 4.364643111115182e-06
Epoch 92: reducing lr to 2.175769117433214e-06
Epoch 95: reducing lr to 7.31452777920534e-07
Epoch 98: reducing lr to 5.446797996929917e-08
[I 2024-06-21 05:17:47,559] Trial 889 finished with value: 0.9730082154273987 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.589038030795497, 'bidirectional': True, 'fc_dropout': 0.2489344763805524, 'learning_rate_model': 0.000987737678581176}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00013271058374103638
Epoch 31: reducing lr to 0.00013044405212889063
Epoch 36: reducing lr to 0.00012510854640577175
Epoch 45: reducing lr to 0.00010928907924957736
Epoch 52: reducing lr to 9.269274445347151e-05
Epoch 55: reducing lr to 8.480736453186754e-05
Epoch 58: reducing lr to 7.663485290618604e-05
Epoch 61: reducing lr to 6.830405881076541e-05
Epoch 64: reducing lr to 5.994640699146398e-05
Epoch 67: reducing lr to 5.169364712028145e-05
Epoch 70: reducing lr to 4.367598526118818e-05
Epoch 73: reducing lr to 3.601983967366547e-05
Epoch 76: reducing lr to 2.8845941843846186e-05
Epoch 79: reducing lr to 2.2267449562206528e-05
Epoch 82: reducing lr to 1.6388090113343602e-05
Epoch 85: reducing lr to 1.1300601830090021e-05
Epoch 88: reducing lr to 7.085196788949102e-06
Epoch 91: reducing lr to 3.808379411596413e-06
Epoch 94: reducing lr to 1.5218104186191795e-06
Epoch 97: reducing lr to 2.615571058103665e-07
[I 2024-06-21 05:18:26,512] Trial 890 finished with value: 0.9796702265739441 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.5513717140848632, 'bidirectional': True, 'fc_dropout': 0.15538226469046523, 'learning_rate_model': 0.0013319759686965438}. Best is trial 690 with value: 0.966281533241272.
Epoch 53: reducing lr to 2.7035150302803052e-05
Epoch 65: reducing lr to 1.7155715945661084e-05
Epoch 68: reducing lr to 1.4698740025889353e-05
Epoch 73: reducing lr to 1.0807532585882903e-05
Epoch 76: reducing lr to 8.655048419767883e-06
Epoch 79: reducing lr to 6.681212046704294e-06
Epoch 82: reducing lr to 4.917146204008158e-06
Epoch 85: reducing lr to 3.390676461230278e-06
Epoch 88: reducing lr to 2.1258699613242303e-06
Epoch 91: reducing lr to 1.1426809492526919e-06
Epoch 94: reducing lr to 4.5660990825529366e-07
Epoch 97: reducing lr to 7.847860983627663e-08
[I 2024-06-21 05:18:59,407] Trial 891 finished with value: 0.974136233329773 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.602021103482188, 'bidirectional': True, 'fc_dropout': 0.22806288021179977, 'learning_rate_model': 0.00039965124264074593}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.376806221453354e-05
Epoch 47: reducing lr to 4.7386459360443865e-05
Epoch 50: reducing lr to 4.416284896350115e-05
Epoch 53: reducing lr to 4.071739644242774e-05
Epoch 56: reducing lr to 3.710442801118293e-05
Epoch 59: reducing lr to 3.338093836162226e-05
Epoch 62: reducing lr to 2.9605635494075652e-05
Epoch 65: reducing lr to 2.5838069313072614e-05
Epoch 72: reducing lr to 1.740879880310595e-05
Epoch 75: reducing lr to 1.4088451076004524e-05
Epoch 78: reducing lr to 1.1020554345537884e-05
Epoch 81: reducing lr to 8.253482096640238e-06
Epoch 84: reducing lr to 5.830880864180225e-06
Epoch 87: reducing lr to 3.7909466094547758e-06
Epoch 90: reducing lr to 2.1658623959044727e-06
Epoch 93: reducing lr to 9.812486294440474e-07
Epoch 96: reducing lr to 2.5579086701345074e-07
Epoch 99: reducing lr to 9.278639634836134e-10
[I 2024-06-21 05:19:35,101] Trial 892 finished with value: 0.9752528071403503 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6302919374892202, 'bidirectional': True, 'fc_dropout': 0.2160063801388896, 'learning_rate_model': 0.0006019111380203772}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.621363382050045e-05
Epoch 40: reducing lr to 7.248265374259981e-05
Epoch 52: reducing lr to 5.6466573120285196e-05
Epoch 65: reducing lr to 3.483130606277483e-05
Epoch 72: reducing lr to 2.3468131149778334e-05
Epoch 77: reducing lr to 1.6192995276990126e-05
Epoch 80: reducing lr to 1.2320760865862596e-05
Epoch 83: reducing lr to 8.894055879153802e-06
Epoch 86: reducing lr to 5.9669073948665265e-06
Epoch 89: reducing lr to 3.585495629422706e-06
Epoch 92: reducing lr to 1.7873650748953051e-06
Epoch 95: reducing lr to 6.008786220537297e-07
Epoch 98: reducing lr to 4.4744713176205365e-08
[I 2024-06-21 05:20:14,033] Trial 893 finished with value: 0.9685307741165161 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.563541409065235, 'bidirectional': True, 'fc_dropout': 0.18364174268660818, 'learning_rate_model': 0.0008114132219768281}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.945174475175798e-05
Epoch 40: reducing lr to 8.507270309120287e-05
Epoch 43: reducing lr to 8.109215099476837e-05
Epoch 49: reducing lr to 7.161842995049283e-05
Epoch 53: reducing lr to 6.442372731180834e-05
Epoch 56: reducing lr to 5.870722986016507e-05
Epoch 61: reducing lr to 4.8836929288851346e-05
Epoch 65: reducing lr to 4.088141377218529e-05
Epoch 68: reducing lr to 3.502653429512752e-05
Epoch 71: reducing lr to 2.9370249491144695e-05
Epoch 74: reducing lr to 2.4001736784402948e-05
Epoch 77: reducing lr to 1.9005676644355443e-05
Epoch 80: reducing lr to 1.4460845138499813e-05
Epoch 84: reducing lr to 9.225714598739957e-06
Epoch 87: reducing lr to 5.998097421736266e-06
Epoch 90: reducing lr to 3.426862731411163e-06
Epoch 93: reducing lr to 1.5525475509656456e-06
Epoch 96: reducing lr to 4.047164726906347e-07
Epoch 99: reducing lr to 1.4680814636549613e-09
[I 2024-06-21 05:20:46,707] Trial 894 finished with value: 0.9702565670013428 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.581484988074719, 'bidirectional': True, 'fc_dropout': 0.14540088674505303, 'learning_rate_model': 0.0009523535984574597}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.028134964256968e-05
Epoch 43: reducing lr to 5.7460785066898584e-05
Epoch 49: reducing lr to 5.074783637789426e-05
Epoch 53: reducing lr to 4.5649768847679556e-05
Epoch 58: reducing lr to 3.882584360055446e-05
Epoch 61: reducing lr to 3.460517772398285e-05
Epoch 65: reducing lr to 2.8968008631883946e-05
Epoch 72: reducing lr to 1.9517643825808506e-05
Epoch 75: reducing lr to 1.579508002066898e-05
Epoch 78: reducing lr to 1.2355548301287678e-05
Epoch 81: reducing lr to 9.253281958555988e-06
Epoch 84: reducing lr to 6.537214726008993e-06
Epoch 87: reducing lr to 4.250169498931428e-06
Epoch 90: reducing lr to 2.428227891945864e-06
Epoch 93: reducing lr to 1.1001138832528005e-06
Epoch 96: reducing lr to 2.86776537125155e-07
Epoch 99: reducing lr to 1.0402623732254716e-09
[I 2024-06-21 05:21:22,857] Trial 895 finished with value: 0.9726958274841309 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.6205446953364423, 'bidirectional': True, 'fc_dropout': 0.11233578601865792, 'learning_rate_model': 0.0006748246871905268}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.0001213131423257712
Epoch 40: reducing lr to 0.0001087653814603942
Epoch 52: reducing lr to 8.473211241684284e-05
Epoch 56: reducing lr to 7.505714545567537e-05
Epoch 59: reducing lr to 6.752503893336096e-05
Epoch 62: reducing lr to 5.988812141011357e-05
Epoch 65: reducing lr to 5.22668541377492e-05
Epoch 68: reducing lr to 4.478139550545294e-05
Epoch 71: reducing lr to 3.7549840000577494e-05
Epoch 74: reducing lr to 3.0686200887127e-05
Epoch 77: reducing lr to 2.4298742076175828e-05
Epoch 80: reducing lr to 1.8488178705717684e-05
Epoch 83: reducing lr to 1.3346163950640255e-05
Epoch 86: reducing lr to 8.95376928728639e-06
Epoch 89: reducing lr to 5.380291417635239e-06
Epoch 92: reducing lr to 2.682068524565047e-06
Epoch 95: reducing lr to 9.016611446258137e-07
Epoch 98: reducing lr to 6.714262717570901e-08
[I 2024-06-21 05:21:57,124] Trial 896 finished with value: 0.9730369448661804 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.6022271730827914, 'bidirectional': True, 'fc_dropout': 0.16502434445230915, 'learning_rate_model': 0.0012175832982567582}. Best is trial 690 with value: 0.966281533241272.
Epoch 49: reducing lr to 4.13824895463641e-05
Epoch 55: reducing lr to 3.503702499798258e-05
Epoch 61: reducing lr to 2.8218905624843383e-05
Epoch 64: reducing lr to 2.476605389040156e-05
Epoch 67: reducing lr to 2.1356536857239697e-05
Epoch 72: reducing lr to 1.591572664451577e-05
Epoch 75: reducing lr to 1.2880149785539309e-05
Epoch 78: reducing lr to 1.0075372368788466e-05
Epoch 81: reducing lr to 7.545619109119387e-06
Epoch 84: reducing lr to 5.330793179967892e-06
Epoch 87: reducing lr to 3.4658146516847285e-06
Epoch 90: reducing lr to 1.9801063952040993e-06
Epoch 93: reducing lr to 8.970914727184369e-07
Epoch 96: reducing lr to 2.338528673686267e-07
Epoch 99: reducing lr to 8.482853626565895e-10
[I 2024-06-21 05:22:27,211] Trial 897 finished with value: 0.9754148721694946 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.5322741891870363, 'bidirectional': True, 'fc_dropout': 0.20532658392164316, 'learning_rate_model': 0.0005502880035188046}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.177550843793215e-05
Epoch 40: reducing lr to 6.826179338933829e-05
Epoch 43: reducing lr to 6.506782382085038e-05
Epoch 49: reducing lr to 5.746617058715365e-05
Epoch 52: reducing lr to 5.317837232379756e-05
Epoch 55: reducing lr to 4.86544835139658e-05
Epoch 65: reducing lr to 3.280298519948561e-05
Epoch 72: reducing lr to 2.2101518598767066e-05
Epoch 75: reducing lr to 1.788613717728649e-05
Epoch 78: reducing lr to 1.3991257500958233e-05
Epoch 81: reducing lr to 1.0478292622403116e-05
Epoch 84: reducing lr to 7.402654446433249e-06
Epoch 87: reducing lr to 4.8128350464277535e-06
Epoch 90: reducing lr to 2.7496927597849236e-06
Epoch 93: reducing lr to 1.2457542349103893e-06
Epoch 96: reducing lr to 3.247419117557884e-07
Epoch 99: reducing lr to 1.1779791861572797e-09
[I 2024-06-21 05:23:02,965] Trial 898 finished with value: 0.9735647439956665 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5686196317284561, 'bidirectional': True, 'fc_dropout': 0.1296542038226491, 'learning_rate_model': 0.0007641624423500702}. Best is trial 690 with value: 0.966281533241272.
Epoch 15: reducing lr to 7.248028797464514e-05
Epoch 18: reducing lr to 8.73128645411473e-05
Epoch 21: reducing lr to 9.72520724004457e-05
Epoch 24: reducing lr to 0.00010090197923618453
Epoch 29: reducing lr to 9.985247136594677e-05
Epoch 32: reducing lr to 9.817703743023729e-05
Epoch 35: reducing lr to 9.574898042899224e-05
Epoch 38: reducing lr to 9.260659162092879e-05
Epoch 41: reducing lr to 8.879942875412202e-05
Epoch 44: reducing lr to 8.438753308092747e-05
Epoch 47: reducing lr to 7.944048478758128e-05
Epoch 50: reducing lr to 7.403630021342875e-05
Epoch 53: reducing lr to 6.826021096175645e-05
Epoch 56: reducing lr to 6.220329158913283e-05
Epoch 59: reducing lr to 5.596109019120441e-05
Epoch 62: reducing lr to 4.963202711990431e-05
Epoch 65: reducing lr to 4.3315934127778006e-05
Epoch 68: reducing lr to 3.711239197124762e-05
Epoch 71: reducing lr to 3.1119270956827896e-05
Epoch 74: reducing lr to 2.5431059094458765e-05
Epoch 77: reducing lr to 2.0137479642175865e-05
Epoch 80: reducing lr to 1.532199984427726e-05
Epoch 83: reducing lr to 1.106057688149498e-05
Epoch 86: reducing lr to 7.420398396683023e-06
Epoch 89: reducing lr to 4.458893738282558e-06
Epoch 92: reducing lr to 2.2227529368816458e-06
Epoch 95: reducing lr to 7.472478569928165e-07
Epoch 98: reducing lr to 5.5644168065762353e-08
[I 2024-06-21 05:23:20,188] Trial 899 finished with value: 1.0930367708206177 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.638833430062352, 'bidirectional': False, 'fc_dropout': 0.23813854820693928, 'learning_rate_model': 0.0010090670045565444}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.14858442721772e-05
Epoch 40: reducing lr to 7.749676716916902e-05
Epoch 43: reducing lr to 7.387069314291616e-05
Epoch 52: reducing lr to 6.03727463605783e-05
Epoch 55: reducing lr to 5.52368315187995e-05
Epoch 61: reducing lr to 4.448787920019259e-05
Epoch 65: reducing lr to 3.7240822138365974e-05
Epoch 72: reducing lr to 2.5091579870522846e-05
Epoch 75: reducing lr to 2.0305909639351557e-05
Epoch 78: reducing lr to 1.588410106326039e-05
Epoch 81: reducing lr to 1.1895875618990493e-05
Epoch 84: reducing lr to 8.40414175462688e-06
Epoch 87: reducing lr to 5.4639519194771585e-06
Epoch 90: reducing lr to 3.1216920770951164e-06
Epoch 93: reducing lr to 1.4142893278853638e-06
Epoch 96: reducing lr to 3.686754636209131e-07
Epoch 99: reducing lr to 1.3373451558510753e-09
[I 2024-06-21 05:23:54,649] Trial 900 finished with value: 0.9724173545837402 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.590601244156701, 'bidirectional': True, 'fc_dropout': 0.19197125916915153, 'learning_rate_model': 0.0008675441404895237}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.5393929101667106e-05
Epoch 47: reducing lr to 4.8819355990886707e-05
Epoch 53: reducing lr to 4.194863044788422e-05
Epoch 56: reducing lr to 3.8226411180833166e-05
Epoch 63: reducing lr to 2.9203115872607768e-05
Epoch 66: reducing lr to 2.5337916076030267e-05
Epoch 72: reducing lr to 1.7935215198880194e-05
Epoch 75: reducing lr to 1.451446505441577e-05
Epoch 78: reducing lr to 1.1353799652329372e-05
Epoch 81: reducing lr to 8.503055220383013e-06
Epoch 84: reducing lr to 6.00719810027598e-06
Epoch 87: reducing lr to 3.905579242831283e-06
Epoch 90: reducing lr to 2.2313548798541157e-06
Epoch 93: reducing lr to 1.0109201405409611e-06
Epoch 96: reducing lr to 2.6352560551023234e-07
Epoch 99: reducing lr to 9.559212010281856e-10
[I 2024-06-21 05:24:27,848] Trial 901 finished with value: 0.9706805348396301 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.6235575555915569, 'bidirectional': True, 'fc_dropout': 0.2722752199993281, 'learning_rate_model': 0.000620112042944187}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.898261993321763e-05
Epoch 40: reducing lr to 6.560562860252743e-05
Epoch 43: reducing lr to 6.2535940994368e-05
Epoch 61: reducing lr to 3.7661639146195066e-05
Epoch 65: reducing lr to 3.152657375667168e-05
Epoch 72: reducing lr to 2.124151664859476e-05
Epoch 75: reducing lr to 1.7190161795107368e-05
Epoch 78: reducing lr to 1.3446837501833908e-05
Epoch 81: reducing lr to 1.0070567150984803e-05
Epoch 84: reducing lr to 7.11460649027428e-06
Epoch 87: reducing lr to 4.625560696600202e-06
Epoch 90: reducing lr to 2.6426982505514412e-06
Epoch 93: reducing lr to 1.1972801417538193e-06
Epoch 96: reducing lr to 3.121057358222365e-07
Epoch 99: reducing lr to 1.1321423178502558e-09
[I 2024-06-21 05:25:05,256] Trial 902 finished with value: 0.9700111746788025 and parameters: {'hidden_size': 144, 'n_layers': 2, 'rnn_dropout': 0.5161797932660415, 'bidirectional': True, 'fc_dropout': 0.15113543821636205, 'learning_rate_model': 0.0007344277801035513}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00013826125604398564
Epoch 35: reducing lr to 0.00013167565295367425
Epoch 40: reducing lr to 0.00012396050391996747
Epoch 45: reducing lr to 0.00011386013792556958
Epoch 49: reducing lr to 0.00010435611358325366
Epoch 52: reducing lr to 9.656965490641788e-05
Epoch 55: reducing lr to 8.835446587166518e-05
Epoch 58: reducing lr to 7.984013573650656e-05
Epoch 61: reducing lr to 7.116090290512755e-05
Epoch 64: reducing lr to 6.245368901501485e-05
Epoch 67: reducing lr to 5.385575421996e-05
Epoch 70: reducing lr to 4.550275050371294e-05
Epoch 73: reducing lr to 3.752638361912372e-05
Epoch 76: reducing lr to 3.005243469416442e-05
Epoch 79: reducing lr to 2.319879438835426e-05
Epoch 82: reducing lr to 1.7073528420719877e-05
Epoch 85: reducing lr to 1.177325394129871e-05
Epoch 88: reducing lr to 7.38153792820672e-06
Epoch 91: reducing lr to 3.9676663766836845e-06
Epoch 94: reducing lr to 1.5854607372512786e-06
Epoch 97: reducing lr to 2.724968345187628e-07
[I 2024-06-21 05:25:42,328] Trial 903 finished with value: 0.9804677963256836 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.6116434532716951, 'bidirectional': True, 'fc_dropout': 0.17160462702286183, 'learning_rate_model': 0.0013876863868804083}. Best is trial 690 with value: 0.966281533241272.
Epoch 16: reducing lr to 0.0008733799713935639
Epoch 23: reducing lr to 0.0011267469307972354
Epoch 26: reducing lr to 0.0011302642784349428
Epoch 29: reducing lr to 0.001120208028873378
Epoch 32: reducing lr to 0.0011014119538143006
Epoch 35: reducing lr to 0.0010741724783146036
Epoch 38: reducing lr to 0.0010389191778756622
Epoch 41: reducing lr to 0.000996208022585419
Epoch 44: reducing lr to 0.0009467125930977345
Epoch 47: reducing lr to 0.0008912134838456355
Epoch 50: reducing lr to 0.000830585931350787
Epoch 53: reducing lr to 0.0007657861175184457
Epoch 56: reducing lr to 0.0006978357741905827
Epoch 59: reducing lr to 0.0006278068201932683
Epoch 62: reducing lr to 0.000556803397135937
Epoch 65: reducing lr to 0.00048594548061066113
Epoch 68: reducing lr to 0.00041635023037662686
Epoch 71: reducing lr to 0.00034911561728669696
Epoch 74: reducing lr to 0.0002853016674565903
Epoch 77: reducing lr to 0.00022591495300869173
Epoch 80: reducing lr to 0.00017189186215584747
Epoch 83: reducing lr to 0.00012408453047910666
Epoch 86: reducing lr to 8.32467113501838e-05
Epoch 89: reducing lr to 5.002268343676456e-05
Epoch 92: reducing lr to 2.4936244962544403e-05
Epoch 95: reducing lr to 8.383097959528858e-06
Epoch 98: reducing lr to 6.242513878179645e-07
[I 2024-06-21 05:26:15,018] Trial 904 finished with value: 1.2632131576538086 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.547521983875439, 'bidirectional': True, 'fc_dropout': 0.25761914077697606, 'learning_rate_model': 0.011320350360010669}. Best is trial 690 with value: 0.966281533241272.
Epoch 34: reducing lr to 0.00010517747720597229
Epoch 40: reducing lr to 9.810216409572653e-05
Epoch 45: reducing lr to 9.010874900886102e-05
Epoch 49: reducing lr to 8.258727784574266e-05
Epoch 60: reducing lr to 5.861405943867922e-05
Epoch 65: reducing lr to 4.7142679339160856e-05
Epoch 68: reducing lr to 4.039108539247202e-05
Epoch 71: reducing lr to 3.386850223888856e-05
Epoch 74: reducing lr to 2.7677765429430904e-05
Epoch 77: reducing lr to 2.1916524821316954e-05
Epoch 80: reducing lr to 1.6675621570636282e-05
Epoch 83: reducing lr to 1.2037723293518207e-05
Epoch 86: reducing lr to 8.075953323590391e-06
Epoch 89: reducing lr to 4.852814603770617e-06
Epoch 92: reducing lr to 2.419121994333049e-06
Epoch 95: reducing lr to 8.132634518551738e-07
Epoch 98: reducing lr to 6.056005082287398e-08
[I 2024-06-21 05:26:50,321] Trial 905 finished with value: 0.9729644656181335 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.6586577889181435, 'bidirectional': True, 'fc_dropout': 0.21509368003674922, 'learning_rate_model': 0.0010982130060316665}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.4489747825225376e-05
Epoch 50: reducing lr to 3.6542027603490555e-05
Epoch 61: reducing lr to 2.5539833456215136e-05
Epoch 64: reducing lr to 2.2414791705162572e-05
Epoch 67: reducing lr to 1.9328970506043475e-05
Epoch 72: reducing lr to 1.4404704889679244e-05
Epoch 75: reducing lr to 1.165732239184257e-05
Epoch 78: reducing lr to 9.118827488535441e-06
Epoch 81: reducing lr to 6.829246248347845e-06
Epoch 84: reducing lr to 4.824693480885581e-06
Epoch 87: reducing lr to 3.1367739830495124e-06
Epoch 90: reducing lr to 1.7921172504498866e-06
Epoch 93: reducing lr to 8.119225852631419e-07
Epoch 96: reducing lr to 2.1165113081475463e-07
Epoch 99: reducing lr to 7.677500741382591e-10
[I 2024-06-21 05:27:26,417] Trial 906 finished with value: 0.974145233631134 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5831274406701121, 'bidirectional': True, 'fc_dropout': 0.1340165716602373, 'learning_rate_model': 0.0004980442597479859}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.910901438933615e-05
Epoch 43: reducing lr to 7.540750330961002e-05
Epoch 52: reducing lr to 6.162874446281167e-05
Epoch 65: reducing lr to 3.80155823530502e-05
Epoch 72: reducing lr to 2.5613586547363244e-05
Epoch 75: reducing lr to 2.072835495629678e-05
Epoch 78: reducing lr to 1.6214554819198272e-05
Epoch 81: reducing lr to 1.214335810243789e-05
Epoch 84: reducing lr to 8.578982005088047e-06
Epoch 87: reducing lr to 5.577624290791351e-06
Epoch 90: reducing lr to 3.1866359393665405e-06
Epoch 93: reducing lr to 1.4437122847477807e-06
Epoch 96: reducing lr to 3.7634540925965845e-07
Epoch 99: reducing lr to 1.3651673617183676e-09
[I 2024-06-21 05:28:00,479] Trial 907 finished with value: 0.9690054655075073 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.6000318757425657, 'bidirectional': True, 'fc_dropout': 0.18657565906005147, 'learning_rate_model': 0.0008855925788950029}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.379542331901937e-05
Epoch 40: reducing lr to 6.005751743031413e-05
Epoch 53: reducing lr to 4.548026553014078e-05
Epoch 58: reducing lr to 3.8681678373376744e-05
Epoch 62: reducing lr to 3.306871954259065e-05
Epoch 65: reducing lr to 2.8860446782403827e-05
Epoch 72: reducing lr to 1.9445172366203665e-05
Epoch 75: reducing lr to 1.573643090739029e-05
Epoch 79: reducing lr to 1.1239563847210487e-05
Epoch 83: reducing lr to 7.369417210983432e-06
Epoch 86: reducing lr to 4.94404697356784e-06
Epoch 89: reducing lr to 2.9708620634265043e-06
Epoch 92: reducing lr to 1.4809710130241896e-06
Epoch 95: reducing lr to 4.978746838608852e-07
Epoch 98: reducing lr to 3.707447579165499e-08
[I 2024-06-21 05:28:39,534] Trial 908 finished with value: 0.9703292846679688 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.5634066813403473, 'bidirectional': True, 'fc_dropout': 0.23489329358332783, 'learning_rate_model': 0.0006723189784843659}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 9.957432247759896e-05
Epoch 36: reducing lr to 9.550147010308086e-05
Epoch 40: reducing lr to 9.082626877094105e-05
Epoch 52: reducing lr to 7.07569036449722e-05
Epoch 55: reducing lr to 6.473760762987795e-05
Epoch 61: reducing lr to 5.2139827516487324e-05
Epoch 65: reducing lr to 4.36462712490502e-05
Epoch 68: reducing lr to 3.739541947542524e-05
Epoch 71: reducing lr to 3.135659356318433e-05
Epoch 74: reducing lr to 2.5625002109225215e-05
Epoch 77: reducing lr to 2.0291052621464514e-05
Epoch 80: reducing lr to 1.5438848884303972e-05
Epoch 83: reducing lr to 1.1144927345134165e-05
Epoch 86: reducing lr to 7.47698803498614e-06
Epoch 89: reducing lr to 4.492898271515465e-06
Epoch 92: reducing lr to 2.239704154055047e-06
Epoch 95: reducing lr to 7.529465383424549e-07
Epoch 98: reducing lr to 5.606852309040632e-08
[I 2024-06-21 05:29:14,057] Trial 909 finished with value: 0.9726556539535522 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6114939754188995, 'bidirectional': True, 'fc_dropout': 0.1536857179313716, 'learning_rate_model': 0.0010167623780067087}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.387498327025136e-05
Epoch 40: reducing lr to 7.025849003905717e-05
Epoch 49: reducing lr to 5.914708907151134e-05
Epoch 65: reducing lr to 3.3762491350679415e-05
Epoch 72: reducing lr to 2.274800070755313e-05
Epoch 78: reducing lr to 1.4400509816059e-05
Epoch 81: reducing lr to 1.078478869780792e-05
Epoch 84: reducing lr to 7.619186339287507e-06
Epoch 87: reducing lr to 4.953613234865329e-06
Epoch 90: reducing lr to 2.8301228517677653e-06
Epoch 93: reducing lr to 1.2821932615417502e-06
Epoch 96: reducing lr to 3.342407991279361e-07
Epoch 99: reducing lr to 1.2124357536935464e-09
[I 2024-06-21 05:29:53,033] Trial 910 finished with value: 0.9686868190765381 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5749141007609629, 'bidirectional': True, 'fc_dropout': 0.2021217576183568, 'learning_rate_model': 0.0007865146325390801}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00011773979480745494
Epoch 35: reducing lr to 0.00011408000602038422
Epoch 40: reducing lr to 0.00010739582235794907
Epoch 43: reducing lr to 0.00010237077142737098
Epoch 51: reducing lr to 8.59604907666084e-05
Epoch 54: reducing lr to 7.895469439305089e-05
Epoch 61: reducing lr to 6.16517636308133e-05
Epoch 64: reducing lr to 5.410808345362597e-05
Epoch 67: reducing lr to 4.665907954758262e-05
Epoch 70: reducing lr to 3.942227689756648e-05
Epoch 73: reducing lr to 3.2511781587284125e-05
Epoch 76: reducing lr to 2.603656677551199e-05
Epoch 79: reducing lr to 2.0098769545651692e-05
Epoch 82: reducing lr to 1.4792014934683303e-05
Epoch 85: reducing lr to 1.0200009268041335e-05
Epoch 88: reducing lr to 6.395152576807645e-06
Epoch 91: reducing lr to 3.437472258430315e-06
Epoch 94: reducing lr to 1.373597672717393e-06
Epoch 97: reducing lr to 2.3608343551085198e-07
[I 2024-06-21 05:30:24,469] Trial 911 finished with value: 0.974109411239624 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.646197046662739, 'bidirectional': True, 'fc_dropout': 0.10125647897928927, 'learning_rate_model': 0.0012022516525922785}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.0983578492719124e-05
Epoch 47: reducing lr to 4.493245935953047e-05
Epoch 50: reducing lr to 4.1875789899383e-05
Epoch 53: reducing lr to 3.8608766841155646e-05
Epoch 56: reducing lr to 3.51829030101118e-05
Epoch 62: reducing lr to 2.8072450054393907e-05
Epoch 65: reducing lr to 2.4499994618874014e-05
Epoch 72: reducing lr to 1.6507250283648827e-05
Epoch 75: reducing lr to 1.3358853224213055e-05
Epoch 78: reducing lr to 1.0449833495340927e-05
Epoch 81: reducing lr to 7.826059466925863e-06
Epoch 84: reducing lr to 5.528917353102433e-06
Epoch 87: reducing lr to 3.594625062991431e-06
Epoch 90: reducing lr to 2.053698997472452e-06
Epoch 93: reducing lr to 9.304327598886512e-07
Epoch 96: reducing lr to 2.425442392561404e-07
Epoch 99: reducing lr to 8.79812722719489e-10
[I 2024-06-21 05:31:00,194] Trial 912 finished with value: 0.9758179187774658 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.589857305968024, 'bidirectional': True, 'fc_dropout': 0.17167830136526901, 'learning_rate_model': 0.0005707399985601289}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.243915802894438e-05
Epoch 39: reducing lr to 7.950788916419969e-05
Epoch 45: reducing lr to 7.201506159197197e-05
Epoch 49: reducing lr to 6.600388936916212e-05
Epoch 52: reducing lr to 6.107905516983742e-05
Epoch 55: reducing lr to 5.58830545755456e-05
Epoch 58: reducing lr to 5.0497851112164444e-05
Epoch 61: reducing lr to 4.500834883058975e-05
Epoch 64: reducing lr to 3.9501148892005076e-05
Epoch 67: reducing lr to 3.406306656476959e-05
Epoch 70: reducing lr to 2.877989997053333e-05
Epoch 73: reducing lr to 2.37349512910459e-05
Epoch 76: reducing lr to 1.900777545960575e-05
Epoch 79: reducing lr to 1.4672936790476564e-05
Epoch 82: reducing lr to 1.0798785450393401e-05
Epoch 85: reducing lr to 7.4464305345808965e-06
Epoch 88: reducing lr to 4.668727073655753e-06
Epoch 91: reducing lr to 2.509497561649322e-06
Epoch 94: reducing lr to 1.002783368481792e-06
Epoch 97: reducing lr to 1.7235071622972088e-07
[I 2024-06-21 05:31:59,650] Trial 913 finished with value: 0.9775834679603577 and parameters: {'hidden_size': 139, 'n_layers': 3, 'rnn_dropout': 0.6292528735347888, 'bidirectional': True, 'fc_dropout': 0.12494668148457475, 'learning_rate_model': 0.0008776936550600417}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.230091580821431e-05
Epoch 43: reducing lr to 5.9385855724084685e-05
Epoch 65: reducing lr to 2.993850465537942e-05
Epoch 72: reducing lr to 2.0171530531023705e-05
Epoch 75: reducing lr to 1.6324252134142464e-05
Epoch 78: reducing lr to 1.2769488059690945e-05
Epoch 81: reducing lr to 9.563288540616191e-06
Epoch 87: reducing lr to 4.392560115087019e-06
Epoch 90: reducing lr to 2.50957920411996e-06
[I 2024-06-21 05:32:33,675] Trial 914 finished with value: 0.9691834449768066 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5402328723613378, 'bidirectional': True, 'fc_dropout': 0.14662276668388968, 'learning_rate_model': 0.0006974328921174663}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.707338913794323e-05
Epoch 40: reducing lr to 9.232123571325583e-05
Epoch 49: reducing lr to 7.772060499575766e-05
Epoch 52: reducing lr to 7.192153622672622e-05
Epoch 55: reducing lr to 6.580316481549005e-05
Epoch 58: reducing lr to 5.946200408694093e-05
Epoch 61: reducing lr to 5.2998029879855585e-05
Epoch 64: reducing lr to 4.6513216406736566e-05
Epoch 67: reducing lr to 4.0109789994611406e-05
Epoch 70: reducing lr to 3.388877926445796e-05
Epoch 73: reducing lr to 2.794827382925092e-05
Epoch 76: reducing lr to 2.2381950858706298e-05
Epoch 83: reducing lr to 1.1328368745743662e-05
Epoch 86: reducing lr to 7.600056505062544e-06
Epoch 89: reducing lr to 4.566849722808034e-06
Epoch 92: reducing lr to 2.2765688597859616e-06
Epoch 95: reducing lr to 7.653397611334309e-07
Epoch 98: reducing lr to 5.6991390336397046e-08
[I 2024-06-21 05:33:09,490] Trial 915 finished with value: 0.9729068279266357 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.598355983517273, 'bidirectional': True, 'fc_dropout': 0.22058514371980162, 'learning_rate_model': 0.0010334979123832536}. Best is trial 690 with value: 0.966281533241272.
Epoch 16: reducing lr to 6.26113912072404e-05
Epoch 19: reducing lr to 7.33917147032473e-05
Epoch 22: reducing lr to 7.979126631709646e-05
Epoch 25: reducing lr to 8.1125888334364e-05
Epoch 28: reducing lr to 8.061683942966711e-05
Epoch 31: reducing lr to 7.947634487221017e-05
Epoch 34: reducing lr to 7.772239245744702e-05
Epoch 37: reducing lr to 7.538264093954634e-05
Epoch 40: reducing lr to 7.249398922015567e-05
Epoch 43: reducing lr to 6.91019951928843e-05
Epoch 46: reducing lr to 6.526014861575934e-05
Epoch 49: reducing lr to 6.1029043396322764e-05
Epoch 52: reducing lr to 5.647540386170002e-05
Epoch 55: reducing lr to 5.167103628901406e-05
Epoch 58: reducing lr to 4.669172644824782e-05
Epoch 61: reducing lr to 4.161597900111388e-05
Epoch 64: reducing lr to 3.652386780499488e-05
Epoch 67: reducing lr to 3.149566468676031e-05
Epoch 70: reducing lr to 2.6610700990964993e-05
Epoch 73: reducing lr to 2.194600024627663e-05
Epoch 76: reducing lr to 1.757512117056873e-05
Epoch 79: reducing lr to 1.3567008015680248e-05
Epoch 82: reducing lr to 9.98485925872649e-06
Epoch 85: reducing lr to 6.88517807944459e-06
Epoch 88: reducing lr to 4.31683571842428e-06
Epoch 91: reducing lr to 2.320351680130191e-06
Epoch 94: reducing lr to 9.272015679242574e-07
Epoch 97: reducing lr to 1.5936029589622018e-07
[I 2024-06-21 05:33:26,756] Trial 916 finished with value: 1.0930583477020264 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.553789175404872, 'bidirectional': False, 'fc_dropout': 0.2437700001975948, 'learning_rate_model': 0.0008115401179428466}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.233617485929697e-05
Epoch 47: reducing lr to 4.612451929466797e-05
Epoch 50: reducing lr to 4.298675627208616e-05
Epoch 53: reducing lr to 3.9633058962095275e-05
Epoch 56: reducing lr to 3.611630682726313e-05
Epoch 59: reducing lr to 3.249197674431074e-05
Epoch 62: reducing lr to 2.8817213271631284e-05
Epoch 65: reducing lr to 2.5149980451221946e-05
Epoch 68: reducing lr to 2.1548096592390022e-05
Epoch 72: reducing lr to 1.6945188290668943e-05
Epoch 75: reducing lr to 1.3713264131938923e-05
Epoch 78: reducing lr to 1.0727067993879713e-05
Epoch 81: reducing lr to 8.033685136063593e-06
Epoch 84: reducing lr to 5.675599750533292e-06
Epoch 87: reducing lr to 3.6899906089800587e-06
Epoch 90: reducing lr to 2.108183713613412e-06
Epoch 93: reducing lr to 9.551171780400888e-07
Epoch 96: reducing lr to 2.489789475769639e-07
Epoch 99: reducing lr to 9.031541892828842e-10
[I 2024-06-21 05:34:02,501] Trial 917 finished with value: 0.9754032492637634 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6823283018787654, 'bidirectional': True, 'fc_dropout': 0.18173385048156115, 'learning_rate_model': 0.0005858817534376004}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.404204236689057e-05
Epoch 40: reducing lr to 7.992782854373852e-05
Epoch 45: reducing lr to 7.341526772073268e-05
Epoch 52: reducing lr to 6.22666299006942e-05
Epoch 58: reducing lr to 5.147969295265497e-05
Epoch 62: reducing lr to 4.4009660386445586e-05
Epoch 65: reducing lr to 3.8409060860636346e-05
Epoch 71: reducing lr to 2.759404815312358e-05
Epoch 77: reducing lr to 1.7856285376982008e-05
Epoch 80: reducing lr to 1.3586308049815443e-05
Epoch 83: reducing lr to 9.807623433489611e-06
Epoch 86: reducing lr to 6.579808086040848e-06
Epoch 89: reducing lr to 3.95378570065229e-06
Epoch 92: reducing lr to 1.9709572135510216e-06
Epoch 95: reducing lr to 6.625988564058641e-07
Epoch 98: reducing lr to 4.934073986428804e-08
[I 2024-06-21 05:34:41,818] Trial 918 finished with value: 0.9693968296051025 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.5746620876438875, 'bidirectional': True, 'fc_dropout': 0.16374920641953422, 'learning_rate_model': 0.0008947588634737005}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00014148789432346647
Epoch 31: reducing lr to 0.00013907145716992458
Epoch 36: reducing lr to 0.00013338306782949353
Epoch 45: reducing lr to 0.00011651732107325293
Epoch 52: reducing lr to 9.882332563148475e-05
Epoch 55: reducing lr to 9.04164166299738e-05
Epoch 63: reducing lr to 6.68758229346823e-05
Epoch 66: reducing lr to 5.802442439451671e-05
Epoch 69: reducing lr to 4.937770003972802e-05
Epoch 72: reducing lr to 4.107206508949239e-05
Epoch 75: reducing lr to 3.3238466717218295e-05
Epoch 78: reducing lr to 2.6000468528676666e-05
Epoch 81: reducing lr to 1.9472196658834856e-05
Epoch 84: reducing lr to 1.3756625088915276e-05
Epoch 87: reducing lr to 8.9438684228195e-06
Epoch 90: reducing lr to 5.109855212043755e-06
Epoch 93: reducing lr to 2.31503092392052e-06
Epoch 96: reducing lr to 6.03479841320212e-07
Epoch 99: reducing lr to 2.1890820574863372e-09
[I 2024-06-21 05:35:14,520] Trial 919 finished with value: 0.9765764474868774 and parameters: {'hidden_size': 126, 'n_layers': 2, 'rnn_dropout': 0.6139293996049405, 'bidirectional': True, 'fc_dropout': 0.11464216717782577, 'learning_rate_model': 0.001420071178860009}. Best is trial 690 with value: 0.966281533241272.
Epoch 53: reducing lr to 2.941323467145848e-05
Epoch 56: reducing lr to 2.680331612032927e-05
Epoch 62: reducing lr to 2.1386374878270267e-05
Epoch 65: reducing lr to 1.8664778757094363e-05
Epoch 73: reducing lr to 1.1758192153829068e-05
Epoch 76: reducing lr to 9.4163697043354e-06
Epoch 79: reducing lr to 7.2689093871660035e-06
Epoch 82: reducing lr to 5.349671579128149e-06
Epoch 87: reducing lr to 2.7384855612888735e-06
Epoch 90: reducing lr to 1.5645651363515104e-06
Epoch 93: reducing lr to 7.088296092235078e-07
Epoch 96: reducing lr to 1.8477696158496985e-07
Epoch 99: reducing lr to 6.702658540442153e-10
[I 2024-06-21 05:35:47,727] Trial 920 finished with value: 0.9727622866630554 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5903875688319571, 'bidirectional': True, 'fc_dropout': 0.19766948770240775, 'learning_rate_model': 0.000434805638395636}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.550541827939974e-05
Epoch 40: reducing lr to 6.229865069276816e-05
Epoch 43: reducing lr to 5.938369659339876e-05
Epoch 50: reducing lr to 5.116951937373266e-05
Epoch 53: reducing lr to 4.717742752127882e-05
Epoch 56: reducing lr to 4.299124247030857e-05
Epoch 61: reducing lr to 3.576323177849256e-05
Epoch 65: reducing lr to 2.9937416161439167e-05
Epoch 72: reducing lr to 2.017079714139716e-05
Epoch 75: reducing lr to 1.632365862255156e-05
Epoch 78: reducing lr to 1.2769023790999733e-05
Epoch 81: reducing lr to 9.56294084183347e-06
Epoch 84: reducing lr to 6.755981064359817e-06
Epoch 87: reducing lr to 4.392400411884652e-06
Epoch 90: reducing lr to 2.5094879616952655e-06
Epoch 93: reducing lr to 1.1369289331012606e-06
Epoch 96: reducing lr to 2.96373445836471e-07
Epoch 99: reducing lr to 1.07507450650135e-09
[I 2024-06-21 05:36:22,164] Trial 921 finished with value: 0.973715603351593 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6324970398000221, 'bidirectional': True, 'fc_dropout': 0.1415035261888156, 'learning_rate_model': 0.0006974075350902687}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00011528118141098526
Epoch 31: reducing lr to 0.00011331232229976793
Epoch 36: reducing lr to 0.00010867754950435581
Epoch 39: reducing lr to 0.00010481332860769138
Epoch 52: reducing lr to 8.051904217130301e-05
Epoch 55: reducing lr to 7.366928017333901e-05
Epoch 58: reducing lr to 6.65700966060208e-05
Epoch 63: reducing lr to 5.4488929336362135e-05
Epoch 66: reducing lr to 4.7277007173488306e-05
Epoch 69: reducing lr to 4.023184897305382e-05
Epoch 72: reducing lr to 3.346460281386965e-05
Epoch 75: reducing lr to 2.708196153298129e-05
Epoch 78: reducing lr to 2.1184602001161173e-05
Epoch 81: reducing lr to 1.586551164840691e-05
Epoch 84: reducing lr to 1.1208591378514352e-05
Epoch 87: reducing lr to 7.2872645613755034e-06
Epoch 90: reducing lr to 4.163396087701805e-06
Epoch 93: reducing lr to 1.886235576468412e-06
Epoch 96: reducing lr to 4.917019183709208e-07
Epoch 99: reducing lr to 1.7836152485083829e-09
[I 2024-06-21 05:37:00,619] Trial 922 finished with value: 0.9749621748924255 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6091746596694926, 'bidirectional': True, 'fc_dropout': 0.1613122514774017, 'learning_rate_model': 0.0011570423319214008}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.906769092770316e-05
Epoch 40: reducing lr to 8.470745032798928e-05
Epoch 43: reducing lr to 8.074398840971427e-05
Epoch 49: reducing lr to 7.131094201974737e-05
Epoch 52: reducing lr to 6.5990125786015e-05
Epoch 55: reducing lr to 6.037633998254939e-05
Epoch 65: reducing lr to 4.070589273192446e-05
Epoch 72: reducing lr to 2.74262247726208e-05
Epoch 75: reducing lr to 2.2195272073546737e-05
Epoch 78: reducing lr to 1.7362036520617345e-05
Epoch 83: reducing lr to 1.039411166253968e-05
Epoch 86: reducing lr to 6.973275475775074e-06
Epoch 89: reducing lr to 4.1902190006606175e-06
Epoch 92: reducing lr to 2.088818917107241e-06
Epoch 95: reducing lr to 7.022217510346514e-07
Epoch 98: reducing lr to 5.2291277610684756e-08
[I 2024-06-21 05:37:36,312] Trial 923 finished with value: 0.9727391004562378 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5756405845746483, 'bidirectional': True, 'fc_dropout': 0.26113692759967044, 'learning_rate_model': 0.0009482647453852812}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.053612151168028e-05
Epoch 40: reducing lr to 6.70830797009161e-05
Epoch 43: reducing lr to 6.394426215032652e-05
Epoch 53: reducing lr to 5.08005726496661e-05
Epoch 62: reducing lr to 3.693711701048658e-05
Epoch 65: reducing lr to 3.223655812870514e-05
Epoch 72: reducing lr to 2.1719879599646463e-05
Epoch 75: reducing lr to 1.757728747268501e-05
Epoch 78: reducing lr to 1.3749662812102685e-05
Epoch 81: reducing lr to 1.029735821778126e-05
Epoch 84: reducing lr to 7.274828766892392e-06
Epoch 87: reducing lr to 4.729729193685411e-06
Epoch 90: reducing lr to 2.7022123123195583e-06
Epoch 93: reducing lr to 1.2242431157881081e-06
Epoch 96: reducing lr to 3.191344157087964e-07
Epoch 99: reducing lr to 1.157638375827788e-09
[I 2024-06-21 05:38:09,512] Trial 924 finished with value: 0.9703046083450317 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5334462784318464, 'bidirectional': True, 'fc_dropout': 0.21333863498199368, 'learning_rate_model': 0.000750967231877958}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.085493111404456e-05
Epoch 47: reducing lr to 4.481908083089639e-05
Epoch 50: reducing lr to 4.177012429567785e-05
Epoch 56: reducing lr to 3.5094125635509544e-05
Epoch 61: reducing lr to 2.9193837559662813e-05
Epoch 65: reducing lr to 2.4438173534939923e-05
Epoch 68: reducing lr to 2.0938231935957214e-05
Epoch 72: reducing lr to 1.6465597372242868e-05
Epoch 75: reducing lr to 1.3325144694913995e-05
Epoch 78: reducing lr to 1.042346532491223e-05
Epoch 81: reducing lr to 7.806311892009867e-06
Epoch 84: reducing lr to 5.514966180089229e-06
Epoch 87: reducing lr to 3.5855547092551386e-06
Epoch 90: reducing lr to 2.0485168780445556e-06
Epoch 93: reducing lr to 9.280849895058917e-07
Epoch 96: reducing lr to 2.419322249269185e-07
Epoch 99: reducing lr to 8.775926824003758e-10
[I 2024-06-21 05:38:39,910] Trial 925 finished with value: 0.9745141267776489 and parameters: {'hidden_size': 122, 'n_layers': 2, 'rnn_dropout': 0.55712031170212, 'bidirectional': True, 'fc_dropout': 0.22727164128872718, 'learning_rate_model': 0.0005692998445558359}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 0.00010032236695066258
Epoch 40: reducing lr to 9.09940460567713e-05
Epoch 43: reducing lr to 8.673643430078852e-05
Epoch 52: reducing lr to 7.088760813617227e-05
Epoch 55: reducing lr to 6.48571930785181e-05
Epoch 58: reducing lr to 5.860719147347959e-05
Epoch 61: reducing lr to 5.223614192929707e-05
Epoch 64: reducing lr to 4.5844552699758224e-05
Epoch 67: reducing lr to 3.953318052883309e-05
Epoch 70: reducing lr to 3.340160167239917e-05
Epoch 73: reducing lr to 2.754649562885956e-05
Epoch 76: reducing lr to 2.206019288566658e-05
Epoch 79: reducing lr to 1.702923187855358e-05
Epoch 82: reducing lr to 1.2532938979254415e-05
Epoch 85: reducing lr to 8.642236660027299e-06
Epoch 88: reducing lr to 5.4184678261932606e-06
Epoch 91: reducing lr to 2.9124923310327445e-06
Epoch 94: reducing lr to 1.1638181742128954e-06
Epoch 97: reducing lr to 2.000281438556753e-07
[I 2024-06-21 05:39:16,074] Trial 926 finished with value: 0.9762000441551208 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5173971005665199, 'bidirectional': True, 'fc_dropout': 0.13549376845544653, 'learning_rate_model': 0.0010186405750792593}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.161262505873674e-05
Epoch 40: reducing lr to 5.8596426183051116e-05
Epoch 49: reducing lr to 4.932938406154896e-05
Epoch 53: reducing lr to 4.4373812570835715e-05
Epoch 56: reducing lr to 4.043640011326149e-05
Epoch 61: reducing lr to 3.3637928713904733e-05
Epoch 65: reducing lr to 2.8158324084195424e-05
Epoch 72: reducing lr to 1.8972106339477755e-05
Epoch 75: reducing lr to 1.5353591881640915e-05
Epoch 78: reducing lr to 1.2010198482289051e-05
Epoch 81: reducing lr to 8.994643558089571e-06
Epoch 84: reducing lr to 6.3544930962334036e-06
Epoch 87: reducing lr to 4.131373049645842e-06
Epoch 90: reducing lr to 2.3603565160649913e-06
Epoch 93: reducing lr to 1.0693646100360375e-06
Epoch 96: reducing lr to 2.787608487255563e-07
Epoch 99: reducing lr to 1.0111860090250038e-09
[I 2024-06-21 05:39:54,531] Trial 927 finished with value: 0.9725725054740906 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.6198200944824919, 'bidirectional': True, 'fc_dropout': 0.18218739413145232, 'learning_rate_model': 0.0006559626684525348}. Best is trial 690 with value: 0.966281533241272.
[I 2024-06-21 05:40:33,523] Trial 928 finished with value: 1.059612512588501 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.5976932805401601, 'bidirectional': True, 'fc_dropout': 0.20222276595052474, 'learning_rate_model': 2.737461028841994e-05}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.710318966447986e-05
Epoch 40: reducing lr to 7.332866206147517e-05
Epoch 43: reducing lr to 6.989761368883067e-05
Epoch 49: reducing lr to 6.173171248106472e-05
Epoch 52: reducing lr to 5.7125643782457566e-05
Epoch 55: reducing lr to 5.226596024253383e-05
Epoch 65: reducing lr to 3.523785263860864e-05
Epoch 72: reducing lr to 2.3742048192767367e-05
Epoch 75: reducing lr to 1.921377162152438e-05
Epoch 78: reducing lr to 1.5029786680979427e-05
Epoch 81: reducing lr to 1.1256064930890834e-05
Epoch 84: reducing lr to 7.952131335962875e-06
Epoch 87: reducing lr to 5.1700773910848715e-06
Epoch 90: reducing lr to 2.95379422578496e-06
Epoch 93: reducing lr to 1.3382228442545413e-06
Epoch 96: reducing lr to 3.4884653218116317e-07
Epoch 99: reducing lr to 1.2654170564261464e-09
[I 2024-06-21 05:41:09,355] Trial 929 finished with value: 0.972760021686554 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6446750874405103, 'bidirectional': True, 'fc_dropout': 0.09648524531272105, 'learning_rate_model': 0.0008208839339388325}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.928929554154528e-05
Epoch 47: reducing lr to 6.987863847326961e-05
Epoch 55: reducing lr to 5.651447949450764e-05
Epoch 61: reducing lr to 4.551690000462324e-05
Epoch 64: reducing lr to 3.9947473988717974e-05
Epoch 67: reducing lr to 3.444794654687906e-05
Epoch 70: reducing lr to 2.910508523724175e-05
Epoch 73: reducing lr to 2.4003133476313837e-05
Epoch 76: reducing lr to 1.9222545091838464e-05
Epoch 79: reducing lr to 1.4838726903315463e-05
Epoch 82: reducing lr to 1.092080136880899e-05
Epoch 85: reducing lr to 7.5305680577096425e-06
Epoch 88: reducing lr to 4.72147921178654e-06
Epoch 91: reducing lr to 2.537852477223202e-06
Epoch 94: reducing lr to 1.01411385877066e-06
Epoch 97: reducing lr to 1.7429811402061872e-07
[I 2024-06-21 05:41:25,331] Trial 930 finished with value: 0.9730345606803894 and parameters: {'hidden_size': 69, 'n_layers': 2, 'rnn_dropout': 0.5821950075452642, 'bidirectional': True, 'fc_dropout': 0.12080845347064366, 'learning_rate_model': 0.0008876107515614616}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00011573512982218699
Epoch 40: reducing lr to 0.00011006940517380163
Epoch 49: reducing lr to 9.266189620989583e-05
Epoch 52: reducing lr to 8.574799341128317e-05
Epoch 55: reducing lr to 7.845340407152562e-05
Epoch 58: reducing lr to 7.089319558741566e-05
Epoch 61: reducing lr to 6.318656351586811e-05
Epoch 64: reducing lr to 5.54550859621423e-05
Epoch 67: reducing lr to 4.782064161343407e-05
Epoch 70: reducing lr to 4.040368119953106e-05
Epoch 73: reducing lr to 3.332115143665127e-05
Epoch 76: reducing lr to 2.6684738333645725e-05
Epoch 79: reducing lr to 2.0599121642197255e-05
Epoch 82: reducing lr to 1.5160257163039192e-05
Epoch 85: reducing lr to 1.0453935062376967e-05
Epoch 88: reducing lr to 6.554357745674642e-06
Epoch 91: reducing lr to 3.5230469722164403e-06
Epoch 94: reducing lr to 1.4077929240133984e-06
Epoch 97: reducing lr to 2.4196065310116455e-07
[I 2024-06-21 05:42:01,125] Trial 931 finished with value: 0.9779700636863708 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5655829228917804, 'bidirectional': True, 'fc_dropout': 0.2871355704680554, 'learning_rate_model': 0.0012321813024439067}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.589968361419467e-05
Epoch 43: reducing lr to 6.281623717093738e-05
Epoch 49: reducing lr to 5.547762917117048e-05
Epoch 53: reducing lr to 4.990441225952227e-05
Epoch 65: reducing lr to 3.1667880946486325e-05
Epoch 72: reducing lr to 2.133672455314476e-05
Epoch 75: reducing lr to 1.7267210873591882e-05
Epoch 78: reducing lr to 1.3507108396918919e-05
Epoch 81: reducing lr to 1.0115705057657715e-05
Epoch 84: reducing lr to 7.146495304375592e-06
Epoch 87: reducing lr to 4.6462932058921626e-06
Epoch 90: reducing lr to 2.6545432504615454e-06
Epoch 93: reducing lr to 1.2026465445084652e-06
Epoch 96: reducing lr to 3.135046440827559e-07
Epoch 99: reducing lr to 1.13721676237152e-09
[I 2024-06-21 05:42:32,569] Trial 932 finished with value: 0.970492422580719 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.6034466467620297, 'bidirectional': True, 'fc_dropout': 0.15941271203122598, 'learning_rate_model': 0.0007377196039004925}. Best is trial 690 with value: 0.966281533241272.
Epoch 47: reducing lr to 3.881425958067335e-05
Epoch 61: reducing lr to 2.528247273682656e-05
Epoch 64: reducing lr to 2.218892151974905e-05
Epoch 67: reducing lr to 1.913419563552585e-05
Epoch 72: reducing lr to 1.4259551037390283e-05
Epoch 75: reducing lr to 1.1539853463078702e-05
Epoch 78: reducing lr to 9.026938557213583e-06
Epoch 81: reducing lr to 6.760429052246321e-06
Epoch 84: reducing lr to 4.77607583476041e-06
Epoch 87: reducing lr to 3.105165225293871e-06
Epoch 90: reducing lr to 1.7740583783904766e-06
Epoch 93: reducing lr to 8.037409743301915e-07
Epoch 96: reducing lr to 2.0951835702908307e-07
Epoch 99: reducing lr to 7.600135823633558e-10
[I 2024-06-21 05:43:07,869] Trial 933 finished with value: 0.9728041291236877 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.6276085025175042, 'bidirectional': True, 'fc_dropout': 0.24887369798407993, 'learning_rate_model': 0.0004930255493011916}. Best is trial 690 with value: 0.966281533241272.
Epoch 32: reducing lr to 0.00010191681515636053
Epoch 35: reducing lr to 9.939626816225474e-05
Epoch 39: reducing lr to 9.489059967073514e-05
Epoch 45: reducing lr to 8.594810466763233e-05
Epoch 48: reducing lr to 8.064503348269452e-05
Epoch 52: reducing lr to 7.289626517965084e-05
Epoch 55: reducing lr to 6.669497349067645e-05
Epoch 58: reducing lr to 6.026787309396202e-05
Epoch 61: reducing lr to 5.371629476798325e-05
Epoch 64: reducing lr to 4.7143594748244325e-05
Epoch 67: reducing lr to 4.065338480159976e-05
Epoch 70: reducing lr to 3.4348062756737744e-05
Epoch 73: reducing lr to 2.8327047602933377e-05
Epoch 76: reducing lr to 2.2685286085809113e-05
Epoch 79: reducing lr to 1.75117687768529e-05
Epoch 82: reducing lr to 1.2888069823954479e-05
Epoch 85: reducing lr to 8.887121344318318e-06
Epoch 88: reducing lr to 5.5720044435247135e-06
Epoch 91: reducing lr to 2.995020129453697e-06
Epoch 94: reducing lr to 1.1967958925253752e-06
Epoch 97: reducing lr to 2.0569610121258432e-07
[I 2024-06-21 05:44:05,807] Trial 934 finished with value: 0.9842933416366577 and parameters: {'hidden_size': 138, 'n_layers': 3, 'rnn_dropout': 0.5479668193723304, 'bidirectional': True, 'fc_dropout': 0.17984190562369973, 'learning_rate_model': 0.0010475045700665854}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.7052678232661583e-05
Epoch 47: reducing lr to 5.0281232150221215e-05
Epoch 53: reducing lr to 4.32047654690005e-05
Epoch 65: reducing lr to 2.7416480973225047e-05
Epoch 72: reducing lr to 1.84722780703501e-05
Epoch 75: reducing lr to 1.4949094926069658e-05
Epoch 85: reducing lr to 5.418626478785507e-06
Epoch 88: reducing lr to 3.397344274689814e-06
Epoch 91: reducing lr to 1.8261138505022006e-06
Epoch 94: reducing lr to 7.297064664347954e-07
Epoch 97: reducing lr to 1.254163522056693e-07
[I 2024-06-21 05:44:45,544] Trial 935 finished with value: 0.969597339630127 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.5878671962388267, 'bidirectional': True, 'fc_dropout': 0.2302880634936725, 'learning_rate_model': 0.0006386810509389986}. Best is trial 690 with value: 0.966281533241272.
Epoch 13: reducing lr to 8.503297297307634e-05
Epoch 16: reducing lr to 0.00010873279907272306
Epoch 19: reducing lr to 0.00012745422860861112
Epoch 22: reducing lr to 0.000138567879756866
Epoch 25: reducing lr to 0.0001408856239379728
Epoch 28: reducing lr to 0.00014000159451129244
Epoch 31: reducing lr to 0.00013802097783486314
Epoch 34: reducing lr to 0.00013497501204780193
Epoch 37: reducing lr to 0.00013091173016297817
Epoch 40: reducing lr to 0.00012589521190744293
Epoch 43: reducing lr to 0.00012000457447052036
Epoch 46: reducing lr to 0.00011333270975254789
Epoch 49: reducing lr to 0.0001059848469306251
Epoch 52: reducing lr to 9.807686145033305e-05
Epoch 55: reducing lr to 8.973345422235309e-05
Epoch 58: reducing lr to 8.108623706270214e-05
Epoch 61: reducing lr to 7.227154349541915e-05
Epoch 64: reducing lr to 6.342843215628725e-05
Epoch 67: reducing lr to 5.469630548077241e-05
Epoch 70: reducing lr to 4.62129326348575e-05
Epoch 73: reducing lr to 3.8112074963004205e-05
Epoch 76: reducing lr to 3.052147671648005e-05
Epoch 79: reducing lr to 2.3560868527968347e-05
Epoch 82: reducing lr to 1.7340002747343172e-05
Epoch 85: reducing lr to 1.1957004472463984e-05
Epoch 88: reducing lr to 7.496744949297713e-06
Epoch 91: reducing lr to 4.0295915511373615e-06
Epoch 94: reducing lr to 1.610205744372018e-06
Epoch 97: reducing lr to 2.7674981660287015e-07
[I 2024-06-21 05:45:00,995] Trial 936 finished with value: 1.092637062072754 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.668623219912774, 'bidirectional': False, 'fc_dropout': 0.14518986566663755, 'learning_rate_model': 0.0014093446397263458}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.523356349383447e-05
Epoch 43: reducing lr to 7.171338477603154e-05
Epoch 49: reducing lr to 6.333535318882217e-05
Epoch 58: reducing lr to 4.845622380792275e-05
Epoch 61: reducing lr to 4.3188662014862424e-05
Epoch 64: reducing lr to 3.790411808077898e-05
Epoch 72: reducing lr to 2.4358809229149442e-05
Epoch 75: reducing lr to 1.9712898975739333e-05
Epoch 78: reducing lr to 1.5420224217568467e-05
Epoch 81: reducing lr to 1.1548470296089068e-05
Epoch 86: reducing lr to 6.193367422055147e-06
Epoch 89: reducing lr to 3.721574737743094e-06
Epoch 92: reducing lr to 1.8552003397437302e-06
Epoch 95: reducing lr to 6.236835660695223e-07
Epoch 98: reducing lr to 4.644289420901136e-08
[I 2024-06-21 05:45:33,247] Trial 937 finished with value: 0.9697062969207764 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.6137575310385204, 'bidirectional': True, 'fc_dropout': 0.19595955777330643, 'learning_rate_model': 0.0008422085147725845}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00011361629412057257
Epoch 40: reducing lr to 0.00010186463999882103
Epoch 45: reducing lr to 9.356465642873208e-05
Epoch 52: reducing lr to 7.935618863088613e-05
Epoch 55: reducing lr to 7.260535068586106e-05
Epoch 58: reducing lr to 6.560869331014558e-05
Epoch 62: reducing lr to 5.6088452462866886e-05
Epoch 65: reducing lr to 4.895072502964966e-05
Epoch 68: reducing lr to 4.194018970520376e-05
Epoch 71: reducing lr to 3.51674483398469e-05
Epoch 74: reducing lr to 2.873928049833518e-05
Epoch 77: reducing lr to 2.2757081166631744e-05
Epoch 80: reducing lr to 1.731517549798307e-05
Epoch 83: reducing lr to 1.2499401628929697e-05
Epoch 86: reducing lr to 8.385687365184768e-06
Epoch 89: reducing lr to 5.038932801846793e-06
Epoch 92: reducing lr to 2.5119016826734763e-06
Epoch 95: reducing lr to 8.444542432987229e-07
Epoch 98: reducing lr to 6.288268798394694e-08
[I 2024-06-21 05:46:10,362] Trial 938 finished with value: 0.9736809134483337 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5700990085904541, 'bidirectional': True, 'fc_dropout': 0.21277136032579877, 'learning_rate_model': 0.0011403323620086375}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.481368908602616e-05
Epoch 65: reducing lr to 3.1146009769867074e-05
Epoch 72: reducing lr to 2.0985105776802675e-05
Epoch 77: reducing lr to 1.4479709379590432e-05
Epoch 83: reducing lr to 7.953027968765169e-06
Epoch 86: reducing lr to 5.335583904934958e-06
Epoch 89: reducing lr to 3.2061353571568836e-06
[I 2024-06-21 05:46:46,910] Trial 939 finished with value: 0.9675372838973999 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6016403078529974, 'bidirectional': True, 'fc_dropout': 0.17154768422629504, 'learning_rate_model': 0.00072556234594082}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 9.411976855359865e-05
Epoch 36: reducing lr to 8.872832218691617e-05
Epoch 40: reducing lr to 8.438469512401276e-05
Epoch 52: reducing lr to 6.573868796766454e-05
Epoch 58: reducing lr to 5.435025915298547e-05
Epoch 62: reducing lr to 4.646368907907087e-05
Epoch 65: reducing lr to 4.055079375703066e-05
Epoch 71: reducing lr to 2.9132723646613753e-05
Epoch 74: reducing lr to 2.380762768084754e-05
Epoch 79: reducing lr to 1.57923139209714e-05
Epoch 82: reducing lr to 1.1622609177224723e-05
Epoch 85: reducing lr to 8.014507952432137e-06
Epoch 88: reducing lr to 5.024897511067051e-06
Epoch 91: reducing lr to 2.700943501862598e-06
Epoch 94: reducing lr to 1.0792842616259454e-06
Epoch 97: reducing lr to 1.8549910314959347e-07
[I 2024-06-21 05:47:26,192] Trial 940 finished with value: 0.9697238206863403 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.6406479193481661, 'bidirectional': True, 'fc_dropout': 0.1579845115431915, 'learning_rate_model': 0.0009446516348485398}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.886272704403249e-05
Epoch 47: reducing lr to 4.306332670264327e-05
Epoch 57: reducing lr to 3.259995913880789e-05
Epoch 61: reducing lr to 2.805019070513842e-05
Epoch 66: reducing lr to 2.2350457841966102e-05
Epoch 72: reducing lr to 1.5820569852166188e-05
Epoch 75: reducing lr to 1.2803142070720554e-05
Epoch 78: reducing lr to 1.0015133830030203e-05
Epoch 81: reducing lr to 7.500505434654322e-06
Epoch 84: reducing lr to 5.298921485321782e-06
Epoch 87: reducing lr to 3.445093272604863e-06
Epoch 90: reducing lr to 1.968267754261894e-06
Epoch 93: reducing lr to 8.917279509079072e-07
Epoch 96: reducing lr to 2.324547101096035e-07
Epoch 99: reducing lr to 8.432136423342299e-10
[I 2024-06-21 05:48:00,313] Trial 941 finished with value: 0.9724256992340088 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.6210298048566628, 'bidirectional': True, 'fc_dropout': 0.13359942900903948, 'learning_rate_model': 0.0005469979469318283}. Best is trial 690 with value: 0.966281533241272.
[I 2024-06-21 05:48:38,755] Trial 942 finished with value: 1.0182570219039917 and parameters: {'hidden_size': 141, 'n_layers': 2, 'rnn_dropout': 0.5075707557983588, 'bidirectional': True, 'fc_dropout': 0.2658671150180782, 'learning_rate_model': 0.00011164571196084215}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.656496655354943e-05
Epoch 40: reducing lr to 7.281678725075934e-05
Epoch 43: reducing lr to 6.940968950242639e-05
Epoch 52: reducing lr to 5.672687504352091e-05
Epoch 55: reducing lr to 5.190111479528429e-05
Epoch 61: reducing lr to 4.180128479277624e-05
Epoch 65: reducing lr to 3.499187284513717e-05
Epoch 72: reducing lr to 2.3576315502670138e-05
Epoch 75: reducing lr to 1.9079648818306452e-05
Epoch 78: reducing lr to 1.4924870417731958e-05
Epoch 81: reducing lr to 1.1177491342556785e-05
Epoch 84: reducing lr to 7.896621040152874e-06
Epoch 87: reducing lr to 5.133987377827442e-06
Epoch 90: reducing lr to 2.9331751006337365e-06
Epoch 93: reducing lr to 1.3288813051367958e-06
Epoch 96: reducing lr to 3.46411389528763e-07
Epoch 99: reducing lr to 1.2565837421594713e-09
[I 2024-06-21 05:49:13,186] Trial 943 finished with value: 0.9725886583328247 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5923149890969786, 'bidirectional': True, 'fc_dropout': 0.23233801437486964, 'learning_rate_model': 0.0008151537079059022}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.77589369445061e-05
Epoch 43: reducing lr to 5.5056396068429765e-05
Epoch 49: reducing lr to 4.862434399363521e-05
Epoch 52: reducing lr to 4.4996272458634654e-05
Epoch 55: reducing lr to 4.11684356038262e-05
Epoch 61: reducing lr to 3.3157158722628575e-05
Epoch 65: reducing lr to 2.7755871325006542e-05
Epoch 68: reducing lr to 2.3780781757551247e-05
Epoch 72: reducing lr to 1.8700947568766925e-05
Epoch 75: reducing lr to 1.5134150717537877e-05
Epoch 78: reducing lr to 1.1838542757923106e-05
Epoch 81: reducing lr to 8.866087643076667e-06
Epoch 86: reducing lr to 4.7548235360931385e-06
Epoch 89: reducing lr to 2.85715830314466e-06
Epoch 92: reducing lr to 1.4242898311132868e-06
Epoch 95: reducing lr to 4.788195333707144e-07
Epoch 98: reducing lr to 3.5655524922178526e-08
[I 2024-06-21 05:49:46,635] Trial 944 finished with value: 0.9749641418457031 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5312262214678687, 'bidirectional': True, 'fc_dropout': 0.19409089367659826, 'learning_rate_model': 0.0006465873240586616}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 9.471657327424674e-05
Epoch 40: reducing lr to 9.007979596527427e-05
Epoch 45: reducing lr to 8.274004758431112e-05
Epoch 49: reducing lr to 7.583364960647171e-05
Epoch 52: reducing lr to 7.017537469856838e-05
Epoch 60: reducing lr to 5.382085669160773e-05
Epoch 65: reducing lr to 4.328755614386033e-05
Epoch 68: reducing lr to 3.708807817347981e-05
Epoch 71: reducing lr to 3.10988834899863e-05
Epoch 74: reducing lr to 2.5414398200482358e-05
Epoch 77: reducing lr to 2.0124286781743903e-05
Epoch 80: reducing lr to 1.5311961795371627e-05
Epoch 83: reducing lr to 1.1053330659540316e-05
Epoch 86: reducing lr to 7.4155369998182385e-06
Epoch 89: reducing lr to 4.455972540405985e-06
Epoch 92: reducing lr to 2.2212967234034806e-06
Epoch 95: reducing lr to 7.467583053279419e-07
Epoch 98: reducing lr to 5.560771336753957e-08
[I 2024-06-21 05:50:21,982] Trial 945 finished with value: 0.972511887550354 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5616239506059633, 'bidirectional': True, 'fc_dropout': 0.14869772073744558, 'learning_rate_model': 0.0010084059247990886}. Best is trial 690 with value: 0.966281533241272.
Epoch 62: reducing lr to 1.9567878710371577e-05
Epoch 65: reducing lr to 1.707770152508812e-05
Epoch 68: reducing lr to 1.4631898531782987e-05
Epoch 73: reducing lr to 1.0758386085953575e-05
Epoch 76: reducing lr to 8.615690191312842e-06
Epoch 79: reducing lr to 6.65082970135656e-06
Epoch 86: reducing lr to 2.925559648372651e-06
Epoch 89: reducing lr to 1.7579594652130857e-06
Epoch 92: reducing lr to 8.763405888489117e-07
Epoch 95: reducing lr to 2.946092730989102e-07
Epoch 98: reducing lr to 2.193822003320172e-08
[I 2024-06-21 05:50:59,019] Trial 946 finished with value: 0.9738789796829224 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5864060698676333, 'bidirectional': True, 'fc_dropout': 0.17150726357025894, 'learning_rate_model': 0.00039783385651564123}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.751208972928231e-05
Epoch 40: reducing lr to 6.420708707856708e-05
Epoch 49: reducing lr to 5.40527172779709e-05
Epoch 65: reducing lr to 3.0854509126759645e-05
Epoch 72: reducing lr to 2.078870271025212e-05
Epoch 79: reducing lr to 1.2016141950927512e-05
Epoch 83: reducing lr to 7.878594268118577e-06
Epoch 86: reducing lr to 5.285647295040648e-06
Epoch 89: reducing lr to 3.17612860748313e-06
Epoch 92: reducing lr to 1.5832961278229888e-06
Epoch 95: reducing lr to 5.322744686868256e-07
Epoch 99: reducing lr to 1.1080079855355085e-09
[I 2024-06-21 05:51:37,897] Trial 947 finished with value: 0.9688171148300171 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.6104842067050344, 'bidirectional': True, 'fc_dropout': 0.12712256918673512, 'learning_rate_model': 0.000718771688260461}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00011303117692896126
Epoch 36: reducing lr to 0.00010655643163856613
Epoch 40: reducing lr to 0.00010134004313054651
Epoch 45: reducing lr to 9.308280398470184e-05
Epoch 52: reducing lr to 7.894750895524554e-05
Epoch 55: reducing lr to 7.223143742616451e-05
Epoch 58: reducing lr to 6.527081242191506e-05
Epoch 61: reducing lr to 5.817537636237895e-05
Epoch 65: reducing lr to 4.869863169234813e-05
Epoch 68: reducing lr to 4.1724200209983014e-05
Epoch 71: reducing lr to 3.498633806188879e-05
Epoch 74: reducing lr to 2.859127490438164e-05
Epoch 77: reducing lr to 2.263988354524692e-05
Epoch 80: reducing lr to 1.722600336877346e-05
Epoch 83: reducing lr to 1.2435030450177999e-05
Epoch 86: reducing lr to 8.34250157146718e-06
Epoch 89: reducing lr to 5.012982596090156e-06
Epoch 92: reducing lr to 2.4989655376465122e-06
Epoch 95: reducing lr to 8.401053539153041e-07
Epoch 98: reducing lr to 6.255884586183966e-08
[I 2024-06-21 05:52:12,386] Trial 948 finished with value: 0.9764909744262695 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5455690241195378, 'bidirectional': True, 'fc_dropout': 0.10852602836743297, 'learning_rate_model': 0.00113445971782211}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.853268266517017e-05
Epoch 40: reducing lr to 7.468817532704836e-05
Epoch 49: reducing lr to 6.287621832182583e-05
Epoch 53: reducing lr to 5.655974790794979e-05
Epoch 56: reducing lr to 5.154104333631724e-05
Epoch 63: reducing lr to 3.9374846192735503e-05
Epoch 66: reducing lr to 3.4163359577460124e-05
Epoch 72: reducing lr to 2.4182225724479117e-05
Epoch 75: reducing lr to 1.956999491357431e-05
Epoch 78: reducing lr to 1.5308438899594792e-05
Epoch 81: reducing lr to 1.1464752354900694e-05
Epoch 84: reducing lr to 8.099563836937163e-06
Epoch 87: reducing lr to 5.265930616817083e-06
Epoch 90: reducing lr to 3.00855756553281e-06
Epoch 93: reducing lr to 1.3630334934319582e-06
Epoch 96: reducing lr to 3.5531414627388734e-07
Epoch 99: reducing lr to 1.2888778864171037e-09
[I 2024-06-21 05:52:45,602] Trial 949 finished with value: 0.9697093367576599 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.653703357534392, 'bidirectional': True, 'fc_dropout': 0.21095923325725593, 'learning_rate_model': 0.0008361031206294356}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.0670380642524554e-05
Epoch 47: reducing lr to 4.465643421395602e-05
Epoch 50: reducing lr to 4.161854221768959e-05
Epoch 53: reducing lr to 3.83715888491275e-05
Epoch 56: reducing lr to 3.496677048446054e-05
Epoch 59: reducing lr to 3.14577982416271e-05
Epoch 62: reducing lr to 2.7899997840040496e-05
Epoch 65: reducing lr to 2.4349488399591945e-05
Epoch 72: reducing lr to 1.6405844390725858e-05
Epoch 75: reducing lr to 1.3276788288118276e-05
Epoch 78: reducing lr to 1.0385638994240941e-05
Epoch 81: reducing lr to 7.777983104438169e-06
Epoch 84: reducing lr to 5.494952592681739e-06
Epoch 87: reducing lr to 3.572542877407984e-06
Epoch 90: reducing lr to 2.041082893817689e-06
Epoch 93: reducing lr to 9.247170069195043e-07
Epoch 96: reducing lr to 2.410542627468782e-07
Epoch 99: reducing lr to 8.744079343296456e-10
[I 2024-06-21 05:53:21,377] Trial 950 finished with value: 0.9755789041519165 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6251935521268346, 'bidirectional': True, 'fc_dropout': 0.3055501824379508, 'learning_rate_model': 0.0005672338786318346}. Best is trial 690 with value: 0.966281533241272.
Epoch 42: reducing lr to 0.00011705387175052542
Epoch 58: reducing lr to 7.776073641856137e-05
Epoch 64: reducing lr to 6.0827111640828315e-05
Epoch 67: reducing lr to 5.245310607113924e-05
Epoch 72: reducing lr to 3.9090106390589366e-05
Epoch 75: reducing lr to 3.163452330446686e-05
Epoch 78: reducing lr to 2.4745799335304433e-05
Epoch 81: reducing lr to 1.8532553388630697e-05
Epoch 84: reducing lr to 1.3092790370521773e-05
Epoch 87: reducing lr to 8.51227634718788e-06
Epoch 90: reducing lr to 4.86327588944146e-06
Epoch 93: reducing lr to 2.2033176300334e-06
Epoch 96: reducing lr to 5.743585366448567e-07
Epoch 99: reducing lr to 2.0834465064853772e-09
[I 2024-06-21 05:53:29,901] Trial 951 finished with value: 0.9855442643165588 and parameters: {'hidden_size': 23, 'n_layers': 2, 'rnn_dropout': 0.5756892553577662, 'bidirectional': True, 'fc_dropout': 0.24445061005469965, 'learning_rate_model': 0.0013515447383250496}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.735730259824007e-05
Epoch 39: reducing lr to 8.425116047674489e-05
Epoch 45: reducing lr to 7.631132677158113e-05
Epoch 61: reducing lr to 4.7693451052235586e-05
Epoch 65: reducing lr to 3.992420766583721e-05
Epoch 72: reducing lr to 2.6899552370050058e-05
Epoch 75: reducing lr to 2.1769050916037822e-05
Epoch 78: reducing lr to 1.702862914998415e-05
Epoch 81: reducing lr to 1.2753032326057688e-05
Epoch 84: reducing lr to 9.009701757340944e-06
Epoch 87: reducing lr to 5.857656694550287e-06
Epoch 90: reducing lr to 3.3466254394622563e-06
Epoch 93: reducing lr to 1.51619587280546e-06
Epoch 96: reducing lr to 3.9524035522664654e-07
Epoch 99: reducing lr to 1.433707492418286e-09
[I 2024-06-21 05:54:07,000] Trial 952 finished with value: 0.9710406064987183 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.6018828631564507, 'bidirectional': True, 'fc_dropout': 0.19132400858861295, 'learning_rate_model': 0.0009300549889982566}. Best is trial 690 with value: 0.966281533241272.
Epoch 47: reducing lr to 3.849551145443565e-05
Epoch 53: reducing lr to 3.3077740398825876e-05
Epoch 56: reducing lr to 3.0142660008633244e-05
Epoch 61: reducing lr to 2.507484953600858e-05
Epoch 64: reducing lr to 2.2006702994030297e-05
Epoch 67: reducing lr to 1.8977062945844807e-05
Epoch 73: reducing lr to 1.3223109663667047e-05
Epoch 79: reducing lr to 8.174520768522572e-06
Epoch 82: reducing lr to 6.016170941072606e-06
Epoch 85: reducing lr to 4.148521998390968e-06
Epoch 88: reducing lr to 2.601020298195056e-06
Epoch 91: reducing lr to 1.3980800319111083e-06
Epoch 94: reducing lr to 5.586661749475987e-07
Epoch 97: reducing lr to 9.601925840804011e-08
[I 2024-06-21 05:54:46,236] Trial 953 finished with value: 0.9724687337875366 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.6369224274837821, 'bidirectional': True, 'fc_dropout': 0.1618015567521468, 'learning_rate_model': 0.0004889767545611957}. Best is trial 690 with value: 0.966281533241272.
Epoch 16: reducing lr to 5.403007125624338e-05
Epoch 19: reducing lr to 6.33328775894662e-05
Epoch 22: reducing lr to 6.885532682813273e-05
Epoch 25: reducing lr to 7.000702975794685e-05
Epoch 28: reducing lr to 6.95677494917945e-05
Epoch 31: reducing lr to 6.858356752396705e-05
Epoch 34: reducing lr to 6.707000629936429e-05
Epoch 37: reducing lr to 6.505093375047868e-05
Epoch 40: reducing lr to 6.255819153178958e-05
Epoch 43: reducing lr to 5.9631093515589754e-05
Epoch 46: reducing lr to 5.631579832225099e-05
Epoch 49: reducing lr to 5.266459504931709e-05
Epoch 52: reducing lr to 4.873506299792798e-05
Epoch 55: reducing lr to 4.4589166903170866e-05
Epoch 58: reducing lr to 4.029230557624346e-05
Epoch 61: reducing lr to 3.591222407734157e-05
Epoch 64: reducing lr to 3.1518021593318054e-05
Epoch 67: reducing lr to 2.7178968147438666e-05
Epoch 70: reducing lr to 2.2963522180196503e-05
Epoch 73: reducing lr to 1.8938150618169647e-05
Epoch 76: reducing lr to 1.5166330453189647e-05
Epoch 79: reducing lr to 1.1707556655225087e-05
Epoch 82: reducing lr to 8.616365917296025e-06
Epoch 85: reducing lr to 5.941517271401838e-06
Epoch 88: reducing lr to 3.725183819921075e-06
Epoch 91: reducing lr to 2.0023315917388586e-06
Epoch 94: reducing lr to 8.001222432197762e-07
Epoch 97: reducing lr to 1.3751887598518987e-07
[I 2024-06-21 05:55:02,472] Trial 954 finished with value: 1.0924839973449707 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5834462154818788, 'bidirectional': False, 'fc_dropout': 0.14400478549632678, 'learning_rate_model': 0.0007003129870508549}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.908019262544868e-05
Epoch 40: reducing lr to 8.471934001469569e-05
Epoch 43: reducing lr to 8.075532177793514e-05
Epoch 52: reducing lr to 6.59993882761298e-05
Epoch 62: reducing lr to 4.6647950409040004e-05
Epoch 65: reducing lr to 4.071160628261939e-05
Epoch 68: reducing lr to 3.488104598374001e-05
Epoch 71: reducing lr to 2.924825546320311e-05
Epoch 74: reducing lr to 2.3902041732482947e-05
Epoch 77: reducing lr to 1.8926733527161312e-05
Epoch 80: reducing lr to 1.4400779705742184e-05
Epoch 83: reducing lr to 1.0395570598333178e-05
Epoch 86: reducing lr to 6.974254256985029e-06
Epoch 89: reducing lr to 4.1908071471690554e-06
Epoch 92: reducing lr to 2.0891121074041406e-06
Epoch 95: reducing lr to 7.023203161146534e-07
Epoch 98: reducing lr to 5.2298617306376255e-08
[I 2024-06-21 05:55:35,358] Trial 955 finished with value: 0.9697107076644897 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.55448912884837, 'bidirectional': True, 'fc_dropout': 0.2166374778266672, 'learning_rate_model': 0.0009483978454926945}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00011820970213196966
Epoch 47: reducing lr to 9.340407702688894e-05
Epoch 50: reducing lr to 8.704997592111902e-05
Epoch 53: reducing lr to 8.025859886922917e-05
Epoch 56: reducing lr to 7.313702899035761e-05
Epoch 62: reducing lr to 5.835605984157954e-05
Epoch 65: reducing lr to 5.0929760292639934e-05
Epoch 68: reducing lr to 4.3635795118868754e-05
Epoch 71: reducing lr to 3.6589237707252406e-05
Epoch 74: reducing lr to 2.9901184627532952e-05
Epoch 77: reducing lr to 2.3677130176819388e-05
Epoch 80: reducing lr to 1.80152129923127e-05
Epoch 83: reducing lr to 1.3004741571799745e-05
Epoch 86: reducing lr to 8.72471341617912e-06
Epoch 89: reducing lr to 5.24265247498041e-06
Epoch 92: reducing lr to 2.6134556842570023e-06
Epoch 95: reducing lr to 8.785947943213594e-07
Epoch 98: reducing lr to 6.542498039896656e-08
[I 2024-06-21 05:56:06,669] Trial 956 finished with value: 0.9732323884963989 and parameters: {'hidden_size': 124, 'n_layers': 2, 'rnn_dropout': 0.612416290641638, 'bidirectional': True, 'fc_dropout': 0.17951539266248245, 'learning_rate_model': 0.0011864350081814401}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 7.530187841710283e-05
Epoch 40: reducing lr to 7.088978550946098e-05
Epoch 43: reducing lr to 6.757285217982452e-05
Epoch 46: reducing lr to 6.381602098950943e-05
Epoch 52: reducing lr to 5.522567194030189e-05
Epoch 55: reducing lr to 5.052761917206459e-05
Epoch 58: reducing lr to 4.565849539512493e-05
Epoch 61: reducing lr to 4.0695068058621316e-05
Epoch 65: reducing lr to 3.406585835796043e-05
Epoch 68: reducing lr to 2.9187076619152727e-05
Epoch 72: reducing lr to 2.295239892048707e-05
Epoch 75: reducing lr to 1.8574730682195544e-05
Epoch 78: reducing lr to 1.4529903098114028e-05
Epoch 81: reducing lr to 1.0881693545185137e-05
Epoch 84: reducing lr to 7.687647215994095e-06
Epoch 87: reducing lr to 4.998123066994728e-06
Epoch 90: reducing lr to 2.855552429545686e-06
Epoch 93: reducing lr to 1.2937141865964154e-06
Epoch 96: reducing lr to 3.3724406182822784e-07
Epoch 99: reducing lr to 1.2233298847660543e-09
[I 2024-06-21 05:57:03,039] Trial 957 finished with value: 0.9771718382835388 and parameters: {'hidden_size': 133, 'n_layers': 3, 'rnn_dropout': 0.5969459232576348, 'bidirectional': True, 'fc_dropout': 0.119410183194204, 'learning_rate_model': 0.0007935817232871479}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.648333256076078e-05
Epoch 47: reducing lr to 4.977946075597001e-05
Epoch 53: reducing lr to 4.277361224381881e-05
Epoch 65: reducing lr to 2.7142883742308014e-05
Epoch 72: reducing lr to 1.8287937704651347e-05
Epoch 75: reducing lr to 1.479991345451308e-05
Epoch 78: reducing lr to 1.157708889748118e-05
Epoch 86: reducing lr to 4.649813401429409e-06
Epoch 89: reducing lr to 2.7940580480266023e-06
Epoch 92: reducing lr to 1.3928344330674724e-06
Epoch 95: reducing lr to 4.6824481838976483e-07
Epoch 98: reducing lr to 3.486807372757436e-08
[I 2024-06-21 05:57:42,902] Trial 958 finished with value: 0.9695045948028564 and parameters: {'hidden_size': 143, 'n_layers': 2, 'rnn_dropout': 0.5621633752207146, 'bidirectional': True, 'fc_dropout': 0.23214685524621026, 'learning_rate_model': 0.0006323074624705609}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 9.763954466907122e-05
Epoch 36: reducing lr to 9.364582980908053e-05
Epoch 40: reducing lr to 8.906146992645053e-05
Epoch 52: reducing lr to 6.938206238503568e-05
Epoch 55: reducing lr to 6.347972423682105e-05
Epoch 61: reducing lr to 5.1126725155263045e-05
Epoch 65: reducing lr to 4.279820284209074e-05
Epoch 72: reducing lr to 2.8835951068352983e-05
Epoch 75: reducing lr to 2.3336123902131063e-05
Epoch 78: reducing lr to 1.8254456809355383e-05
Epoch 81: reducing lr to 1.3671075676960754e-05
Epoch 84: reducing lr to 9.658276667250488e-06
Epoch 87: reducing lr to 6.279327607225447e-06
Epoch 90: reducing lr to 3.5875365540983865e-06
Epoch 93: reducing lr to 1.6253411728492373e-06
Epoch 96: reducing lr to 4.23692237951274e-07
Epoch 99: reducing lr to 1.5369147608225442e-09
[I 2024-06-21 05:58:17,337] Trial 959 finished with value: 0.9725732207298279 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.6280424521040558, 'bidirectional': True, 'fc_dropout': 0.13728634526566802, 'learning_rate_model': 0.0009970061874892604}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 7.194922394768769e-05
Epoch 43: reducing lr to 6.858271948955653e-05
Epoch 52: reducing lr to 5.6051012279674845e-05
Epoch 61: reducing lr to 4.130325045101693e-05
Epoch 65: reducing lr to 3.457496809099514e-05
Epoch 72: reducing lr to 2.329541947684963e-05
Epoch 75: reducing lr to 1.8852327567600156e-05
Epoch 78: reducing lr to 1.4747050571973954e-05
Epoch 81: reducing lr to 1.104431901134957e-05
Epoch 84: reducing lr to 7.802538083579867e-06
Epoch 87: reducing lr to 5.072819353040838e-06
Epoch 90: reducing lr to 2.8982282817081806e-06
Epoch 93: reducing lr to 1.3130485734549673e-06
Epoch 96: reducing lr to 3.422841295840713e-07
Epoch 99: reducing lr to 1.241612387578063e-09
[I 2024-06-21 05:58:51,490] Trial 960 finished with value: 0.9682494401931763 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.5339510593386221, 'bidirectional': True, 'fc_dropout': 0.2592487438105243, 'learning_rate_model': 0.0008054416968430333}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.355406166497416e-05
Epoch 49: reducing lr to 4.508447098249003e-05
Epoch 61: reducing lr to 3.074330340555008e-05
Epoch 65: reducing lr to 2.5735232037470445e-05
Epoch 72: reducing lr to 1.7339510598220142e-05
Epoch 75: reducing lr to 1.4032378081209292e-05
Epoch 78: reducing lr to 1.097669178867311e-05
Epoch 81: reducing lr to 8.220632675781213e-06
Epoch 84: reducing lr to 5.807673561220874e-06
Epoch 87: reducing lr to 3.7758583837616855e-06
Epoch 90: reducing lr to 2.1572421160598236e-06
Epoch 93: reducing lr to 9.7734319306961e-07
Epoch 96: reducing lr to 2.547728019417633e-07
Epoch 99: reducing lr to 9.241710017160627e-10
[I 2024-06-21 05:59:27,705] Trial 961 finished with value: 0.972450852394104 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.5768362294242962, 'bidirectional': True, 'fc_dropout': 0.16509143945082674, 'learning_rate_model': 0.0005995154906970959}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00012193708214826083
Epoch 36: reducing lr to 0.00011495218143489237
Epoch 40: reducing lr to 0.00010932478542521095
Epoch 45: reducing lr to 0.00010041694534603086
Epoch 52: reducing lr to 8.516790806245035e-05
Epoch 55: reducing lr to 7.79226666343279e-05
Epoch 58: reducing lr to 7.041360297590172e-05
Epoch 61: reducing lr to 6.275910628590305e-05
Epoch 65: reducing lr to 5.253567391331173e-05
Epoch 68: reducing lr to 4.50117159425205e-05
Epoch 71: reducing lr to 3.7742966978045113e-05
Epoch 74: reducing lr to 3.0844026678853515e-05
Epoch 77: reducing lr to 2.4423715780813852e-05
Epoch 80: reducing lr to 1.858326742173516e-05
Epoch 83: reducing lr to 1.341480616867726e-05
Epoch 86: reducing lr to 8.99982046618274e-06
Epoch 89: reducing lr to 5.407963424210161e-06
Epoch 92: reducing lr to 2.695862985140703e-06
Epoch 95: reducing lr to 9.06298583601829e-07
Epoch 98: reducing lr to 6.748795628086415e-08
[I 2024-06-21 06:00:06,582] Trial 962 finished with value: 0.9746545553207397 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.6046922461583238, 'bidirectional': True, 'fc_dropout': 0.20744298761537694, 'learning_rate_model': 0.001223845593441072}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.394502871040637e-05
Epoch 43: reducing lr to 6.0953040577422116e-05
Epoch 49: reducing lr to 5.383210351819724e-05
Epoch 52: reducing lr to 4.981545863617805e-05
Epoch 55: reducing lr to 4.557765318947029e-05
Epoch 65: reducing lr to 3.0728577821041565e-05
Epoch 68: reducing lr to 2.632774861669549e-05
Epoch 71: reducing lr to 2.2076193405182318e-05
Epoch 74: reducing lr to 1.8040942535149934e-05
Epoch 77: reducing lr to 1.428564621228876e-05
Epoch 80: reducing lr to 1.0869516589437389e-05
Epoch 86: reducing lr to 5.264074160861291e-06
Epoch 89: reducing lr to 3.163165379935879e-06
Epoch 92: reducing lr to 1.576833975147141e-06
Epoch 95: reducing lr to 5.301020141335196e-07
Epoch 98: reducing lr to 3.9474299311009414e-08
[I 2024-06-21 06:00:40,034] Trial 963 finished with value: 0.9745340347290039 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5871625382349281, 'bidirectional': True, 'fc_dropout': 0.18511068803705777, 'learning_rate_model': 0.0007158380536061561}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00014851744064028167
Epoch 30: reducing lr to 0.00014680643257988533
Epoch 36: reducing lr to 0.00014000994186468663
Epoch 49: reducing lr to 0.00011209722338715554
Epoch 52: reducing lr to 0.00010373316719800112
Epoch 55: reducing lr to 9.490857754267915e-05
Epoch 58: reducing lr to 8.576265657666468e-05
Epoch 61: reducing lr to 7.643960047461802e-05
Epoch 64: reducing lr to 6.708648768606036e-05
Epoch 67: reducing lr to 5.785076028787057e-05
Epoch 70: reducing lr to 4.887813289324377e-05
Epoch 73: reducing lr to 4.031008115407675e-05
Epoch 76: reducing lr to 3.228171661023083e-05
Epoch 79: reducing lr to 2.4919675020183504e-05
Epoch 82: reducing lr to 1.8340038390347976e-05
Epoch 85: reducing lr to 1.2646590906229926e-05
Epoch 88: reducing lr to 7.929098522999576e-06
Epoch 91: reducing lr to 4.261986853295306e-06
Epoch 94: reducing lr to 1.703069809067155e-06
Epoch 97: reducing lr to 2.927105799793413e-07
[I 2024-06-21 06:01:15,774] Trial 964 finished with value: 0.9858074188232422 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.6195234064841889, 'bidirectional': True, 'fc_dropout': 0.14556735964995887, 'learning_rate_model': 0.0014906246079904838}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.340292407002678e-05
Epoch 40: reducing lr to 7.931999779364933e-05
Epoch 43: reducing lr to 7.560861479964476e-05
Epoch 52: reducing lr to 6.179310806170974e-05
Epoch 55: reducing lr to 5.6536362925116014e-05
Epoch 58: reducing lr to 5.1088203017899e-05
Epoch 61: reducing lr to 4.5534524973156405e-05
Epoch 65: reducing lr to 3.811696974921116e-05
Epoch 72: reducing lr to 2.5681897873552147e-05
Epoch 75: reducing lr to 2.078363739064703e-05
Epoch 78: reducing lr to 1.625779896781502e-05
Epoch 81: reducing lr to 1.2175744386757368e-05
Epoch 84: reducing lr to 8.60186211354278e-06
Epoch 87: reducing lr to 5.592499791010075e-06
Epoch 90: reducing lr to 3.195134683839395e-06
Epoch 93: reducing lr to 1.4475626592599217e-06
Epoch 96: reducing lr to 3.773491208626501e-07
Epoch 99: reducing lr to 1.3688082572569856e-09
[I 2024-06-21 06:01:51,869] Trial 965 finished with value: 0.9734346866607666 and parameters: {'hidden_size': 138, 'n_layers': 2, 'rnn_dropout': 0.6524294828897216, 'bidirectional': True, 'fc_dropout': 0.0962071149410611, 'learning_rate_model': 0.0008879544505296335}. Best is trial 690 with value: 0.966281533241272.
Epoch 47: reducing lr to 3.8227700917670514e-05
Epoch 53: reducing lr to 3.284762091017659e-05
Epoch 65: reducing lr to 2.0844139851788423e-05
Epoch 68: reducing lr to 1.7858921989333022e-05
Epoch 73: reducing lr to 1.3131117429691339e-05
Epoch 76: reducing lr to 1.0515856071356178e-05
Epoch 79: reducing lr to 8.117651208615388e-06
Epoch 82: reducing lr to 5.974316867490375e-06
Epoch 85: reducing lr to 4.11966102574261e-06
Epoch 88: reducing lr to 2.58292518487201e-06
Epoch 91: reducing lr to 1.388353688510529e-06
Epoch 94: reducing lr to 5.547795740808376e-07
Epoch 97: reducing lr to 9.53512592527495e-08
[I 2024-06-21 06:02:26,102] Trial 966 finished with value: 0.9729781150817871 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5704699634767851, 'bidirectional': True, 'fc_dropout': 0.24597373519826055, 'learning_rate_model': 0.0004855749780382442}. Best is trial 690 with value: 0.966281533241272.
Epoch 27: reducing lr to 0.00010695512653330714
Epoch 36: reducing lr to 0.00010082843458317041
Epoch 40: reducing lr to 9.589245578439376e-05
Epoch 45: reducing lr to 8.807908887400013e-05
Epoch 48: reducing lr to 8.264453415043108e-05
Epoch 52: reducing lr to 7.470364406719872e-05
Epoch 55: reducing lr to 6.834859849732858e-05
Epoch 58: reducing lr to 6.176214555303787e-05
Epoch 61: reducing lr to 5.504812839267839e-05
Epoch 64: reducing lr to 4.831246585050316e-05
Epoch 67: reducing lr to 4.166133862772092e-05
Epoch 70: reducing lr to 3.519968338917624e-05
Epoch 73: reducing lr to 2.902938410341019e-05
Epoch 76: reducing lr to 2.3247741611183836e-05
Epoch 79: reducing lr to 1.7945952902650055e-05
Epoch 82: reducing lr to 1.3207614662687336e-05
Epoch 85: reducing lr to 9.107467276297316e-06
Epoch 88: reducing lr to 5.7101558723768145e-06
Epoch 91: reducing lr to 3.069278202023191e-06
Epoch 94: reducing lr to 1.2264690674613534e-06
Epoch 97: reducing lr to 2.1079609899255468e-07
[I 2024-06-21 06:03:05,323] Trial 967 finished with value: 0.9740467667579651 and parameters: {'hidden_size': 146, 'n_layers': 2, 'rnn_dropout': 0.5150032217940598, 'bidirectional': True, 'fc_dropout': 0.22321128978501822, 'learning_rate_model': 0.0010734762387094499}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.987615927496597e-05
Epoch 52: reducing lr to 5.4436020942009146e-05
Epoch 72: reducing lr to 2.262421124826884e-05
Epoch 77: reducing lr to 1.5610691092132605e-05
Epoch 80: reducing lr to 1.1877703204812363e-05
Epoch 83: reducing lr to 8.574223391698743e-06
Epoch 86: reducing lr to 5.752335903474483e-06
Epoch 89: reducing lr to 3.456560304358541e-06
[I 2024-06-21 06:03:41,821] Trial 968 finished with value: 0.9667459726333618 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5967702159940949, 'bidirectional': True, 'fc_dropout': 0.15675673118636857, 'learning_rate_model': 0.000782234598335948}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.149525412987836e-05
Epoch 43: reducing lr to 4.908579099617066e-05
Epoch 49: reducing lr to 4.335126446763729e-05
Epoch 52: reducing lr to 4.0116640086033215e-05
Epoch 55: reducing lr to 3.6703913986253e-05
Epoch 61: reducing lr to 2.956142209277415e-05
Epoch 65: reducing lr to 2.4745878700133642e-05
Epoch 68: reducing lr to 2.120186874611026e-05
Epoch 72: reducing lr to 1.6672918486163085e-05
Epoch 78: reducing lr to 1.0554708935041584e-05
Epoch 81: reducing lr to 7.904602481805802e-06
Epoch 84: reducing lr to 5.584406049523652e-06
Epoch 87: reducing lr to 3.6307010334084135e-06
Epoch 90: reducing lr to 2.0743100995985055e-06
Epoch 93: reducing lr to 9.397706641575405e-07
Epoch 96: reducing lr to 2.4497843437993986e-07
Epoch 99: reducing lr to 8.886425998918049e-10
[I 2024-06-21 06:04:15,252] Trial 969 finished with value: 0.9755004644393921 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5492488463772508, 'bidirectional': True, 'fc_dropout': 0.1679679955071605, 'learning_rate_model': 0.0005764679949277677}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.994787249882258e-05
Epoch 40: reducing lr to 6.65236279679911e-05
Epoch 49: reducing lr to 5.600289654098085e-05
Epoch 53: reducing lr to 5.037691189155618e-05
Epoch 56: reducing lr to 4.590682764672617e-05
Epoch 64: reducing lr to 3.3515884833332255e-05
Epoch 67: reducing lr to 2.890178762081565e-05
Epoch 72: reducing lr to 2.153874265222068e-05
Epoch 75: reducing lr to 1.7430698437408793e-05
Epoch 78: reducing lr to 1.3634994959617959e-05
Epoch 81: reducing lr to 1.0211481497076562e-05
Epoch 84: reducing lr to 7.214158988782681e-06
Epoch 87: reducing lr to 4.690284743527932e-06
Epoch 90: reducing lr to 2.679676713661914e-06
Epoch 93: reducing lr to 1.2140333142151562e-06
Epoch 96: reducing lr to 3.164729353071806e-07
Epoch 99: reducing lr to 1.1479840367916752e-09
[I 2024-06-21 06:04:48,426] Trial 970 finished with value: 0.9702646732330322 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.589474007081664, 'bidirectional': True, 'fc_dropout': 0.17775376683875918, 'learning_rate_model': 0.0007447044019495002}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 6.339236298840609e-05
Epoch 40: reducing lr to 6.028903840532906e-05
Epoch 49: reducing lr to 5.0754309160550515e-05
Epoch 61: reducing lr to 3.460959154356956e-05
Epoch 65: reducing lr to 2.8971703442091466e-05
Epoch 72: reducing lr to 1.952013326132859e-05
Epoch 75: reducing lr to 1.5797094650795297e-05
Epoch 78: reducing lr to 1.2357124226183424e-05
Epoch 81: reducing lr to 9.254462195729628e-06
Epoch 84: reducing lr to 6.538048534366541e-06
Epoch 87: reducing lr to 4.2507115993515145e-06
Epoch 90: reducing lr to 2.428537607443242e-06
Epoch 93: reducing lr to 1.100254200526833e-06
Epoch 96: reducing lr to 2.868131148854743e-07
Epoch 99: reducing lr to 1.0403950565646136e-09
[I 2024-06-21 06:05:18,458] Trial 971 finished with value: 0.9744555950164795 and parameters: {'hidden_size': 121, 'n_layers': 2, 'rnn_dropout': 0.5651992682774994, 'bidirectional': True, 'fc_dropout': 0.19490630004349827, 'learning_rate_model': 0.0006749107596981058}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.430778229203086e-05
Epoch 47: reducing lr to 3.904899746164172e-05
Epoch 50: reducing lr to 3.639256868628115e-05
Epoch 53: reducing lr to 3.3553330039514634e-05
Epoch 56: reducing lr to 3.057604925076594e-05
Epoch 62: reducing lr to 2.4396639902230953e-05
Epoch 65: reducing lr to 2.12919622321925e-05
Epoch 68: reducing lr to 1.8242608963877455e-05
Epoch 72: reducing lr to 1.4345788848705996e-05
Epoch 75: reducing lr to 1.1609643297481658e-05
Epoch 78: reducing lr to 9.081530978953528e-06
Epoch 81: reducing lr to 6.801314252873758e-06
Epoch 84: reducing lr to 4.804960217276205e-06
Epoch 87: reducing lr to 3.123944403691614e-06
Epoch 90: reducing lr to 1.7847873916179907e-06
Epoch 93: reducing lr to 8.086017769115041e-07
Epoch 96: reducing lr to 2.1078546596492573e-07
Epoch 99: reducing lr to 7.646099337986663e-10
[I 2024-06-21 06:05:54,196] Trial 972 finished with value: 0.9762164354324341 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5971956354117313, 'bidirectional': True, 'fc_dropout': 0.16060077296553454, 'learning_rate_model': 0.000496007231135562}. Best is trial 690 with value: 0.966281533241272.
Epoch 17: reducing lr to 5.3410723173277675e-05
Epoch 20: reducing lr to 6.0997546563525097e-05
Epoch 23: reducing lr to 6.476995482288648e-05
Epoch 26: reducing lr to 6.497214614141929e-05
Epoch 29: reducing lr to 6.439407238591381e-05
Epoch 32: reducing lr to 6.331359823581994e-05
Epoch 35: reducing lr to 6.174776339811938e-05
Epoch 38: reducing lr to 5.9721261603992205e-05
Epoch 41: reducing lr to 5.726605225487513e-05
Epoch 44: reducing lr to 5.442085548155142e-05
Epoch 47: reducing lr to 5.1230542998139144e-05
Epoch 50: reducing lr to 4.774542692745673e-05
Epoch 53: reducing lr to 4.4020472459213776e-05
Epoch 56: reducing lr to 4.011441285767463e-05
Epoch 59: reducing lr to 3.608886633722328e-05
Epoch 62: reducing lr to 3.200730340770221e-05
Epoch 65: reducing lr to 2.793410478009318e-05
Epoch 68: reducing lr to 2.3933489299954753e-05
Epoch 71: reducing lr to 2.006857275717104e-05
Epoch 74: reducing lr to 1.6400289725202653e-05
Epoch 77: reducing lr to 1.298650202653242e-05
Epoch 80: reducing lr to 9.881037029653634e-06
Epoch 83: reducing lr to 7.132878922212131e-06
Epoch 86: reducing lr to 4.785356485941536e-06
Epoch 89: reducing lr to 2.8755054553610646e-06
Epoch 92: reducing lr to 1.4334358634850186e-06
Epoch 95: reducing lr to 4.818942579504757e-07
Epoch 98: reducing lr to 3.588448575447085e-08
[I 2024-06-21 06:06:09,184] Trial 973 finished with value: 1.092915654182434 and parameters: {'hidden_size': 129, 'n_layers': 2, 'rnn_dropout': 0.5769884930290883, 'bidirectional': False, 'fc_dropout': 0.1564067740608372, 'learning_rate_model': 0.0006507393642318113}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.627404414640945e-05
Epoch 40: reducing lr to 7.254010672726725e-05
Epoch 52: reducing lr to 5.651133104500499e-05
Epoch 61: reducing lr to 4.164245326785219e-05
Epoch 65: reducing lr to 3.485891490971741e-05
Epoch 72: reducing lr to 2.348673303739561e-05
Epoch 77: reducing lr to 1.6205830567384883e-05
Epoch 80: reducing lr to 1.2330526850530197e-05
Epoch 83: reducing lr to 8.901105704590256e-06
Epoch 86: reducing lr to 5.971637031839891e-06
Epoch 89: reducing lr to 3.5883376532006106e-06
Epoch 92: reducing lr to 1.7887818201845645e-06
Epoch 95: reducing lr to 6.013549052535992e-07
Epoch 98: reducing lr to 4.478017983184854e-08
[I 2024-06-21 06:06:45,656] Trial 974 finished with value: 0.9665114879608154 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5417512597475862, 'bidirectional': True, 'fc_dropout': 0.20132210453650856, 'learning_rate_model': 0.0008120563842921421}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 8.145228947300932e-05
Epoch 40: reducing lr to 7.667983125218712e-05
Epoch 61: reducing lr to 4.40189080712466e-05
Epoch 65: reducing lr to 3.684824621172951e-05
Epoch 72: reducing lr to 2.4827075768496184e-05
Epoch 75: reducing lr to 2.009185391138598e-05
Epoch 78: reducing lr to 1.571665804413133e-05
Epoch 81: reducing lr to 1.1770474671156253e-05
Epoch 84: reducing lr to 8.315549088099581e-06
Epoch 87: reducing lr to 5.406353406213514e-06
Epoch 90: reducing lr to 3.0887846091749457e-06
Epoch 93: reducing lr to 1.3993805285746627e-06
Epoch 96: reducing lr to 3.6478905340092333e-07
Epoch 99: reducing lr to 1.3232474672529754e-09
[I 2024-06-21 06:07:19,971] Trial 975 finished with value: 0.9694185256958008 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5266823915666504, 'bidirectional': True, 'fc_dropout': 0.18736131574847226, 'learning_rate_model': 0.0008583988819991147}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00010104794578184991
Epoch 34: reducing lr to 9.881793270316713e-05
Epoch 37: reducing lr to 9.584312196037341e-05
Epoch 49: reducing lr to 7.759364724366055e-05
Epoch 52: reducing lr to 7.18040513388099e-05
Epoch 55: reducing lr to 6.569567437731982e-05
Epoch 61: reducing lr to 5.291145681805958e-05
Epoch 64: reducing lr to 4.643723638318775e-05
Epoch 67: reducing lr to 4.004427006234787e-05
Epoch 70: reducing lr to 3.383342144477854e-05
Epoch 73: reducing lr to 2.7902619912628037e-05
Epoch 76: reducing lr to 2.234538961257698e-05
Epoch 79: reducing lr to 1.7249387759271974e-05
Epoch 82: reducing lr to 1.269496626496203e-05
Epoch 85: reducing lr to 8.753964495835396e-06
Epoch 88: reducing lr to 5.488518405393026e-06
Epoch 91: reducing lr to 2.9501453689851664e-06
Epoch 94: reducing lr to 1.1788641502714004e-06
Epoch 97: reducing lr to 2.0261413085102733e-07
[I 2024-06-21 06:08:09,609] Trial 976 finished with value: 0.9823062419891357 and parameters: {'hidden_size': 124, 'n_layers': 3, 'rnn_dropout': 0.5384316873877009, 'bidirectional': True, 'fc_dropout': 0.1740479760776431, 'learning_rate_model': 0.0010318096783330892}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 7.818914461721804e-05
Epoch 40: reducing lr to 7.360788080717928e-05
Epoch 49: reducing lr to 6.19667726996008e-05
Epoch 52: reducing lr to 5.734316514662899e-05
Epoch 55: reducing lr to 5.2464977045127426e-05
Epoch 61: reducing lr to 4.2255420827860584e-05
Epoch 65: reducing lr to 3.537203030854635e-05
Epoch 72: reducing lr to 2.383245247303739e-05
Epoch 75: reducing lr to 1.9286933262028745e-05
Epoch 78: reducing lr to 1.5087016665371416e-05
Epoch 81: reducing lr to 1.1298925447409369e-05
Epoch 84: reducing lr to 7.982411230275427e-06
Epoch 87: reducing lr to 5.189763861337417e-06
Epoch 90: reducing lr to 2.9650415974893627e-06
Epoch 93: reducing lr to 1.3433184902617176e-06
Epoch 96: reducing lr to 3.501748598557755e-07
Epoch 99: reducing lr to 1.2702354746708422e-09
[I 2024-06-21 06:08:44,011] Trial 977 finished with value: 0.9721762537956238 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5256839958590155, 'bidirectional': True, 'fc_dropout': 0.20046408472931365, 'learning_rate_model': 0.0008240096718966709}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.452969000470724e-05
Epoch 43: reducing lr to 5.197824560504521e-05
Epoch 47: reducing lr to 4.805769136770589e-05
Epoch 50: reducing lr to 4.478841833830288e-05
Epoch 53: reducing lr to 4.129416077792373e-05
Epoch 56: reducing lr to 3.7630014434570486e-05
Epoch 59: reducing lr to 3.3853781333289087e-05
Epoch 65: reducing lr to 2.620406709731025e-05
Epoch 72: reducing lr to 1.765539547064224e-05
Epoch 75: reducing lr to 1.4288014821061475e-05
Epoch 78: reducing lr to 1.1176661151455331e-05
Epoch 81: reducing lr to 8.370393160041049e-06
Epoch 84: reducing lr to 5.913475637442313e-06
Epoch 87: reducing lr to 3.844645593013189e-06
Epoch 90: reducing lr to 2.1965419651965723e-06
Epoch 93: reducing lr to 9.951480744765298e-07
Epoch 96: reducing lr to 2.594141598152768e-07
Epoch 99: reducing lr to 9.410072115511973e-10
[I 2024-06-21 06:09:18,405] Trial 978 finished with value: 0.9738677144050598 and parameters: {'hidden_size': 136, 'n_layers': 2, 'rnn_dropout': 0.5480285015183373, 'bidirectional': True, 'fc_dropout': 0.19320186953890858, 'learning_rate_model': 0.0006104372449889018}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 8.729113572511556e-05
Epoch 52: reducing lr to 6.800290888464072e-05
Epoch 61: reducing lr to 5.011044516101455e-05
Epoch 64: reducing lr to 4.397895516674865e-05
Epoch 67: reducing lr to 3.7924417879328564e-05
Epoch 70: reducing lr to 3.2042357399983695e-05
Epoch 73: reducing lr to 2.6425518953073814e-05
Epoch 76: reducing lr to 2.1162475730593714e-05
Epoch 79: reducing lr to 1.633624457448408e-05
Epoch 82: reducing lr to 1.202292375030923e-05
Epoch 85: reducing lr to 8.290549612315805e-06
Epoch 88: reducing lr to 5.197968778565055e-06
Epoch 91: reducing lr to 2.793971412238551e-06
Epoch 94: reducing lr to 1.116457775063545e-06
Epoch 97: reducing lr to 1.9188820160008763e-07
[I 2024-06-21 06:09:52,563] Trial 979 finished with value: 0.9707975387573242 and parameters: {'hidden_size': 131, 'n_layers': 2, 'rnn_dropout': 0.516238256903461, 'bidirectional': True, 'fc_dropout': 0.17236185192915973, 'learning_rate_model': 0.000977188030949598}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00011656738255383251
Epoch 39: reducing lr to 0.0001124226248040643
Epoch 47: reducing lr to 9.770309837965904e-05
Epoch 51: reducing lr to 8.873397962293856e-05
Epoch 54: reducing lr to 8.150214337922096e-05
Epoch 58: reducing lr to 7.140298942247123e-05
Epoch 61: reducing lr to 6.364093886560132e-05
Epoch 65: reducing lr to 5.3273856331686425e-05
Epoch 68: reducing lr to 4.564417870267223e-05
Epoch 71: reducing lr to 3.827329603952176e-05
Epoch 74: reducing lr to 3.1277418249004114e-05
Epoch 77: reducing lr to 2.4766895114737065e-05
Epoch 80: reducing lr to 1.8844382208409735e-05
Epoch 83: reducing lr to 1.3603298545799181e-05
Epoch 86: reducing lr to 9.126277571265811e-06
Epoch 89: reducing lr to 5.4839510954743025e-06
Epoch 92: reducing lr to 2.7337427439739527e-06
Epoch 95: reducing lr to 9.190330482118734e-07
Epoch 98: reducing lr to 6.843623426167761e-08
[I 2024-06-21 06:10:29,807] Trial 980 finished with value: 0.9778882265090942 and parameters: {'hidden_size': 140, 'n_layers': 2, 'rnn_dropout': 0.5413186528962478, 'bidirectional': True, 'fc_dropout': 0.15485955302052803, 'learning_rate_model': 0.0012410419332343763}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.849449067130969e-05
Epoch 43: reducing lr to 6.528963319612634e-05
Epoch 49: reducing lr to 5.766206672519775e-05
Epoch 53: reducing lr to 5.1869403804402657e-05
Epoch 65: reducing lr to 3.291480712969672e-05
Epoch 72: reducing lr to 2.2176860353649478e-05
Epoch 75: reducing lr to 1.7947109139777774e-05
Epoch 78: reducing lr to 1.4038952227835174e-05
Epoch 81: reducing lr to 1.051401202108675e-05
Epoch 84: reducing lr to 7.427889317706464e-06
Epoch 87: reducing lr to 4.829241495456899e-06
Epoch 90: reducing lr to 2.7590661735159155e-06
Epoch 93: reducing lr to 1.2500008802162807e-06
Epoch 96: reducing lr to 3.258489228150628e-07
Epoch 99: reducing lr to 1.1819947934375678e-09
[I 2024-06-21 06:11:01,292] Trial 981 finished with value: 0.9703059792518616 and parameters: {'hidden_size': 128, 'n_layers': 2, 'rnn_dropout': 0.5566137557278701, 'bidirectional': True, 'fc_dropout': 0.20553062568941935, 'learning_rate_model': 0.000766767391831917}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 8.665596905089761e-05
Epoch 36: reducing lr to 8.264417965774219e-05
Epoch 40: reducing lr to 7.859839713300791e-05
Epoch 49: reducing lr to 6.616803739334141e-05
Epoch 52: reducing lr to 6.123095540360685e-05
Epoch 55: reducing lr to 5.602203264307088e-05
Epoch 65: reducing lr to 3.777020687329386e-05
Epoch 72: reducing lr to 2.544826102297771e-05
Epoch 75: reducing lr to 2.0594561660833744e-05
Epoch 78: reducing lr to 1.6109896310199348e-05
Epoch 81: reducing lr to 1.2064977550679769e-05
Epoch 84: reducing lr to 8.523608084842141e-06
Epoch 87: reducing lr to 5.5416229420932634e-06
Epoch 90: reducing lr to 3.166067470490564e-06
Epoch 93: reducing lr to 1.4343936955648044e-06
Epoch 96: reducing lr to 3.7391624917227676e-07
Epoch 99: reducing lr to 1.356355748809851e-09
[I 2024-06-21 06:11:37,073] Trial 982 finished with value: 0.9728851318359375 and parameters: {'hidden_size': 134, 'n_layers': 2, 'rnn_dropout': 0.5006487514961174, 'bidirectional': True, 'fc_dropout': 0.18156599589847255, 'learning_rate_model': 0.0008798764306614462}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.925184844460112e-05
Epoch 49: reducing lr to 4.1462654054620106e-05
Epoch 61: reducing lr to 2.8273570163341086e-05
Epoch 65: reducing lr to 2.366781731562151e-05
Epoch 68: reducing lr to 2.0278203183385557e-05
Epoch 72: reducing lr to 1.5946557955390992e-05
Epoch 75: reducing lr to 1.2905100697993825e-05
Epoch 78: reducing lr to 1.0094889978296592e-05
Epoch 86: reducing lr to 4.05450412644335e-06
Epoch 89: reducing lr to 2.4363386026982524e-06
Epoch 92: reducing lr to 1.2145117381674775e-06
Epoch 95: reducing lr to 4.0829607221731267e-07
Epoch 98: reducing lr to 3.040396175169654e-08
[I 2024-06-21 06:12:15,991] Trial 983 finished with value: 0.9710310101509094 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5648864509654673, 'bidirectional': True, 'fc_dropout': 0.14998057369262147, 'learning_rate_model': 0.0005513539994916143}. Best is trial 690 with value: 0.966281533241272.
Epoch 57: reducing lr to 2.481318180515029e-05
Epoch 61: reducing lr to 2.1350164234015297e-05
Epoch 65: reducing lr to 1.787223133937174e-05
Epoch 73: reducing lr to 1.1258913541964377e-05
Epoch 76: reducing lr to 9.016530006762988e-06
Epoch 79: reducing lr to 6.960255561721195e-06
Epoch 86: reducing lr to 3.0616695552405244e-06
Epoch 89: reducing lr to 1.8397474742939316e-06
Epoch 92: reducing lr to 9.171118088098908e-07
Epoch 95: reducing lr to 3.083157927202807e-07
Epoch 98: reducing lr to 2.295888255403832e-08
[I 2024-06-21 06:12:51,814] Trial 984 finished with value: 0.975504994392395 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.553238889459574, 'bidirectional': True, 'fc_dropout': 0.21111320549244278, 'learning_rate_model': 0.0004163428379303095}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 6.201318573867194e-05
Epoch 65: reducing lr to 2.9800236896154107e-05
Epoch 72: reducing lr to 2.007837048984014e-05
Epoch 75: reducing lr to 1.6248860333863915e-05
Epoch 78: reducing lr to 1.2710513554424518e-05
Epoch 81: reducing lr to 9.519121522505014e-06
Epoch 84: reducing lr to 6.725023799588183e-06
Epoch 87: reducing lr to 4.372273549296021e-06
Epoch 90: reducing lr to 2.497988982859867e-06
Epoch 93: reducing lr to 1.1317192959407621e-06
Epoch 96: reducing lr to 2.950154030671617e-07
Epoch 99: reducing lr to 1.0701482987848613e-09
[I 2024-06-21 06:13:26,179] Trial 985 finished with value: 0.9698870182037354 and parameters: {'hidden_size': 132, 'n_layers': 2, 'rnn_dropout': 0.5400729720089252, 'bidirectional': True, 'fc_dropout': 0.33019402975613543, 'learning_rate_model': 0.000694211873422206}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 8.684696037240106e-05
Epoch 45: reducing lr to 7.977062510815788e-05
Epoch 52: reducing lr to 6.765688043869902e-05
Epoch 61: reducing lr to 4.985546137063102e-05
Epoch 64: reducing lr to 4.375517107044911e-05
Epoch 67: reducing lr to 3.773144190819343e-05
Epoch 72: reducing lr to 2.811894640636272e-05
Epoch 75: reducing lr to 2.275587220205887e-05
Epoch 79: reducing lr to 1.6253118640382917e-05
Epoch 82: reducing lr to 1.1961745872932658e-05
Epoch 85: reducing lr to 8.248363681663658e-06
Epoch 88: reducing lr to 5.171519247390566e-06
Epoch 91: reducing lr to 2.779754467674856e-06
Epoch 94: reducing lr to 1.1107767511897028e-06
Epoch 97: reducing lr to 1.9091179077762327e-07
[I 2024-06-21 06:13:58,455] Trial 986 finished with value: 0.9702033996582031 and parameters: {'hidden_size': 125, 'n_layers': 2, 'rnn_dropout': 0.5059943676806344, 'bidirectional': True, 'fc_dropout': 0.18887060128898908, 'learning_rate_model': 0.0009722156722477679}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.000103992508170832
Epoch 40: reducing lr to 9.890163457267626e-05
Epoch 43: reducing lr to 9.42740267204018e-05
Epoch 49: reducing lr to 8.326031183044548e-05
Epoch 52: reducing lr to 7.704790169722392e-05
Epoch 62: reducing lr to 5.445696985031889e-05
Epoch 65: reducing lr to 4.752686230478016e-05
Epoch 68: reducing lr to 4.072024714543525e-05
Epoch 71: reducing lr to 3.414450907205154e-05
Epoch 74: reducing lr to 2.7903321680230596e-05
Epoch 77: reducing lr to 2.2095130611652845e-05
Epoch 80: reducing lr to 1.681151732027031e-05
Epoch 83: reducing lr to 1.2135823110903145e-05
Epoch 86: reducing lr to 8.141767226014932e-06
Epoch 89: reducing lr to 4.892361967904568e-06
Epoch 92: reducing lr to 2.4388363057596544e-06
Epoch 95: reducing lr to 8.198910336800142e-07
Epoch 98: reducing lr to 6.105357686443571e-08
[I 2024-06-21 06:14:19,683] Trial 987 finished with value: 0.9738904237747192 and parameters: {'hidden_size': 91, 'n_layers': 2, 'rnn_dropout': 0.5714170964673182, 'bidirectional': True, 'fc_dropout': 0.1680692101162062, 'learning_rate_model': 0.001107162746170608}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.296312875684256e-05
Epoch 40: reducing lr to 6.939127466503761e-05
Epoch 52: reducing lr to 5.405827853237481e-05
Epoch 65: reducing lr to 3.334575379272399e-05
Epoch 72: reducing lr to 2.2467217332749155e-05
Epoch 75: reducing lr to 1.8182086873789625e-05
Epoch 78: reducing lr to 1.4222761283472219e-05
Epoch 83: reducing lr to 8.514725144973944e-06
Epoch 86: reducing lr to 5.712419297014223e-06
Epoch 89: reducing lr to 3.432574542801758e-06
Epoch 92: reducing lr to 1.7111341049846452e-06
Epoch 95: reducing lr to 5.752511994297255e-07
Epoch 98: reducing lr to 4.283635492768542e-08
[I 2024-06-21 06:14:56,595] Trial 988 finished with value: 0.9676626920700073 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5302559600145128, 'bidirectional': True, 'fc_dropout': 0.13923594665980335, 'learning_rate_model': 0.000776806516397528}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 5.705532389711272e-05
Epoch 61: reducing lr to 3.275324184455048e-05
Epoch 65: reducing lr to 2.7417752338765334e-05
Epoch 72: reducing lr to 1.8473134672545404e-05
Epoch 75: reducing lr to 1.4949788150125878e-05
Epoch 79: reducing lr to 1.067771335218706e-05
Epoch 86: reducing lr to 4.696900796253536e-06
Epoch 89: reducing lr to 2.8223527134487718e-06
[I 2024-06-21 06:15:33,047] Trial 989 finished with value: 0.9693014025688171 and parameters: {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.5818910183858739, 'bidirectional': True, 'fc_dropout': 0.1576786392753156, 'learning_rate_model': 0.0006387106680543457}. Best is trial 690 with value: 0.966281533241272.
Epoch 31: reducing lr to 0.00014682682245181303
Epoch 36: reducing lr to 0.00014082121821985507
Epoch 39: reducing lr to 0.00013581407280094685
Epoch 45: reducing lr to 0.00012301494758108514
Epoch 48: reducing lr to 0.00011542482065092651
Epoch 51: reducing lr to 0.00010719660023443816
Epoch 54: reducing lr to 9.846005689362409e-05
Epoch 57: reducing lr to 8.935294430123755e-05
Epoch 60: reducing lr to 8.001892744792941e-05
Epoch 63: reducing lr to 7.060517506731757e-05
Epoch 66: reducing lr to 6.126017539337885e-05
Epoch 69: reducing lr to 5.213126362768779e-05
Epoch 72: reducing lr to 4.336246222872216e-05
Epoch 75: reducing lr to 3.509202068183201e-05
Epoch 78: reducing lr to 2.745039315766617e-05
Epoch 81: reducing lr to 2.0558070072424787e-05
Epoch 84: reducing lr to 1.4523767785063012e-05
Epoch 87: reducing lr to 9.442626169834127e-06
Epoch 90: reducing lr to 5.394807958735283e-06
Epoch 93: reducing lr to 2.4441293803488093e-06
Epoch 96: reducing lr to 6.371330919939045e-07
Epoch 99: reducing lr to 2.311156934190969e-09
[I 2024-06-21 06:16:06,340] Trial 990 finished with value: 0.986136794090271 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5650538429439061, 'bidirectional': True, 'fc_dropout': 0.20184078906491493, 'learning_rate_model': 0.0014992619124760728}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 8.31440597153591e-05
Epoch 40: reducing lr to 7.907380594522139e-05
Epoch 52: reducing lr to 6.160131582876747e-05
Epoch 65: reducing lr to 3.799866304850445e-05
Epoch 68: reducing lr to 3.255664008721219e-05
Epoch 71: reducing lr to 2.7299207906157013e-05
Epoch 76: reducing lr to 1.917030274999563e-05
Epoch 79: reducing lr to 1.4798398744911279e-05
Epoch 82: reducing lr to 1.089112120754099e-05
Epoch 85: reducing lr to 7.510101750627925e-06
Epoch 88: reducing lr to 4.708647345360542e-06
Epoch 91: reducing lr to 2.530955193017154e-06
Epoch 94: reducing lr to 1.011357736590971e-06
Epoch 97: reducing lr to 1.738244128737668e-07
[I 2024-06-21 06:16:45,182] Trial 991 finished with value: 0.9697961807250977 and parameters: {'hidden_size': 142, 'n_layers': 2, 'rnn_dropout': 0.5474604332609672, 'bidirectional': True, 'fc_dropout': 0.17745821899001432, 'learning_rate_model': 0.0008851984349777369}. Best is trial 690 with value: 0.966281533241272.
Epoch 13: reducing lr to 7.158645791851168e-05
Epoch 16: reducing lr to 9.153856054928279e-05
Epoch 19: reducing lr to 0.00010729951516237827
Epoch 22: reducing lr to 0.00011665573184431729
Epoch 25: reducing lr to 0.00011860696429551256
Epoch 28: reducing lr to 0.00011786272905194631
Epoch 31: reducing lr to 0.00011619531313783026
Epoch 34: reducing lr to 0.00011363101491312013
Epoch 37: reducing lr to 0.00011021027178855496
Epoch 40: reducing lr to 0.00010598703037476804
Epoch 43: reducing lr to 0.00010102789682636223
Epoch 46: reducing lr to 9.541107377323478e-05
Epoch 49: reducing lr to 8.922515019204797e-05
Epoch 52: reducing lr to 8.256767780208025e-05
Epoch 55: reducing lr to 7.554363819085902e-05
Epoch 58: reducing lr to 6.826383100938384e-05
Epoch 61: reducing lr to 6.0843030959047195e-05
Epoch 64: reducing lr to 5.33983069230204e-05
Epoch 67: reducing lr to 4.604701721809876e-05
Epoch 70: reducing lr to 3.890514516531969e-05
Epoch 73: reducing lr to 3.208530003284861e-05
Epoch 76: reducing lr to 2.5695025496367588e-05
Epoch 79: reducing lr to 1.9835118830139273e-05
Epoch 82: reducing lr to 1.4597976920936164e-05
Epoch 85: reducing lr to 1.0066208055203689e-05
Epoch 88: reducing lr to 6.311262538223096e-06
Epoch 91: reducing lr to 3.392380342806772e-06
Epoch 94: reducing lr to 1.3555791562895256e-06
Epoch 97: reducing lr to 2.329865510696449e-07
[I 2024-06-21 06:17:01,265] Trial 992 finished with value: 1.0928634405136108 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.5818246228997964, 'bidirectional': False, 'fc_dropout': 0.217041458797253, 'learning_rate_model': 0.0011864808111130543}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 7.248667216590812e-05
Epoch 40: reducing lr to 6.82395280191145e-05
Epoch 43: reducing lr to 6.504660024738279e-05
Epoch 49: reducing lr to 5.744742649181223e-05
Epoch 52: reducing lr to 5.316102680606503e-05
Epoch 65: reducing lr to 3.2792285647457335e-05
Epoch 68: reducing lr to 2.809590011360522e-05
Epoch 71: reducing lr to 2.35588136999799e-05
Epoch 74: reducing lr to 1.925255846226948e-05
Epoch 79: reducing lr to 1.2770799808105467e-05
Epoch 82: reducing lr to 9.398876934245762e-06
Epoch 85: reducing lr to 6.481107020362743e-06
Epoch 88: reducing lr to 4.0634931961976564e-06
Epoch 91: reducing lr to 2.1841769944482612e-06
Epoch 94: reducing lr to 8.727868069390607e-07
Epoch 97: reducing lr to 1.5000790401973295e-07
[I 2024-06-21 06:17:36,603] Trial 993 finished with value: 0.9713022708892822 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5986433721671351, 'bidirectional': True, 'fc_dropout': 0.13408862234061764, 'learning_rate_model': 0.0007639131907725003}. Best is trial 690 with value: 0.966281533241272.
Epoch 30: reducing lr to 0.00010168805214427905
Epoch 40: reducing lr to 9.223274511446102e-05
Epoch 43: reducing lr to 8.791717462492711e-05
Epoch 49: reducing lr to 7.764610921131939e-05
Epoch 52: reducing lr to 7.185259889331886e-05
Epoch 55: reducing lr to 6.574009198709766e-05
Epoch 58: reducing lr to 5.9405009308799796e-05
Epoch 61: reducing lr to 5.2947230869608593e-05
Epoch 64: reducing lr to 4.646863313897693e-05
Epoch 67: reducing lr to 4.0071344459229054e-05
Epoch 70: reducing lr to 3.3856296614649735e-05
Epoch 73: reducing lr to 2.7921485198581886e-05
Epoch 76: reducing lr to 2.2360497590469794e-05
Epoch 79: reducing lr to 1.726105027102273e-05
Epoch 82: reducing lr to 1.2703549479352403e-05
Epoch 85: reducing lr to 8.759883153078384e-06
Epoch 88: reducing lr to 5.4922292565426745e-06
Epoch 91: reducing lr to 2.9521399965924114e-06
Epoch 94: reducing lr to 1.1796611940388235e-06
Epoch 97: reducing lr to 2.0275112062222503e-07
[I 2024-06-21 06:18:10,079] Trial 994 finished with value: 0.9759728908538818 and parameters: {'hidden_size': 127, 'n_layers': 2, 'rnn_dropout': 0.5891452899104724, 'bidirectional': True, 'fc_dropout': 0.15240003596888818, 'learning_rate_model': 0.0010325072968611214}. Best is trial 690 with value: 0.966281533241272.
Epoch 40: reducing lr to 4.9566455118028536e-05
Epoch 65: reducing lr to 2.3819000540375623e-05
Epoch 68: reducing lr to 2.0407734525824126e-05
Epoch 73: reducing lr to 1.5005181090584464e-05
Epoch 76: reducing lr to 1.2016671507059296e-05
Epoch 87: reducing lr to 3.4947100050332943e-06
Epoch 90: reducing lr to 1.99661503161642e-06
Epoch 93: reducing lr to 9.04570746048175e-07
Epoch 96: reducing lr to 2.358025565220561e-07
Epoch 99: reducing lr to 8.553577273773607e-10
[I 2024-06-21 06:18:47,070] Trial 995 finished with value: 0.9709718823432922 and parameters: {'hidden_size': 139, 'n_layers': 2, 'rnn_dropout': 0.5660884851001579, 'bidirectional': True, 'fc_dropout': 0.19056275448827334, 'learning_rate_model': 0.0005548758906112149}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 6.547653971095951e-05
Epoch 40: reducing lr to 6.164013373346877e-05
Epoch 53: reducing lr to 4.667874679908966e-05
Epoch 65: reducing lr to 2.962096795524712e-05
Epoch 72: reducing lr to 1.995758526838721e-05
Epoch 75: reducing lr to 1.61511122524259e-05
Epoch 83: reducing lr to 7.5636137133011236e-06
Epoch 86: reducing lr to 5.0743309026864545e-06
Epoch 89: reducing lr to 3.049149261052648e-06
Epoch 92: reducing lr to 1.5199970828651657e-06
Epoch 95: reducing lr to 5.109945167364292e-07
Epoch 98: reducing lr to 3.805145040413878e-08
[I 2024-06-21 06:19:26,125] Trial 996 finished with value: 0.9695682525634766 and parameters: {'hidden_size': 145, 'n_layers': 2, 'rnn_dropout': 0.5274648888016201, 'bidirectional': True, 'fc_dropout': 0.16998174219486475, 'learning_rate_model': 0.0006900357110733251}. Best is trial 690 with value: 0.966281533241272.
Epoch 35: reducing lr to 8.273486747925555e-05
Epoch 40: reducing lr to 7.788726035851468e-05
Epoch 49: reducing lr to 6.556936710994084e-05
Epoch 52: reducing lr to 6.067695448611773e-05
Epoch 65: reducing lr to 3.742847238420036e-05
Epoch 68: reducing lr to 3.2068109945634816e-05
Epoch 71: reducing lr to 2.6889568401969256e-05
Epoch 74: reducing lr to 2.1974493040137617e-05
Epoch 79: reducing lr to 1.4576340700389373e-05
Epoch 82: reducing lr to 1.0727693993577777e-05
Epoch 85: reducing lr to 7.397408577694032e-06
Epoch 88: reducing lr to 4.637991523749206e-06
Epoch 91: reducing lr to 2.49297682991033e-06
Epoch 94: reducing lr to 9.961817621378868e-07
Epoch 97: reducing lr to 1.7121608275115323e-07
[I 2024-06-21 06:20:01,448] Trial 997 finished with value: 0.9705995917320251 and parameters: {'hidden_size': 133, 'n_layers': 2, 'rnn_dropout': 0.5506499436352409, 'bidirectional': True, 'fc_dropout': 0.22148797424888722, 'learning_rate_model': 0.0008719155496552555}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 0.00012366706072842334
Epoch 40: reducing lr to 0.00011761303447693963
Epoch 49: reducing lr to 9.901249830890379e-05
Epoch 52: reducing lr to 9.162474976116303e-05
Epoch 55: reducing lr to 8.383022424194797e-05
Epoch 58: reducing lr to 7.575187531573686e-05
Epoch 61: reducing lr to 6.751706763143268e-05
Epoch 64: reducing lr to 5.925571167472337e-05
Epoch 67: reducing lr to 5.109803911367807e-05
Epoch 70: reducing lr to 4.31727557936034e-05
Epoch 73: reducing lr to 3.560482339794621e-05
Epoch 76: reducing lr to 2.851358235912587e-05
Epoch 79: reducing lr to 2.2010886677118793e-05
Epoch 82: reducing lr to 1.61992685031807e-05
Epoch 85: reducing lr to 1.1170397650188054e-05
Epoch 88: reducing lr to 7.003562000712179e-06
Epoch 91: reducing lr to 3.764499720452694e-06
Epoch 94: reducing lr to 1.5042763013657948e-06
Epoch 97: reducing lr to 2.585434761850263e-07
[I 2024-06-21 06:20:37,276] Trial 998 finished with value: 0.9780258536338806 and parameters: {'hidden_size': 137, 'n_layers': 2, 'rnn_dropout': 0.6022124691331626, 'bidirectional': True, 'fc_dropout': 0.13848877504607646, 'learning_rate_model': 0.0013166291012233896}. Best is trial 690 with value: 0.966281533241272.
Epoch 36: reducing lr to 7.544583628850515e-05
Epoch 40: reducing lr to 7.175244315079008e-05
Epoch 53: reducing lr to 5.433658110694894e-05
Epoch 56: reducing lr to 4.951514434149266e-05
Epoch 64: reducing lr to 3.6150262615100155e-05
Epoch 67: reducing lr to 3.1173493337083365e-05
Epoch 72: reducing lr to 2.323170661162051e-05
Epoch 75: reducing lr to 1.880076653832728e-05
Epoch 78: reducing lr to 1.4706717456420789e-05
Epoch 81: reducing lr to 1.101411284960105e-05
Epoch 84: reducing lr to 7.781198177773138e-06
Epoch 87: reducing lr to 5.058945215419351e-06
Epoch 90: reducing lr to 2.890301640674775e-06
Epoch 93: reducing lr to 1.3094573916398949e-06
Epoch 96: reducing lr to 3.413479840624228e-07
Epoch 99: reducing lr to 1.238216583399846e-09
[I 2024-06-21 06:21:10,535] Trial 999 finished with value: 0.9698539972305298 and parameters: {'hidden_size': 130, 'n_layers': 2, 'rnn_dropout': 0.5740153369235231, 'bidirectional': True, 'fc_dropout': 0.20349779941431495, 'learning_rate_model': 0.0008032388175030897}. Best is trial 690 with value: 0.966281533241272.

Optuna study saved to optuna/no-name-3974d02f-3fde-4596-ae4d-9f35a2fdf068.pkl
To reload the study run: study = joblib.load('optuna/no-name-3974d02f-3fde-4596-ae4d-9f35a2fdf068.pkl')

Study statistics    : 
  Study name        : no-name-3974d02f-3fde-4596-ae4d-9f35a2fdf068
  # finished trials : 1000
  # pruned trials   : 0
  # complete trials : 1000

Best trial          :
  value             : 0.966281533241272
  best_params = {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6193926562216368, 'bidirectional': True, 'fc_dropout': 0.209474313213079, 'learning_rate_model': 0.0008327152231865279}

O Melhor modelo foi o de nmero 690
Best hyperparameters:  {'hidden_size': 135, 'n_layers': 2, 'rnn_dropout': 0.6193926562216368, 'bidirectional': True, 'fc_dropout': 0.209474313213079, 'learning_rate_model': 0.0008327152231865279}
Epoch 44: reducing lr to 6.963936302181141e-05
Epoch 48: reducing lr to 6.410888217046945e-05
Epoch 51: reducing lr to 5.953879048500258e-05
Epoch 54: reducing lr to 5.468636771791594e-05
Epoch 57: reducing lr to 4.9628124570506545e-05
Epoch 60: reducing lr to 4.4443854989221275e-05
Epoch 63: reducing lr to 3.921529895814221e-05
Epoch 66: reducing lr to 3.402492933399149e-05
Epoch 69: reducing lr to 2.895457855994762e-05
Epoch 72: reducing lr to 2.408423912608668e-05
Epoch 75: reducing lr to 1.9490697116341478e-05
Epoch 78: reducing lr to 1.5246408966057643e-05
Epoch 81: reducing lr to 1.141829707417229e-05
Epoch 84: reducing lr to 8.06674433066482e-06
Epoch 87: reducing lr to 5.244593018103411e-06
Epoch 90: reducing lr to 2.9963668629369032e-06
Epoch 93: reducing lr to 1.357510469329954e-06
Epoch 96: reducing lr to 3.538744101242515e-07
Epoch 99: reducing lr to 1.283655341505381e-09
Mtricas de Treinamento para <class 'tsai.models.RNNPlus.LSTMPlus'>
train_loss    0.421660
valid_loss    0.399561
mae           0.760018
_rmse         0.983053
dtype: float64
RMSE: 1.219503999151989
MAE: 0.9452834501236813
R: 0.7150578472116624
MAPE: 0.10618604347021499
Correlao Linear: 0.8503575409921148
count    1096.000000
mean        0.012432
std         1.219997
min        -4.172318
25%        -0.738878
50%         0.029279
75%         0.747709
max         4.184459
dtype: float64
Estatstica de teste: 0.9963, p-valor: 0.0095
Os resduos no parecem seguir uma distribuio normal (rejeitamos H0)
====================================================================================================
Default LSTM
[I 2024-06-21 06:27:52,444] Trial 0 finished with value: 2.1446685791015625 and parameters: {'hidden_size': 93, 'n_layers': 6, 'rnn_dropout': 9.149985387590931e-05, 'bidirectional': True, 'fc_dropout': 0.07387087581503825, 'learning_rate_model': 5.5595654267125665e-05}. Best is trial 0 with value: 2.1446685791015625.
Epoch 13: reducing lr to 0.00196354066941974
Epoch 16: reducing lr to 0.0025108056982405735
Epoch 19: reducing lr to 0.002943111978946899
Epoch 23: reducing lr to 0.0032391887918001598
Epoch 26: reducing lr to 0.003249300514968436
Epoch 33: reducing lr to 0.0031428963833818628
Epoch 36: reducing lr to 0.003056752910061986
Epoch 52: reducing lr to 0.002264743892322208
Epoch 55: reducing lr to 0.002072081930251843
Epoch 60: reducing lr to 0.0017369405862873728
Epoch 68: reducing lr to 0.0011969298187882783
Epoch 71: reducing lr to 0.0010036427556606015
Epoch 75: reducing lr to 0.0007617292173376554
Epoch 78: reducing lr to 0.0005958552995617456
Epoch 81: reducing lr to 0.00044624624977347634
Epoch 84: reducing lr to 0.00031526193284838605
Epoch 87: reducing lr to 0.00020496751404468563
Epoch 90: reducing lr to 0.00011710305545961016
Epoch 93: reducing lr to 5.305379182478781e-05
Epoch 96: reducing lr to 1.3830006995171201e-05
Epoch 99: reducing lr to 5.0167409240686744e-08
[I 2024-06-21 06:28:05,909] Trial 1 finished with value: 2.0360779762268066 and parameters: {'hidden_size': 79, 'n_layers': 3, 'rnn_dropout': 0.4310533872026856, 'bidirectional': False, 'fc_dropout': 0.16356179978521396, 'learning_rate_model': 0.032543911150884876}. Best is trial 1 with value: 2.0360779762268066.
Epoch 47: reducing lr to 0.006793072879733128
Epoch 50: reducing lr to 0.006330953095772714
Epoch 56: reducing lr to 0.005319095096840677
Epoch 59: reducing lr to 0.004785315259777943
Epoch 62: reducing lr to 0.004244107753067331
Epoch 65: reducing lr to 0.003704009336933383
Epoch 68: reducing lr to 0.003173535308552384
Epoch 71: reducing lr to 0.002661054702009337
Epoch 74: reducing lr to 0.0021746473262265987
Epoch 77: reducing lr to 0.0017219855491721239
Epoch 80: reducing lr to 0.0013102067778632959
Epoch 83: reducing lr to 0.0009458062227187272
Epoch 86: reducing lr to 0.0006345291980544745
Epoch 89: reducing lr to 0.0003812865720561942
Epoch 92: reducing lr to 0.00019007087801959505
Epoch 95: reducing lr to 6.389826503891278e-05
Epoch 98: reducing lr to 4.7582147819663636e-06
[I 2024-06-21 06:28:09,279] Trial 2 finished with value: 1.9977259635925293 and parameters: {'hidden_size': 19, 'n_layers': 1, 'rnn_dropout': 0.6063679444552919, 'bidirectional': False, 'fc_dropout': 0.30196428199351444, 'learning_rate_model': 0.08628680603870356}. Best is trial 2 with value: 1.9977259635925293.

Optuna study saved to optuna/no-name-644c45d6-3dd1-456e-ab1e-9d96e2a7c61d.pkl
To reload the study run: study = joblib.load('optuna/no-name-644c45d6-3dd1-456e-ab1e-9d96e2a7c61d.pkl')

Study statistics    : 
  Study name        : no-name-644c45d6-3dd1-456e-ab1e-9d96e2a7c61d
  # finished trials : 3
  # pruned trials   : 0
  # complete trials : 3

Best trial          :
  value             : 1.9977259635925293
  best_params = {'hidden_size': 19, 'n_layers': 1, 'rnn_dropout': 0.6063679444552919, 'bidirectional': False, 'fc_dropout': 0.30196428199351444, 'learning_rate_model': 0.08628680603870356}

O Melhor modelo foi o de nmero 2
Best hyperparameters:  {'hidden_size': 19, 'n_layers': 1, 'rnn_dropout': 0.6063679444552919, 'bidirectional': False, 'fc_dropout': 0.30196428199351444, 'learning_rate_model': 0.08628680603870356}
Epoch 38: reducing lr to 0.007918926070337822
Epoch 42: reducing lr to 0.0074730820530098845
Epoch 63: reducing lr to 0.004063529524538341
Epoch 66: reducing lr to 0.0035256980972294275
Epoch 69: reducing lr to 0.003000303146343423
Epoch 72: reducing lr to 0.0024956335757979667
Epoch 75: reducing lr to 0.002019646038415411
Epoch 78: reducing lr to 0.001579848544388003
Epoch 81: reducing lr to 0.0011831756613757808
Epoch 84: reducing lr to 0.0008358843264091156
Epoch 87: reducing lr to 0.0005434501110395326
Epoch 90: reducing lr to 0.00031048660949617446
Epoch 93: reducing lr to 0.00014066662803923074
Epoch 96: reducing lr to 3.6668829556887886e-05
Epoch 99: reducing lr to 1.330136838978424e-07
Mtricas de Treinamento para <class 'tsai.models.RNNPlus.LSTMPlus'>
train_loss    1.196419
valid_loss    1.156440
mae           1.586364
_rmse         2.013380
dtype: float64
RMSE: 2.005187749862671
MAE: 1.559914231300354
R: 0.3169576403723592
MAPE: 0.18734663724899292
Correlao Linear: 0.5634972333308548
count    1096.000000
mean       -0.057026
std         2.005292
min        -7.048611
25%        -1.320519
50%        -0.013398
75%         1.223992
max         6.013191
dtype: float64
Estatstica de teste: 0.9967, p-valor: 0.0206
Os resduos no parecem seguir uma distribuio normal (rejeitamos H0)
